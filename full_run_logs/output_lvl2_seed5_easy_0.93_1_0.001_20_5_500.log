==> Playing in 11_vs_11_easy_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 26.54984211921692, eps 0.9, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
=== ep: 1, time 25.638461589813232, eps 0.8561552526261419, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
goal_identified
goal_identified
goal_identified
=== ep: 2, time 25.858819484710693, eps 0.8144488388143276, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
goal_identified
=== ep: 3, time 26.272472620010376, eps 0.774776470806127, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 4, time 26.982481479644775, eps 0.7370389470171057, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
goal_identified
goal_identified
goal_identified
=== ep: 5, time 28.579145669937134, eps 0.701141903981193, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
goal_identified
=== ep: 6, time 29.89559555053711, eps 0.6669955803928644, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
goal_identified
=== ep: 7, time 30.436222553253174, eps 0.6345145926571234, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
=== ep: 8, time 30.21758484840393, eps 0.6036177213860398, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
goal_identified
goal_identified
goal_identified
=== ep: 9, time 28.013548374176025, eps 0.5742277083079742, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
goal_identified
goal_identified
goal_identified
=== ep: 10, time 30.90554714202881, eps 0.5462710630816575, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
goal_identified
goal_identified
=== ep: 11, time 32.884904623031616, eps 0.5196778795320575, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 12, time 32.25125050544739, eps 0.49438166084852986, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 13, time 33.736164808273315, eps 0.47031915330815344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
goal_identified
goal_identified
=== ep: 14, time 31.851988792419434, eps 0.4474301881084772, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
=== ep: 15, time 35.3770706653595, eps 0.42565753091417224, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
=== ep: 16, time 36.47919750213623, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
goal_identified
goal_identified
=== ep: 17, time 40.48038363456726, eps 0.3852460238219053, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
goal_identified
=== ep: 18, time 39.508089780807495, eps 0.3665061241067986, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
goal_identified
goal_identified
=== ep: 19, time 38.93683123588562, eps 0.3486801800855966, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 20, time 35.52315425872803, eps 0.3317236176131267, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
=== ep: 21, time 40.07159614562988, eps 0.31559403645092865, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 22, time 40.891077756881714, eps 0.3002511042445735, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
goal_identified
goal_identified
=== ep: 23, time 39.14433813095093, eps 0.2856564556717689, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
=== ep: 24, time 35.52302646636963, eps 0.27177359650906974, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
goal_identified
=== ep: 25, time 40.672956466674805, eps 0.2585678123773109, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
goal_identified
=== ep: 26, time 38.896239042282104, eps 0.24600608193757734, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 27, time 38.17419791221619, eps 0.23405699432065646, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
=== ep: 28, time 39.876333236694336, eps 0.22269067058350425, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 29, time 40.28486680984497, eps 0.2118786889963241, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
goal_identified
goal_identified
goal_identified
=== ep: 30, time 44.63085913658142, eps 0.2015940139734384, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
goal_identified
goal_identified
=== ep: 31, time 44.57040190696716, eps 0.191810928470242, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
goal_identified
=== ep: 32, time 44.40861129760742, eps 0.1825049696771952, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
=== ep: 33, time 39.66192317008972, eps 0.17365286785005798, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
=== ep: 34, time 37.88394546508789, eps 0.16523248812340846, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
=== ep: 35, time 39.59684777259827, eps 0.15722277516195018, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
=== ep: 36, time 40.706117153167725, eps 0.1496037005112063, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
goal_identified
goal_identified
=== ep: 37, time 44.34219241142273, eps 0.14235621251595124, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 38, time 45.867841958999634, eps 0.13546218868114893, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
goal_identified
goal_identified
=== ep: 39, time 42.411134243011475, eps 0.1289043903562757, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 40, time 54.22748565673828, eps 0.12266641962971482, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
=== ep: 41, time 51.321545362472534, eps 0.116732678325436, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
goal_identified
goal_identified
=== ep: 42, time 56.276500940322876, eps 0.11108832899943073, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
goal_identified
goal_identified
=== ep: 43, time 50.79003310203552, eps 0.10571925783837377, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
=== ep: 44, time 49.98340940475464, eps 0.10061203936773815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
=== ep: 45, time 46.510034799575806, eps 0.09575390288111604, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
goal_identified
goal_identified
=== ep: 46, time 49.34150671958923, eps 0.09113270050680057, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
=== ep: 47, time 47.437734603881836, eps 0.08673687683177911, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
goal_identified
=== ep: 48, time 41.15983438491821, eps 0.08255544000718185, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
goal_identified
goal_identified
=== ep: 49, time 47.4422287940979, eps 0.07857793426293408, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
=== ep: 50, time 51.981112241744995, eps 0.07479441376288502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
=== ep: 51, time 64.62455630302429, eps 0.0711954177350367, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
=== ep: 52, time 59.81028199195862, eps 0.06777194681468615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
goal_identified
=== ep: 53, time 56.361461877822876, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
=== ep: 54, time 54.80107235908508, eps 0.06141775595303503, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
=== ep: 55, time 54.52768802642822, eps 0.05847114722483011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 56.97752261161804, eps 0.05566824630007096, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
=== ep: 57, time 44.22654366493225, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
goal_identified
goal_identified
goal_identified
=== ep: 58, time 52.97942662239075, eps 0.050465874830710106, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
goal_identified
=== ep: 59, time 61.9175500869751, eps 0.04805339564764071, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
goal_identified
goal_identified
goal_identified
=== ep: 60, time 58.068212032318115, eps 0.045758574462709686, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
=== ep: 61, time 57.60710954666138, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 62, time 53.87294888496399, eps 0.04149923295180846, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
=== ep: 63, time 55.43431615829468, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
goal_identified
=== ep: 64, time 57.48386740684509, eps 0.03764522137655123, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
goal_identified
=== ep: 65, time 54.68689584732056, eps 0.03585801284071809, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
goal_identified
=== ep: 66, time 56.91447877883911, eps 0.034157967493714775, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
=== ep: 67, time 52.32668900489807, eps 0.03254083433665968, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
goal_identified
=== ep: 68, time 68.55072855949402, eps 0.031002569694333147, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
goal_identified
goal_identified
=== ep: 69, time 66.38642263412476, eps 0.02953932710388308, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
=== ep: 70, time 66.97085976600647, eps 0.028147447696664333, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 66.62653493881226, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 61.301655769348145, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
goal_identified
=== ep: 73, time 64.93089461326599, eps 0.02436602477210106, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
goal_identified
=== ep: 74, time 63.84773540496826, eps 0.02322645029683511, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 75, time 71.62096548080444, eps 0.02214245352455219, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
goal_identified
=== ep: 76, time 79.2318422794342, eps 0.02111132389869288, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
=== ep: 77, time 79.60493469238281, eps 0.020130483058101077, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
goal_identified
goal_identified
=== ep: 78, time 74.91343092918396, eps 0.019197478389778148, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
=== ep: 79, time 68.66628050804138, eps 0.018309976896072843, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 80, time 65.56504344940186, eps 0.017465759360972027, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
=== ep: 81, time 67.83077120780945, eps 0.01666271480090467, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
=== ep: 82, time 88.20416021347046, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
goal_identified
=== ep: 83, time 92.24287390708923, eps 0.015172210419884185, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
goal_identified
=== ep: 84, time 94.64261865615845, eps 0.014481023561609456, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
=== ep: 85, time 90.58345580101013, eps 0.01382354628419033, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
=== ep: 86, time 93.29374432563782, eps 0.013198134551968641, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 87, time 82.05476450920105, eps 0.012603224509851407, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 88, time 75.50135660171509, eps 0.012037328572858524, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
=== ep: 89, time 92.95126628875732, eps 0.011499031706385502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
=== ep: 90, time 98.68310356140137, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
=== ep: 91, time 87.88132572174072, eps 0.010499916741083536, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
goal_identified
goal_identified
goal_identified
=== ep: 92, time 84.88663029670715, eps 0.010036600334425595, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
=== ep: 93, time 91.38487768173218, eps 0.00959588013555861, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
=== ep: 94, time 87.52533435821533, eps 0.009176654114424539, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
=== ep: 95, time 83.83938598632812, eps 0.00877787398760545, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 96, time 95.96808958053589, eps 0.008398542597069007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
=== ep: 97, time 93.65957999229431, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
=== ep: 98, time 89.95685768127441, eps 0.00769447818076098, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
goal_identified
=== ep: 99, time 83.36473894119263, eps 0.007367984627217855, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
goal_identified
goal_identified
goal_identified
=== ep: 100, time 85.07712578773499, eps 0.007057414352177835, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
goal_identified
goal_identified
=== ep: 101, time 91.97962284088135, eps 0.006761990768184489, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
=== ep: 102, time 78.93208813667297, eps 0.006480975162398559, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
=== ep: 103, time 86.76257920265198, eps 0.006213664849431085, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
goal_identified
goal_identified
=== ep: 104, time 84.57075548171997, eps 0.005959391414263934, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
=== ep: 105, time 86.44739937782288, eps 0.005717519040864065, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
goal_identified
goal_identified
=== ep: 106, time 78.74683594703674, eps 0.005487442922312285, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
=== ep: 107, time 94.92642593383789, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
=== ep: 108, time 96.83614706993103, eps 0.005060406267408787, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
goal_identified
=== ep: 109, time 83.7640733718872, eps 0.004862377916986354, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
goal_identified
=== ep: 110, time 86.55593419075012, eps 0.004674007523179196, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
=== ep: 111, time 91.88317036628723, eps 0.004494824061885041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
goal_identified
goal_identified
=== ep: 112, time 92.15020084381104, eps 0.0043243794811181555, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
goal_identified
=== ep: 113, time 92.56235647201538, eps 0.0041622475806460035, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 89.76355385780334, eps 0.0040080229462666735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
goal_identified
=== ep: 115, time 90.47547197341919, eps 0.0038613199360621906, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
goal_identified
=== ep: 116, time 90.02635908126831, eps 0.003721771716092858, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
goal_identified
goal_identified
goal_identified
=== ep: 117, time 87.82658553123474, eps 0.0035890293431213305, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
=== ep: 118, time 91.40484881401062, eps 0.0034627608920727634, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
goal_identified
goal_identified
=== ep: 119, time 94.56961631774902, eps 0.00334265062604924, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
=== ep: 120, time 93.4498450756073, eps 0.0032283982068230565, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 90.73011827468872, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
=== ep: 122, time 93.62745380401611, eps 0.0030163380798177374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
goal_identified
=== ep: 123, time 93.20802187919617, eps 0.0029180001112638996, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
goal_identified
=== ep: 124, time 94.22505974769592, eps 0.002824458142029865, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 98.04115772247314, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
=== ep: 126, time 99.97474932670593, eps 0.0026508379945489875, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
=== ep: 127, time 88.43414902687073, eps 0.0025703256754987464, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
=== ep: 128, time 99.4851405620575, eps 0.0024937399885833667, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
goal_identified
=== ep: 129, time 92.16997456550598, eps 0.0024208894296938593, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
goal_identified
goal_identified
=== ep: 130, time 100.3646171092987, eps 0.0023515918344868374, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
=== ep: 131, time 110.06732654571533, eps 0.002285673922878779, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
goal_identified
goal_identified
goal_identified
=== ep: 132, time 87.66975259780884, eps 0.0022229708657555565, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
=== ep: 133, time 93.48872232437134, eps 0.0021633258728137976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
=== ep: 134, time 91.903160572052, eps 0.0021065898005034594, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
=== ep: 135, time 98.32054448127747, eps 0.002052620779091266, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
goal_identified
goal_identified
=== ep: 136, time 97.62048983573914, eps 0.0020012838579124784, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
=== ep: 137, time 83.53775548934937, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
=== ep: 138, time 100.07743644714355, eps 0.001905999100714611, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 139, time 91.45261240005493, eps 0.001861813003170924, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
goal_identified
=== ep: 140, time 99.00290369987488, eps 0.0018197818870335101, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
=== ep: 141, time 109.79289031028748, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
=== ep: 142, time 99.29141211509705, eps 0.0017417693260160481, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
=== ep: 143, time 86.6173825263977, eps 0.0017055928090985275, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
goal_identified
goal_identified
=== ep: 144, time 90.73329138755798, eps 0.0016711806417306348, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
=== ep: 145, time 108.63508701324463, eps 0.0016384467755694515, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
=== ep: 146, time 114.08024072647095, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
=== ep: 147, time 103.9426498413086, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
=== ep: 148, time 102.37643194198608, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
goal_identified
goal_identified
=== ep: 149, time 101.30264139175415, eps 0.0015227160093621863, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
=== ep: 150, time 108.4886109828949, eps 0.0014972228487629025, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 111.6524338722229, eps 0.0014729730042773413, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
=== ep: 152, time 103.75863528251648, eps 0.001449905838663109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
=== ep: 153, time 92.5021321773529, eps 0.00142796367199102, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
=== ep: 154, time 102.70835280418396, eps 0.0014070916374152305, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
=== ep: 155, time 117.85989713668823, eps 0.001387237543977543, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
=== ep: 156, time 110.6037106513977, eps 0.0013683517461028282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 99.10908246040344, eps 0.0013503870194592265, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
=== ep: 158, time 88.66453790664673, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 159, time 95.28360271453857, eps 0.001317043286000802, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
goal_identified
=== ep: 160, time 111.34970164299011, eps 0.0013015809024843582, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 110.21737241744995, eps 0.0012868726283106018, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
goal_identified
=== ep: 162, time 94.63797187805176, eps 0.0012728816851329014, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 163, time 105.52245378494263, eps 0.0012595730883057546, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
=== ep: 164, time 101.79109692573547, eps 0.001246913559404956, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 165, time 118.82356214523315, eps 0.0012348714430141991, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
goal_identified
=== ep: 166, time 110.54529643058777, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
goal_identified
=== ep: 167, time 98.72765731811523, eps 0.001212520470067348, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
goal_identified
=== ep: 168, time 111.67710137367249, eps 0.0012021557244367845, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 100.66892218589783, eps 0.0011922964734155277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 170, time 103.28797960281372, eps 0.001182918063740569, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
goal_identified
goal_identified
=== ep: 171, time 114.95906114578247, eps 0.0011739970445027263, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
goal_identified
=== ep: 172, time 111.09623956680298, eps 0.0011655111085071537, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 94.00366806983948, eps 0.001157439036493735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
=== ep: 174, time 102.0722017288208, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
goal_identified
=== ep: 175, time 112.72387051582336, eps 0.0011424567312790603, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
=== ep: 176, time 116.69988489151001, eps 0.0011355090345108335, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
=== ep: 177, time 100.23017930984497, eps 0.0011289001809123877, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
=== ep: 178, time 102.76893019676208, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
goal_identified
goal_identified
goal_identified
=== ep: 179, time 104.31322741508484, eps 0.001116633706881133, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
=== ep: 180, time 112.7338056564331, eps 0.001110945413873925, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
=== ep: 181, time 109.78923559188843, eps 0.001105534542190287, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
=== ep: 182, time 99.1043348312378, eps 0.0011003875618326132, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
goal_identified
=== ep: 183, time 106.34928488731384, eps 0.0010954916026690664, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
goal_identified
goal_identified
goal_identified
=== ep: 184, time 104.48447346687317, eps 0.001090834422251547, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
goal_identified
goal_identified
goal_identified
=== ep: 185, time 108.1073386669159, eps 0.0010864043752031938, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
=== ep: 186, time 121.5774941444397, eps 0.0010821903840988777, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
goal_identified
goal_identified
=== ep: 187, time 109.06591844558716, eps 0.0010781819117658682, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
=== ep: 188, time 106.85513281822205, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
goal_identified
goal_identified
goal_identified
=== ep: 189, time 93.02787375450134, eps 0.0010707419191793434, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
goal_identified
=== ep: 190, time 111.79416489601135, eps 0.0010672917950690429, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
goal_identified
goal_identified
=== ep: 191, time 109.6658501625061, eps 0.0010640099354971456, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 103.71794867515564, eps 0.0010608881341052777, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
=== ep: 193, time 96.83302187919617, eps 0.0010579185847638855, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
goal_identified
=== ep: 194, time 89.72051000595093, eps 0.0010550938620528466, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
=== ep: 195, time 103.22356724739075, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
goal_identified
=== ep: 196, time 113.59660196304321, eps 0.001049850987889527, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
=== ep: 197, time 104.77038908004761, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
=== ep: 198, time 102.11254930496216, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
goal_identified
goal_identified
=== ep: 199, time 103.74573612213135, eps 0.001042907142909185, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
goal_identified
=== ep: 200, time 104.68402290344238, eps 0.001040814536856474, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
=== ep: 201, time 103.85014963150024, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
goal_identified
goal_identified
goal_identified
=== ep: 202, time 101.96976017951965, eps 0.0010369305201475454, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
=== ep: 203, time 104.39771103858948, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
goal_identified
goal_identified
goal_identified
=== ep: 204, time 92.13770985603333, eps 0.00103341611649703, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 108.18789315223694, eps 0.0010317863932645186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
=== ep: 206, time 116.13589549064636, eps 0.0010302361525719613, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
goal_identified
goal_identified
goal_identified
=== ep: 207, time 100.17991757392883, eps 0.0010287615180101426, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
goal_identified
goal_identified
=== ep: 208, time 107.16800451278687, eps 0.001027358802224555, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
=== ep: 209, time 80.03424906730652, eps 0.0010260244976950921, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
goal_identified
=== ep: 210, time 97.73138189315796, eps 0.0010247552679654227, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
=== ep: 211, time 116.01734161376953, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
=== ep: 212, time 96.75940942764282, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
=== ep: 213, time 92.9216742515564, eps 0.001021307056596379, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
goal_identified
=== ep: 214, time 98.96120929718018, eps 0.0010202678991839778, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
=== ep: 215, time 88.93748378753662, eps 0.0010192794220766138, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
=== ep: 216, time 107.29365849494934, eps 0.0010183391535666436, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
=== ep: 217, time 99.85004329681396, eps 0.0010174447424930286, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
=== ep: 218, time 92.39485597610474, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
goal_identified
=== ep: 219, time 102.9486026763916, eps 0.0010157846557556941, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 98.1290774345398, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 110.47016263008118, eps 0.0010142825471585687, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
=== ep: 222, time 110.41556072235107, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
=== ep: 223, time 101.54777383804321, eps 0.0010129233830939361, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
goal_identified
goal_identified
=== ep: 224, time 95.23897051811218, eps 0.0010122931022630473, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
goal_identified
goal_identified
=== ep: 225, time 107.82428741455078, eps 0.001011693560591007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
goal_identified
=== ep: 226, time 126.87665724754333, eps 0.0010111232589113477, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
goal_identified
goal_identified
=== ep: 227, time 106.4164526462555, eps 0.0010105807711728136, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
=== ep: 228, time 106.07416844367981, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 99.7894868850708, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 106.94661259651184, eps 0.001009106954145169, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
goal_identified
=== ep: 231, time 126.64801788330078, eps 0.0010086628027504636, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
=== ep: 232, time 101.7074031829834, eps 0.0010082403128748867, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
=== ep: 233, time 114.83728241920471, eps 0.0010078384280736842, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
=== ep: 234, time 101.58018255233765, eps 0.001007456143425521, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
goal_identified
goal_identified
goal_identified
=== ep: 235, time 114.6634132862091, eps 0.001007092503019653, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
=== ep: 236, time 117.54416275024414, eps 0.001006746597565654, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 237, time 106.9630765914917, eps 0.001006417562119715, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
=== ep: 238, time 119.66080355644226, eps 0.0010061045739218342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
goal_identified
=== ep: 239, time 107.30871415138245, eps 0.0010058068503384884, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
goal_identified
=== ep: 240, time 120.76292061805725, eps 0.001005523646905642, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
=== ep: 241, time 121.05469560623169, eps 0.001005254255467199, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
=== ep: 242, time 120.86199736595154, eps 0.0010049980024042435, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
goal_identified
goal_identified
=== ep: 243, time 113.30520820617676, eps 0.0010047542469506416, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
=== ep: 244, time 115.15687918663025, eps 0.0010045223795907931, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
goal_identified
goal_identified
=== ep: 245, time 130.08805656433105, eps 0.001004301820535524, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
goal_identified
goal_identified
goal_identified
=== ep: 246, time 111.8022358417511, eps 0.0010040920182723119, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
=== ep: 247, time 115.72890067100525, eps 0.0010038924481862177, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
=== ep: 248, time 117.7047712802887, eps 0.0010037026112480747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
goal_identified
=== ep: 249, time 122.52508234977722, eps 0.0010035220327666559, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
=== ep: 250, time 109.76358270645142, eps 0.0010033502612016988, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
=== ep: 251, time 118.78905844688416, eps 0.001003186867034819, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
=== ep: 252, time 113.56348323822021, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
goal_identified
=== ep: 253, time 129.87271928787231, eps 0.0010028835965394094, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
goal_identified
goal_identified
=== ep: 254, time 123.79409193992615, eps 0.0010027429618766747, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
=== ep: 255, time 106.176034450531, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 256, time 114.25269436836243, eps 0.0010024819345422614, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
goal_identified
goal_identified
=== ep: 257, time 117.15561509132385, eps 0.0010023608891662839, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
goal_identified
goal_identified
=== ep: 258, time 122.36145830154419, eps 0.001002245747242954, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
goal_identified
=== ep: 259, time 110.19539213180542, eps 0.0010021362208574892, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 117.05885863304138, eps 0.001002032036136876, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
=== ep: 261, time 114.63674712181091, eps 0.0010019329325650452, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
goal_identified
goal_identified
=== ep: 262, time 123.59263443946838, eps 0.0010018386623314465, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
goal_identified
goal_identified
goal_identified
=== ep: 263, time 111.57559585571289, eps 0.0010017489897113931, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
goal_identified
goal_identified
=== ep: 264, time 109.16557145118713, eps 0.0010016636904766263, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
=== ep: 265, time 104.66007137298584, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
goal_identified
=== ep: 266, time 121.96426916122437, eps 0.0010015053693952815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
goal_identified
goal_identified
goal_identified
=== ep: 267, time 121.1781370639801, eps 0.0010014319516635345, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
=== ep: 268, time 106.67756509780884, eps 0.0010013621145568167, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
=== ep: 269, time 109.11260080337524, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 108.91982936859131, eps 0.0010012324922186594, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 124.42080020904541, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
goal_identified
=== ep: 272, time 113.67190265655518, eps 0.0010011152050768812, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
goal_identified
=== ep: 273, time 114.6137056350708, eps 0.0010010608158834819, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
goal_identified
=== ep: 274, time 118.97688436508179, eps 0.0010010090792823456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
=== ep: 275, time 110.11355972290039, eps 0.0010009598659050213, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
=== ep: 276, time 120.70729160308838, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
goal_identified
=== ep: 277, time 117.61986970901489, eps 0.0010008685225871602, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
goal_identified
goal_identified
=== ep: 278, time 117.80372500419617, eps 0.0010008261642407504, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
goal_identified
=== ep: 279, time 111.569171667099, eps 0.001000785871735272, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
goal_identified
=== ep: 280, time 106.2606611251831, eps 0.0010007475443184742, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
=== ep: 281, time 117.46107530593872, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
goal_identified
=== ep: 282, time 114.3730800151825, eps 0.0010006764060709957, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
goal_identified
goal_identified
=== ep: 283, time 115.41466903686523, eps 0.001000643417357642, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
goal_identified
goal_identified
=== ep: 284, time 110.601003408432, eps 0.0010006120375228235, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
goal_identified
=== ep: 285, time 103.613525390625, eps 0.0010005821881006083, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 122.10639429092407, eps 0.0010005537944518927, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 120.8974359035492, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
=== ep: 288, time 112.96925854682922, eps 0.0010005010939419733, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
goal_identified
=== ep: 289, time 109.8392014503479, eps 0.001000476655302044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
=== ep: 290, time 99.32295656204224, eps 0.0010004534085486486, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 127.85210657119751, eps 0.0010004312955527947, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
goal_identified
=== ep: 292, time 114.01255559921265, eps 0.0010004102610204745, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
=== ep: 293, time 107.94154214859009, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
goal_identified
goal_identified
=== ep: 294, time 113.72369503974915, eps 0.0010003712195224871, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
goal_identified
goal_identified
=== ep: 295, time 120.84412002563477, eps 0.0010003531149327387, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
goal_identified
goal_identified
=== ep: 296, time 109.34560823440552, eps 0.0010003358933142518, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
goal_identified
goal_identified
=== ep: 297, time 115.4383430480957, eps 0.0010003195116040093, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
goal_identified
=== ep: 298, time 111.39136171340942, eps 0.0010003039288392032, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
goal_identified
=== ep: 299, time 126.1583161354065, eps 0.0010002891060548044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
goal_identified
goal_identified
goal_identified
=== ep: 300, time 106.60120677947998, eps 0.0010002750061861312, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
=== ep: 301, time 107.70274567604065, eps 0.0010002615939761676, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
goal_identified
goal_identified
goal_identified
=== ep: 302, time 108.20545983314514, eps 0.001000248835887403, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
goal_identified
goal_identified
=== ep: 303, time 111.4780547618866, eps 0.0010002367000179694, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
goal_identified
goal_identified
=== ep: 304, time 111.37004375457764, eps 0.0010002251560218723, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
=== ep: 305, time 109.4706916809082, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 306, time 92.51932573318481, eps 0.0010002037295934862, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
=== ep: 307, time 117.0941846370697, eps 0.0010001937935839656, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
goal_identified
=== ep: 308, time 132.95450258255005, eps 0.0010001843421593476, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
goal_identified
goal_identified
=== ep: 309, time 98.12823104858398, eps 0.0010001753516861473, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
goal_identified
=== ep: 310, time 106.33604121208191, eps 0.0010001667996834991, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
goal_identified
goal_identified
=== ep: 311, time 110.2500376701355, eps 0.001000158664766942, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
=== ep: 312, time 123.86909556388855, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
goal_identified
goal_identified
goal_identified
=== ep: 313, time 105.62662363052368, eps 0.001000143565818053, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
goal_identified
goal_identified
=== ep: 314, time 111.28406572341919, eps 0.0010001365640304844, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 315, time 105.70645093917847, eps 0.0010001299037241253, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
goal_identified
=== ep: 316, time 101.07012796401978, eps 0.0010001235682447402, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
goal_identified
goal_identified
goal_identified
=== ep: 317, time 111.7171618938446, eps 0.0010001175417503308, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
=== ep: 318, time 112.2827570438385, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 98.88802313804626, eps 0.0010001063561738807, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
goal_identified
goal_identified
goal_identified
=== ep: 320, time 98.76405620574951, eps 0.0010001011691220727, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
goal_identified
=== ep: 321, time 106.27670335769653, eps 0.0010000962350457665, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
=== ep: 322, time 105.1103765964508, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
=== ep: 323, time 93.42803454399109, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 9/9)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
=== ep: 324, time 95.92232489585876, eps 0.0010000828302715028, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
=== ep: 325, time 106.84709763526917, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
goal_identified
goal_identified
=== ep: 326, time 111.8595404624939, eps 0.0010000749479290019, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
=== ep: 327, time 113.24017190933228, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
goal_identified
goal_identified
=== ep: 328, time 112.53031969070435, eps 0.001000067815690565, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
goal_identified
goal_identified
=== ep: 329, time 113.80059576034546, eps 0.0010000645082803084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
=== ep: 330, time 120.06482076644897, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
goal_identified
=== ep: 331, time 106.369145154953, eps 0.0010000583695057963, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
=== ep: 332, time 117.01932048797607, eps 0.0010000555227914069, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
=== ep: 333, time 116.97023510932922, eps 0.0010000528149129166, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 118.07335424423218, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 335, time 118.26975345611572, eps 0.0010000477889094373, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
=== ep: 336, time 115.70710396766663, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
=== ep: 337, time 110.39626002311707, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 338, time 110.42305588722229, eps 0.0010000411322955373, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
goal_identified
=== ep: 339, time 116.19026565551758, eps 0.0010000391262498123, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 114.5350649356842, eps 0.001000037218040092, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
goal_identified
=== ep: 341, time 123.9328362941742, eps 0.0010000354028948577, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
goal_identified
=== ep: 342, time 126.37323117256165, eps 0.0010000336762753012, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
goal_identified
goal_identified
goal_identified
=== ep: 343, time 117.52467036247253, eps 0.001000032033863974, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
goal_identified
goal_identified
=== ep: 344, time 125.25261688232422, eps 0.0010000304715539925, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
=== ep: 345, time 112.607492685318, eps 0.001000028985438768, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
=== ep: 346, time 110.89816474914551, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
goal_identified
goal_identified
=== ep: 347, time 122.50434756278992, eps 0.0010000262271095755, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
goal_identified
=== ep: 348, time 124.28991627693176, eps 0.0010000249479983478, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
goal_identified
=== ep: 349, time 126.96241998672485, eps 0.0010000237312701107, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 115.12869644165039, eps 0.00100002257388241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
=== ep: 351, time 128.38346910476685, eps 0.0010000214729411737, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
=== ep: 352, time 116.30612635612488, eps 0.0010000204256934752, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
=== ep: 353, time 130.21448826789856, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
goal_identified
=== ep: 354, time 127.08715629577637, eps 0.0010000184819317455, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
goal_identified
=== ep: 355, time 130.0462474822998, eps 0.001000017580557298, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
=== ep: 356, time 117.69297742843628, eps 0.001000016723143401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
goal_identified
=== ep: 357, time 119.64537692070007, eps 0.0010000159075460732, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
goal_identified
goal_identified
=== ep: 358, time 124.53091764450073, eps 0.0010000151317258964, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
goal_identified
goal_identified
=== ep: 359, time 124.72882556915283, eps 0.0010000143937429161, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
goal_identified
goal_identified
=== ep: 360, time 114.38606834411621, eps 0.0010000136917517905, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
goal_identified
goal_identified
goal_identified
=== ep: 361, time 113.46375131607056, eps 0.001000013023997176, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
goal_identified
goal_identified
goal_identified
=== ep: 362, time 116.28852796554565, eps 0.0010000123888093385, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
goal_identified
goal_identified
=== ep: 363, time 118.99025750160217, eps 0.0010000117845999773, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 120.81592035293579, eps 0.0010000112098582543, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
=== ep: 365, time 119.14807295799255, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
goal_identified
goal_identified
=== ep: 366, time 120.65360236167908, eps 0.0010000101430991996, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
goal_identified
=== ep: 367, time 131.2113742828369, eps 0.0010000096484144142, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 368, time 107.42509841918945, eps 0.0010000091778556905, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
=== ep: 369, time 119.56385517120361, eps 0.0010000087302463867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
goal_identified
=== ep: 370, time 120.23593640327454, eps 0.001000008304467246, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
goal_identified
=== ep: 371, time 108.97925233840942, eps 0.0010000078994535993, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 114.0790102481842, eps 0.0010000075141927012, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
goal_identified
=== ep: 373, time 113.58304500579834, eps 0.0010000071477211988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
goal_identified
=== ep: 374, time 126.38785696029663, eps 0.0010000067991227223, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
goal_identified
=== ep: 375, time 111.2764642238617, eps 0.0010000064675255943, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
goal_identified
=== ep: 376, time 129.86943435668945, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
goal_identified
goal_identified
goal_identified
=== ep: 377, time 109.93272042274475, eps 0.0010000058520591598, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
goal_identified
=== ep: 378, time 119.12761449813843, eps 0.0010000055666508666, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
goal_identified
goal_identified
=== ep: 379, time 116.42093062400818, eps 0.0010000052951621003, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
goal_identified
=== ep: 380, time 106.19943833351135, eps 0.0010000050369139975, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
goal_identified
goal_identified
=== ep: 381, time 116.11001133918762, eps 0.001000004791260803, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
goal_identified
=== ep: 382, time 102.08616709709167, eps 0.0010000045575882562, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
goal_identified
=== ep: 383, time 110.04156851768494, eps 0.001000004335312054, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
goal_identified
=== ep: 384, time 111.08423924446106, eps 0.0010000041238763903, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
goal_identified
goal_identified
=== ep: 385, time 118.44285035133362, eps 0.0010000039227525655, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
goal_identified
goal_identified
goal_identified
=== ep: 386, time 109.01909160614014, eps 0.0010000037314376652, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
goal_identified
=== ep: 387, time 106.24279999732971, eps 0.001000003549453303, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
goal_identified
goal_identified
=== ep: 388, time 116.61750960350037, eps 0.0010000033763444226, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 110.63674426078796, eps 0.001000003211678162, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
goal_identified
goal_identified
=== ep: 390, time 112.67698168754578, eps 0.0010000030550427698, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 103.03176665306091, eps 0.0010000029060465757, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
=== ep: 392, time 105.071706533432, eps 0.0010000027643170119, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
goal_identified
goal_identified
=== ep: 393, time 112.87651467323303, eps 0.0010000026294996803, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
goal_identified
goal_identified
goal_identified
=== ep: 394, time 107.26552677154541, eps 0.0010000025012574677, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
goal_identified
goal_identified
=== ep: 395, time 107.56263518333435, eps 0.0010000023792697014, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
=== ep: 396, time 112.20520162582397, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 397, time 106.03566360473633, eps 0.0010000021528522535, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
=== ep: 398, time 105.07138276100159, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
goal_identified
=== ep: 399, time 109.78235054016113, eps 0.0010000019479812744, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
goal_identified
=== ep: 400, time 107.33654594421387, eps 0.0010000018529771066, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
goal_identified
=== ep: 401, time 97.27498483657837, eps 0.0010000017626063467, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
goal_identified
=== ep: 402, time 112.77873682975769, eps 0.0010000016766430208, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
=== ep: 403, time 110.56666922569275, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 103.01283717155457, eps 0.001000001517089342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
=== ep: 405, time 102.22988295555115, eps 0.0010000014431000217, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
goal_identified
=== ep: 406, time 109.7157666683197, eps 0.001000001372719203, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
=== ep: 407, time 102.5161645412445, eps 0.0010000013057708975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
goal_identified
goal_identified
=== ep: 408, time 116.59588265419006, eps 0.0010000012420876994, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
=== ep: 409, time 109.81379890441895, eps 0.0010000011815103674, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
goal_identified
goal_identified
=== ep: 410, time 105.65112948417664, eps 0.001000001123887427, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
goal_identified
goal_identified
=== ep: 411, time 105.6065239906311, eps 0.0010000010690747903, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
goal_identified
goal_identified
goal_identified
=== ep: 412, time 101.71521258354187, eps 0.0010000010169353975, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
goal_identified
goal_identified
=== ep: 413, time 107.55862617492676, eps 0.0010000009673388729, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
=== ep: 414, time 114.0373854637146, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
goal_identified
=== ep: 415, time 104.28839468955994, eps 0.0010000008752844081, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
goal_identified
=== ep: 416, time 105.43590521812439, eps 0.0010000008325962838, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
goal_identified
=== ep: 417, time 109.1713638305664, eps 0.001000000791990084, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
goal_identified
goal_identified
goal_identified
=== ep: 418, time 116.48304772377014, eps 0.0010000007533642718, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 419, time 104.6826593875885, eps 0.0010000007166222626, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
goal_identified
=== ep: 420, time 123.81922030448914, eps 0.0010000006816721825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
=== ep: 421, time 105.69807267189026, eps 0.001000000648426638, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
=== ep: 422, time 86.89793491363525, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
=== ep: 423, time 119.47491908073425, eps 0.0010000005867206849, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
=== ep: 424, time 110.70377326011658, eps 0.0010000005581059794, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 425, time 109.61809945106506, eps 0.0010000005308868295, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
goal_identified
=== ep: 426, time 108.48469042778015, eps 0.0010000005049951733, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
=== ep: 427, time 83.8764419555664, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
=== ep: 428, time 86.16869115829468, eps 0.0010000004569385287, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 92.96764993667603, eps 0.0010000004346533736, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
goal_identified
=== ep: 430, time 92.38210082054138, eps 0.0010000004134550786, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 431, time 90.6768753528595, eps 0.0010000003932906364, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
goal_identified
goal_identified
=== ep: 432, time 86.01485991477966, eps 0.0010000003741096257, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
goal_identified
goal_identified
=== ep: 433, time 83.43689918518066, eps 0.001000000355864084, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
=== ep: 434, time 80.9367904663086, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
=== ep: 435, time 85.26667904853821, eps 0.001000000321999139, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 89.14970684051514, eps 0.0010000003062950555, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
=== ep: 437, time 94.32626557350159, eps 0.0010000002913568694, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 92.86302828788757, eps 0.0010000002771472273, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
=== ep: 439, time 87.86061239242554, eps 0.0010000002636305976, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
goal_identified
goal_identified
=== ep: 440, time 78.39876389503479, eps 0.0010000002507731815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
goal_identified
goal_identified
=== ep: 441, time 78.78062438964844, eps 0.0010000002385428292, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
=== ep: 442, time 84.03120160102844, eps 0.0010000002269089582, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
=== ep: 443, time 88.08474373817444, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
goal_identified
=== ep: 444, time 84.2311782836914, eps 0.0010000002053157158, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
=== ep: 445, time 85.58947038650513, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
goal_identified
=== ep: 446, time 84.64316248893738, eps 0.001000000185777342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
goal_identified
=== ep: 447, time 86.86177468299866, eps 0.0010000001767168742, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
goal_identified
=== ep: 448, time 86.3298020362854, eps 0.0010000001680982905, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
goal_identified
=== ep: 449, time 90.98078513145447, eps 0.0010000001599000403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 84.5477888584137, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
=== ep: 451, time 85.47368693351746, eps 0.0010000001446835395, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
=== ep: 452, time 80.75435638427734, eps 0.0010000001376272401, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
goal_identified
=== ep: 453, time 76.2730438709259, eps 0.0010000001309150804, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
goal_identified
goal_identified
=== ep: 454, time 85.15331840515137, eps 0.0010000001245302765, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
goal_identified
=== ep: 455, time 88.30117058753967, eps 0.0010000001184568633, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
goal_identified
goal_identified
goal_identified
=== ep: 456, time 91.80863070487976, eps 0.0010000001126796538, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
goal_identified
goal_identified
=== ep: 457, time 89.23832440376282, eps 0.0010000001071842023, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
=== ep: 458, time 91.92622804641724, eps 0.001000000101956767, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
goal_identified
=== ep: 459, time 87.1690149307251, eps 0.001000000096984277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
goal_identified
=== ep: 460, time 83.6851577758789, eps 0.001000000092254298, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
=== ep: 461, time 82.64927792549133, eps 0.0010000000877550027, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
=== ep: 462, time 92.62856864929199, eps 0.0010000000834751407, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
goal_identified
=== ep: 463, time 88.9552571773529, eps 0.00100000007940401, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
=== ep: 464, time 88.8662497997284, eps 0.0010000000755314307, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
goal_identified
=== ep: 465, time 82.83706855773926, eps 0.0010000000718477194, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
goal_identified
=== ep: 466, time 74.72241020202637, eps 0.0010000000683436647, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
goal_identified
=== ep: 467, time 85.76072573661804, eps 0.001000000065010505, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 88.87453484535217, eps 0.0010000000618399052, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
goal_identified
=== ep: 469, time 89.59594202041626, eps 0.0010000000588239375, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
goal_identified
=== ep: 470, time 82.33610987663269, eps 0.0010000000559550603, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
=== ep: 471, time 86.72892594337463, eps 0.0010000000532260998, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
goal_identified
=== ep: 472, time 81.61615538597107, eps 0.0010000000506302322, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
goal_identified
goal_identified
=== ep: 473, time 83.58616662025452, eps 0.0010000000481609666, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
goal_identified
goal_identified
goal_identified
=== ep: 474, time 81.68334817886353, eps 0.0010000000458121286, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
goal_identified
=== ep: 475, time 90.85833501815796, eps 0.0010000000435778447, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
goal_identified
=== ep: 476, time 89.59399485588074, eps 0.001000000041452528, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
goal_identified
=== ep: 477, time 93.71524429321289, eps 0.0010000000394308644, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
goal_identified
=== ep: 478, time 88.41630101203918, eps 0.0010000000375077985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
=== ep: 479, time 92.38355660438538, eps 0.0010000000356785216, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
goal_identified
=== ep: 480, time 86.52268242835999, eps 0.0010000000339384595, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 87.94617629051208, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
goal_identified
=== ep: 482, time 82.3422737121582, eps 0.0010000000307087882, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 87.34369158744812, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
goal_identified
=== ep: 484, time 82.4479308128357, eps 0.0010000000277864607, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
goal_identified
=== ep: 485, time 92.09755182266235, eps 0.0010000000264312988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
=== ep: 486, time 90.53147411346436, eps 0.0010000000251422292, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
goal_identified
=== ep: 487, time 90.6873505115509, eps 0.0010000000239160282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
=== ep: 488, time 74.72179651260376, eps 0.00100000002274963, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
=== ep: 489, time 73.93029165267944, eps 0.0010000000216401172, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
goal_identified
goal_identified
=== ep: 490, time 85.77333402633667, eps 0.0010000000205847162, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
goal_identified
=== ep: 491, time 84.24586009979248, eps 0.0010000000195807877, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
goal_identified
goal_identified
goal_identified
=== ep: 492, time 88.75757694244385, eps 0.0010000000186258216, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
goal_identified
goal_identified
goal_identified
=== ep: 493, time 90.98714756965637, eps 0.0010000000177174295, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
goal_identified
=== ep: 494, time 90.65074610710144, eps 0.0010000000168533404, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
goal_identified
=== ep: 495, time 84.90442633628845, eps 0.0010000000160313932, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
=== ep: 496, time 78.56203269958496, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
goal_identified
goal_identified
=== ep: 497, time 84.16395330429077, eps 0.0010000000145058043, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
=== ep: 498, time 84.88756108283997, eps 0.001000000013798348, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
goal_identified
=== ep: 499, time 85.6756956577301, eps 0.0010000000131253947, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
goal_identified
goal_identified
goal_identified
=== ep: 500, time 88.37755060195923, eps 0.0010000000124852615, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
=== ep: 501, time 89.41199469566345, eps 0.0010000000118763482, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
goal_identified
=== ep: 502, time 79.7318708896637, eps 0.0010000000112971319, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
=== ep: 503, time 81.11965870857239, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 504, time 86.60723090171814, eps 0.0010000000102220676, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
=== ep: 505, time 95.23090934753418, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
=== ep: 506, time 93.74738097190857, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
goal_identified
goal_identified
=== ep: 507, time 96.15175104141235, eps 0.0010000000087982152, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
=== ep: 508, time 92.94772839546204, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
goal_identified
=== ep: 509, time 86.28605270385742, eps 0.0010000000079609542, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
=== ep: 510, time 87.2348837852478, eps 0.001000000007572694, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
goal_identified
=== ep: 511, time 82.88348436355591, eps 0.0010000000072033692, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
=== ep: 512, time 83.33204364776611, eps 0.001000000006852057, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
goal_identified
goal_identified
goal_identified
=== ep: 513, time 87.07732820510864, eps 0.001000000006517878, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
=== ep: 514, time 90.64818954467773, eps 0.0010000000061999974, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
goal_identified
goal_identified
=== ep: 515, time 93.95146775245667, eps 0.0010000000058976199, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
goal_identified
goal_identified
goal_identified
=== ep: 516, time 93.41880512237549, eps 0.0010000000056099897, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
=== ep: 517, time 94.8862657546997, eps 0.0010000000053363872, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 518, time 95.47937798500061, eps 0.0010000000050761286, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
goal_identified
=== ep: 519, time 90.66840147972107, eps 0.001000000004828563, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
=== ep: 520, time 92.64945840835571, eps 0.001000000004593071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
goal_identified
=== ep: 521, time 85.50135040283203, eps 0.0010000000043690644, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
goal_identified
=== ep: 522, time 80.3962185382843, eps 0.0010000000041559827, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
goal_identified
goal_identified
=== ep: 523, time 79.57269239425659, eps 0.0010000000039532928, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
=== ep: 524, time 85.2280375957489, eps 0.0010000000037604885, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
=== ep: 525, time 92.55216264724731, eps 0.0010000000035770874, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
=== ep: 526, time 91.42498469352722, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
=== ep: 527, time 91.87304186820984, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 93.07087922096252, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
=== ep: 529, time 94.79266023635864, eps 0.0010000000029286714, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
goal_identified
=== ep: 530, time 93.6266782283783, eps 0.0010000000027858384, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
=== ep: 531, time 92.99437379837036, eps 0.0010000000026499714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
goal_identified
goal_identified
=== ep: 532, time 87.99055099487305, eps 0.0010000000025207308, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
goal_identified
=== ep: 533, time 85.23748683929443, eps 0.0010000000023977934, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
goal_identified
goal_identified
=== ep: 534, time 91.14604711532593, eps 0.0010000000022808515, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
goal_identified
=== ep: 535, time 89.40794634819031, eps 0.0010000000021696133, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
goal_identified
=== ep: 536, time 94.0709719657898, eps 0.0010000000020637999, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
goal_identified
=== ep: 537, time 90.99070763587952, eps 0.0010000000019631471, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
goal_identified
goal_identified
goal_identified
=== ep: 538, time 79.97798919677734, eps 0.0010000000018674034, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
goal_identified
=== ep: 539, time 81.00280451774597, eps 0.001000000001776329, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
goal_identified
goal_identified
=== ep: 540, time 90.19670844078064, eps 0.0010000000016896964, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
goal_identified
goal_identified
goal_identified
=== ep: 541, time 89.21615290641785, eps 0.001000000001607289, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
=== ep: 542, time 93.62115120887756, eps 0.0010000000015289005, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
=== ep: 543, time 95.53762435913086, eps 0.0010000000014543352, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
=== ep: 544, time 92.9727053642273, eps 0.0010000000013834064, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
=== ep: 545, time 92.57273006439209, eps 0.001000000001315937, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 546, time 94.89736580848694, eps 0.0010000000012517578, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
=== ep: 547, time 88.70098423957825, eps 0.001000000001190709, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
=== ep: 548, time 84.95983910560608, eps 0.0010000000011326374, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
goal_identified
=== ep: 549, time 84.44391942024231, eps 0.001000000001077398, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
=== ep: 550, time 84.88570713996887, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
goal_identified
goal_identified
=== ep: 551, time 80.25982666015625, eps 0.00100000000097487, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 92.68444776535034, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
goal_identified
goal_identified
goal_identified
=== ep: 553, time 90.42520427703857, eps 0.0010000000008820989, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
goal_identified
goal_identified
goal_identified
=== ep: 554, time 90.35004353523254, eps 0.0010000000008390784, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
=== ep: 555, time 91.03201627731323, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
goal_identified
goal_identified
=== ep: 556, time 82.68539500236511, eps 0.0010000000007592295, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 557, time 83.69647550582886, eps 0.0010000000007222014, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
=== ep: 558, time 83.51146650314331, eps 0.0010000000006869794, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
goal_identified
goal_identified
=== ep: 559, time 85.33558368682861, eps 0.001000000000653475, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
=== ep: 560, time 90.53166007995605, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
goal_identified
goal_identified
=== ep: 561, time 90.81120729446411, eps 0.0010000000005912885, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
goal_identified
=== ep: 562, time 90.13444566726685, eps 0.0010000000005624511, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
goal_identified
goal_identified
goal_identified
=== ep: 563, time 89.86971545219421, eps 0.00100000000053502, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
goal_identified
=== ep: 564, time 94.83849120140076, eps 0.001000000000508927, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
goal_identified
=== ep: 565, time 90.43356943130493, eps 0.001000000000484106, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 566, time 92.7575204372406, eps 0.001000000000460496, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
goal_identified
=== ep: 567, time 93.27208590507507, eps 0.0010000000004380374, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
goal_identified
goal_identified
=== ep: 568, time 92.87425637245178, eps 0.001000000000416674, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
goal_identified
goal_identified
=== ep: 569, time 90.41209411621094, eps 0.0010000000003963527, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 87.25866484642029, eps 0.0010000000003770222, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
goal_identified
goal_identified
goal_identified
=== ep: 571, time 68.70161867141724, eps 0.0010000000003586346, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
goal_identified
=== ep: 572, time 85.55989241600037, eps 0.0010000000003411438, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
goal_identified
goal_identified
goal_identified
=== ep: 573, time 78.9191575050354, eps 0.001000000000324506, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
goal_identified
goal_identified
=== ep: 574, time 83.95978951454163, eps 0.0010000000003086798, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
goal_identified
=== ep: 575, time 89.6532211303711, eps 0.0010000000002936252, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
goal_identified
=== ep: 576, time 89.89946341514587, eps 0.001000000000279305, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
=== ep: 577, time 78.0105550289154, eps 0.0010000000002656831, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 84.79873728752136, eps 0.0010000000002527256, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
goal_identified
=== ep: 579, time 83.96989583969116, eps 0.0010000000002404, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
=== ep: 580, time 88.30037641525269, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
=== ep: 581, time 91.87235951423645, eps 0.0010000000002175229, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
goal_identified
goal_identified
=== ep: 582, time 88.55546641349792, eps 0.0010000000002069142, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
goal_identified
=== ep: 583, time 94.56777906417847, eps 0.0010000000001968228, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
goal_identified
=== ep: 584, time 92.46776723861694, eps 0.0010000000001872237, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
=== ep: 585, time 88.75909757614136, eps 0.0010000000001780928, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
goal_identified
goal_identified
goal_identified
=== ep: 586, time 89.15456128120422, eps 0.001000000000169407, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
goal_identified
goal_identified
goal_identified
=== ep: 587, time 84.24544668197632, eps 0.001000000000161145, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
=== ep: 588, time 78.45351600646973, eps 0.0010000000001532858, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
goal_identified
goal_identified
goal_identified
=== ep: 589, time 82.17313385009766, eps 0.00100000000014581, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
goal_identified
goal_identified
=== ep: 590, time 82.35396027565002, eps 0.0010000000001386988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
goal_identified
goal_identified
goal_identified
=== ep: 591, time 85.42429995536804, eps 0.0010000000001319344, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 88.76915907859802, eps 0.0010000000001255, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
=== ep: 593, time 91.18622589111328, eps 0.0010000000001193791, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
goal_identified
=== ep: 594, time 95.02745318412781, eps 0.001000000000113557, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
goal_identified
=== ep: 595, time 89.55612230300903, eps 0.0010000000001080186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
goal_identified
=== ep: 596, time 91.29325246810913, eps 0.0010000000001027505, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
goal_identified
goal_identified
=== ep: 597, time 88.7776608467102, eps 0.0010000000000977393, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
goal_identified
=== ep: 598, time 86.86633896827698, eps 0.0010000000000929725, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
goal_identified
=== ep: 599, time 88.40823030471802, eps 0.0010000000000884382, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
=== ep: 600, time 91.35316324234009, eps 0.001000000000084125, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
=== ep: 601, time 87.37368774414062, eps 0.0010000000000800222, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
=== ep: 602, time 88.77872228622437, eps 0.0010000000000761195, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
goal_identified
=== ep: 603, time 83.41034412384033, eps 0.0010000000000724072, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
goal_identified
goal_identified
=== ep: 604, time 84.62462115287781, eps 0.0010000000000688757, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
goal_identified
=== ep: 605, time 85.15639901161194, eps 0.0010000000000655166, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
=== ep: 606, time 84.35377979278564, eps 0.0010000000000623215, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
goal_identified
goal_identified
goal_identified
=== ep: 607, time 87.30279493331909, eps 0.001000000000059282, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
goal_identified
goal_identified
goal_identified
=== ep: 608, time 90.3223729133606, eps 0.0010000000000563907, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
=== ep: 609, time 90.11323833465576, eps 0.0010000000000536405, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
goal_identified
=== ep: 610, time 94.2578432559967, eps 0.0010000000000510245, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 94.34236979484558, eps 0.0010000000000485358, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
=== ep: 612, time 92.3512670993805, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
goal_identified
goal_identified
=== ep: 613, time 94.62164187431335, eps 0.0010000000000439171, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
goal_identified
goal_identified
=== ep: 614, time 89.10680866241455, eps 0.0010000000000417752, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 615, time 89.13442587852478, eps 0.0010000000000397378, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
goal_identified
goal_identified
=== ep: 616, time 88.28645873069763, eps 0.0010000000000377999, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 83.52384757995605, eps 0.0010000000000359563, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
=== ep: 618, time 75.77632522583008, eps 0.0010000000000342027, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
goal_identified
goal_identified
goal_identified
=== ep: 619, time 80.84470343589783, eps 0.0010000000000325345, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
goal_identified
goal_identified
=== ep: 620, time 79.63846015930176, eps 0.001000000000030948, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
goal_identified
goal_identified
=== ep: 621, time 83.7698724269867, eps 0.0010000000000294385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
goal_identified
=== ep: 622, time 89.02712345123291, eps 0.0010000000000280028, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 90.85502433776855, eps 0.0010000000000266371, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
goal_identified
goal_identified
=== ep: 624, time 93.32399368286133, eps 0.001000000000025338, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
goal_identified
goal_identified
=== ep: 625, time 92.9829032421112, eps 0.0010000000000241023, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
=== ep: 626, time 93.93307828903198, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
goal_identified
=== ep: 627, time 93.43474817276001, eps 0.0010000000000218085, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
=== ep: 628, time 96.41301727294922, eps 0.001000000000020745, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 89.6511459350586, eps 0.0010000000000197332, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
goal_identified
=== ep: 630, time 89.66469430923462, eps 0.0010000000000187708, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
goal_identified
goal_identified
goal_identified
=== ep: 631, time 87.0823130607605, eps 0.0010000000000178553, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
goal_identified
=== ep: 632, time 83.55379676818848, eps 0.0010000000000169845, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
goal_identified
=== ep: 633, time 80.56032133102417, eps 0.0010000000000161562, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
goal_identified
goal_identified
=== ep: 634, time 80.51318573951721, eps 0.0010000000000153684, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
=== ep: 635, time 79.41115546226501, eps 0.0010000000000146188, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
goal_identified
=== ep: 636, time 87.00059938430786, eps 0.0010000000000139058, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
goal_identified
=== ep: 637, time 87.93030762672424, eps 0.0010000000000132275, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
=== ep: 638, time 88.09448719024658, eps 0.0010000000000125824, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
=== ep: 639, time 90.90508246421814, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
goal_identified
goal_identified
=== ep: 640, time 93.26905059814453, eps 0.001000000000011385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
=== ep: 641, time 88.60131740570068, eps 0.00100000000001083, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
goal_identified
=== ep: 642, time 86.51716446876526, eps 0.0010000000000103017, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 642
=== ep: 643, time 85.35388326644897, eps 0.0010000000000097993, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
goal_identified
=== ep: 644, time 80.7099769115448, eps 0.0010000000000093213, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
goal_identified
=== ep: 645, time 85.34813499450684, eps 0.0010000000000088666, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
goal_identified
goal_identified
goal_identified
=== ep: 646, time 81.24397921562195, eps 0.0010000000000084342, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
=== ep: 647, time 82.49894118309021, eps 0.001000000000008023, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
goal_identified
goal_identified
=== ep: 648, time 89.3666558265686, eps 0.0010000000000076317, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
goal_identified
=== ep: 649, time 83.83333230018616, eps 0.0010000000000072594, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
=== ep: 650, time 93.44960761070251, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
goal_identified
=== ep: 651, time 91.08463931083679, eps 0.0010000000000065686, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
=== ep: 652, time 98.03303027153015, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
goal_identified
goal_identified
goal_identified
=== ep: 653, time 94.01014065742493, eps 0.0010000000000059436, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 95.10346245765686, eps 0.0010000000000056537, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
goal_identified
=== ep: 655, time 96.62257671356201, eps 0.0010000000000053779, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
goal_identified
=== ep: 656, time 93.2586030960083, eps 0.0010000000000051157, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
=== ep: 657, time 97.00253176689148, eps 0.0010000000000048661, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
goal_identified
goal_identified
=== ep: 658, time 94.10676312446594, eps 0.001000000000004629, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
goal_identified
goal_identified
goal_identified
=== ep: 659, time 89.32627153396606, eps 0.0010000000000044032, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
goal_identified
goal_identified
=== ep: 660, time 90.25531387329102, eps 0.0010000000000041883, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
goal_identified
goal_identified
=== ep: 661, time 85.43531942367554, eps 0.001000000000003984, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
goal_identified
goal_identified
goal_identified
=== ep: 662, time 75.40960693359375, eps 0.0010000000000037897, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 82.91897058486938, eps 0.001000000000003605, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
goal_identified
=== ep: 664, time 78.04531002044678, eps 0.0010000000000034291, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
=== ep: 665, time 87.39754152297974, eps 0.001000000000003262, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
goal_identified
goal_identified
=== ep: 666, time 85.4267590045929, eps 0.0010000000000031028, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
goal_identified
=== ep: 667, time 83.47573065757751, eps 0.0010000000000029514, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
goal_identified
=== ep: 668, time 88.51604747772217, eps 0.0010000000000028075, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
=== ep: 669, time 93.24304533004761, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 84.05792331695557, eps 0.0010000000000025403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
=== ep: 671, time 88.33975553512573, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
goal_identified
=== ep: 672, time 85.97629380226135, eps 0.0010000000000022985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
=== ep: 673, time 79.32855105400085, eps 0.0010000000000021864, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
goal_identified
=== ep: 674, time 83.78372192382812, eps 0.00100000000000208, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
goal_identified
=== ep: 675, time 83.24446773529053, eps 0.0010000000000019785, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
goal_identified
=== ep: 676, time 86.28715753555298, eps 0.001000000000001882, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
goal_identified
=== ep: 677, time 89.13399028778076, eps 0.0010000000000017903, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
goal_identified
=== ep: 678, time 88.03362393379211, eps 0.0010000000000017029, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
goal_identified
=== ep: 679, time 86.08644556999207, eps 0.0010000000000016198, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
=== ep: 680, time 86.21629452705383, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
goal_identified
=== ep: 681, time 76.26987767219543, eps 0.0010000000000014656, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
goal_identified
goal_identified
goal_identified
=== ep: 682, time 84.43374371528625, eps 0.0010000000000013943, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
goal_identified
=== ep: 683, time 75.60192227363586, eps 0.0010000000000013262, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
goal_identified
goal_identified
=== ep: 684, time 91.11004734039307, eps 0.0010000000000012616, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
goal_identified
=== ep: 685, time 87.07906937599182, eps 0.0010000000000012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
=== ep: 686, time 85.27049922943115, eps 0.0010000000000011415, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
goal_identified
=== ep: 687, time 89.17732262611389, eps 0.0010000000000010857, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
goal_identified
=== ep: 688, time 88.49458622932434, eps 0.0010000000000010328, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
=== ep: 689, time 93.78435635566711, eps 0.0010000000000009825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
=== ep: 690, time 89.46520829200745, eps 0.0010000000000009346, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
goal_identified
=== ep: 691, time 92.9245126247406, eps 0.001000000000000889, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
=== ep: 692, time 91.92340970039368, eps 0.0010000000000008457, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 89.85409688949585, eps 0.0010000000000008045, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
=== ep: 694, time 91.79599356651306, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
=== ep: 695, time 88.91269969940186, eps 0.0010000000000007277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
goal_identified
=== ep: 696, time 81.43435049057007, eps 0.0010000000000006924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
goal_identified
goal_identified
goal_identified
=== ep: 697, time 64.85656476020813, eps 0.0010000000000006586, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 698, time 85.99090385437012, eps 0.0010000000000006265, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
goal_identified
goal_identified
goal_identified
=== ep: 699, time 78.20305061340332, eps 0.001000000000000596, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
goal_identified
goal_identified
=== ep: 700, time 87.85778284072876, eps 0.0010000000000005668, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
goal_identified
=== ep: 701, time 87.09160017967224, eps 0.0010000000000005393, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
=== ep: 702, time 86.97731328010559, eps 0.0010000000000005128, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 702
goal_identified
=== ep: 703, time 87.84326577186584, eps 0.001000000000000488, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
=== ep: 704, time 87.72876381874084, eps 0.001000000000000464, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
=== ep: 705, time 86.54926133155823, eps 0.0010000000000004415, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
goal_identified
=== ep: 706, time 81.68401217460632, eps 0.00100000000000042, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
goal_identified
=== ep: 707, time 89.172687292099, eps 0.0010000000000003994, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 708, time 82.41101241111755, eps 0.00100000000000038, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
=== ep: 709, time 88.16224265098572, eps 0.0010000000000003615, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
goal_identified
=== ep: 710, time 76.87913870811462, eps 0.0010000000000003437, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 711, time 74.95769309997559, eps 0.001000000000000327, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
goal_identified
=== ep: 712, time 89.47192716598511, eps 0.0010000000000003112, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 85.87644839286804, eps 0.001000000000000296, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
=== ep: 714, time 88.22353315353394, eps 0.0010000000000002815, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
=== ep: 715, time 92.26128554344177, eps 0.0010000000000002678, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
goal_identified
=== ep: 716, time 86.59310674667358, eps 0.0010000000000002548, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
goal_identified
=== ep: 717, time 86.4205973148346, eps 0.0010000000000002422, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
=== ep: 718, time 85.09878468513489, eps 0.0010000000000002305, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
goal_identified
=== ep: 719, time 79.14423251152039, eps 0.0010000000000002192, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
=== ep: 720, time 86.19498658180237, eps 0.0010000000000002086, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
goal_identified
goal_identified
goal_identified
=== ep: 721, time 81.0027904510498, eps 0.0010000000000001984, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
=== ep: 722, time 92.52462267875671, eps 0.0010000000000001887, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
goal_identified
=== ep: 723, time 87.648672580719, eps 0.0010000000000001796, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
goal_identified
=== ep: 724, time 96.07536363601685, eps 0.0010000000000001707, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
=== ep: 725, time 96.1986038684845, eps 0.0010000000000001624, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
goal_identified
goal_identified
=== ep: 726, time 91.02511644363403, eps 0.0010000000000001544, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
=== ep: 727, time 86.2590081691742, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
goal_identified
goal_identified
=== ep: 728, time 82.8292019367218, eps 0.0010000000000001399, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
=== ep: 729, time 81.16454315185547, eps 0.001000000000000133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
goal_identified
goal_identified
=== ep: 730, time 80.39528441429138, eps 0.0010000000000001264, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
=== ep: 731, time 90.76312756538391, eps 0.0010000000000001204, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
goal_identified
goal_identified
goal_identified
=== ep: 732, time 87.75222730636597, eps 0.0010000000000001145, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 733, time 88.26562213897705, eps 0.0010000000000001089, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
=== ep: 734, time 88.41393184661865, eps 0.0010000000000001037, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
=== ep: 735, time 80.06201195716858, eps 0.0010000000000000985, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
=== ep: 736, time 82.10445022583008, eps 0.0010000000000000937, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
goal_identified
goal_identified
=== ep: 737, time 86.70937204360962, eps 0.0010000000000000891, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
goal_identified
=== ep: 738, time 91.07697582244873, eps 0.0010000000000000848, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
=== ep: 739, time 89.95826172828674, eps 0.0010000000000000807, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 740, time 88.44792914390564, eps 0.0010000000000000768, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 741, time 80.56756019592285, eps 0.001000000000000073, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
goal_identified
goal_identified
=== ep: 742, time 89.34626984596252, eps 0.0010000000000000694, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 743, time 80.45986151695251, eps 0.001000000000000066, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
goal_identified
=== ep: 744, time 97.28283405303955, eps 0.001000000000000063, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
goal_identified
goal_identified
goal_identified
=== ep: 745, time 93.66804099082947, eps 0.0010000000000000599, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
goal_identified
=== ep: 746, time 93.90467476844788, eps 0.0010000000000000568, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
=== ep: 747, time 96.20438528060913, eps 0.001000000000000054, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
goal_identified
=== ep: 748, time 95.66296291351318, eps 0.0010000000000000514, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
goal_identified
=== ep: 749, time 89.26380062103271, eps 0.001000000000000049, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
goal_identified
=== ep: 750, time 85.4083845615387, eps 0.0010000000000000466, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
=== ep: 751, time 86.82673287391663, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
=== ep: 752, time 88.02619075775146, eps 0.001000000000000042, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
goal_identified
=== ep: 753, time 92.49626278877258, eps 0.0010000000000000401, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
=== ep: 754, time 97.22336959838867, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
goal_identified
goal_identified
goal_identified
=== ep: 755, time 97.36254143714905, eps 0.0010000000000000362, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
goal_identified
=== ep: 756, time 92.94812607765198, eps 0.0010000000000000345, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
goal_identified
goal_identified
=== ep: 757, time 100.7856674194336, eps 0.0010000000000000328, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 97.07807564735413, eps 0.0010000000000000312, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
=== ep: 759, time 98.09200549125671, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
goal_identified
goal_identified
=== ep: 760, time 102.08866357803345, eps 0.0010000000000000282, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
=== ep: 761, time 104.61453747749329, eps 0.001000000000000027, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
goal_identified
=== ep: 762, time 100.84912252426147, eps 0.0010000000000000256, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
goal_identified
=== ep: 763, time 98.90413069725037, eps 0.0010000000000000243, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
goal_identified
=== ep: 764, time 93.68081903457642, eps 0.0010000000000000232, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
goal_identified
=== ep: 765, time 101.018381357193, eps 0.001000000000000022, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
=== ep: 766, time 94.04365420341492, eps 0.0010000000000000208, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
=== ep: 767, time 90.93625140190125, eps 0.00100000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
goal_identified
goal_identified
goal_identified
=== ep: 768, time 86.02476477622986, eps 0.0010000000000000189, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
goal_identified
goal_identified
=== ep: 769, time 96.90174293518066, eps 0.001000000000000018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
goal_identified
goal_identified
=== ep: 770, time 76.92548084259033, eps 0.0010000000000000172, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
=== ep: 771, time 90.30665040016174, eps 0.0010000000000000163, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
=== ep: 772, time 80.57645106315613, eps 0.0010000000000000154, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
goal_identified
=== ep: 773, time 93.10460019111633, eps 0.0010000000000000148, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
=== ep: 774, time 89.74817991256714, eps 0.0010000000000000141, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
goal_identified
goal_identified
=== ep: 775, time 92.1285285949707, eps 0.0010000000000000132, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
goal_identified
goal_identified
=== ep: 776, time 93.81990313529968, eps 0.0010000000000000126, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
goal_identified
goal_identified
=== ep: 777, time 100.03020453453064, eps 0.0010000000000000122, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
goal_identified
=== ep: 778, time 95.64852714538574, eps 0.0010000000000000115, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
goal_identified
=== ep: 779, time 97.37971520423889, eps 0.0010000000000000109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
=== ep: 780, time 95.98367762565613, eps 0.0010000000000000104, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
goal_identified
goal_identified
goal_identified
=== ep: 781, time 96.94974827766418, eps 0.00100000000000001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
goal_identified
goal_identified
=== ep: 782, time 98.66147184371948, eps 0.0010000000000000093, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
goal_identified
=== ep: 783, time 99.35096073150635, eps 0.001000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
goal_identified
goal_identified
=== ep: 784, time 97.1893219947815, eps 0.0010000000000000085, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 97.33841872215271, eps 0.001000000000000008, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
=== ep: 786, time 95.55508971214294, eps 0.0010000000000000076, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
goal_identified
goal_identified
goal_identified
=== ep: 787, time 89.72019457817078, eps 0.0010000000000000074, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
=== ep: 788, time 94.19145894050598, eps 0.001000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
goal_identified
=== ep: 789, time 97.23306059837341, eps 0.0010000000000000067, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
goal_identified
=== ep: 790, time 97.09608435630798, eps 0.0010000000000000063, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
=== ep: 791, time 88.30618286132812, eps 0.001000000000000006, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
goal_identified
=== ep: 792, time 94.78726148605347, eps 0.0010000000000000057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
goal_identified
goal_identified
=== ep: 793, time 91.91566634178162, eps 0.0010000000000000054, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
=== ep: 794, time 95.29436421394348, eps 0.0010000000000000052, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
goal_identified
=== ep: 795, time 100.27627468109131, eps 0.001000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
=== ep: 796, time 91.94135189056396, eps 0.0010000000000000048, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
=== ep: 797, time 98.78479886054993, eps 0.0010000000000000044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
goal_identified
=== ep: 798, time 98.50574612617493, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
=== ep: 799, time 93.21326041221619, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
=== ep: 800, time 96.89549612998962, eps 0.001000000000000004, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
=== ep: 801, time 97.22876238822937, eps 0.0010000000000000037, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
goal_identified
=== ep: 802, time 94.58280181884766, eps 0.0010000000000000035, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
goal_identified
=== ep: 803, time 89.18091869354248, eps 0.0010000000000000033, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
goal_identified
goal_identified
=== ep: 804, time 94.38657093048096, eps 0.001000000000000003, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
goal_identified
goal_identified
goal_identified
=== ep: 805, time 87.9410171508789, eps 0.001000000000000003, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
goal_identified
=== ep: 806, time 91.44109201431274, eps 0.0010000000000000028, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
=== ep: 807, time 93.51386189460754, eps 0.0010000000000000026, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
goal_identified
=== ep: 808, time 92.60578203201294, eps 0.0010000000000000026, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
goal_identified
=== ep: 809, time 92.69031572341919, eps 0.0010000000000000024, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
=== ep: 810, time 94.48080563545227, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
=== ep: 811, time 96.0643982887268, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
goal_identified
goal_identified
=== ep: 812, time 93.82858920097351, eps 0.0010000000000000022, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
goal_identified
=== ep: 813, time 92.59842228889465, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
goal_identified
=== ep: 814, time 99.87657284736633, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
goal_identified
goal_identified
=== ep: 815, time 92.8390760421753, eps 0.0010000000000000018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
goal_identified
=== ep: 816, time 93.06198287010193, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
goal_identified
=== ep: 817, time 93.57968163490295, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
goal_identified
=== ep: 818, time 87.30505609512329, eps 0.0010000000000000015, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
goal_identified
goal_identified
=== ep: 819, time 79.56794118881226, eps 0.0010000000000000015, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
=== ep: 820, time 91.57586359977722, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
goal_identified
=== ep: 821, time 76.10271430015564, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 96.8804543018341, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
goal_identified
=== ep: 823, time 92.80669116973877, eps 0.0010000000000000013, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
goal_identified
=== ep: 824, time 87.71329832077026, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
goal_identified
goal_identified
=== ep: 825, time 97.11057639122009, eps 0.001000000000000001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
=== ep: 826, time 96.71062016487122, eps 0.001000000000000001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
goal_identified
=== ep: 827, time 86.82242679595947, eps 0.001000000000000001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
goal_identified
=== ep: 828, time 91.80035328865051, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
goal_identified
goal_identified
goal_identified
=== ep: 829, time 90.93657183647156, eps 0.0010000000000000009, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
goal_identified
goal_identified
goal_identified
=== ep: 830, time 92.63077473640442, eps 0.0010000000000000009, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
goal_identified
goal_identified
=== ep: 831, time 87.89784073829651, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
=== ep: 832, time 91.63800048828125, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
=== ep: 833, time 85.43912076950073, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
goal_identified
goal_identified
goal_identified
=== ep: 834, time 93.42030644416809, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
goal_identified
goal_identified
goal_identified
=== ep: 835, time 87.81771898269653, eps 0.0010000000000000007, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
goal_identified
goal_identified
goal_identified
=== ep: 836, time 86.68386030197144, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
goal_identified
=== ep: 837, time 89.03855991363525, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
goal_identified
goal_identified
=== ep: 838, time 95.20964765548706, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
=== ep: 839, time 92.60542821884155, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
goal_identified
goal_identified
=== ep: 840, time 93.59153437614441, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
=== ep: 841, time 92.9545226097107, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
=== ep: 842, time 96.42857718467712, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
=== ep: 843, time 93.17346000671387, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
goal_identified
=== ep: 844, time 87.87784194946289, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
goal_identified
goal_identified
goal_identified
=== ep: 845, time 91.63601183891296, eps 0.0010000000000000005, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
goal_identified
goal_identified
=== ep: 846, time 81.45202326774597, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
goal_identified
goal_identified
goal_identified
=== ep: 847, time 91.32696676254272, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
=== ep: 848, time 84.7416365146637, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
goal_identified
=== ep: 849, time 93.77286577224731, eps 0.0010000000000000005, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
goal_identified
=== ep: 850, time 96.22321343421936, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
=== ep: 851, time 98.32390332221985, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
=== ep: 852, time 97.36680340766907, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
goal_identified
=== ep: 853, time 92.9921452999115, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
goal_identified
=== ep: 854, time 97.76559972763062, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
=== ep: 855, time 92.64972186088562, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
goal_identified
=== ep: 856, time 92.88861751556396, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
goal_identified
=== ep: 857, time 89.84496665000916, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
goal_identified
=== ep: 858, time 88.4038393497467, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
=== ep: 859, time 89.55224943161011, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
goal_identified
=== ep: 860, time 93.99226927757263, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
=== ep: 861, time 90.78242039680481, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
goal_identified
goal_identified
=== ep: 862, time 95.2998902797699, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
goal_identified
=== ep: 863, time 90.00375318527222, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
=== ep: 864, time 98.64592456817627, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
=== ep: 865, time 93.23728942871094, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
=== ep: 866, time 94.84510612487793, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
goal_identified
goal_identified
goal_identified
=== ep: 867, time 90.82231998443604, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
=== ep: 868, time 90.65696334838867, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
goal_identified
=== ep: 869, time 90.34847736358643, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
goal_identified
=== ep: 870, time 83.7321572303772, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 93.6965172290802, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
=== ep: 872, time 94.14799094200134, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
=== ep: 873, time 95.663325548172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
goal_identified
=== ep: 874, time 95.04714679718018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
goal_identified
goal_identified
goal_identified
=== ep: 875, time 95.24125647544861, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
=== ep: 876, time 99.97488069534302, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
goal_identified
=== ep: 877, time 96.0606107711792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
goal_identified
=== ep: 878, time 98.36300563812256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
goal_identified
goal_identified
=== ep: 879, time 99.54836201667786, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
=== ep: 880, time 103.16543841362, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
=== ep: 881, time 91.74226903915405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
goal_identified
=== ep: 882, time 87.35878825187683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
=== ep: 883, time 89.17016315460205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
goal_identified
goal_identified
=== ep: 884, time 87.12956237792969, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
goal_identified
=== ep: 885, time 86.53441190719604, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
goal_identified
=== ep: 886, time 91.8213517665863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
=== ep: 887, time 95.67997550964355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
goal_identified
=== ep: 888, time 97.7786431312561, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
=== ep: 889, time 101.9089298248291, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
goal_identified
goal_identified
=== ep: 890, time 91.59466123580933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
=== ep: 891, time 99.98307585716248, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
=== ep: 892, time 96.08535361289978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
goal_identified
=== ep: 893, time 91.94334936141968, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
=== ep: 894, time 96.76339054107666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
goal_identified
=== ep: 895, time 85.37077927589417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
=== ep: 896, time 92.69204783439636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
goal_identified
goal_identified
=== ep: 897, time 91.43708205223083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
goal_identified
goal_identified
=== ep: 898, time 96.01275563240051, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 898
goal_identified
goal_identified
=== ep: 899, time 91.97095608711243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
goal_identified
goal_identified
=== ep: 900, time 95.16994261741638, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
=== ep: 901, time 92.83410096168518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
goal_identified
=== ep: 902, time 90.0229287147522, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
goal_identified
=== ep: 903, time 85.38927149772644, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
goal_identified
goal_identified
=== ep: 904, time 93.13058638572693, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
goal_identified
goal_identified
goal_identified
=== ep: 905, time 87.8863410949707, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
=== ep: 906, time 93.76840567588806, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
goal_identified
goal_identified
=== ep: 907, time 93.80778050422668, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
goal_identified
goal_identified
=== ep: 908, time 100.1152594089508, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
goal_identified
goal_identified
goal_identified
=== ep: 909, time 98.62789821624756, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
goal_identified
goal_identified
=== ep: 910, time 93.48898148536682, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
goal_identified
=== ep: 911, time 92.67484021186829, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
goal_identified
goal_identified
goal_identified
=== ep: 912, time 93.47956347465515, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
goal_identified
=== ep: 913, time 90.8651921749115, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
goal_identified
=== ep: 914, time 73.60059928894043, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
goal_identified
goal_identified
=== ep: 915, time 94.34811878204346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
goal_identified
=== ep: 916, time 93.45674109458923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
=== ep: 917, time 95.36861324310303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
=== ep: 918, time 103.05150818824768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
=== ep: 919, time 98.3104293346405, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
goal_identified
goal_identified
=== ep: 920, time 99.96724057197571, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
goal_identified
=== ep: 921, time 100.85476636886597, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
goal_identified
=== ep: 922, time 99.68191766738892, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
goal_identified
=== ep: 923, time 95.01508092880249, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
goal_identified
=== ep: 924, time 93.25364398956299, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
=== ep: 925, time 96.10402870178223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
=== ep: 926, time 83.53552532196045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
=== ep: 927, time 90.25501942634583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
goal_identified
goal_identified
=== ep: 928, time 90.53584694862366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
goal_identified
goal_identified
=== ep: 929, time 94.96927618980408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
goal_identified
=== ep: 930, time 92.1990110874176, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
goal_identified
=== ep: 931, time 94.16218876838684, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
=== ep: 932, time 91.90566396713257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
=== ep: 933, time 93.73277950286865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
=== ep: 934, time 81.97405934333801, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
goal_identified
goal_identified
=== ep: 935, time 96.8684446811676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
=== ep: 936, time 93.42654013633728, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
=== ep: 937, time 96.31030082702637, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
goal_identified
=== ep: 938, time 100.86562776565552, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
=== ep: 939, time 98.71819019317627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
goal_identified
goal_identified
goal_identified
=== ep: 940, time 102.48819756507874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
goal_identified
=== ep: 941, time 97.53241658210754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
=== ep: 942, time 95.00751566886902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
=== ep: 943, time 91.40810012817383, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
goal_identified
goal_identified
goal_identified
=== ep: 944, time 90.2210066318512, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
goal_identified
=== ep: 945, time 88.99321126937866, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
goal_identified
=== ep: 946, time 94.59533429145813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
goal_identified
=== ep: 947, time 95.11722445487976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
=== ep: 948, time 94.3097562789917, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
goal_identified
=== ep: 949, time 92.36547040939331, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
goal_identified
=== ep: 950, time 92.84896183013916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
=== ep: 951, time 90.60794186592102, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
=== ep: 952, time 87.54654788970947, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
goal_identified
goal_identified
=== ep: 953, time 92.66325259208679, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
=== ep: 954, time 94.55118775367737, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
goal_identified
=== ep: 955, time 99.56245636940002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
=== ep: 956, time 96.29305529594421, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
goal_identified
=== ep: 957, time 92.65922379493713, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
goal_identified
goal_identified
=== ep: 958, time 94.09232306480408, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
goal_identified
=== ep: 959, time 97.13439083099365, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
goal_identified
=== ep: 960, time 97.94557189941406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
=== ep: 961, time 96.56446886062622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
=== ep: 962, time 99.04053378105164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
goal_identified
=== ep: 963, time 95.062490940094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
goal_identified
goal_identified
=== ep: 964, time 95.54967141151428, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
goal_identified
=== ep: 965, time 92.66569805145264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
=== ep: 966, time 85.71970868110657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
=== ep: 967, time 90.67415738105774, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
=== ep: 968, time 94.08894348144531, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
goal_identified
=== ep: 969, time 92.30344200134277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
goal_identified
=== ep: 970, time 92.5146713256836, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
goal_identified
=== ep: 971, time 97.42531394958496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
=== ep: 972, time 96.53766536712646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 973, time 99.14112186431885, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
=== ep: 974, time 96.87354493141174, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
=== ep: 975, time 98.72535157203674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
goal_identified
=== ep: 976, time 96.16054773330688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
goal_identified
goal_identified
=== ep: 977, time 99.86491394042969, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
goal_identified
goal_identified
goal_identified
=== ep: 978, time 100.38874173164368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
=== ep: 979, time 100.85433983802795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
goal_identified
=== ep: 980, time 103.29104208946228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
goal_identified
goal_identified
=== ep: 981, time 101.2974443435669, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
=== ep: 982, time 103.86935234069824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
=== ep: 983, time 103.51367020606995, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
goal_identified
=== ep: 984, time 101.14312529563904, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
goal_identified
=== ep: 985, time 101.34221601486206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
goal_identified
=== ep: 986, time 102.29487729072571, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
=== ep: 987, time 104.74297070503235, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
=== ep: 988, time 97.7843451499939, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 989, time 99.12063550949097, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
=== ep: 990, time 104.03066802024841, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
=== ep: 991, time 101.31263017654419, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
=== ep: 992, time 103.45004367828369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
goal_identified
=== ep: 993, time 106.29326272010803, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
goal_identified
=== ep: 994, time 99.15280342102051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
goal_identified
=== ep: 995, time 97.36891984939575, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
goal_identified
=== ep: 996, time 99.76178503036499, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
=== ep: 997, time 94.74426198005676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
goal_identified
goal_identified
=== ep: 998, time 104.144455909729, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
goal_identified
goal_identified
=== ep: 999, time 99.42608380317688, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
goal_identified
goal_identified
=== ep: 1000, time 101.64185762405396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
=== ep: 1001, time 96.46354174613953, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
=== ep: 1002, time 101.09608554840088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
=== ep: 1003, time 100.99222874641418, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
=== ep: 1004, time 101.79516792297363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
=== ep: 1005, time 102.97997164726257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
goal_identified
goal_identified
=== ep: 1006, time 96.87059378623962, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
=== ep: 1007, time 98.82107186317444, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
=== ep: 1008, time 102.2539873123169, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
=== ep: 1009, time 94.62427258491516, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
goal_identified
goal_identified
=== ep: 1010, time 103.92360424995422, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
goal_identified
=== ep: 1011, time 96.210205078125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
goal_identified
=== ep: 1012, time 97.78852939605713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
goal_identified
goal_identified
=== ep: 1013, time 97.66465425491333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
goal_identified
goal_identified
goal_identified
=== ep: 1014, time 94.3884379863739, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
goal_identified
=== ep: 1015, time 103.90211892127991, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
goal_identified
=== ep: 1016, time 94.0869255065918, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
=== ep: 1017, time 101.84617352485657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
goal_identified
=== ep: 1018, time 98.93609595298767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
=== ep: 1019, time 104.40812921524048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
goal_identified
goal_identified
=== ep: 1020, time 96.70335292816162, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
goal_identified
=== ep: 1021, time 101.14681887626648, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
goal_identified
=== ep: 1022, time 107.48894739151001, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
goal_identified
goal_identified
=== ep: 1023, time 104.58507323265076, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
goal_identified
=== ep: 1024, time 105.12140822410583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
goal_identified
=== ep: 1025, time 105.33572602272034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
goal_identified
=== ep: 1026, time 101.03244304656982, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
=== ep: 1027, time 103.20760202407837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
goal_identified
=== ep: 1028, time 101.66960287094116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
goal_identified
=== ep: 1029, time 99.00183844566345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
=== ep: 1030, time 99.88771605491638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
goal_identified
=== ep: 1031, time 104.42424774169922, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
goal_identified
=== ep: 1032, time 99.74561357498169, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
=== ep: 1033, time 100.726238489151, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
goal_identified
=== ep: 1034, time 100.61773824691772, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
goal_identified
=== ep: 1035, time 102.56360793113708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
=== ep: 1036, time 101.64490079879761, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
goal_identified
=== ep: 1037, time 100.66153764724731, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
=== ep: 1038, time 96.84640574455261, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
goal_identified
=== ep: 1039, time 95.56694793701172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
=== ep: 1040, time 96.41367816925049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
goal_identified
=== ep: 1041, time 91.14217591285706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
=== ep: 1042, time 97.29343938827515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
=== ep: 1043, time 96.02394533157349, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
goal_identified
goal_identified
=== ep: 1044, time 91.2089159488678, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
=== ep: 1045, time 99.27337884902954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
=== ep: 1046, time 99.9318151473999, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
goal_identified
goal_identified
=== ep: 1047, time 100.63300681114197, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
goal_identified
=== ep: 1048, time 103.49809718132019, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
goal_identified
goal_identified
=== ep: 1049, time 97.90600275993347, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
goal_identified
=== ep: 1050, time 96.65090727806091, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
=== ep: 1051, time 93.25905203819275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
goal_identified
goal_identified
=== ep: 1052, time 100.23623085021973, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
goal_identified
=== ep: 1053, time 94.71499633789062, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
goal_identified
=== ep: 1054, time 95.96371412277222, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
=== ep: 1055, time 101.24984765052795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
=== ep: 1056, time 104.69442057609558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
=== ep: 1057, time 95.69903874397278, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
goal_identified
goal_identified
=== ep: 1058, time 89.4801573753357, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
goal_identified
=== ep: 1059, time 97.71641087532043, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
goal_identified
=== ep: 1060, time 83.77104353904724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
=== ep: 1061, time 91.51510810852051, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
goal_identified
=== ep: 1062, time 97.6487169265747, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
=== ep: 1063, time 101.62287282943726, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
goal_identified
=== ep: 1064, time 96.48964834213257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
goal_identified
=== ep: 1065, time 100.12110209465027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
goal_identified
=== ep: 1066, time 98.90371012687683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
goal_identified
=== ep: 1067, time 94.26691389083862, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
goal_identified
goal_identified
=== ep: 1068, time 98.60359764099121, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
=== ep: 1069, time 93.49384832382202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
=== ep: 1070, time 95.75037431716919, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
=== ep: 1071, time 97.02326369285583, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
=== ep: 1072, time 104.37215781211853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1072
goal_identified
goal_identified
=== ep: 1073, time 98.68947744369507, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
=== ep: 1074, time 104.79755592346191, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
goal_identified
goal_identified
=== ep: 1075, time 102.9026825428009, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
=== ep: 1076, time 102.59008598327637, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
goal_identified
=== ep: 1077, time 106.2924919128418, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
=== ep: 1078, time 104.35540914535522, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
goal_identified
goal_identified
=== ep: 1079, time 106.25529289245605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
goal_identified
goal_identified
goal_identified
=== ep: 1080, time 97.50249218940735, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
goal_identified
goal_identified
=== ep: 1081, time 105.19457650184631, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
goal_identified
=== ep: 1082, time 102.60050654411316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
=== ep: 1083, time 97.95945286750793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
=== ep: 1084, time 98.95074319839478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
=== ep: 1085, time 97.4330506324768, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
goal_identified
goal_identified
=== ep: 1086, time 97.06598448753357, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
goal_identified
=== ep: 1087, time 97.67006087303162, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
goal_identified
=== ep: 1088, time 97.80027031898499, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
goal_identified
=== ep: 1089, time 89.64394092559814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
goal_identified
goal_identified
=== ep: 1090, time 92.32707405090332, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
goal_identified
goal_identified
=== ep: 1091, time 93.43078064918518, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
goal_identified
goal_identified
=== ep: 1092, time 97.53693509101868, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
=== ep: 1093, time 98.84871220588684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
=== ep: 1094, time 100.42757225036621, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
goal_identified
goal_identified
=== ep: 1095, time 100.37256979942322, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
=== ep: 1096, time 98.78052806854248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
goal_identified
=== ep: 1097, time 94.49382591247559, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
goal_identified
=== ep: 1098, time 91.28696727752686, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
=== ep: 1099, time 97.12473368644714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
goal_identified
goal_identified
=== ep: 1100, time 94.8768196105957, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
goal_identified
=== ep: 1101, time 95.91258263587952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
goal_identified
goal_identified
=== ep: 1102, time 101.07198238372803, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
goal_identified
goal_identified
=== ep: 1103, time 99.54190039634705, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
goal_identified
=== ep: 1104, time 100.94423460960388, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
goal_identified
=== ep: 1105, time 101.96607089042664, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
goal_identified
goal_identified
goal_identified
=== ep: 1106, time 102.06767535209656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
goal_identified
goal_identified
=== ep: 1107, time 95.7844078540802, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
goal_identified
goal_identified
=== ep: 1108, time 102.7462432384491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
goal_identified
=== ep: 1109, time 97.64254355430603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
=== ep: 1110, time 102.90626502037048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
=== ep: 1111, time 102.41268277168274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
goal_identified
goal_identified
goal_identified
=== ep: 1112, time 99.98809361457825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
goal_identified
=== ep: 1113, time 94.32683610916138, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
goal_identified
=== ep: 1114, time 97.68013978004456, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
goal_identified
goal_identified
=== ep: 1115, time 102.87712740898132, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
goal_identified
=== ep: 1116, time 102.78137373924255, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
goal_identified
=== ep: 1117, time 102.33161759376526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
goal_identified
=== ep: 1118, time 99.51010394096375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
goal_identified
=== ep: 1119, time 103.04137182235718, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
=== ep: 1120, time 103.39786982536316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
goal_identified
goal_identified
=== ep: 1121, time 98.26924562454224, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
=== ep: 1122, time 97.3580379486084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 102.40393877029419, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
goal_identified
goal_identified
goal_identified
=== ep: 1124, time 101.4681339263916, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
=== ep: 1125, time 100.32375812530518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
goal_identified
goal_identified
goal_identified
=== ep: 1126, time 95.56071996688843, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
goal_identified
=== ep: 1127, time 105.24274373054504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
goal_identified
goal_identified
=== ep: 1128, time 101.15571618080139, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
goal_identified
=== ep: 1129, time 97.18976831436157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
=== ep: 1130, time 106.80928635597229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
goal_identified
goal_identified
=== ep: 1131, time 101.34869146347046, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
=== ep: 1132, time 94.70073223114014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
=== ep: 1133, time 102.76831412315369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
goal_identified
goal_identified
=== ep: 1134, time 107.5389723777771, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
=== ep: 1135, time 103.02379155158997, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
goal_identified
=== ep: 1136, time 101.61884140968323, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
goal_identified
=== ep: 1137, time 104.34172177314758, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
goal_identified
goal_identified
=== ep: 1138, time 98.88022375106812, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
goal_identified
goal_identified
goal_identified
=== ep: 1139, time 107.76816058158875, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
=== ep: 1140, time 96.52671432495117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
goal_identified
goal_identified
=== ep: 1141, time 103.82694840431213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
=== ep: 1142, time 105.35884094238281, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
=== ep: 1143, time 104.76903223991394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
goal_identified
goal_identified
=== ep: 1144, time 106.58696961402893, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
goal_identified
=== ep: 1145, time 104.32495450973511, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
goal_identified
=== ep: 1146, time 96.9669041633606, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
goal_identified
=== ep: 1147, time 100.7762062549591, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
goal_identified
=== ep: 1148, time 91.96897888183594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
goal_identified
goal_identified
=== ep: 1149, time 101.68512749671936, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
=== ep: 1150, time 97.62203907966614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
goal_identified
goal_identified
=== ep: 1151, time 98.9337911605835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1152, time 102.6180169582367, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
=== ep: 1153, time 100.2497444152832, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
goal_identified
=== ep: 1154, time 87.85447406768799, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
goal_identified
goal_identified
=== ep: 1155, time 98.78186655044556, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
goal_identified
=== ep: 1156, time 97.3479266166687, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
=== ep: 1157, time 101.09222841262817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
=== ep: 1158, time 101.44361686706543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
=== ep: 1159, time 96.59243726730347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1160, time 87.65742373466492, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
=== ep: 1161, time 88.55756068229675, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
=== ep: 1162, time 102.58231544494629, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
goal_identified
=== ep: 1163, time 102.5560851097107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
goal_identified
=== ep: 1164, time 95.5889253616333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
goal_identified
=== ep: 1165, time 97.00381255149841, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
goal_identified
=== ep: 1166, time 97.59765100479126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
goal_identified
=== ep: 1167, time 84.49022698402405, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
goal_identified
=== ep: 1168, time 100.26840162277222, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
goal_identified
goal_identified
=== ep: 1169, time 94.93600225448608, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
goal_identified
goal_identified
=== ep: 1170, time 98.30371069908142, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
=== ep: 1171, time 92.08486104011536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
goal_identified
=== ep: 1172, time 94.62127828598022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
goal_identified
goal_identified
goal_identified
=== ep: 1173, time 98.31384897232056, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
=== ep: 1174, time 99.494952917099, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
goal_identified
=== ep: 1175, time 98.7323842048645, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
=== ep: 1176, time 96.7387466430664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
goal_identified
goal_identified
goal_identified
=== ep: 1177, time 87.60314679145813, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
goal_identified
goal_identified
=== ep: 1178, time 97.87947964668274, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
goal_identified
=== ep: 1179, time 96.23410058021545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
=== ep: 1180, time 102.87941002845764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
=== ep: 1181, time 102.86829328536987, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
goal_identified
goal_identified
=== ep: 1182, time 98.52137565612793, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1183, time 92.24381995201111, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
goal_identified
=== ep: 1184, time 103.02180600166321, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
goal_identified
goal_identified
=== ep: 1185, time 102.68816184997559, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
=== ep: 1186, time 98.91522264480591, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
=== ep: 1187, time 101.22788095474243, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
=== ep: 1188, time 95.51139211654663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
goal_identified
goal_identified
=== ep: 1189, time 105.65076875686646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
goal_identified
goal_identified
=== ep: 1190, time 105.36092853546143, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
=== ep: 1191, time 103.07821893692017, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
=== ep: 1192, time 109.26885509490967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
goal_identified
goal_identified
goal_identified
=== ep: 1193, time 101.33732986450195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
=== ep: 1194, time 107.64330339431763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 108.10945677757263, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
=== ep: 1196, time 91.86069965362549, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
goal_identified
=== ep: 1197, time 101.89384007453918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
goal_identified
goal_identified
goal_identified
=== ep: 1198, time 95.24912071228027, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
goal_identified
=== ep: 1199, time 98.41756129264832, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
=== ep: 1200, time 100.08032774925232, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1201, time 100.05910205841064, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
=== ep: 1202, time 102.64759063720703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
goal_identified
goal_identified
=== ep: 1203, time 94.31437921524048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
goal_identified
=== ep: 1204, time 106.08267879486084, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
goal_identified
=== ep: 1205, time 99.00782203674316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
goal_identified
=== ep: 1206, time 94.45169687271118, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
=== ep: 1207, time 105.80809116363525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
=== ep: 1208, time 101.70897603034973, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
=== ep: 1209, time 106.9028799533844, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
=== ep: 1210, time 98.55520105361938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
goal_identified
=== ep: 1211, time 111.10993957519531, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
goal_identified
=== ep: 1212, time 105.51568627357483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
goal_identified
goal_identified
=== ep: 1213, time 99.9844400882721, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
goal_identified
goal_identified
goal_identified
=== ep: 1214, time 107.60147905349731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
goal_identified
goal_identified
=== ep: 1215, time 100.38200116157532, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
=== ep: 1216, time 102.70437383651733, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
goal_identified
=== ep: 1217, time 102.07643008232117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
goal_identified
=== ep: 1218, time 103.40048098564148, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
goal_identified
=== ep: 1219, time 93.94428133964539, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
goal_identified
=== ep: 1220, time 102.08433556556702, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
goal_identified
goal_identified
goal_identified
=== ep: 1221, time 99.96218705177307, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
goal_identified
=== ep: 1222, time 99.93546390533447, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
goal_identified
goal_identified
=== ep: 1223, time 96.22566556930542, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 102.3271119594574, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
=== ep: 1225, time 106.19270038604736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
goal_identified
goal_identified
=== ep: 1226, time 97.70243883132935, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
goal_identified
=== ep: 1227, time 107.96071529388428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
goal_identified
goal_identified
=== ep: 1228, time 110.36663937568665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
goal_identified
goal_identified
goal_identified
=== ep: 1229, time 99.1174840927124, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
goal_identified
=== ep: 1230, time 106.05364084243774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
=== ep: 1231, time 112.72442865371704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 110.56969857215881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
=== ep: 1233, time 107.20920896530151, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
goal_identified
=== ep: 1234, time 107.74379539489746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
goal_identified
goal_identified
=== ep: 1235, time 109.0108962059021, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1236, time 102.94973731040955, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
goal_identified
=== ep: 1237, time 108.93365931510925, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
goal_identified
=== ep: 1238, time 106.78015518188477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
goal_identified
=== ep: 1239, time 100.64322233200073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
goal_identified
=== ep: 1240, time 104.3884744644165, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
goal_identified
=== ep: 1241, time 102.38393449783325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
goal_identified
=== ep: 1242, time 106.13979411125183, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
goal_identified
goal_identified
=== ep: 1243, time 98.27834606170654, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
goal_identified
goal_identified
=== ep: 1244, time 102.3531289100647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
=== ep: 1245, time 109.51256442070007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
goal_identified
goal_identified
=== ep: 1246, time 101.0716187953949, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
=== ep: 1247, time 106.85225248336792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
goal_identified
=== ep: 1248, time 102.58002710342407, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
=== ep: 1249, time 101.97446823120117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
goal_identified
goal_identified
goal_identified
=== ep: 1250, time 106.02782773971558, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
goal_identified
goal_identified
=== ep: 1251, time 91.93433952331543, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
=== ep: 1252, time 102.53442430496216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
=== ep: 1253, time 107.8047034740448, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
=== ep: 1254, time 100.38495326042175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
goal_identified
goal_identified
goal_identified
=== ep: 1255, time 104.35762071609497, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
=== ep: 1256, time 98.49625062942505, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
goal_identified
=== ep: 1257, time 102.38279914855957, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
=== ep: 1258, time 100.79000806808472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
goal_identified
=== ep: 1259, time 102.81258177757263, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
goal_identified
goal_identified
=== ep: 1260, time 107.21710252761841, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
=== ep: 1261, time 102.45131492614746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
=== ep: 1262, time 104.87240934371948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
=== ep: 1263, time 100.32687306404114, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
goal_identified
=== ep: 1264, time 100.01322603225708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
=== ep: 1265, time 105.34043598175049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
goal_identified
goal_identified
=== ep: 1266, time 106.68278694152832, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
goal_identified
goal_identified
=== ep: 1267, time 100.03400373458862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
goal_identified
goal_identified
=== ep: 1268, time 105.5792760848999, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1269, time 107.66405963897705, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
=== ep: 1270, time 104.20598769187927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
goal_identified
goal_identified
=== ep: 1271, time 106.62704205513, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
=== ep: 1272, time 110.96040534973145, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
goal_identified
goal_identified
=== ep: 1273, time 105.7421498298645, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 109.2920708656311, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
goal_identified
=== ep: 1275, time 108.96880221366882, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
=== ep: 1276, time 100.70810890197754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
=== ep: 1277, time 107.67056441307068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
=== ep: 1278, time 109.06956148147583, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
goal_identified
=== ep: 1279, time 112.00225234031677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
=== ep: 1280, time 110.37184977531433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
=== ep: 1281, time 109.9307279586792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
=== ep: 1282, time 114.43179607391357, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
goal_identified
=== ep: 1283, time 114.13737654685974, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
=== ep: 1284, time 103.72338461875916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
=== ep: 1285, time 109.97204542160034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
goal_identified
goal_identified
=== ep: 1286, time 113.1231837272644, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
=== ep: 1287, time 120.76033878326416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
goal_identified
goal_identified
goal_identified
=== ep: 1288, time 110.97479510307312, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
goal_identified
=== ep: 1289, time 108.82737064361572, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1290, time 115.10567855834961, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
=== ep: 1291, time 113.07354474067688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
goal_identified
goal_identified
goal_identified
=== ep: 1292, time 106.6353747844696, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
=== ep: 1293, time 107.65749049186707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1293
goal_identified
=== ep: 1294, time 110.29145431518555, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
goal_identified
=== ep: 1295, time 105.93678164482117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
goal_identified
=== ep: 1296, time 100.89806008338928, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
=== ep: 1297, time 108.72626829147339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
=== ep: 1298, time 105.59622716903687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 111.6659460067749, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
goal_identified
=== ep: 1300, time 106.76104879379272, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
goal_identified
=== ep: 1301, time 100.87469673156738, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
goal_identified
=== ep: 1302, time 113.63241577148438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
goal_identified
=== ep: 1303, time 111.43585801124573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
goal_identified
=== ep: 1304, time 103.5395131111145, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
=== ep: 1305, time 119.03660655021667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
=== ep: 1306, time 114.46682858467102, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
=== ep: 1307, time 112.76773881912231, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
=== ep: 1308, time 107.19601774215698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
goal_identified
=== ep: 1309, time 114.72193217277527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
=== ep: 1310, time 106.0355896949768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
goal_identified
=== ep: 1311, time 107.11257100105286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
goal_identified
goal_identified
goal_identified
=== ep: 1312, time 112.12497186660767, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
=== ep: 1313, time 112.50261211395264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
=== ep: 1314, time 101.92354035377502, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1315, time 111.29327964782715, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
goal_identified
=== ep: 1316, time 118.63031840324402, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
=== ep: 1317, time 119.14686393737793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
=== ep: 1318, time 104.70846223831177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
goal_identified
goal_identified
goal_identified
=== ep: 1319, time 114.4141743183136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
goal_identified
=== ep: 1320, time 110.4494743347168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
=== ep: 1321, time 109.66007328033447, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
goal_identified
goal_identified
=== ep: 1322, time 106.63568210601807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
=== ep: 1323, time 106.60193967819214, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
goal_identified
=== ep: 1324, time 104.77502417564392, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
goal_identified
=== ep: 1325, time 109.1799008846283, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
goal_identified
=== ep: 1326, time 110.8557710647583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
=== ep: 1327, time 103.65818452835083, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
goal_identified
=== ep: 1328, time 111.40194416046143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
=== ep: 1329, time 116.73298287391663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
goal_identified
goal_identified
=== ep: 1330, time 118.66928124427795, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
goal_identified
=== ep: 1331, time 111.4947280883789, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
goal_identified
goal_identified
=== ep: 1332, time 110.48297572135925, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
=== ep: 1333, time 120.69147062301636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
=== ep: 1334, time 115.01323580741882, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
=== ep: 1335, time 107.87925815582275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
goal_identified
goal_identified
=== ep: 1336, time 109.34920310974121, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
goal_identified
goal_identified
=== ep: 1337, time 114.78593635559082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
=== ep: 1338, time 112.41348528862, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
=== ep: 1339, time 103.44248342514038, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
=== ep: 1340, time 109.83538675308228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
=== ep: 1341, time 115.10015988349915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
goal_identified
=== ep: 1342, time 105.1295759677887, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
goal_identified
=== ep: 1343, time 107.23201513290405, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
goal_identified
goal_identified
goal_identified
=== ep: 1344, time 111.48715591430664, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
=== ep: 1345, time 107.17767095565796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
goal_identified
goal_identified
=== ep: 1346, time 110.40471935272217, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
=== ep: 1347, time 108.32658743858337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
goal_identified
goal_identified
=== ep: 1348, time 119.80043840408325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
goal_identified
=== ep: 1349, time 109.7447669506073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
goal_identified
=== ep: 1350, time 108.92098593711853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
goal_identified
=== ep: 1351, time 106.32768106460571, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
goal_identified
=== ep: 1352, time 117.46811008453369, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
=== ep: 1353, time 111.00728225708008, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
=== ep: 1354, time 114.48139905929565, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
goal_identified
=== ep: 1355, time 106.50158715248108, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
goal_identified
=== ep: 1356, time 103.30851674079895, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
goal_identified
=== ep: 1357, time 110.16129422187805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
=== ep: 1358, time 102.59553456306458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
=== ep: 1359, time 112.93647170066833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
goal_identified
=== ep: 1360, time 109.50468611717224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
=== ep: 1361, time 103.6849799156189, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
goal_identified
=== ep: 1362, time 103.83116960525513, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
=== ep: 1363, time 110.04334163665771, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
goal_identified
=== ep: 1364, time 108.35038638114929, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
goal_identified
goal_identified
=== ep: 1365, time 103.06651425361633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
=== ep: 1366, time 106.50935649871826, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
goal_identified
=== ep: 1367, time 102.8404746055603, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
goal_identified
=== ep: 1368, time 108.21811556816101, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
goal_identified
goal_identified
=== ep: 1369, time 109.23308324813843, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
goal_identified
=== ep: 1370, time 114.06781148910522, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
goal_identified
goal_identified
=== ep: 1371, time 103.97978901863098, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
goal_identified
goal_identified
=== ep: 1372, time 116.4315996170044, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
=== ep: 1373, time 114.77278161048889, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
goal_identified
=== ep: 1374, time 113.54384636878967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
=== ep: 1375, time 103.68860459327698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
=== ep: 1376, time 114.63740062713623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1376
=== ep: 1377, time 108.1184196472168, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
goal_identified
goal_identified
goal_identified
=== ep: 1378, time 110.18786263465881, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1379, time 109.3737359046936, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
=== ep: 1380, time 104.47568273544312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
=== ep: 1381, time 104.57394599914551, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
goal_identified
=== ep: 1382, time 109.64926195144653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
goal_identified
=== ep: 1383, time 110.02433967590332, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1384, time 108.88530135154724, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
goal_identified
=== ep: 1385, time 98.4944498538971, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
=== ep: 1386, time 115.83946371078491, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
=== ep: 1387, time 107.28936123847961, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
goal_identified
goal_identified
goal_identified
=== ep: 1388, time 107.55386781692505, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
goal_identified
goal_identified
goal_identified
=== ep: 1389, time 112.92508482933044, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
goal_identified
=== ep: 1390, time 115.06784343719482, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
=== ep: 1391, time 107.65552282333374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
goal_identified
goal_identified
=== ep: 1392, time 110.92223238945007, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
=== ep: 1393, time 114.50643181800842, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
goal_identified
=== ep: 1394, time 105.70806193351746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
goal_identified
=== ep: 1395, time 109.98264145851135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
goal_identified
=== ep: 1396, time 117.84012508392334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
goal_identified
goal_identified
goal_identified
=== ep: 1397, time 117.51713466644287, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
goal_identified
=== ep: 1398, time 104.18504166603088, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
goal_identified
=== ep: 1399, time 104.76354336738586, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
=== ep: 1400, time 115.2732458114624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
goal_identified
goal_identified
=== ep: 1401, time 103.84838724136353, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
goal_identified
goal_identified
=== ep: 1402, time 108.5046033859253, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
=== ep: 1403, time 109.73696875572205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
=== ep: 1404, time 101.3637523651123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
=== ep: 1405, time 110.56878209114075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
goal_identified
=== ep: 1406, time 110.1284568309784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
goal_identified
goal_identified
=== ep: 1407, time 103.41430425643921, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
goal_identified
=== ep: 1408, time 112.03135323524475, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
goal_identified
=== ep: 1409, time 109.54931211471558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
goal_identified
goal_identified
goal_identified
=== ep: 1410, time 107.74821901321411, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
goal_identified
goal_identified
=== ep: 1411, time 113.94478178024292, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
=== ep: 1412, time 115.79708957672119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
goal_identified
goal_identified
=== ep: 1413, time 121.97097730636597, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1414, time 115.55936694145203, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
goal_identified
goal_identified
goal_identified
=== ep: 1415, time 117.30918145179749, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
goal_identified
goal_identified
=== ep: 1416, time 107.3969554901123, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
goal_identified
=== ep: 1417, time 111.69238305091858, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
=== ep: 1418, time 113.55940198898315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
goal_identified
=== ep: 1419, time 104.27124619483948, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
goal_identified
goal_identified
=== ep: 1420, time 102.39549803733826, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 107.51725149154663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
goal_identified
=== ep: 1422, time 102.43364953994751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
=== ep: 1423, time 104.02738070487976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
goal_identified
goal_identified
goal_identified
=== ep: 1424, time 114.31778764724731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
goal_identified
goal_identified
=== ep: 1425, time 112.73494625091553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
goal_identified
=== ep: 1426, time 106.37201571464539, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
=== ep: 1427, time 102.06115674972534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
goal_identified
=== ep: 1428, time 108.307692527771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
goal_identified
=== ep: 1429, time 104.57748508453369, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
goal_identified
goal_identified
goal_identified
=== ep: 1430, time 111.62251472473145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
goal_identified
goal_identified
=== ep: 1431, time 105.36669898033142, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
goal_identified
=== ep: 1432, time 110.04560208320618, eps 0.001, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
goal_identified
goal_identified
=== ep: 1433, time 104.0820255279541, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
=== ep: 1434, time 114.15906810760498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
=== ep: 1435, time 115.51384091377258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
=== ep: 1436, time 115.43581438064575, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
=== ep: 1437, time 116.53618383407593, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
=== ep: 1438, time 115.99736905097961, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
goal_identified
=== ep: 1439, time 113.70087552070618, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
goal_identified
=== ep: 1440, time 116.09508919715881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
goal_identified
goal_identified
=== ep: 1441, time 114.17991328239441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
=== ep: 1442, time 113.95568418502808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
=== ep: 1443, time 105.59837245941162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
goal_identified
goal_identified
=== ep: 1444, time 103.76258707046509, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
=== ep: 1445, time 108.3727855682373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
goal_identified
=== ep: 1446, time 110.15605902671814, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
=== ep: 1447, time 113.0179169178009, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
goal_identified
goal_identified
=== ep: 1448, time 104.58756256103516, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
=== ep: 1449, time 109.53905916213989, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
goal_identified
=== ep: 1450, time 105.03444266319275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
=== ep: 1451, time 104.96839356422424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
=== ep: 1452, time 111.61121773719788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
goal_identified
goal_identified
goal_identified
=== ep: 1453, time 109.42670798301697, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
=== ep: 1454, time 99.70475316047668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1454
=== ep: 1455, time 112.83394694328308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
=== ep: 1456, time 116.21280431747437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
=== ep: 1457, time 118.47623658180237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
goal_identified
goal_identified
=== ep: 1458, time 108.39378619194031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
goal_identified
goal_identified
=== ep: 1459, time 111.36387348175049, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
goal_identified
=== ep: 1460, time 112.11987853050232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
=== ep: 1461, time 115.87709927558899, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
=== ep: 1462, time 105.60235834121704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
=== ep: 1463, time 111.33723378181458, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
goal_identified
goal_identified
=== ep: 1464, time 116.57982611656189, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
=== ep: 1465, time 117.18706512451172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
=== ep: 1466, time 121.23032760620117, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
goal_identified
goal_identified
=== ep: 1467, time 113.08478927612305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
goal_identified
=== ep: 1468, time 110.99810481071472, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
goal_identified
=== ep: 1469, time 110.87456917762756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
=== ep: 1470, time 111.67948341369629, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
goal_identified
goal_identified
=== ep: 1471, time 108.51394629478455, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
goal_identified
=== ep: 1472, time 106.7599663734436, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
goal_identified
goal_identified
goal_identified
=== ep: 1473, time 108.79454827308655, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
goal_identified
goal_identified
=== ep: 1474, time 110.99360632896423, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
goal_identified
goal_identified
=== ep: 1475, time 111.91130638122559, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
=== ep: 1476, time 106.97595071792603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
goal_identified
goal_identified
=== ep: 1477, time 109.36533784866333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
goal_identified
goal_identified
=== ep: 1478, time 109.0639021396637, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
goal_identified
goal_identified
=== ep: 1479, time 111.86821579933167, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
=== ep: 1480, time 112.21261715888977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
=== ep: 1481, time 118.34132432937622, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
goal_identified
goal_identified
=== ep: 1482, time 114.0521719455719, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
goal_identified
goal_identified
=== ep: 1483, time 114.07238411903381, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
goal_identified
=== ep: 1484, time 113.40951418876648, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
goal_identified
goal_identified
=== ep: 1485, time 113.98810362815857, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
=== ep: 1486, time 112.5196521282196, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
goal_identified
=== ep: 1487, time 103.83383131027222, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
=== ep: 1488, time 114.56808137893677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
goal_identified
=== ep: 1489, time 108.16932225227356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
=== ep: 1490, time 114.25363063812256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
goal_identified
goal_identified
=== ep: 1491, time 110.85265231132507, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
=== ep: 1492, time 112.61461639404297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
=== ep: 1493, time 116.24665403366089, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
goal_identified
=== ep: 1494, time 106.76903820037842, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
goal_identified
=== ep: 1495, time 110.11933493614197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
=== ep: 1496, time 106.89440035820007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
goal_identified
=== ep: 1497, time 109.47917795181274, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
goal_identified
=== ep: 1498, time 106.68983292579651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
=== ep: 1499, time 116.97950911521912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
=== ep: 1500, time 108.38525104522705, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
goal_identified
goal_identified
goal_identified
=== ep: 1501, time 114.52008605003357, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
goal_identified
=== ep: 1502, time 109.68714141845703, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
goal_identified
=== ep: 1503, time 115.24693489074707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
goal_identified
=== ep: 1504, time 107.7801878452301, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
goal_identified
=== ep: 1505, time 112.18476915359497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
goal_identified
goal_identified
goal_identified
=== ep: 1506, time 108.81396460533142, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
goal_identified
=== ep: 1507, time 105.393639087677, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
=== ep: 1508, time 115.49968981742859, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
goal_identified
=== ep: 1509, time 114.3111515045166, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
goal_identified
goal_identified
goal_identified
=== ep: 1510, time 120.94786500930786, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
goal_identified
goal_identified
=== ep: 1511, time 116.00391411781311, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1512, time 117.98328995704651, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
goal_identified
goal_identified
goal_identified
=== ep: 1513, time 114.60250115394592, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
goal_identified
=== ep: 1514, time 114.79496312141418, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
=== ep: 1515, time 112.82120299339294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
=== ep: 1516, time 115.27161121368408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1516
goal_identified
=== ep: 1517, time 107.07256293296814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
goal_identified
goal_identified
=== ep: 1518, time 108.57007050514221, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
=== ep: 1519, time 108.30005836486816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
=== ep: 1520, time 108.46575331687927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
=== ep: 1521, time 108.6310350894928, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
goal_identified
goal_identified
=== ep: 1522, time 115.75054430961609, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
goal_identified
goal_identified
=== ep: 1523, time 110.14510798454285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
goal_identified
=== ep: 1524, time 117.27682948112488, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
=== ep: 1525, time 116.61345505714417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
=== ep: 1526, time 118.22621154785156, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
goal_identified
=== ep: 1527, time 110.03143239021301, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
=== ep: 1528, time 115.73403477668762, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
=== ep: 1529, time 107.73725700378418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
=== ep: 1530, time 115.03118801116943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
goal_identified
=== ep: 1531, time 107.47478008270264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
goal_identified
=== ep: 1532, time 109.58863854408264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
=== ep: 1533, time 113.12279605865479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
goal_identified
=== ep: 1534, time 117.19770216941833, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
=== ep: 1535, time 115.30152750015259, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
=== ep: 1536, time 106.93649053573608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
goal_identified
goal_identified
goal_identified
=== ep: 1537, time 111.31165385246277, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
=== ep: 1538, time 110.8445656299591, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
goal_identified
=== ep: 1539, time 106.45436096191406, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
goal_identified
=== ep: 1540, time 113.14805316925049, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1541, time 113.16691493988037, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
goal_identified
=== ep: 1542, time 120.91011500358582, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
goal_identified
=== ep: 1543, time 123.70737409591675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
goal_identified
=== ep: 1544, time 122.9509162902832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 111.00148224830627, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
=== ep: 1546, time 107.24331021308899, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
goal_identified
=== ep: 1547, time 120.74129009246826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
=== ep: 1548, time 110.45485830307007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
goal_identified
=== ep: 1549, time 114.07749843597412, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
=== ep: 1550, time 108.2075846195221, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
goal_identified
goal_identified
=== ep: 1551, time 111.81810355186462, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
=== ep: 1552, time 112.4120352268219, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
goal_identified
goal_identified
=== ep: 1553, time 113.55983400344849, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
goal_identified
goal_identified
goal_identified
=== ep: 1554, time 107.22253060340881, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
goal_identified
=== ep: 1555, time 121.3648087978363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
goal_identified
goal_identified
=== ep: 1556, time 120.81757044792175, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
goal_identified
goal_identified
=== ep: 1557, time 122.87859725952148, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
=== ep: 1558, time 115.87660002708435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
=== ep: 1559, time 110.52569961547852, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
goal_identified
goal_identified
=== ep: 1560, time 113.00607180595398, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
=== ep: 1561, time 115.1550521850586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
goal_identified
=== ep: 1562, time 109.10382080078125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
goal_identified
=== ep: 1563, time 115.723304271698, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
goal_identified
=== ep: 1564, time 111.29605150222778, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
goal_identified
goal_identified
=== ep: 1565, time 109.30454468727112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
goal_identified
=== ep: 1566, time 115.85206723213196, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
=== ep: 1567, time 113.42982840538025, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
goal_identified
=== ep: 1568, time 112.91684246063232, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
goal_identified
=== ep: 1569, time 124.76692342758179, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
goal_identified
=== ep: 1570, time 118.62777304649353, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
goal_identified
=== ep: 1571, time 122.80832099914551, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
goal_identified
=== ep: 1572, time 113.62730574607849, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
goal_identified
=== ep: 1573, time 113.85641193389893, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
goal_identified
goal_identified
=== ep: 1574, time 111.26547884941101, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
=== ep: 1575, time 111.2395589351654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
goal_identified
goal_identified
=== ep: 1576, time 111.17555928230286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
goal_identified
=== ep: 1577, time 110.8002679347992, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
=== ep: 1578, time 112.63359904289246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
=== ep: 1579, time 123.4656891822815, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
=== ep: 1580, time 120.10752511024475, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
=== ep: 1581, time 121.44852781295776, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
goal_identified
goal_identified
=== ep: 1582, time 110.48152112960815, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
=== ep: 1583, time 115.01712536811829, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
=== ep: 1584, time 112.77828884124756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
goal_identified
goal_identified
=== ep: 1585, time 121.06233716011047, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
goal_identified
goal_identified
goal_identified
=== ep: 1586, time 120.15428686141968, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
goal_identified
=== ep: 1587, time 120.79247379302979, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
=== ep: 1588, time 112.08885264396667, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
goal_identified
=== ep: 1589, time 112.72110986709595, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
goal_identified
=== ep: 1590, time 113.95674061775208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
goal_identified
goal_identified
=== ep: 1591, time 119.65736794471741, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
=== ep: 1592, time 111.30577063560486, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
goal_identified
=== ep: 1593, time 110.69725465774536, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
goal_identified
=== ep: 1594, time 113.14826083183289, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
goal_identified
=== ep: 1595, time 118.88259291648865, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
goal_identified
goal_identified
=== ep: 1596, time 116.52784824371338, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
goal_identified
=== ep: 1597, time 109.98648405075073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
=== ep: 1598, time 115.84367275238037, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
goal_identified
=== ep: 1599, time 120.03191065788269, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
goal_identified
goal_identified
goal_identified
=== ep: 1600, time 107.74296307563782, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1600
goal_identified
goal_identified
=== ep: 1601, time 112.22687435150146, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
goal_identified
=== ep: 1602, time 104.91743993759155, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
=== ep: 1603, time 116.55511140823364, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
goal_identified
goal_identified
=== ep: 1604, time 113.94095420837402, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
goal_identified
=== ep: 1605, time 113.88740062713623, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
=== ep: 1606, time 112.57064485549927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
=== ep: 1607, time 116.99474287033081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
goal_identified
goal_identified
goal_identified
=== ep: 1608, time 116.08115768432617, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
goal_identified
=== ep: 1609, time 119.3785080909729, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
=== ep: 1610, time 116.90457725524902, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1610
=== ep: 1611, time 114.03149485588074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
=== ep: 1612, time 109.24051117897034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
goal_identified
=== ep: 1613, time 117.05603170394897, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
goal_identified
=== ep: 1614, time 115.95193195343018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
goal_identified
goal_identified
goal_identified
=== ep: 1615, time 119.76576972007751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1615
=== ep: 1616, time 111.99831128120422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
goal_identified
=== ep: 1617, time 117.3999125957489, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
=== ep: 1618, time 119.32801342010498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
goal_identified
=== ep: 1619, time 108.4601936340332, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
=== ep: 1620, time 115.26994252204895, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
goal_identified
=== ep: 1621, time 111.74688291549683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
goal_identified
=== ep: 1622, time 106.51900124549866, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
=== ep: 1623, time 121.8913323879242, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
goal_identified
=== ep: 1624, time 122.85152745246887, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1625, time 123.9500834941864, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
goal_identified
=== ep: 1626, time 116.62304592132568, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
goal_identified
goal_identified
goal_identified
=== ep: 1627, time 115.58982968330383, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1627
=== ep: 1628, time 109.76968908309937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 121.19800209999084, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
=== ep: 1630, time 113.28725099563599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
=== ep: 1631, time 113.4447329044342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
goal_identified
=== ep: 1632, time 107.00363612174988, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
=== ep: 1633, time 107.40878391265869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
goal_identified
goal_identified
=== ep: 1634, time 112.83713054656982, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1634
goal_identified
goal_identified
=== ep: 1635, time 119.3792724609375, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 111.66808223724365, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
goal_identified
=== ep: 1637, time 116.72345805168152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
goal_identified
=== ep: 1638, time 114.25046372413635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
goal_identified
=== ep: 1639, time 121.64809465408325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1639
=== ep: 1640, time 117.41221570968628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
goal_identified
=== ep: 1641, time 125.45300793647766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1641
goal_identified
goal_identified
goal_identified
=== ep: 1642, time 109.18492984771729, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
=== ep: 1643, time 122.10107445716858, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
goal_identified
goal_identified
=== ep: 1644, time 117.62406086921692, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
=== ep: 1645, time 116.89128851890564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
=== ep: 1646, time 110.28889632225037, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
=== ep: 1647, time 111.52638697624207, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
goal_identified
=== ep: 1648, time 113.75264096260071, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 107.18803930282593, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
=== ep: 1650, time 103.44978594779968, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
=== ep: 1651, time 106.91513967514038, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 111.91210007667542, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
goal_identified
=== ep: 1653, time 118.90840005874634, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
goal_identified
=== ep: 1654, time 126.84601092338562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
=== ep: 1655, time 117.61985993385315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
=== ep: 1656, time 116.5787456035614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
goal_identified
goal_identified
=== ep: 1657, time 115.26897501945496, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
=== ep: 1658, time 114.99490213394165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
=== ep: 1659, time 120.5204074382782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
=== ep: 1660, time 114.9411928653717, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
=== ep: 1661, time 117.38202595710754, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
goal_identified
=== ep: 1662, time 118.08368158340454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
goal_identified
=== ep: 1663, time 108.06948184967041, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
goal_identified
=== ep: 1664, time 122.94894528388977, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
goal_identified
goal_identified
=== ep: 1665, time 119.38458323478699, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
goal_identified
=== ep: 1666, time 122.84504508972168, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1667, time 113.2306125164032, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
goal_identified
goal_identified
goal_identified
=== ep: 1668, time 114.75211548805237, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
goal_identified
goal_identified
=== ep: 1669, time 117.34294438362122, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
goal_identified
goal_identified
=== ep: 1670, time 114.35281372070312, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
=== ep: 1671, time 118.8130693435669, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
goal_identified
goal_identified
=== ep: 1672, time 113.32245302200317, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
goal_identified
=== ep: 1673, time 106.82332682609558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
=== ep: 1674, time 121.19470071792603, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
goal_identified
goal_identified
=== ep: 1675, time 113.61448240280151, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
goal_identified
=== ep: 1676, time 115.02499485015869, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
goal_identified
=== ep: 1677, time 116.84575939178467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
goal_identified
goal_identified
=== ep: 1678, time 120.44846796989441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
goal_identified
=== ep: 1679, time 124.91241502761841, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
goal_identified
goal_identified
goal_identified
=== ep: 1680, time 121.55035972595215, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
goal_identified
=== ep: 1681, time 120.64077877998352, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
=== ep: 1682, time 114.91401648521423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
goal_identified
goal_identified
=== ep: 1683, time 119.07045984268188, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
goal_identified
=== ep: 1684, time 115.81479501724243, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
=== ep: 1685, time 117.46376633644104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
goal_identified
=== ep: 1686, time 117.88446402549744, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
goal_identified
=== ep: 1687, time 119.2422742843628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
=== ep: 1688, time 114.83634734153748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
=== ep: 1689, time 122.62529134750366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
goal_identified
goal_identified
goal_identified
=== ep: 1690, time 113.57103967666626, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
=== ep: 1691, time 116.1947169303894, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
goal_identified
goal_identified
=== ep: 1692, time 113.25162839889526, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
goal_identified
goal_identified
=== ep: 1693, time 119.57377290725708, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
goal_identified
=== ep: 1694, time 114.48869323730469, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
goal_identified
goal_identified
=== ep: 1695, time 105.84816575050354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 111.96072030067444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
=== ep: 1697, time 117.9517195224762, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
goal_identified
goal_identified
=== ep: 1698, time 121.28168797492981, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
=== ep: 1699, time 112.07664251327515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 112.12581419944763, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
goal_identified
goal_identified
=== ep: 1701, time 121.849773645401, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
goal_identified
goal_identified
goal_identified
=== ep: 1702, time 122.93293309211731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
=== ep: 1703, time 111.15976643562317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
=== ep: 1704, time 111.46837091445923, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
=== ep: 1705, time 116.05012440681458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
goal_identified
goal_identified
goal_identified
=== ep: 1706, time 118.70797324180603, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
=== ep: 1707, time 125.2021415233612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
goal_identified
=== ep: 1708, time 111.67746210098267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
goal_identified
goal_identified
=== ep: 1709, time 118.44364762306213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
=== ep: 1710, time 115.90568494796753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
=== ep: 1711, time 109.41183638572693, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
=== ep: 1712, time 109.92201209068298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
=== ep: 1713, time 108.23811936378479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
goal_identified
=== ep: 1714, time 121.36037278175354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
=== ep: 1715, time 129.59799361228943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
=== ep: 1716, time 114.74352812767029, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
=== ep: 1717, time 112.59086537361145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
=== ep: 1718, time 116.26636338233948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
=== ep: 1719, time 109.23051691055298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
goal_identified
=== ep: 1720, time 115.28119945526123, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1721, time 108.389484167099, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
=== ep: 1722, time 117.07002472877502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
goal_identified
=== ep: 1723, time 124.91547131538391, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
goal_identified
goal_identified
=== ep: 1724, time 121.87794375419617, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
=== ep: 1725, time 112.39291477203369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
goal_identified
goal_identified
=== ep: 1726, time 114.93264746665955, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
goal_identified
=== ep: 1727, time 120.4731092453003, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
=== ep: 1728, time 123.3838517665863, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
=== ep: 1729, time 115.90533351898193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
goal_identified
goal_identified
=== ep: 1730, time 112.82861828804016, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
goal_identified
goal_identified
=== ep: 1731, time 121.49357271194458, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
goal_identified
=== ep: 1732, time 120.55694556236267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
goal_identified
=== ep: 1733, time 115.11716485023499, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
goal_identified
goal_identified
=== ep: 1734, time 115.51513051986694, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
goal_identified
goal_identified
=== ep: 1735, time 119.21579241752625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
=== ep: 1736, time 118.38233923912048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
goal_identified
=== ep: 1737, time 119.00584864616394, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
=== ep: 1738, time 117.46178913116455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
goal_identified
=== ep: 1739, time 112.23642611503601, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
=== ep: 1740, time 118.84908699989319, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
=== ep: 1741, time 115.90902471542358, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
=== ep: 1742, time 116.31182432174683, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
=== ep: 1743, time 109.8209240436554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
goal_identified
goal_identified
=== ep: 1744, time 119.25523662567139, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
goal_identified
=== ep: 1745, time 124.06089282035828, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
goal_identified
=== ep: 1746, time 124.78706765174866, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
goal_identified
goal_identified
=== ep: 1747, time 118.96268844604492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
goal_identified
=== ep: 1748, time 117.41620588302612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
goal_identified
goal_identified
=== ep: 1749, time 112.4550473690033, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
=== ep: 1750, time 114.15943264961243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
goal_identified
goal_identified
=== ep: 1751, time 112.26112365722656, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
goal_identified
goal_identified
=== ep: 1752, time 116.93945050239563, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
goal_identified
=== ep: 1753, time 116.5885021686554, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
goal_identified
goal_identified
=== ep: 1754, time 112.16618251800537, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
goal_identified
=== ep: 1755, time 117.87188029289246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
goal_identified
goal_identified
=== ep: 1756, time 113.56380820274353, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
=== ep: 1757, time 119.03243803977966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
goal_identified
goal_identified
goal_identified
=== ep: 1758, time 118.83819532394409, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
goal_identified
goal_identified
=== ep: 1759, time 102.24755048751831, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
goal_identified
goal_identified
=== ep: 1760, time 117.62933492660522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
goal_identified
=== ep: 1761, time 115.77643275260925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
goal_identified
goal_identified
=== ep: 1762, time 115.32597041130066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
goal_identified
=== ep: 1763, time 116.9672179222107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
=== ep: 1764, time 117.05861210823059, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
=== ep: 1765, time 113.29366731643677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
goal_identified
=== ep: 1766, time 122.00973224639893, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
goal_identified
=== ep: 1767, time 110.33143711090088, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
=== ep: 1768, time 114.85930728912354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
goal_identified
=== ep: 1769, time 120.77106857299805, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
goal_identified
=== ep: 1770, time 116.54185152053833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
goal_identified
goal_identified
=== ep: 1771, time 109.91517877578735, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
=== ep: 1772, time 116.77836656570435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
=== ep: 1773, time 129.1894030570984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
goal_identified
=== ep: 1774, time 119.27630186080933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
=== ep: 1775, time 118.15809202194214, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
goal_identified
=== ep: 1776, time 112.0488076210022, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
=== ep: 1777, time 114.95108938217163, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
goal_identified
goal_identified
goal_identified
=== ep: 1778, time 113.84443163871765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
goal_identified
goal_identified
=== ep: 1779, time 117.23558759689331, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
=== ep: 1780, time 112.78774785995483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
=== ep: 1781, time 118.24510526657104, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
=== ep: 1782, time 122.52247977256775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
goal_identified
goal_identified
=== ep: 1783, time 123.92941427230835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
=== ep: 1784, time 113.68479752540588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
goal_identified
goal_identified
=== ep: 1785, time 117.32699346542358, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
goal_identified
=== ep: 1786, time 118.77262759208679, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
=== ep: 1787, time 115.89539003372192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
goal_identified
goal_identified
=== ep: 1788, time 112.77377486228943, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
goal_identified
=== ep: 1789, time 108.76205515861511, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
goal_identified
=== ep: 1790, time 114.14646792411804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
=== ep: 1791, time 124.83036017417908, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
=== ep: 1792, time 125.54701447486877, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
goal_identified
goal_identified
goal_identified
=== ep: 1793, time 113.73257684707642, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
goal_identified
=== ep: 1794, time 113.71784591674805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
=== ep: 1795, time 115.29464149475098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
goal_identified
=== ep: 1796, time 121.41577744483948, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
goal_identified
=== ep: 1797, time 118.3378255367279, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
=== ep: 1798, time 113.42376708984375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
goal_identified
goal_identified
goal_identified
=== ep: 1799, time 125.25824666023254, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
goal_identified
goal_identified
=== ep: 1800, time 125.07466149330139, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 123.70661354064941, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 119.42980170249939, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
goal_identified
=== ep: 1803, time 114.2221245765686, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
goal_identified
=== ep: 1804, time 103.66265559196472, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
=== ep: 1805, time 125.42539358139038, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
goal_identified
=== ep: 1806, time 125.86562633514404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
goal_identified
goal_identified
goal_identified
=== ep: 1807, time 116.74964714050293, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
=== ep: 1808, time 112.57202100753784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 116.43339562416077, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
=== ep: 1810, time 120.04527449607849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
goal_identified
=== ep: 1811, time 119.74971795082092, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
goal_identified
=== ep: 1812, time 119.47060942649841, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
=== ep: 1813, time 115.07649636268616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
=== ep: 1814, time 122.02287268638611, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 117.54172015190125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
=== ep: 1816, time 111.7171573638916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
goal_identified
=== ep: 1817, time 113.26014971733093, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
=== ep: 1818, time 118.28322196006775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
goal_identified
=== ep: 1819, time 125.35517501831055, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
goal_identified
goal_identified
goal_identified
=== ep: 1820, time 126.88414907455444, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
goal_identified
=== ep: 1821, time 105.74429488182068, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
goal_identified
=== ep: 1822, time 112.9765145778656, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
=== ep: 1823, time 117.81137943267822, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
goal_identified
goal_identified
=== ep: 1824, time 116.27822089195251, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
goal_identified
=== ep: 1825, time 109.44953203201294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
goal_identified
=== ep: 1826, time 115.35441970825195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
goal_identified
=== ep: 1827, time 123.21815133094788, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
=== ep: 1828, time 121.05559778213501, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
goal_identified
=== ep: 1829, time 121.44573974609375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
goal_identified
=== ep: 1830, time 101.2699625492096, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
goal_identified
=== ep: 1831, time 110.03357195854187, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
goal_identified
=== ep: 1832, time 119.11509895324707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
goal_identified
goal_identified
goal_identified
=== ep: 1833, time 117.94387793540955, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
goal_identified
=== ep: 1834, time 117.9807620048523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
=== ep: 1835, time 123.96008491516113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
=== ep: 1836, time 115.64976954460144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
goal_identified
=== ep: 1837, time 116.37752223014832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
goal_identified
goal_identified
goal_identified
=== ep: 1838, time 117.773934841156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
=== ep: 1839, time 112.36964178085327, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
goal_identified
goal_identified
goal_identified
=== ep: 1840, time 112.1991970539093, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
=== ep: 1841, time 118.06174802780151, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
goal_identified
goal_identified
goal_identified
=== ep: 1842, time 121.98864889144897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
=== ep: 1843, time 124.79226303100586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
goal_identified
=== ep: 1844, time 117.01176452636719, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
=== ep: 1845, time 117.36857485771179, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
=== ep: 1846, time 109.15884208679199, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
goal_identified
goal_identified
=== ep: 1847, time 119.10471081733704, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1847
goal_identified
goal_identified
=== ep: 1848, time 116.48910474777222, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
goal_identified
goal_identified
=== ep: 1849, time 118.48208808898926, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1850, time 117.04036593437195, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
goal_identified
=== ep: 1851, time 119.15056300163269, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
goal_identified
=== ep: 1852, time 117.67999482154846, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
goal_identified
=== ep: 1853, time 128.11131024360657, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
=== ep: 1854, time 120.50085878372192, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
goal_identified
goal_identified
=== ep: 1855, time 123.16695094108582, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
=== ep: 1856, time 108.26666712760925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
goal_identified
goal_identified
=== ep: 1857, time 117.4241693019867, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
=== ep: 1858, time 117.75729322433472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
goal_identified
goal_identified
=== ep: 1859, time 125.98118209838867, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
=== ep: 1860, time 118.637624502182, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
goal_identified
=== ep: 1861, time 122.49378561973572, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1861
goal_identified
=== ep: 1862, time 116.0159547328949, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
goal_identified
=== ep: 1863, time 119.24287676811218, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
=== ep: 1864, time 109.53338503837585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
goal_identified
=== ep: 1865, time 115.75342273712158, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
=== ep: 1866, time 114.7636866569519, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
goal_identified
goal_identified
=== ep: 1867, time 115.14717674255371, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
goal_identified
goal_identified
goal_identified
=== ep: 1868, time 123.91526293754578, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
goal_identified
=== ep: 1869, time 117.11968398094177, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
goal_identified
goal_identified
goal_identified
=== ep: 1870, time 115.56224465370178, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
=== ep: 1871, time 116.22770929336548, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
=== ep: 1872, time 112.33369207382202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
goal_identified
goal_identified
=== ep: 1873, time 113.6836006641388, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
goal_identified
=== ep: 1874, time 112.90910744667053, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
=== ep: 1875, time 118.19906044006348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
goal_identified
=== ep: 1876, time 124.62270855903625, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
goal_identified
=== ep: 1877, time 126.90736770629883, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
goal_identified
goal_identified
=== ep: 1878, time 120.04418516159058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
=== ep: 1879, time 114.79049706459045, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
=== ep: 1880, time 116.43799662590027, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
goal_identified
goal_identified
=== ep: 1881, time 115.84365177154541, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
goal_identified
goal_identified
=== ep: 1882, time 112.94953155517578, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
=== ep: 1883, time 109.35863709449768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
=== ep: 1884, time 115.73015832901001, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
goal_identified
=== ep: 1885, time 118.10358881950378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
=== ep: 1886, time 126.78921008110046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
goal_identified
goal_identified
=== ep: 1887, time 114.33577084541321, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
=== ep: 1888, time 121.52004790306091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
=== ep: 1889, time 119.23994255065918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 122.48554730415344, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1891, time 115.01180291175842, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
goal_identified
goal_identified
=== ep: 1892, time 117.36349201202393, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
goal_identified
=== ep: 1893, time 118.92283344268799, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
goal_identified
=== ep: 1894, time 116.51238226890564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
=== ep: 1895, time 122.99841022491455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
goal_identified
=== ep: 1896, time 124.69234538078308, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
goal_identified
goal_identified
=== ep: 1897, time 122.93613147735596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
=== ep: 1898, time 126.51493644714355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
=== ep: 1899, time 118.87723541259766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1899
goal_identified
goal_identified
=== ep: 1900, time 120.03548336029053, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1900
goal_identified
goal_identified
=== ep: 1901, time 109.83672595024109, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1901
goal_identified
goal_identified
=== ep: 1902, time 117.87799334526062, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1902
goal_identified
=== ep: 1903, time 114.78231811523438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1903
goal_identified
=== ep: 1904, time 114.66943001747131, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1904
goal_identified
goal_identified
=== ep: 1905, time 117.78504610061646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1905
goal_identified
=== ep: 1906, time 117.6133484840393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1906
=== ep: 1907, time 128.52602362632751, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1907
goal_identified
goal_identified
=== ep: 1908, time 126.01308965682983, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1908
goal_identified
=== ep: 1909, time 119.45284104347229, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1909
goal_identified
goal_identified
=== ep: 1910, time 123.22194218635559, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1910
=== ep: 1911, time 114.793865442276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1911
goal_identified
goal_identified
=== ep: 1912, time 115.15581464767456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1912
=== ep: 1913, time 116.96189403533936, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1913
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1914, time 117.40455889701843, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
goal_identified
=== ep: 1915, time 116.72694396972656, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1915
goal_identified
goal_identified
=== ep: 1916, time 115.47051477432251, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1916
goal_identified
goal_identified
=== ep: 1917, time 119.51346158981323, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1917
=== ep: 1918, time 118.48742890357971, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1918
=== ep: 1919, time 115.56093072891235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1919
=== ep: 1920, time 113.36478233337402, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1920
=== ep: 1921, time 115.70852494239807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1921
goal_identified
=== ep: 1922, time 116.45380139350891, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1922
goal_identified
goal_identified
=== ep: 1923, time 114.00045585632324, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1923
=== ep: 1924, time 109.25503706932068, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1924
goal_identified
=== ep: 1925, time 115.18904733657837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1925
=== ep: 1926, time 117.94438767433167, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1926
goal_identified
goal_identified
=== ep: 1927, time 118.7789511680603, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1927
=== ep: 1928, time 115.33266043663025, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1928
goal_identified
=== ep: 1929, time 105.32842469215393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1929
=== ep: 1930, time 120.59332060813904, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1930
goal_identified
goal_identified
=== ep: 1931, time 110.63300132751465, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1931
=== ep: 1932, time 117.97843337059021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1932
goal_identified
=== ep: 1933, time 115.81308364868164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1933
goal_identified
=== ep: 1934, time 119.60511112213135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1934
goal_identified
=== ep: 1935, time 109.1753716468811, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1935
=== ep: 1936, time 122.32438492774963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1936
goal_identified
=== ep: 1937, time 114.96983408927917, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1937
goal_identified
goal_identified
=== ep: 1938, time 113.29230451583862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1938
goal_identified
goal_identified
=== ep: 1939, time 115.87693238258362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1939
goal_identified
goal_identified
=== ep: 1940, time 112.05781984329224, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1940
=== ep: 1941, time 110.69012784957886, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1941
=== ep: 1942, time 117.1111171245575, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1942
goal_identified
=== ep: 1943, time 114.1610255241394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1943
=== ep: 1944, time 117.90927290916443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1944
=== ep: 1945, time 120.48963284492493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1945
goal_identified
goal_identified
=== ep: 1946, time 108.60420250892639, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1946
=== ep: 1947, time 117.95628833770752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1947
goal_identified
goal_identified
goal_identified
=== ep: 1948, time 114.56346726417542, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1948
=== ep: 1949, time 126.83530139923096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1949
=== ep: 1950, time 110.52121186256409, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1950
goal_identified
goal_identified
=== ep: 1951, time 119.57236075401306, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1951
goal_identified
goal_identified
goal_identified
=== ep: 1952, time 117.19377493858337, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1952
goal_identified
=== ep: 1953, time 119.00646328926086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1953
goal_identified
=== ep: 1954, time 111.22613763809204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1954
goal_identified
goal_identified
goal_identified
=== ep: 1955, time 116.51576232910156, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1955
=== ep: 1956, time 118.81645464897156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1956
goal_identified
=== ep: 1957, time 122.70309448242188, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1957
goal_identified
goal_identified
=== ep: 1958, time 111.76330161094666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1958
goal_identified
goal_identified
goal_identified
=== ep: 1959, time 121.47810959815979, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1959
goal_identified
goal_identified
=== ep: 1960, time 119.79185366630554, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1960
=== ep: 1961, time 119.93643951416016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1961
goal_identified
goal_identified
goal_identified
=== ep: 1962, time 108.17808651924133, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1962
=== ep: 1963, time 113.93157362937927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1963
goal_identified
=== ep: 1964, time 119.25682950019836, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1964
=== ep: 1965, time 114.1434473991394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1965
goal_identified
=== ep: 1966, time 117.83274388313293, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1966
goal_identified
=== ep: 1967, time 123.07176947593689, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1967
=== ep: 1968, time 117.7975709438324, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1968
goal_identified
=== ep: 1969, time 121.07036137580872, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1969
goal_identified
=== ep: 1970, time 115.9398763179779, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1970
=== ep: 1971, time 120.28756713867188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1971
=== ep: 1972, time 114.05160140991211, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1972
=== ep: 1973, time 119.10614156723022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1973
goal_identified
=== ep: 1974, time 114.77990102767944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1974
goal_identified
goal_identified
=== ep: 1975, time 112.72789359092712, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1975
goal_identified
goal_identified
=== ep: 1976, time 110.37820720672607, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1976
goal_identified
goal_identified
=== ep: 1977, time 117.87253093719482, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1977
=== ep: 1978, time 112.52918863296509, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1978
=== ep: 1979, time 116.56246066093445, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1979
goal_identified
=== ep: 1980, time 118.0281970500946, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1980
=== ep: 1981, time 121.64301466941833, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1981
goal_identified
goal_identified
=== ep: 1982, time 116.210529088974, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1982
goal_identified
goal_identified
goal_identified
=== ep: 1983, time 115.63654637336731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1983
goal_identified
=== ep: 1984, time 117.15713906288147, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1984
goal_identified
goal_identified
=== ep: 1985, time 109.76251029968262, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1985
=== ep: 1986, time 112.76716566085815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1986
=== ep: 1987, time 117.64296293258667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1987
goal_identified
goal_identified
=== ep: 1988, time 119.55639433860779, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1988
=== ep: 1989, time 119.35219478607178, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1989
goal_identified
=== ep: 1990, time 114.30097436904907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1990
goal_identified
=== ep: 1991, time 122.57266235351562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1991
goal_identified
=== ep: 1992, time 122.28830933570862, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1992
=== ep: 1993, time 113.29740905761719, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1993
=== ep: 1994, time 108.2720558643341, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1994
goal_identified
goal_identified
=== ep: 1995, time 114.51881766319275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1995
goal_identified
=== ep: 1996, time 118.41061568260193, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1996
=== ep: 1997, time 109.23237323760986, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1997
goal_identified
=== ep: 1998, time 107.70488142967224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1998
goal_identified
goal_identified
=== ep: 1999, time 111.8624804019928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1999
goal_identified
=== ep: 2000, time 115.24248337745667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2000
goal_identified
=== ep: 2001, time 119.07552289962769, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2001
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2002, time 124.4962911605835, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
goal_identified
=== ep: 2003, time 113.75385332107544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2003
=== ep: 2004, time 113.15659809112549, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2004
goal_identified
goal_identified
=== ep: 2005, time 119.1876609325409, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2005
goal_identified
goal_identified
=== ep: 2006, time 121.7156400680542, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2006
goal_identified
=== ep: 2007, time 119.35394620895386, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2007
goal_identified
=== ep: 2008, time 112.33239817619324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2008
=== ep: 2009, time 116.68127846717834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2009
=== ep: 2010, time 114.60642671585083, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2010
=== ep: 2011, time 116.08716154098511, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2011
goal_identified
=== ep: 2012, time 107.66602301597595, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2012
goal_identified
=== ep: 2013, time 116.80149698257446, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2013
goal_identified
goal_identified
=== ep: 2014, time 105.34229016304016, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2014
goal_identified
goal_identified
=== ep: 2015, time 116.5325255393982, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2015
goal_identified
=== ep: 2016, time 126.34538435935974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2016
goal_identified
=== ep: 2017, time 118.13167119026184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2017
goal_identified
goal_identified
=== ep: 2018, time 113.66184902191162, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2018
=== ep: 2019, time 118.4861569404602, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2019
goal_identified
=== ep: 2020, time 115.68204689025879, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2020
goal_identified
goal_identified
=== ep: 2021, time 112.03252053260803, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2021
=== ep: 2022, time 113.68236041069031, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2022
goal_identified
=== ep: 2023, time 127.06670069694519, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2023
=== ep: 2024, time 121.87286901473999, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2024
=== ep: 2025, time 122.30968618392944, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2025
goal_identified
=== ep: 2026, time 109.64926862716675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2026
goal_identified
=== ep: 2027, time 121.6333110332489, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2027
=== ep: 2028, time 119.39964056015015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2028
goal_identified
goal_identified
=== ep: 2029, time 116.57516098022461, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2029
=== ep: 2030, time 114.32721638679504, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2030
goal_identified
=== ep: 2031, time 118.48953437805176, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2031
goal_identified
=== ep: 2032, time 118.24652004241943, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2032
goal_identified
goal_identified
=== ep: 2033, time 113.37101602554321, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2033
goal_identified
=== ep: 2034, time 114.9907054901123, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2034
goal_identified
=== ep: 2035, time 121.29818558692932, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2035
goal_identified
goal_identified
=== ep: 2036, time 120.40306401252747, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2036
goal_identified
=== ep: 2037, time 112.24365234375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2037
=== ep: 2038, time 119.7162561416626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2038
=== ep: 2039, time 117.03380727767944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2039
goal_identified
=== ep: 2040, time 118.81261610984802, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2040
goal_identified
goal_identified
=== ep: 2041, time 114.6868634223938, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2041
=== ep: 2042, time 110.58818244934082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2042
goal_identified
=== ep: 2043, time 111.13487601280212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2043
goal_identified
=== ep: 2044, time 111.71263122558594, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2044
=== ep: 2045, time 112.7344172000885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2045
goal_identified
=== ep: 2046, time 125.6753740310669, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2046
goal_identified
goal_identified
=== ep: 2047, time 120.58081388473511, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2047
=== ep: 2048, time 123.68820381164551, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2048
goal_identified
=== ep: 2049, time 102.44013357162476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2049
goal_identified
=== ep: 2050, time 120.0564455986023, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2050
=== ep: 2051, time 114.76158881187439, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2051
goal_identified
goal_identified
=== ep: 2052, time 112.28026127815247, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2052
goal_identified
goal_identified
=== ep: 2053, time 108.91187310218811, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2053
=== ep: 2054, time 128.29319620132446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2054
=== ep: 2055, time 124.10252666473389, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2055
=== ep: 2056, time 108.08556151390076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2056
goal_identified
=== ep: 2057, time 113.38796377182007, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2057
goal_identified
=== ep: 2058, time 112.122079372406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2058
goal_identified
goal_identified
=== ep: 2059, time 111.1348671913147, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2059
=== ep: 2060, time 115.42324662208557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2060
goal_identified
goal_identified
=== ep: 2061, time 122.69232201576233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2061
goal_identified
=== ep: 2062, time 114.51277279853821, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2062
goal_identified
=== ep: 2063, time 114.92769122123718, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2063
goal_identified
=== ep: 2064, time 106.76118922233582, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2064
=== ep: 2065, time 114.27319812774658, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2065
goal_identified
=== ep: 2066, time 114.14987087249756, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2066
goal_identified
=== ep: 2067, time 113.59899425506592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2067
goal_identified
=== ep: 2068, time 108.30228853225708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2068
=== ep: 2069, time 117.7112250328064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2069
goal_identified
goal_identified
=== ep: 2070, time 122.07160139083862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2070
goal_identified
=== ep: 2071, time 109.95615029335022, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2071
goal_identified
goal_identified
=== ep: 2072, time 117.61254525184631, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2072
goal_identified
=== ep: 2073, time 118.18912267684937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2073
=== ep: 2074, time 114.98900198936462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2074
=== ep: 2075, time 108.62167882919312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2075
goal_identified
goal_identified
goal_identified
=== ep: 2076, time 119.57677292823792, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2076
goal_identified
goal_identified
=== ep: 2077, time 117.83496761322021, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2077
=== ep: 2078, time 114.04752802848816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2078
goal_identified
goal_identified
goal_identified
=== ep: 2079, time 124.11909937858582, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2079
=== ep: 2080, time 111.46100211143494, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2080
goal_identified
=== ep: 2081, time 116.05183839797974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2081
goal_identified
=== ep: 2082, time 116.63440752029419, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2082
goal_identified
=== ep: 2083, time 120.80615997314453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2083
goal_identified
=== ep: 2084, time 117.57053756713867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2084
goal_identified
=== ep: 2085, time 118.17697334289551, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2085
=== ep: 2086, time 115.12689256668091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2086
goal_identified
=== ep: 2087, time 121.50814843177795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2087
goal_identified
=== ep: 2088, time 115.61833381652832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2088
=== ep: 2089, time 112.2415223121643, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2089
goal_identified
goal_identified
=== ep: 2090, time 116.24306774139404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2090
goal_identified
=== ep: 2091, time 120.58861255645752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2091
goal_identified
=== ep: 2092, time 120.43264102935791, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2092
=== ep: 2093, time 106.40225148200989, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2093
goal_identified
goal_identified
=== ep: 2094, time 115.64180278778076, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2094
goal_identified
=== ep: 2095, time 117.85782885551453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2095
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2096, time 117.4143795967102, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
=== ep: 2097, time 117.73349523544312, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2097
goal_identified
goal_identified
=== ep: 2098, time 120.62215256690979, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2098
=== ep: 2099, time 119.24577212333679, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2099
=== ep: 2100, time 110.19113492965698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2100
goal_identified
=== ep: 2101, time 113.36637425422668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2101
goal_identified
=== ep: 2102, time 119.60693407058716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2102
goal_identified
=== ep: 2103, time 117.72973585128784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2103
goal_identified
goal_identified
=== ep: 2104, time 118.56545877456665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2104
goal_identified
=== ep: 2105, time 119.11993265151978, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2105
goal_identified
goal_identified
=== ep: 2106, time 111.4975311756134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2106
=== ep: 2107, time 117.29476976394653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2107
=== ep: 2108, time 110.58982539176941, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2108
goal_identified
goal_identified
=== ep: 2109, time 112.91259241104126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2109
goal_identified
goal_identified
goal_identified
=== ep: 2110, time 117.1460874080658, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2110
goal_identified
=== ep: 2111, time 116.94805669784546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2111
goal_identified
=== ep: 2112, time 112.71632933616638, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2112
=== ep: 2113, time 125.76050877571106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2113
=== ep: 2114, time 117.04465699195862, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2114
goal_identified
=== ep: 2115, time 105.0896384716034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2115
goal_identified
goal_identified
=== ep: 2116, time 109.92141771316528, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2116
goal_identified
=== ep: 2117, time 111.79953932762146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2117
goal_identified
goal_identified
=== ep: 2118, time 123.35359573364258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2118
=== ep: 2119, time 123.61621880531311, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2119
goal_identified
goal_identified
=== ep: 2120, time 120.56846499443054, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2120
goal_identified
goal_identified
=== ep: 2121, time 113.0010757446289, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2121
=== ep: 2122, time 116.75927305221558, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2122
goal_identified
goal_identified
=== ep: 2123, time 115.52764439582825, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2123
goal_identified
goal_identified
goal_identified
=== ep: 2124, time 104.20078444480896, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2124
goal_identified
goal_identified
=== ep: 2125, time 121.42697525024414, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2125
=== ep: 2126, time 118.17817997932434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 110.66150069236755, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2127
goal_identified
=== ep: 2128, time 113.73789811134338, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2128
=== ep: 2129, time 110.42851758003235, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2129
=== ep: 2130, time 108.42514562606812, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2130
=== ep: 2131, time 124.8615255355835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2131
goal_identified
=== ep: 2132, time 110.4937424659729, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2132
goal_identified
=== ep: 2133, time 117.14901924133301, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2133
goal_identified
goal_identified
=== ep: 2134, time 123.22256541252136, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2134
goal_identified
=== ep: 2135, time 124.345698595047, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2135
=== ep: 2136, time 124.94270873069763, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2136
=== ep: 2137, time 99.55188035964966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2137
=== ep: 2138, time 112.53242206573486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2138
goal_identified
=== ep: 2139, time 115.65645456314087, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2139
=== ep: 2140, time 117.57735347747803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2140
goal_identified
goal_identified
=== ep: 2141, time 113.9533383846283, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2141
goal_identified
=== ep: 2142, time 126.67236876487732, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2142
goal_identified
=== ep: 2143, time 105.81658101081848, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2143
=== ep: 2144, time 119.08815097808838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2144
goal_identified
=== ep: 2145, time 117.98349475860596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2145
=== ep: 2146, time 117.69210910797119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2146
goal_identified
goal_identified
=== ep: 2147, time 116.8314197063446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2147
goal_identified
=== ep: 2148, time 120.44736814498901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2148
goal_identified
=== ep: 2149, time 111.85678887367249, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2149
goal_identified
goal_identified
goal_identified
=== ep: 2150, time 121.5514702796936, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2150
=== ep: 2151, time 115.31405782699585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2151
goal_identified
=== ep: 2152, time 111.31251764297485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2152
goal_identified
=== ep: 2153, time 116.98289012908936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2153
goal_identified
=== ep: 2154, time 119.22040796279907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2154
=== ep: 2155, time 107.26334142684937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2155
=== ep: 2156, time 118.46263003349304, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2156
goal_identified
=== ep: 2157, time 117.91197371482849, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2157
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2158, time 124.55112743377686, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
goal_identified
goal_identified
=== ep: 2159, time 114.40361499786377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2159
goal_identified
=== ep: 2160, time 116.70496773719788, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2160
=== ep: 2161, time 116.40492939949036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2161
goal_identified
goal_identified
goal_identified
=== ep: 2162, time 120.9939661026001, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2162
goal_identified
goal_identified
=== ep: 2163, time 117.32545614242554, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2163
=== ep: 2164, time 122.511798620224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2164
goal_identified
=== ep: 2165, time 109.34221863746643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2165
=== ep: 2166, time 115.06246614456177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2166
goal_identified
=== ep: 2167, time 117.34476566314697, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2167
goal_identified
=== ep: 2168, time 118.63664031028748, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2168
goal_identified
=== ep: 2169, time 122.09234952926636, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2169
=== ep: 2170, time 109.43262815475464, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2170
=== ep: 2171, time 122.31659531593323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2171
=== ep: 2172, time 117.16786408424377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2172
goal_identified
goal_identified
goal_identified
=== ep: 2173, time 105.2689778804779, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2173
=== ep: 2174, time 127.57829880714417, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2174
goal_identified
=== ep: 2175, time 119.10183072090149, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2175
goal_identified
=== ep: 2176, time 113.9331705570221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2176
goal_identified
goal_identified
goal_identified
=== ep: 2177, time 111.6519763469696, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2177
=== ep: 2178, time 116.7548565864563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2178
goal_identified
goal_identified
=== ep: 2179, time 115.46536421775818, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2179
goal_identified
goal_identified
goal_identified
=== ep: 2180, time 119.42400741577148, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2180
goal_identified
=== ep: 2181, time 112.90746760368347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2181
goal_identified
=== ep: 2182, time 125.12115812301636, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2182
goal_identified
=== ep: 2183, time 113.72295784950256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2183
goal_identified
=== ep: 2184, time 113.26818084716797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2184
goal_identified
=== ep: 2185, time 116.85740566253662, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2185
goal_identified
goal_identified
=== ep: 2186, time 125.74604320526123, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2186
=== ep: 2187, time 102.7148745059967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2187
=== ep: 2188, time 116.72979736328125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2188
goal_identified
=== ep: 2189, time 116.96172404289246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2189
goal_identified
goal_identified
=== ep: 2190, time 116.13241076469421, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2190
goal_identified
goal_identified
=== ep: 2191, time 123.72542762756348, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2191
goal_identified
=== ep: 2192, time 123.14823174476624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2192
=== ep: 2193, time 116.4598879814148, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2193
=== ep: 2194, time 115.15858745574951, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2194
goal_identified
goal_identified
=== ep: 2195, time 107.32240796089172, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2195
=== ep: 2196, time 123.55869174003601, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2196
goal_identified
goal_identified
goal_identified
=== ep: 2197, time 111.155930519104, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2197
goal_identified
=== ep: 2198, time 122.17764258384705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2198
=== ep: 2199, time 120.40643739700317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2199
goal_identified
=== ep: 2200, time 118.08905506134033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2200
goal_identified
goal_identified
=== ep: 2201, time 118.32072186470032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2201
goal_identified
=== ep: 2202, time 107.79159617424011, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2202
goal_identified
=== ep: 2203, time 118.98197889328003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2203
=== ep: 2204, time 116.13757157325745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2204
goal_identified
goal_identified
=== ep: 2205, time 118.9984838962555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2205
goal_identified
goal_identified
=== ep: 2206, time 113.83233690261841, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2206
=== ep: 2207, time 125.03572249412537, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2207
=== ep: 2208, time 112.98123168945312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2208
goal_identified
goal_identified
=== ep: 2209, time 108.4300594329834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2209
=== ep: 2210, time 119.97198843955994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2210
goal_identified
goal_identified
=== ep: 2211, time 122.67410635948181, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2211
goal_identified
goal_identified
goal_identified
=== ep: 2212, time 119.13402509689331, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2212
goal_identified
goal_identified
goal_identified
=== ep: 2213, time 110.49484443664551, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2213
goal_identified
=== ep: 2214, time 114.87512588500977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2214
=== ep: 2215, time 120.71388339996338, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2215
goal_identified
=== ep: 2216, time 115.62129998207092, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2216
goal_identified
=== ep: 2217, time 115.45349526405334, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2217
goal_identified
goal_identified
=== ep: 2218, time 124.92803597450256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2218
goal_identified
=== ep: 2219, time 105.70409369468689, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2219
=== ep: 2220, time 105.44943308830261, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2220
goal_identified
=== ep: 2221, time 120.263418674469, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2221
=== ep: 2222, time 123.46889781951904, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2222
goal_identified
goal_identified
=== ep: 2223, time 121.34447002410889, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2223
=== ep: 2224, time 119.54806900024414, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2224
=== ep: 2225, time 121.86265659332275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2225
goal_identified
goal_identified
=== ep: 2226, time 112.7720115184784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2226
=== ep: 2227, time 126.095205783844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2227
goal_identified
=== ep: 2228, time 121.04579496383667, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2228
=== ep: 2229, time 122.24277138710022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2229
goal_identified
goal_identified
goal_identified
=== ep: 2230, time 113.8958477973938, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2230
goal_identified
=== ep: 2231, time 112.82244515419006, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2231
=== ep: 2232, time 115.01792788505554, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2232
goal_identified
=== ep: 2233, time 116.92490363121033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2233
goal_identified
goal_identified
goal_identified
=== ep: 2234, time 118.36812424659729, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2234
goal_identified
goal_identified
=== ep: 2235, time 122.64192581176758, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2235
goal_identified
=== ep: 2236, time 102.15940761566162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2236
goal_identified
=== ep: 2237, time 118.72783088684082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2237
=== ep: 2238, time 118.68819403648376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2238
goal_identified
=== ep: 2239, time 124.12926769256592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2239
=== ep: 2240, time 114.10378193855286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2240
goal_identified
=== ep: 2241, time 116.5765151977539, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2241
goal_identified
=== ep: 2242, time 114.63997507095337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2242
goal_identified
goal_identified
goal_identified
=== ep: 2243, time 113.26112270355225, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2243
goal_identified
=== ep: 2244, time 123.30699014663696, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2244
goal_identified
=== ep: 2245, time 121.5744833946228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2245
goal_identified
=== ep: 2246, time 118.09885787963867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2246
goal_identified
goal_identified
=== ep: 2247, time 108.2274341583252, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2247
goal_identified
goal_identified
=== ep: 2248, time 119.80223560333252, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2248
=== ep: 2249, time 111.06744456291199, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2249
=== ep: 2250, time 120.2604911327362, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2250
goal_identified
=== ep: 2251, time 116.0035412311554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2251
=== ep: 2252, time 119.07418203353882, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2252
goal_identified
=== ep: 2253, time 106.41078281402588, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2253
=== ep: 2254, time 116.91360378265381, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2254
goal_identified
=== ep: 2255, time 122.25821542739868, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2255
goal_identified
goal_identified
=== ep: 2256, time 117.23434233665466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2256
=== ep: 2257, time 116.1028082370758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2258, time 118.79055905342102, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
=== ep: 2259, time 116.45054268836975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2259
goal_identified
=== ep: 2260, time 116.92516779899597, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2260
goal_identified
=== ep: 2261, time 107.60652494430542, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2261
goal_identified
goal_identified
=== ep: 2262, time 116.96542572975159, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2262
goal_identified
=== ep: 2263, time 112.94726037979126, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2263
=== ep: 2264, time 118.17224264144897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2264
=== ep: 2265, time 120.40263652801514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2265
goal_identified
=== ep: 2266, time 115.97521805763245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2266
goal_identified
=== ep: 2267, time 114.5443823337555, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2267
goal_identified
goal_identified
=== ep: 2268, time 125.7951238155365, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2268
goal_identified
=== ep: 2269, time 106.10238218307495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2269
goal_identified
=== ep: 2270, time 119.78462243080139, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2270
goal_identified
=== ep: 2271, time 116.46493911743164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2271
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2272, time 118.44656872749329, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
goal_identified
=== ep: 2273, time 122.01227355003357, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2273
goal_identified
=== ep: 2274, time 116.21122145652771, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2274
goal_identified
goal_identified
goal_identified
=== ep: 2275, time 102.9554054737091, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 98.75855684280396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2276
goal_identified
goal_identified
goal_identified
=== ep: 2277, time 111.48355603218079, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2277
=== ep: 2278, time 111.52409219741821, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2278
=== ep: 2279, time 107.04669380187988, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2279
goal_identified
=== ep: 2280, time 99.56234240531921, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2280
goal_identified
=== ep: 2281, time 110.26670670509338, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2281
goal_identified
goal_identified
=== ep: 2282, time 112.06087732315063, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2282
goal_identified
goal_identified
=== ep: 2283, time 107.05715298652649, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2283
=== ep: 2284, time 107.42081952095032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2284
goal_identified
=== ep: 2285, time 99.56669092178345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2285
goal_identified
=== ep: 2286, time 102.85884284973145, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2286
goal_identified
=== ep: 2287, time 105.26783227920532, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2287
=== ep: 2288, time 109.05565810203552, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2288
goal_identified
goal_identified
goal_identified
=== ep: 2289, time 115.01722860336304, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2289
goal_identified
goal_identified
=== ep: 2290, time 106.8531060218811, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2290
goal_identified
=== ep: 2291, time 107.73383164405823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2291
goal_identified
goal_identified
goal_identified
=== ep: 2292, time 108.16410255432129, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2292
=== ep: 2293, time 110.2588906288147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2293
=== ep: 2294, time 103.34831428527832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2294
goal_identified
=== ep: 2295, time 108.85054206848145, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2295
goal_identified
goal_identified
=== ep: 2296, time 102.84237813949585, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2296
=== ep: 2297, time 107.32724237442017, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2297
goal_identified
goal_identified
=== ep: 2298, time 112.84072923660278, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2298
goal_identified
=== ep: 2299, time 108.12222456932068, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2299
=== ep: 2300, time 103.78623294830322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2300
=== ep: 2301, time 105.23688316345215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2301
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2302, time 102.91429448127747, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
goal_identified
goal_identified
=== ep: 2303, time 104.17457890510559, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2303
goal_identified
goal_identified
=== ep: 2304, time 106.01926183700562, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2304
=== ep: 2305, time 99.33414602279663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2305
=== ep: 2306, time 113.06190490722656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2306
goal_identified
=== ep: 2307, time 97.4159107208252, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2307
goal_identified
goal_identified
=== ep: 2308, time 105.04388666152954, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2308
=== ep: 2309, time 109.35480666160583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2309
=== ep: 2310, time 108.50574159622192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2310
goal_identified
=== ep: 2311, time 98.26200914382935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2311
goal_identified
goal_identified
=== ep: 2312, time 103.0213131904602, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2312
goal_identified
=== ep: 2313, time 105.59142827987671, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2313
=== ep: 2314, time 98.2124412059784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2314
goal_identified
=== ep: 2315, time 108.34538054466248, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2315
goal_identified
=== ep: 2316, time 98.00314593315125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2316
goal_identified
=== ep: 2317, time 91.01512789726257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2317
goal_identified
=== ep: 2318, time 94.50484538078308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2318
goal_identified
=== ep: 2319, time 96.61588478088379, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2319
=== ep: 2320, time 96.86662912368774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2320
goal_identified
=== ep: 2321, time 95.3115394115448, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2321
goal_identified
goal_identified
=== ep: 2322, time 100.1328547000885, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2322
goal_identified
goal_identified
=== ep: 2323, time 96.58533191680908, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2323
goal_identified
=== ep: 2324, time 92.16288828849792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2324
goal_identified
goal_identified
=== ep: 2325, time 92.66130042076111, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2325
goal_identified
=== ep: 2326, time 99.71808552742004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2326
goal_identified
=== ep: 2327, time 97.41009092330933, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2327
=== ep: 2328, time 96.98886227607727, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2328
=== ep: 2329, time 95.63550806045532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2329
goal_identified
goal_identified
goal_identified
=== ep: 2330, time 96.30090928077698, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2330
goal_identified
=== ep: 2331, time 99.93518877029419, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2331
=== ep: 2332, time 102.22913646697998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2332
goal_identified
=== ep: 2333, time 94.65539789199829, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2333
=== ep: 2334, time 97.97020530700684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2334
=== ep: 2335, time 96.0119206905365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2335
=== ep: 2336, time 91.82614469528198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2336
goal_identified
=== ep: 2337, time 93.43704199790955, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2337
goal_identified
goal_identified
=== ep: 2338, time 93.25784540176392, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2338
=== ep: 2339, time 99.3285539150238, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2339
goal_identified
=== ep: 2340, time 100.67686176300049, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2340
goal_identified
goal_identified
goal_identified
=== ep: 2341, time 90.91298222541809, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2341
=== ep: 2342, time 86.35395336151123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2342
goal_identified
=== ep: 2343, time 87.25642538070679, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2343
=== ep: 2344, time 95.68417882919312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2344
goal_identified
goal_identified
=== ep: 2345, time 97.24047040939331, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2345
goal_identified
=== ep: 2346, time 95.8806505203247, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2346
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2347, time 99.06875252723694, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
goal_identified
goal_identified
=== ep: 2348, time 95.21378469467163, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2348
goal_identified
goal_identified
=== ep: 2349, time 91.83455109596252, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2349
goal_identified
=== ep: 2350, time 87.70641922950745, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2350
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2351, time 100.05552625656128, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2351
=== ep: 2352, time 99.8813066482544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2352
goal_identified
=== ep: 2353, time 97.6384961605072, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2353
goal_identified
=== ep: 2354, time 94.64014077186584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2354
=== ep: 2355, time 94.66926980018616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2355
goal_identified
goal_identified
=== ep: 2356, time 95.3124430179596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2356
=== ep: 2357, time 104.16623568534851, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2357
goal_identified
=== ep: 2358, time 102.0584762096405, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2358
goal_identified
goal_identified
=== ep: 2359, time 87.77859282493591, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2359
=== ep: 2360, time 95.12585830688477, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2360
goal_identified
goal_identified
goal_identified
=== ep: 2361, time 95.78004121780396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2361
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2362, time 98.53091788291931, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
goal_identified
goal_identified
=== ep: 2363, time 98.05980038642883, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2363
goal_identified
=== ep: 2364, time 94.68444299697876, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2364
goal_identified
=== ep: 2365, time 88.42308139801025, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2365
=== ep: 2366, time 84.37721824645996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2366
goal_identified
=== ep: 2367, time 84.58061504364014, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2367
goal_identified
goal_identified
=== ep: 2368, time 83.61705040931702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2368
=== ep: 2369, time 75.2601752281189, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2369
=== ep: 2370, time 80.57379674911499, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2370
goal_identified
goal_identified
=== ep: 2371, time 82.46114730834961, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2371
goal_identified
=== ep: 2372, time 80.75859880447388, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2372
goal_identified
goal_identified
=== ep: 2373, time 80.70543622970581, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2373
=== ep: 2374, time 78.04723191261292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2374
goal_identified
=== ep: 2375, time 75.28870582580566, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2375
goal_identified
=== ep: 2376, time 73.7948088645935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2376
goal_identified
goal_identified
=== ep: 2377, time 76.86150312423706, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2377
goal_identified
=== ep: 2378, time 73.19990158081055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2378
goal_identified
=== ep: 2379, time 78.53805184364319, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2379
goal_identified
=== ep: 2380, time 79.15914177894592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2380
goal_identified
=== ep: 2381, time 82.46778774261475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2381
goal_identified
goal_identified
=== ep: 2382, time 80.31847786903381, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2382
goal_identified
goal_identified
=== ep: 2383, time 81.33324384689331, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2383
goal_identified
goal_identified
=== ep: 2384, time 76.26625967025757, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2384
=== ep: 2385, time 75.38676524162292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2385
=== ep: 2386, time 68.90574669837952, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2386
goal_identified
=== ep: 2387, time 72.70708727836609, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2387
goal_identified
=== ep: 2388, time 77.1656847000122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2388
goal_identified
goal_identified
=== ep: 2389, time 74.36363244056702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2389
=== ep: 2390, time 75.48664736747742, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2390
goal_identified
goal_identified
=== ep: 2391, time 82.14889311790466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2391
=== ep: 2392, time 77.18461966514587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2392
goal_identified
goal_identified
=== ep: 2393, time 83.67041826248169, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2393
=== ep: 2394, time 79.49987864494324, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2394
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2395, time 78.99263525009155, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
goal_identified
goal_identified
=== ep: 2396, time 76.36847043037415, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2396
goal_identified
=== ep: 2397, time 75.50082492828369, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2397
goal_identified
=== ep: 2398, time 71.76913237571716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2398
goal_identified
goal_identified
goal_identified
=== ep: 2399, time 71.95322561264038, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2399
goal_identified
goal_identified
=== ep: 2400, time 80.1047613620758, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2400
=== ep: 2401, time 66.39679169654846, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2401
goal_identified
=== ep: 2402, time 70.5180311203003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2402
goal_identified
=== ep: 2403, time 76.39837837219238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2403
=== ep: 2404, time 72.58129501342773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2404
goal_identified
=== ep: 2405, time 78.64008593559265, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2405
goal_identified
=== ep: 2406, time 79.12862205505371, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2406
goal_identified
=== ep: 2407, time 80.04595994949341, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2407
goal_identified
=== ep: 2408, time 81.00118350982666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2408
=== ep: 2409, time 83.30733394622803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2409
=== ep: 2410, time 81.19351625442505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2410
goal_identified
goal_identified
goal_identified
=== ep: 2411, time 80.95299100875854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2411
goal_identified
=== ep: 2412, time 78.25843071937561, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2412
=== ep: 2413, time 75.34337210655212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2413
goal_identified
goal_identified
goal_identified
=== ep: 2414, time 76.75527262687683, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2414
=== ep: 2415, time 66.7378294467926, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2415
goal_identified
=== ep: 2416, time 79.01876616477966, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 75.67582440376282, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2417
goal_identified
goal_identified
goal_identified
=== ep: 2418, time 81.52581071853638, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2418
goal_identified
=== ep: 2419, time 80.64810276031494, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2419
goal_identified
=== ep: 2420, time 80.19633054733276, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2420
=== ep: 2421, time 80.1452865600586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2421
goal_identified
goal_identified
=== ep: 2422, time 81.58220028877258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2422
=== ep: 2423, time 75.12458992004395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2423
=== ep: 2424, time 78.6272325515747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2424
goal_identified
=== ep: 2425, time 73.67867136001587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2425
=== ep: 2426, time 67.40616011619568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2426
=== ep: 2427, time 77.89535164833069, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2427
=== ep: 2428, time 76.98381233215332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2428
goal_identified
=== ep: 2429, time 75.01190185546875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2429
goal_identified
=== ep: 2430, time 78.2565667629242, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2430
=== ep: 2431, time 79.87773776054382, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2431
goal_identified
goal_identified
=== ep: 2432, time 80.1866135597229, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2432
=== ep: 2433, time 81.3349244594574, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2433
=== ep: 2434, time 80.95183944702148, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2434
=== ep: 2435, time 77.2360200881958, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2435
goal_identified
=== ep: 2436, time 77.99854683876038, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2436
goal_identified
=== ep: 2437, time 76.41183805465698, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2437
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2438, time 69.1900737285614, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
=== ep: 2439, time 68.24387860298157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2439
goal_identified
=== ep: 2440, time 75.68121457099915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2440
goal_identified
goal_identified
=== ep: 2441, time 76.78849077224731, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2441
goal_identified
goal_identified
=== ep: 2442, time 77.02236104011536, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2442
=== ep: 2443, time 78.22714328765869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2443
=== ep: 2444, time 81.79503607749939, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2444
goal_identified
=== ep: 2445, time 81.64075541496277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2445
goal_identified
goal_identified
=== ep: 2446, time 78.28273677825928, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2446
goal_identified
=== ep: 2447, time 70.37569427490234, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2447
=== ep: 2448, time 69.4939374923706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2448
goal_identified
goal_identified
=== ep: 2449, time 73.20082712173462, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2449
goal_identified
goal_identified
=== ep: 2450, time 75.43074584007263, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2450
goal_identified
=== ep: 2451, time 71.56946849822998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2451
=== ep: 2452, time 68.53588819503784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2452
goal_identified
=== ep: 2453, time 73.48407626152039, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2453
goal_identified
goal_identified
=== ep: 2454, time 76.32444047927856, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2454
goal_identified
=== ep: 2455, time 72.93566608428955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2455
goal_identified
goal_identified
=== ep: 2456, time 77.51357364654541, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2456
goal_identified
=== ep: 2457, time 75.31409764289856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2457
goal_identified
goal_identified
=== ep: 2458, time 70.72914934158325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2458
goal_identified
goal_identified
=== ep: 2459, time 71.70441198348999, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2459
goal_identified
goal_identified
=== ep: 2460, time 71.44056820869446, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2460
=== ep: 2461, time 71.95577907562256, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2461
=== ep: 2462, time 74.20359086990356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2462
goal_identified
goal_identified
=== ep: 2463, time 73.69520926475525, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2463
=== ep: 2464, time 76.16523480415344, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2464
=== ep: 2465, time 69.3817765712738, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2465
=== ep: 2466, time 68.91877055168152, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2466
goal_identified
=== ep: 2467, time 71.41598629951477, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2467
goal_identified
goal_identified
=== ep: 2468, time 67.7614049911499, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2468
=== ep: 2469, time 65.64910507202148, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2469
goal_identified
=== ep: 2470, time 70.61533498764038, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2470
goal_identified
=== ep: 2471, time 66.79298663139343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2471
=== ep: 2472, time 61.3818895816803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2472
=== ep: 2473, time 66.73445844650269, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2473
goal_identified
goal_identified
goal_identified
=== ep: 2474, time 67.5640435218811, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2474
=== ep: 2475, time 67.9963173866272, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2475
goal_identified
=== ep: 2476, time 61.23037672042847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2476
goal_identified
goal_identified
goal_identified
=== ep: 2477, time 64.86186385154724, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2477
=== ep: 2478, time 67.7169234752655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2478
goal_identified
goal_identified
=== ep: 2479, time 63.58918118476868, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2479
goal_identified
goal_identified
goal_identified
=== ep: 2480, time 63.85794281959534, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2480
goal_identified
=== ep: 2481, time 66.31146335601807, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2481
=== ep: 2482, time 74.04241824150085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2482
goal_identified
=== ep: 2483, time 69.54133009910583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2483
goal_identified
=== ep: 2484, time 71.9741325378418, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2484
=== ep: 2485, time 71.70500183105469, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2485
=== ep: 2486, time 71.85990476608276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2486
goal_identified
goal_identified
=== ep: 2487, time 71.94996356964111, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2487
goal_identified
=== ep: 2488, time 74.36264848709106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2488
goal_identified
goal_identified
=== ep: 2489, time 73.58478617668152, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2489
goal_identified
goal_identified
=== ep: 2490, time 74.29315829277039, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2490
=== ep: 2491, time 71.0446949005127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2491
goal_identified
=== ep: 2492, time 74.75298643112183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2492
goal_identified
goal_identified
=== ep: 2493, time 68.30312156677246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2493
goal_identified
goal_identified
=== ep: 2494, time 63.49720621109009, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2494
goal_identified
=== ep: 2495, time 63.74322867393494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2495
=== ep: 2496, time 69.41477012634277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2496
goal_identified
goal_identified
=== ep: 2497, time 66.59570479393005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2497
=== ep: 2498, time 59.69645953178406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 61.965325117111206, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2499
goal_identified
=== ep: 2500, time 68.08374834060669, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2500
=== ep: 2501, time 71.70824241638184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2501
goal_identified
goal_identified
=== ep: 2502, time 59.176031827926636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 63.51787447929382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2503
=== ep: 2504, time 67.05624532699585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2504
goal_identified
=== ep: 2505, time 69.76434206962585, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2505
goal_identified
=== ep: 2506, time 63.37434983253479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2506
goal_identified
=== ep: 2507, time 65.05524325370789, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2507
goal_identified
=== ep: 2508, time 67.55133128166199, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2508
goal_identified
=== ep: 2509, time 67.1449134349823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2509
=== ep: 2510, time 66.7074728012085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2510
=== ep: 2511, time 73.00760436058044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2511
=== ep: 2512, time 67.20382642745972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2512
=== ep: 2513, time 67.93169069290161, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2513
goal_identified
=== ep: 2514, time 71.97208333015442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2514
goal_identified
goal_identified
goal_identified
=== ep: 2515, time 70.03847908973694, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2515
=== ep: 2516, time 72.32611918449402, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2516
=== ep: 2517, time 73.21308422088623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2517
goal_identified
=== ep: 2518, time 74.3862714767456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2518
goal_identified
=== ep: 2519, time 72.3102216720581, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2519
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2520, time 76.80891466140747, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
goal_identified
=== ep: 2521, time 69.16422033309937, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2521
=== ep: 2522, time 72.6857316493988, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2522
goal_identified
=== ep: 2523, time 72.94687390327454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2523
=== ep: 2524, time 73.41389465332031, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2524
=== ep: 2525, time 74.90520119667053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2525
goal_identified
=== ep: 2526, time 70.57295775413513, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2526
goal_identified
=== ep: 2527, time 73.65205478668213, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2527
goal_identified
goal_identified
=== ep: 2528, time 69.64162278175354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2528
goal_identified
=== ep: 2529, time 68.35592246055603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2529
goal_identified
goal_identified
=== ep: 2530, time 71.16339445114136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2530
goal_identified
=== ep: 2531, time 62.88528108596802, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2531
goal_identified
=== ep: 2532, time 60.606571197509766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2532
goal_identified
=== ep: 2533, time 67.60525465011597, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2533
goal_identified
=== ep: 2534, time 66.98667049407959, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2534
goal_identified
=== ep: 2535, time 62.18394923210144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2535
goal_identified
=== ep: 2536, time 60.837687969207764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2536
=== ep: 2537, time 66.88500928878784, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2537
goal_identified
goal_identified
=== ep: 2538, time 67.19763326644897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2538
goal_identified
goal_identified
=== ep: 2539, time 66.02328705787659, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2539
goal_identified
goal_identified
=== ep: 2540, time 59.69792819023132, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2540
=== ep: 2541, time 66.17821884155273, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2541
=== ep: 2542, time 68.84080719947815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2542
goal_identified
=== ep: 2543, time 65.17710423469543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2543
goal_identified
goal_identified
=== ep: 2544, time 58.09398293495178, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2544
goal_identified
=== ep: 2545, time 62.770806074142456, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2545
=== ep: 2546, time 69.41827750205994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2546
goal_identified
goal_identified
=== ep: 2547, time 64.95700478553772, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2547
goal_identified
goal_identified
=== ep: 2548, time 61.54665493965149, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2548
goal_identified
goal_identified
goal_identified
=== ep: 2549, time 67.96976327896118, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2549
=== ep: 2550, time 70.63947558403015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2550
goal_identified
=== ep: 2551, time 64.85737013816833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2551
goal_identified
goal_identified
goal_identified
=== ep: 2552, time 67.83045744895935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2552
=== ep: 2553, time 70.60194206237793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2553
=== ep: 2554, time 67.46321749687195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2554
goal_identified
goal_identified
=== ep: 2555, time 67.62890672683716, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2555
goal_identified
goal_identified
=== ep: 2556, time 71.46387696266174, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2556
=== ep: 2557, time 66.99619698524475, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2557
=== ep: 2558, time 67.66423058509827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2558
goal_identified
=== ep: 2559, time 73.16522192955017, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2559
goal_identified
=== ep: 2560, time 69.48715662956238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2560
goal_identified
goal_identified
=== ep: 2561, time 72.9161970615387, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2561
goal_identified
=== ep: 2562, time 73.44450306892395, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2562
goal_identified
=== ep: 2563, time 74.47209525108337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2563
goal_identified
goal_identified
=== ep: 2564, time 74.35269141197205, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2564
=== ep: 2565, time 72.66304206848145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2565
=== ep: 2566, time 71.065833568573, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2566
goal_identified
goal_identified
=== ep: 2567, time 67.75873827934265, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2567
goal_identified
goal_identified
goal_identified
=== ep: 2568, time 72.04615998268127, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2568
goal_identified
goal_identified
=== ep: 2569, time 68.75880765914917, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 63.9570050239563, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2570
=== ep: 2571, time 69.66109466552734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2571
goal_identified
goal_identified
=== ep: 2572, time 66.66176295280457, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2572
goal_identified
=== ep: 2573, time 62.56339240074158, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2573
=== ep: 2574, time 61.119192123413086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2574
goal_identified
=== ep: 2575, time 66.47887110710144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2575
=== ep: 2576, time 70.09574913978577, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2576
=== ep: 2577, time 66.34350895881653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2577
goal_identified
goal_identified
=== ep: 2578, time 70.59673309326172, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2578
goal_identified
=== ep: 2579, time 67.57461953163147, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2579
=== ep: 2580, time 69.04180097579956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2580
goal_identified
goal_identified
goal_identified
=== ep: 2581, time 68.95122694969177, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2581
=== ep: 2582, time 61.48767447471619, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2582
goal_identified
=== ep: 2583, time 61.0601110458374, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2583
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2584, time 67.0571563243866, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2584
goal_identified
=== ep: 2585, time 67.95989751815796, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2585
goal_identified
=== ep: 2586, time 62.21320724487305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2586
goal_identified
goal_identified
=== ep: 2587, time 62.55098748207092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2587
=== ep: 2588, time 70.28401303291321, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2588
=== ep: 2589, time 67.2558970451355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2590, time 62.72962164878845, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1914
goal_identified
=== ep: 2591, time 68.88595247268677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2591
=== ep: 2592, time 68.21371102333069, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2592
goal_identified
goal_identified
=== ep: 2593, time 59.86943960189819, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2593
=== ep: 2594, time 61.78151226043701, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2594
=== ep: 2595, time 63.97136425971985, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 2/2)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2595
=== ep: 2596, time 67.51005721092224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2596
goal_identified
goal_identified
=== ep: 2597, time 61.15212559700012, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2597
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2598, time 66.78840255737305, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2598
goal_identified
=== ep: 2599, time 67.32539367675781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2599
=== ep: 2600, time 67.21236872673035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2600
goal_identified
goal_identified
=== ep: 2601, time 63.46299934387207, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2601
=== ep: 2602, time 66.1133680343628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2602
goal_identified
goal_identified
=== ep: 2603, time 69.95579648017883, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2603
=== ep: 2604, time 68.88167667388916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2604
goal_identified
=== ep: 2605, time 66.1289930343628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2605
goal_identified
goal_identified
goal_identified
=== ep: 2606, time 70.60122013092041, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2606
goal_identified
=== ep: 2607, time 71.31351494789124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2607
=== ep: 2608, time 75.70058631896973, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2608
goal_identified
goal_identified
goal_identified
=== ep: 2609, time 73.02979516983032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2609
goal_identified
goal_identified
=== ep: 2610, time 74.74419689178467, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2610
goal_identified
=== ep: 2611, time 73.71435165405273, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2611
goal_identified
=== ep: 2612, time 72.82822823524475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2612
=== ep: 2613, time 73.71084260940552, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2613
goal_identified
=== ep: 2614, time 73.5378007888794, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2614
=== ep: 2615, time 69.69812965393066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2615
goal_identified
=== ep: 2616, time 67.22878837585449, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2616
=== ep: 2617, time 75.67213034629822, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2617
goal_identified
goal_identified
=== ep: 2618, time 68.11940097808838, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2618
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2619, time 62.708725929260254, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2096
=== ep: 2620, time 59.409295082092285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2620
goal_identified
goal_identified
=== ep: 2621, time 60.39778184890747, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2621
=== ep: 2622, time 66.49815821647644, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2622
goal_identified
goal_identified
=== ep: 2623, time 67.93867301940918, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2623
goal_identified
=== ep: 2624, time 62.0531530380249, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2624
goal_identified
=== ep: 2625, time 58.773303270339966, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2625
goal_identified
=== ep: 2626, time 64.21123099327087, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2626
goal_identified
goal_identified
goal_identified
=== ep: 2627, time 73.24620294570923, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2627
goal_identified
goal_identified
=== ep: 2628, time 64.45191216468811, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2628
goal_identified
=== ep: 2629, time 61.076666831970215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2629
=== ep: 2630, time 66.47789669036865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2630
goal_identified
goal_identified
=== ep: 2631, time 67.72401237487793, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2631
=== ep: 2632, time 61.90230870246887, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2632
goal_identified
goal_identified
goal_identified
=== ep: 2633, time 60.35834002494812, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2633
goal_identified
=== ep: 2634, time 66.12092351913452, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2634
goal_identified
=== ep: 2635, time 68.18197417259216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2635
=== ep: 2636, time 62.64235258102417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2636
goal_identified
=== ep: 2637, time 65.08407473564148, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2637
goal_identified
=== ep: 2638, time 65.07300114631653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2638
goal_identified
=== ep: 2639, time 68.62804746627808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2639
goal_identified
goal_identified
goal_identified
=== ep: 2640, time 63.78960061073303, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2640
goal_identified
=== ep: 2641, time 61.23071241378784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2641
goal_identified
goal_identified
=== ep: 2642, time 63.44404888153076, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2642
goal_identified
goal_identified
=== ep: 2643, time 67.38873291015625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2643
goal_identified
=== ep: 2644, time 69.52644395828247, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2644
goal_identified
=== ep: 2645, time 66.76676511764526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2645
=== ep: 2646, time 68.71321845054626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2646
goal_identified
=== ep: 2647, time 70.95790457725525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2647
goal_identified
goal_identified
=== ep: 2648, time 74.8292670249939, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2648
goal_identified
goal_identified
=== ep: 2649, time 73.08681178092957, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2649
=== ep: 2650, time 72.5554609298706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2650
goal_identified
goal_identified
=== ep: 2651, time 73.79841542243958, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2651
goal_identified
goal_identified
=== ep: 2652, time 71.43109035491943, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2652
goal_identified
=== ep: 2653, time 72.32875466346741, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2653
=== ep: 2654, time 71.32512402534485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2654
goal_identified
=== ep: 2655, time 73.4158103466034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2655
goal_identified
=== ep: 2656, time 70.05388069152832, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2656
goal_identified
=== ep: 2657, time 69.02574419975281, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2657
goal_identified
=== ep: 2658, time 72.88252997398376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2658
=== ep: 2659, time 65.53659152984619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2659
=== ep: 2660, time 63.307247161865234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2660
=== ep: 2661, time 66.20276880264282, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2661
goal_identified
=== ep: 2662, time 68.18472671508789, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2662
=== ep: 2663, time 63.46543836593628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2663
=== ep: 2664, time 62.314536333084106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2664
=== ep: 2665, time 67.98688697814941, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2665
goal_identified
=== ep: 2666, time 66.38842105865479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2666
=== ep: 2667, time 63.60802388191223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2667
=== ep: 2668, time 69.01028418540955, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2668
=== ep: 2669, time 68.62817239761353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2669
goal_identified
=== ep: 2670, time 61.50806474685669, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2670
goal_identified
=== ep: 2671, time 63.514737129211426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2671
goal_identified
goal_identified
=== ep: 2672, time 68.19741654396057, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2672
goal_identified
=== ep: 2673, time 67.98187112808228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2673
goal_identified
=== ep: 2674, time 57.867284297943115, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2674
=== ep: 2675, time 61.09997057914734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2675
=== ep: 2676, time 66.81205368041992, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2676
goal_identified
goal_identified
=== ep: 2677, time 65.85759544372559, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2677
=== ep: 2678, time 65.47298073768616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2678
=== ep: 2679, time 62.137455463409424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2679
=== ep: 2680, time 67.28221535682678, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2680
=== ep: 2681, time 65.2420494556427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2681
goal_identified
=== ep: 2682, time 60.137733697891235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2682
=== ep: 2683, time 63.892706871032715, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2683
goal_identified
=== ep: 2684, time 67.95129108428955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2684
goal_identified
=== ep: 2685, time 63.10121488571167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2685
goal_identified
goal_identified
=== ep: 2686, time 63.226195335388184, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2686
goal_identified
goal_identified
goal_identified
=== ep: 2687, time 67.05873417854309, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2687
goal_identified
=== ep: 2688, time 71.76487922668457, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2688
=== ep: 2689, time 66.22540783882141, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2689
=== ep: 2690, time 67.79275560379028, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2690
=== ep: 2691, time 65.69477248191833, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2691
=== ep: 2692, time 62.88993430137634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2692
goal_identified
goal_identified
goal_identified
=== ep: 2693, time 67.8963348865509, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2693
=== ep: 2694, time 68.2186210155487, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2694
goal_identified
=== ep: 2695, time 65.05198431015015, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2695
goal_identified
goal_identified
=== ep: 2696, time 69.3637535572052, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2696
goal_identified
=== ep: 2697, time 67.130530834198, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2697
goal_identified
=== ep: 2698, time 70.28512501716614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2698
goal_identified
goal_identified
=== ep: 2699, time 72.0592634677887, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2699
goal_identified
goal_identified
=== ep: 2700, time 73.6185371875763, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2700
=== ep: 2701, time 73.51279997825623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2701
goal_identified
=== ep: 2702, time 73.88214302062988, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2702
=== ep: 2703, time 72.97677111625671, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2703
=== ep: 2704, time 72.51105523109436, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2704
=== ep: 2705, time 73.03215217590332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2706, time 72.81742930412292, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2158
goal_identified
=== ep: 2707, time 70.68873715400696, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2707
goal_identified
goal_identified
goal_identified
=== ep: 2708, time 69.20540404319763, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2708
=== ep: 2709, time 72.72629809379578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2709
goal_identified
=== ep: 2710, time 67.34284400939941, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2710
=== ep: 2711, time 70.15759873390198, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2711
goal_identified
goal_identified
goal_identified
=== ep: 2712, time 68.27369618415833, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2712
goal_identified
goal_identified
=== ep: 2713, time 64.92482995986938, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2713
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2714, time 66.43906927108765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2272
goal_identified
goal_identified
=== ep: 2715, time 67.72832202911377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2715
=== ep: 2716, time 68.39709854125977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2716
goal_identified
=== ep: 2717, time 66.12633037567139, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2717
goal_identified
=== ep: 2718, time 69.90121746063232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2718
=== ep: 2719, time 69.38666152954102, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2719
goal_identified
=== ep: 2720, time 59.82047390937805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2720
=== ep: 2721, time 65.29951095581055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2721
=== ep: 2722, time 67.55596280097961, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2722
goal_identified
=== ep: 2723, time 64.83024406433105, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2723
goal_identified
=== ep: 2724, time 60.118520975112915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2724
goal_identified
=== ep: 2725, time 59.35844850540161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2725
goal_identified
goal_identified
=== ep: 2726, time 65.78023505210876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2726
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2727, time 68.30028486251831, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2302
goal_identified
=== ep: 2728, time 62.86097002029419, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2728
goal_identified
=== ep: 2729, time 64.85579991340637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2729
goal_identified
=== ep: 2730, time 66.07360124588013, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2730
goal_identified
=== ep: 2731, time 69.1106309890747, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2731
goal_identified
=== ep: 2732, time 66.2602310180664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2732
goal_identified
=== ep: 2733, time 64.35436749458313, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2733
goal_identified
goal_identified
=== ep: 2734, time 68.80773997306824, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2734
goal_identified
goal_identified
=== ep: 2735, time 68.48124313354492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2735
=== ep: 2736, time 67.14165735244751, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2736
goal_identified
goal_identified
=== ep: 2737, time 69.42721009254456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2737
goal_identified
=== ep: 2738, time 70.98740077018738, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2738
=== ep: 2739, time 69.04837846755981, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2739
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2740, time 74.00343298912048, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 68.59382629394531, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2741
goal_identified
goal_identified
=== ep: 2742, time 70.77083659172058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 71.33100962638855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2743
=== ep: 2744, time 72.04340147972107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2744
goal_identified
=== ep: 2745, time 73.36149740219116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2745
goal_identified
=== ep: 2746, time 72.80799531936646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2746
goal_identified
goal_identified
=== ep: 2747, time 73.89801216125488, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2747
goal_identified
goal_identified
=== ep: 2748, time 72.1370689868927, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2748
goal_identified
goal_identified
=== ep: 2749, time 73.22662091255188, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2749
goal_identified
=== ep: 2750, time 72.86422991752625, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 71.0352828502655, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2751
goal_identified
=== ep: 2752, time 67.71762800216675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2752
=== ep: 2753, time 65.40894341468811, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2753
=== ep: 2754, time 70.77005219459534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2754
=== ep: 2755, time 66.78012895584106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2755
goal_identified
=== ep: 2756, time 62.90618419647217, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2756
=== ep: 2757, time 59.15659308433533, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2757
goal_identified
=== ep: 2758, time 67.09463620185852, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2758
goal_identified
=== ep: 2759, time 67.32068133354187, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2759
goal_identified
=== ep: 2760, time 62.3776741027832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2760
goal_identified
=== ep: 2761, time 64.51126432418823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2761
goal_identified
=== ep: 2762, time 66.20308899879456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2762
goal_identified
goal_identified
=== ep: 2763, time 67.43581891059875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2763
=== ep: 2764, time 65.10206151008606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2764
goal_identified
=== ep: 2765, time 62.27789759635925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2765
goal_identified
=== ep: 2766, time 64.05132341384888, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2766
=== ep: 2767, time 67.7418487071991, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2767
=== ep: 2768, time 72.66717195510864, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2768
=== ep: 2769, time 69.66691899299622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2769
=== ep: 2770, time 73.20559501647949, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2770
goal_identified
goal_identified
=== ep: 2771, time 74.01908683776855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2771
goal_identified
=== ep: 2772, time 70.8946623802185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2772
=== ep: 2773, time 66.74943947792053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2773
goal_identified
goal_identified
=== ep: 2774, time 64.4195704460144, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2774
goal_identified
=== ep: 2775, time 57.67835736274719, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2775
goal_identified
=== ep: 2776, time 59.368699073791504, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2776
goal_identified
goal_identified
=== ep: 2777, time 63.8986439704895, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2777
goal_identified
goal_identified
=== ep: 2778, time 68.66106986999512, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2778
goal_identified
goal_identified
goal_identified
=== ep: 2779, time 70.06594729423523, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2779
goal_identified
=== ep: 2780, time 69.0202488899231, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2780
=== ep: 2781, time 69.14330792427063, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2781
=== ep: 2782, time 75.76404547691345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2782
goal_identified
=== ep: 2783, time 72.66713762283325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2783
=== ep: 2784, time 70.0331723690033, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2784
=== ep: 2785, time 66.90972781181335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2785
=== ep: 2786, time 69.28416085243225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2786
goal_identified
goal_identified
=== ep: 2787, time 69.21284604072571, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2787
=== ep: 2788, time 69.33755850791931, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2788
=== ep: 2789, time 66.06590914726257, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2789
=== ep: 2790, time 66.61912107467651, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2790
goal_identified
=== ep: 2791, time 70.53086352348328, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2791
goal_identified
=== ep: 2792, time 75.52149248123169, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2792
=== ep: 2793, time 72.5950620174408, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2793
goal_identified
=== ep: 2794, time 69.98573780059814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2794
goal_identified
goal_identified
=== ep: 2795, time 66.68115997314453, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2795
=== ep: 2796, time 70.46382689476013, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2796
goal_identified
=== ep: 2797, time 68.69408988952637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2797
goal_identified
=== ep: 2798, time 65.2584125995636, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2798
goal_identified
=== ep: 2799, time 60.77271366119385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2799
=== ep: 2800, time 62.64211440086365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2800
goal_identified
goal_identified
=== ep: 2801, time 65.2975721359253, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2801
goal_identified
goal_identified
=== ep: 2802, time 69.57746958732605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2802
=== ep: 2803, time 76.61187744140625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2803
goal_identified
goal_identified
=== ep: 2804, time 73.36680340766907, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2804
=== ep: 2805, time 71.71181869506836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2805
=== ep: 2806, time 70.07434916496277, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2806
goal_identified
=== ep: 2807, time 69.19566106796265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 31/31)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2807
=== ep: 2808, time 67.54732322692871, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2808
goal_identified
goal_identified
=== ep: 2809, time 61.96098279953003, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2809
=== ep: 2810, time 61.38381791114807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2810
goal_identified
goal_identified
=== ep: 2811, time 58.898207664489746, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2811
=== ep: 2812, time 62.810473918914795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2812
goal_identified
goal_identified
=== ep: 2813, time 72.03457474708557, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2813
goal_identified
goal_identified
=== ep: 2814, time 69.40676188468933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2814
goal_identified
=== ep: 2815, time 67.24171376228333, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2815
=== ep: 2816, time 69.72839188575745, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2816
goal_identified
goal_identified
=== ep: 2817, time 73.05690717697144, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2817
goal_identified
=== ep: 2818, time 72.61142873764038, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2818
goal_identified
=== ep: 2819, time 70.71268391609192, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2819
goal_identified
goal_identified
goal_identified
=== ep: 2820, time 67.19564461708069, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2820
=== ep: 2821, time 69.2778913974762, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2821
goal_identified
=== ep: 2822, time 69.24090099334717, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2822
goal_identified
=== ep: 2823, time 67.46752953529358, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2823
goal_identified
=== ep: 2824, time 65.78966522216797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2824
goal_identified
=== ep: 2825, time 65.7972502708435, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2825
=== ep: 2826, time 68.11244082450867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2826
=== ep: 2827, time 69.4301278591156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2827
goal_identified
goal_identified
=== ep: 2828, time 68.63857531547546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2828
goal_identified
goal_identified
=== ep: 2829, time 71.18980479240417, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2829
goal_identified
goal_identified
=== ep: 2830, time 72.7024085521698, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2830
goal_identified
goal_identified
=== ep: 2831, time 73.65210938453674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2831
goal_identified
=== ep: 2832, time 69.94356322288513, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2832
goal_identified
=== ep: 2833, time 66.02670788764954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2833
=== ep: 2834, time 70.79868125915527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2834
goal_identified
=== ep: 2835, time 68.16102385520935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2835
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2836, time 71.05531358718872, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2347
goal_identified
goal_identified
=== ep: 2837, time 67.12234139442444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2837
goal_identified
=== ep: 2838, time 67.95905232429504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2838
=== ep: 2839, time 72.65503168106079, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2839
=== ep: 2840, time 72.3977563381195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 72.80842924118042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2841
=== ep: 2842, time 70.26764583587646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2842
goal_identified
=== ep: 2843, time 69.34867668151855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 68.43959593772888, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2844
goal_identified
=== ep: 2845, time 73.8473150730133, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 63.38435912132263, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2846
goal_identified
goal_identified
=== ep: 2847, time 66.37559366226196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2847
goal_identified
goal_identified
=== ep: 2848, time 69.8357286453247, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2848
=== ep: 2849, time 70.71557378768921, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2849
goal_identified
goal_identified
=== ep: 2850, time 71.53583121299744, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2850
=== ep: 2851, time 73.00657963752747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2851
goal_identified
goal_identified
=== ep: 2852, time 72.58727264404297, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2852
goal_identified
goal_identified
=== ep: 2853, time 71.24291682243347, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2853
=== ep: 2854, time 66.76515102386475, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2854
goal_identified
=== ep: 2855, time 63.99195384979248, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2855
goal_identified
=== ep: 2856, time 68.54565167427063, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2856
=== ep: 2857, time 66.3675217628479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2857
goal_identified
goal_identified
goal_identified
=== ep: 2858, time 68.80259537696838, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2858
=== ep: 2859, time 67.18785953521729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2859
goal_identified
=== ep: 2860, time 67.30801272392273, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 71.67283415794373, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2861
goal_identified
goal_identified
=== ep: 2862, time 71.02182078361511, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2862
goal_identified
=== ep: 2863, time 69.58894634246826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2863
goal_identified
goal_identified
=== ep: 2864, time 70.67178773880005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2864
goal_identified
=== ep: 2865, time 72.58339190483093, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2865
goal_identified
=== ep: 2866, time 73.30157089233398, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2866
goal_identified
goal_identified
=== ep: 2867, time 75.68760395050049, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2867
goal_identified
goal_identified
goal_identified
=== ep: 2868, time 71.25305914878845, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2868
goal_identified
=== ep: 2869, time 69.9546947479248, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2869
=== ep: 2870, time 65.77172613143921, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2870
goal_identified
goal_identified
=== ep: 2871, time 62.14314126968384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2871
goal_identified
goal_identified
=== ep: 2872, time 64.55972647666931, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2872
goal_identified
=== ep: 2873, time 70.26555228233337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2873
goal_identified
=== ep: 2874, time 68.8094379901886, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2874
goal_identified
=== ep: 2875, time 63.633868932724, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2875
goal_identified
=== ep: 2876, time 58.54764938354492, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2876
goal_identified
=== ep: 2877, time 67.13522958755493, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2877
=== ep: 2878, time 66.66899180412292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2878
goal_identified
goal_identified
goal_identified
=== ep: 2879, time 70.7076849937439, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2879
goal_identified
=== ep: 2880, time 69.6482093334198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2880
goal_identified
=== ep: 2881, time 72.00868248939514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2881
goal_identified
=== ep: 2882, time 74.11662268638611, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2882
goal_identified
=== ep: 2883, time 66.13061285018921, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2883
goal_identified
=== ep: 2884, time 69.74224901199341, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2884
goal_identified
=== ep: 2885, time 71.35632157325745, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2885
=== ep: 2886, time 72.05021405220032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2886
goal_identified
=== ep: 2887, time 74.17913174629211, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2887
goal_identified
goal_identified
=== ep: 2888, time 74.33844351768494, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2888
=== ep: 2889, time 70.64380645751953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2889
=== ep: 2890, time 69.44437956809998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2890
goal_identified
goal_identified
=== ep: 2891, time 63.660520792007446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2891
goal_identified
goal_identified
=== ep: 2892, time 60.98085141181946, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2892
=== ep: 2893, time 65.9330062866211, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2893
=== ep: 2894, time 69.79204082489014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2894
goal_identified
=== ep: 2895, time 69.01273560523987, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2895
goal_identified
=== ep: 2896, time 62.41107678413391, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2896
=== ep: 2897, time 58.60900926589966, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2897
=== ep: 2898, time 64.3257052898407, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2898
=== ep: 2899, time 70.63910722732544, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2899
goal_identified
=== ep: 2900, time 69.70736813545227, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2900
goal_identified
goal_identified
=== ep: 2901, time 65.06015825271606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2901
goal_identified
=== ep: 2902, time 62.42320418357849, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2902
goal_identified
=== ep: 2903, time 66.17310070991516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2903
goal_identified
=== ep: 2904, time 70.7803966999054, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2904
goal_identified
=== ep: 2905, time 69.53353595733643, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2905
=== ep: 2906, time 68.92396688461304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2906
goal_identified
=== ep: 2907, time 69.3087854385376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2907
goal_identified
goal_identified
goal_identified
=== ep: 2908, time 71.98715591430664, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2908
=== ep: 2909, time 71.312753200531, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2909
=== ep: 2910, time 76.21407318115234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2911, time 71.73176050186157, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2362
goal_identified
goal_identified
=== ep: 2912, time 72.97543978691101, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2912
goal_identified
goal_identified
=== ep: 2913, time 74.64550113677979, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2913
goal_identified
goal_identified
=== ep: 2914, time 69.91534757614136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2914
goal_identified
goal_identified
=== ep: 2915, time 72.23635649681091, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2915
goal_identified
goal_identified
goal_identified
=== ep: 2916, time 70.51884603500366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2916
goal_identified
=== ep: 2917, time 65.80240321159363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2917
=== ep: 2918, time 66.721360206604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2918
=== ep: 2919, time 72.187251329422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2919
goal_identified
goal_identified
=== ep: 2920, time 68.88322257995605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2920
=== ep: 2921, time 69.71155977249146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2921
=== ep: 2922, time 60.00989365577698, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2922
goal_identified
=== ep: 2923, time 61.45045185089111, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2923
goal_identified
=== ep: 2924, time 67.60201025009155, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2924
=== ep: 2925, time 68.63715171813965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2925
goal_identified
goal_identified
=== ep: 2926, time 69.21608781814575, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2926
goal_identified
=== ep: 2927, time 66.88880443572998, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2927
goal_identified
=== ep: 2928, time 68.10334420204163, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2928
goal_identified
goal_identified
=== ep: 2929, time 72.34363961219788, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2929
goal_identified
goal_identified
=== ep: 2930, time 72.50396013259888, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2930
goal_identified
goal_identified
=== ep: 2931, time 77.29974293708801, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2931
=== ep: 2932, time 74.35187768936157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2932
goal_identified
goal_identified
goal_identified
=== ep: 2933, time 73.38296008110046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2933
goal_identified
goal_identified
=== ep: 2934, time 72.00048875808716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2934
goal_identified
=== ep: 2935, time 66.86275291442871, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2935
=== ep: 2936, time 66.03726601600647, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2936
goal_identified
=== ep: 2937, time 70.35428285598755, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2937
goal_identified
=== ep: 2938, time 72.02528429031372, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2938
=== ep: 2939, time 67.1512885093689, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2939
goal_identified
goal_identified
=== ep: 2940, time 63.46692633628845, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2940
goal_identified
goal_identified
=== ep: 2941, time 64.06013464927673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2941
goal_identified
goal_identified
=== ep: 2942, time 66.83847284317017, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2942
=== ep: 2943, time 73.58408284187317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2943
goal_identified
goal_identified
=== ep: 2944, time 72.13914680480957, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2944
goal_identified
=== ep: 2945, time 72.61804223060608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2945
goal_identified
goal_identified
=== ep: 2946, time 73.17854166030884, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2946
=== ep: 2947, time 72.80790042877197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2947
goal_identified
=== ep: 2948, time 73.29326319694519, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2948
goal_identified
goal_identified
=== ep: 2949, time 74.61368942260742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2949
goal_identified
goal_identified
=== ep: 2950, time 71.32262349128723, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2950
goal_identified
=== ep: 2951, time 74.09033632278442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2951
goal_identified
goal_identified
goal_identified
=== ep: 2952, time 70.48800873756409, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2953, time 67.70074105262756, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2395
=== ep: 2954, time 72.14337491989136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2954
=== ep: 2955, time 71.78367018699646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2955
goal_identified
goal_identified
goal_identified
=== ep: 2956, time 71.05103707313538, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2956
=== ep: 2957, time 67.56349778175354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2957
goal_identified
=== ep: 2958, time 67.79330229759216, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2958
=== ep: 2959, time 71.01798009872437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2959
=== ep: 2960, time 70.80407357215881, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2960
goal_identified
=== ep: 2961, time 69.27360010147095, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2961
goal_identified
goal_identified
=== ep: 2962, time 73.38197684288025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2962
goal_identified
=== ep: 2963, time 74.990647315979, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2963
=== ep: 2964, time 71.96098375320435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2964
=== ep: 2965, time 77.70877838134766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2965
goal_identified
=== ep: 2966, time 69.6927330493927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2966
goal_identified
goal_identified
goal_identified
=== ep: 2967, time 68.27263379096985, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2967
goal_identified
goal_identified
=== ep: 2968, time 71.9781539440155, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2968
=== ep: 2969, time 69.55252575874329, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2969
=== ep: 2970, time 64.79269123077393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2970
goal_identified
goal_identified
=== ep: 2971, time 59.14450716972351, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2971
goal_identified
=== ep: 2972, time 63.85331058502197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2972
goal_identified
goal_identified
goal_identified
=== ep: 2973, time 69.46609592437744, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2973
=== ep: 2974, time 68.64734125137329, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2974
goal_identified
=== ep: 2975, time 65.04939770698547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2975
goal_identified
=== ep: 2976, time 74.23709011077881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2976
goal_identified
=== ep: 2977, time 70.68219590187073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2977
goal_identified
=== ep: 2978, time 71.15948104858398, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2978
goal_identified
=== ep: 2979, time 72.30181622505188, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2979
goal_identified
goal_identified
goal_identified
=== ep: 2980, time 73.37627863883972, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2980
goal_identified
=== ep: 2981, time 74.88765788078308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2981
=== ep: 2982, time 73.37739610671997, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2982
goal_identified
=== ep: 2983, time 74.18591165542603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2983
goal_identified
goal_identified
=== ep: 2984, time 70.67420434951782, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2984
=== ep: 2985, time 71.71017932891846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2985
=== ep: 2986, time 77.3746657371521, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2986
goal_identified
=== ep: 2987, time 68.89318466186523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2987
goal_identified
=== ep: 2988, time 65.87092065811157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2988
goal_identified
=== ep: 2989, time 71.92017412185669, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2989
goal_identified
goal_identified
goal_identified
=== ep: 2990, time 67.74617195129395, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2990
goal_identified
goal_identified
goal_identified
=== ep: 2991, time 64.31919717788696, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2991
goal_identified
goal_identified
goal_identified
=== ep: 2992, time 60.049497842788696, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2992
=== ep: 2993, time 66.71182298660278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2993
=== ep: 2994, time 67.46555590629578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2994
goal_identified
=== ep: 2995, time 70.82210206985474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2995
goal_identified
goal_identified
=== ep: 2996, time 68.05960464477539, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2996
goal_identified
=== ep: 2997, time 73.05658173561096, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2997
=== ep: 2998, time 72.1654543876648, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2998
goal_identified
=== ep: 2999, time 74.83778738975525, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2999
goal_identified
=== ep: 3000, time 73.09311032295227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3000
goal_identified
=== ep: 3001, time 73.36013460159302, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3001
=== ep: 3002, time 69.69498443603516, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3002
goal_identified
goal_identified
goal_identified
=== ep: 3003, time 63.857932806015015, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3003
=== ep: 3004, time 68.67478275299072, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3004
=== ep: 3005, time 68.06630778312683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3005
goal_identified
=== ep: 3006, time 64.90165686607361, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3007, time 59.770355463027954, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2520
goal_identified
goal_identified
=== ep: 3008, time 71.2334852218628, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3008
goal_identified
=== ep: 3009, time 70.13985419273376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3009
=== ep: 3010, time 68.99955892562866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3010
goal_identified
=== ep: 3011, time 69.42646336555481, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3011
goal_identified
=== ep: 3012, time 72.76471161842346, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3012
=== ep: 3013, time 72.33530902862549, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3013
goal_identified
goal_identified
goal_identified
=== ep: 3014, time 72.85917782783508, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3014
=== ep: 3015, time 71.02628374099731, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3015
goal_identified
=== ep: 3016, time 71.35923504829407, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3016
goal_identified
=== ep: 3017, time 68.67735576629639, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3017
goal_identified
goal_identified
=== ep: 3018, time 63.4295768737793, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3018
=== ep: 3019, time 72.852698802948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3019
goal_identified
=== ep: 3020, time 69.00304961204529, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3020
goal_identified
=== ep: 3021, time 68.45167374610901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3021
=== ep: 3022, time 66.60724830627441, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3022
goal_identified
=== ep: 3023, time 68.4305989742279, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3023
goal_identified
goal_identified
=== ep: 3024, time 70.2350709438324, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3024
=== ep: 3025, time 69.70986652374268, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3025
goal_identified
=== ep: 3026, time 73.6232967376709, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3026
goal_identified
goal_identified
goal_identified
=== ep: 3027, time 73.97294020652771, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3027
goal_identified
=== ep: 3028, time 72.274977684021, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3028
=== ep: 3029, time 72.1263678073883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3029
goal_identified
goal_identified
goal_identified
=== ep: 3030, time 73.5253918170929, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3030
goal_identified
=== ep: 3031, time 62.1006178855896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3031
goal_identified
goal_identified
=== ep: 3032, time 66.85518741607666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3032
=== ep: 3033, time 68.0209310054779, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3033
=== ep: 3034, time 65.70763850212097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3034
goal_identified
=== ep: 3035, time 65.81813025474548, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3035
goal_identified
=== ep: 3036, time 68.3946762084961, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3036
=== ep: 3037, time 68.363685131073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3037
=== ep: 3038, time 69.45963788032532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3038
goal_identified
=== ep: 3039, time 72.63623118400574, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3039
goal_identified
=== ep: 3040, time 72.67695903778076, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 73.93389344215393, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3041
=== ep: 3042, time 71.60941934585571, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3042
goal_identified
=== ep: 3043, time 67.58492803573608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3043
=== ep: 3044, time 62.66370439529419, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3044
goal_identified
goal_identified
=== ep: 3045, time 63.28418731689453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3045
goal_identified
=== ep: 3046, time 69.21253538131714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3046
goal_identified
=== ep: 3047, time 66.12633395195007, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 33/33)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3047
=== ep: 3048, time 63.42408752441406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3048
=== ep: 3049, time 66.50967264175415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3049
goal_identified
=== ep: 3050, time 70.32624244689941, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3050
=== ep: 3051, time 68.90705299377441, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3051
=== ep: 3052, time 76.43184232711792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3052
goal_identified
=== ep: 3053, time 72.92783832550049, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3053
=== ep: 3054, time 70.98977136611938, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3054
goal_identified
=== ep: 3055, time 71.04907011985779, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3055
=== ep: 3056, time 69.31028962135315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3056
goal_identified
=== ep: 3057, time 62.739707231521606, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3057
goal_identified
=== ep: 3058, time 64.72855973243713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3058
goal_identified
goal_identified
=== ep: 3059, time 70.25291562080383, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3059
goal_identified
=== ep: 3060, time 65.82637023925781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3060
goal_identified
=== ep: 3061, time 63.69575881958008, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3061
goal_identified
=== ep: 3062, time 67.59669208526611, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3062
goal_identified
=== ep: 3063, time 71.19906663894653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3063
=== ep: 3064, time 67.59500360488892, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3064
goal_identified
goal_identified
=== ep: 3065, time 71.78911256790161, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3065
goal_identified
goal_identified
=== ep: 3066, time 72.25183248519897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3066
goal_identified
goal_identified
goal_identified
=== ep: 3067, time 74.19561958312988, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3067
goal_identified
=== ep: 3068, time 72.99279880523682, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3068
=== ep: 3069, time 71.02500772476196, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3069
goal_identified
=== ep: 3070, time 66.03698587417603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3070
=== ep: 3071, time 69.59604239463806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3071
goal_identified
=== ep: 3072, time 68.35302877426147, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3072
goal_identified
goal_identified
=== ep: 3073, time 64.18670320510864, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3073
=== ep: 3074, time 69.17071294784546, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3074
=== ep: 3075, time 70.98150992393494, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3075
goal_identified
=== ep: 3076, time 65.91050481796265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3076
=== ep: 3077, time 70.02859044075012, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3077
goal_identified
goal_identified
goal_identified
=== ep: 3078, time 72.76718068122864, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3078
goal_identified
=== ep: 3079, time 73.63679146766663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3079
goal_identified
=== ep: 3080, time 73.98686194419861, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3080
goal_identified
=== ep: 3081, time 74.02095580101013, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3081
goal_identified
goal_identified
=== ep: 3082, time 71.90365386009216, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3082
goal_identified
=== ep: 3083, time 69.94927883148193, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3083
goal_identified
=== ep: 3084, time 64.17523193359375, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3084
goal_identified
=== ep: 3085, time 68.93488669395447, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3085
goal_identified
=== ep: 3086, time 70.24638199806213, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3086
goal_identified
=== ep: 3087, time 58.34793448448181, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3087
goal_identified
goal_identified
goal_identified
=== ep: 3088, time 63.62133264541626, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3088
goal_identified
goal_identified
=== ep: 3089, time 68.87844610214233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3089
=== ep: 3090, time 65.56921601295471, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3090
=== ep: 3091, time 65.93495440483093, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3091
goal_identified
=== ep: 3092, time 69.90998983383179, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3092
goal_identified
=== ep: 3093, time 69.72061610221863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3093
goal_identified
=== ep: 3094, time 73.54867506027222, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3094
=== ep: 3095, time 72.45519995689392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3095
=== ep: 3096, time 71.24760675430298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3096
=== ep: 3097, time 68.58449006080627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 73.73896741867065, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3098
goal_identified
=== ep: 3099, time 68.49686455726624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3099
=== ep: 3100, time 61.552923917770386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3100
goal_identified
=== ep: 3101, time 66.45851278305054, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3101
goal_identified
goal_identified
goal_identified
=== ep: 3102, time 70.53561997413635, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3102
goal_identified
goal_identified
=== ep: 3103, time 67.73649740219116, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3103
=== ep: 3104, time 70.16514706611633, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3104
goal_identified
=== ep: 3105, time 71.76923871040344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3105
goal_identified
=== ep: 3106, time 72.29652142524719, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3106
goal_identified
=== ep: 3107, time 72.88104248046875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3107
=== ep: 3108, time 69.55963802337646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3108
goal_identified
goal_identified
=== ep: 3109, time 70.16132831573486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3109
goal_identified
=== ep: 3110, time 64.47896909713745, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3110
goal_identified
=== ep: 3111, time 67.71665668487549, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3111
goal_identified
goal_identified
=== ep: 3112, time 67.83142304420471, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3112
=== ep: 3113, time 66.76340985298157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3113
goal_identified
goal_identified
=== ep: 3114, time 71.95731568336487, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3114
=== ep: 3115, time 71.60857582092285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3115
goal_identified
=== ep: 3116, time 72.83883500099182, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3116
goal_identified
=== ep: 3117, time 73.31812477111816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3117
=== ep: 3118, time 71.72836780548096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3118
goal_identified
=== ep: 3119, time 71.59856581687927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3119
goal_identified
=== ep: 3120, time 72.92076659202576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3120
goal_identified
goal_identified
=== ep: 3121, time 61.377110958099365, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3121
goal_identified
goal_identified
=== ep: 3122, time 59.301135540008545, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3122
=== ep: 3123, time 66.46873068809509, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3123
goal_identified
goal_identified
goal_identified
=== ep: 3124, time 69.57894802093506, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3124
=== ep: 3125, time 69.78678154945374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3125
goal_identified
goal_identified
=== ep: 3126, time 71.22107982635498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3126
goal_identified
=== ep: 3127, time 72.28422021865845, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3127
=== ep: 3128, time 73.94196105003357, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3128
=== ep: 3129, time 68.88240647315979, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3129
goal_identified
goal_identified
=== ep: 3130, time 63.3135871887207, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3130
goal_identified
=== ep: 3131, time 61.05970573425293, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3131
goal_identified
=== ep: 3132, time 64.2008638381958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3132
goal_identified
goal_identified
=== ep: 3133, time 57.50162601470947, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3133
=== ep: 3134, time 58.05448007583618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3134
goal_identified
=== ep: 3135, time 59.33631944656372, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3135
=== ep: 3136, time 61.82112002372742, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3136
goal_identified
=== ep: 3137, time 63.742891788482666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3137
goal_identified
=== ep: 3138, time 65.45394539833069, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3138
=== ep: 3139, time 65.1862404346466, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3139
goal_identified
=== ep: 3140, time 67.1407413482666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3140
=== ep: 3141, time 67.93951272964478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3141
=== ep: 3142, time 65.48240208625793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 63.07377338409424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3143
goal_identified
=== ep: 3144, time 65.48708629608154, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3144
goal_identified
=== ep: 3145, time 58.87259316444397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3145
goal_identified
=== ep: 3146, time 55.44186329841614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3146
=== ep: 3147, time 57.226884603500366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3147
goal_identified
=== ep: 3148, time 57.550578117370605, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3148
goal_identified
goal_identified
=== ep: 3149, time 59.46243214607239, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3149
=== ep: 3150, time 61.01273798942566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3150
goal_identified
goal_identified
=== ep: 3151, time 63.909998178482056, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3151
goal_identified
=== ep: 3152, time 66.64602828025818, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3152
=== ep: 3153, time 66.57924318313599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3153
=== ep: 3154, time 65.79296541213989, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3154
=== ep: 3155, time 67.1181652545929, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3155
=== ep: 3156, time 59.969921350479126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3156
=== ep: 3157, time 60.54626488685608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3157
goal_identified
goal_identified
=== ep: 3158, time 57.90230584144592, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3158
=== ep: 3159, time 56.09971070289612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3159
goal_identified
=== ep: 3160, time 57.09007954597473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3160
goal_identified
=== ep: 3161, time 58.348819732666016, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3161
goal_identified
=== ep: 3162, time 59.72363305091858, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3162
goal_identified
=== ep: 3163, time 60.67306876182556, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3163
=== ep: 3164, time 61.436097145080566, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3164
=== ep: 3165, time 64.02046227455139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3165
goal_identified
=== ep: 3166, time 63.30661725997925, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3166
=== ep: 3167, time 73.42424464225769, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3167
goal_identified
=== ep: 3168, time 68.02807950973511, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3168
goal_identified
=== ep: 3169, time 67.5235288143158, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3169
=== ep: 3170, time 65.12191534042358, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3170
goal_identified
=== ep: 3171, time 66.86398768424988, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3171
goal_identified
goal_identified
=== ep: 3172, time 64.96295976638794, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3172
goal_identified
=== ep: 3173, time 64.18604683876038, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3173
goal_identified
goal_identified
=== ep: 3174, time 62.18589472770691, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3174
=== ep: 3175, time 59.98198962211609, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3175
=== ep: 3176, time 57.57559251785278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3176
goal_identified
=== ep: 3177, time 57.43560171127319, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3177
goal_identified
goal_identified
goal_identified
=== ep: 3178, time 64.38947129249573, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3178
=== ep: 3179, time 61.132551193237305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3179
goal_identified
=== ep: 3180, time 61.368422746658325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3180
=== ep: 3181, time 62.63900375366211, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3181
goal_identified
=== ep: 3182, time 63.94456911087036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3182
=== ep: 3183, time 65.4954662322998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3183
goal_identified
=== ep: 3184, time 63.18364429473877, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3184
goal_identified
=== ep: 3185, time 65.48063468933105, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3185
goal_identified
=== ep: 3186, time 65.94386339187622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3186
goal_identified
goal_identified
=== ep: 3187, time 67.41487646102905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3187
goal_identified
=== ep: 3188, time 69.56759691238403, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3188
=== ep: 3189, time 67.01075005531311, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3189
goal_identified
=== ep: 3190, time 68.29694843292236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3190
goal_identified
=== ep: 3191, time 61.98291730880737, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3191
goal_identified
=== ep: 3192, time 58.52830123901367, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3192
=== ep: 3193, time 57.36134338378906, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3193
=== ep: 3194, time 56.132826805114746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3194
goal_identified
goal_identified
=== ep: 3195, time 57.47284436225891, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3195
=== ep: 3196, time 57.56595182418823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3196
goal_identified
=== ep: 3197, time 56.5890736579895, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3197
goal_identified
goal_identified
goal_identified
=== ep: 3198, time 58.17118573188782, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 60.97000741958618, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3199
=== ep: 3200, time 62.7628698348999, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3200
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3201, time 65.14734053611755, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3201
goal_identified
=== ep: 3202, time 70.25989866256714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3202
goal_identified
goal_identified
=== ep: 3203, time 66.55067181587219, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3203
goal_identified
goal_identified
=== ep: 3204, time 63.62735891342163, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3204
goal_identified
=== ep: 3205, time 61.71594524383545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3205
goal_identified
=== ep: 3206, time 59.49482011795044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3206
goal_identified
goal_identified
=== ep: 3207, time 56.96285796165466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3207
goal_identified
goal_identified
goal_identified
=== ep: 3208, time 56.75110387802124, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3208
goal_identified
=== ep: 3209, time 57.81713509559631, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3209
=== ep: 3210, time 60.84424161911011, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3210
goal_identified
=== ep: 3211, time 59.86353898048401, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3211
=== ep: 3212, time 60.7773756980896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3212
goal_identified
=== ep: 3213, time 67.31591796875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3213
=== ep: 3214, time 67.45286703109741, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3214
goal_identified
=== ep: 3215, time 67.69479441642761, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3215
goal_identified
=== ep: 3216, time 66.91380429267883, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3216
=== ep: 3217, time 65.16227316856384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3217
goal_identified
=== ep: 3218, time 63.18079948425293, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3218
goal_identified
=== ep: 3219, time 62.65248942375183, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3219
=== ep: 3220, time 60.26048922538757, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3220
=== ep: 3221, time 58.738340616226196, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3221
=== ep: 3222, time 56.504037857055664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3222
=== ep: 3223, time 58.096853256225586, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3223
=== ep: 3224, time 58.767624616622925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3224
goal_identified
goal_identified
goal_identified
=== ep: 3225, time 65.25922846794128, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3225
goal_identified
=== ep: 3226, time 62.15543723106384, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3226
=== ep: 3227, time 63.43315887451172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3227
goal_identified
=== ep: 3228, time 65.66505289077759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3228
goal_identified
=== ep: 3229, time 66.98169779777527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3229
goal_identified
=== ep: 3230, time 67.39120054244995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3230
=== ep: 3231, time 66.30055713653564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3231
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3232, time 64.27764892578125, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3232
goal_identified
goal_identified
=== ep: 3233, time 61.85306787490845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3233
goal_identified
=== ep: 3234, time 60.14700436592102, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3234
goal_identified
goal_identified
=== ep: 3235, time 62.45602869987488, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3235
goal_identified
goal_identified
=== ep: 3236, time 59.96200752258301, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3236
goal_identified
=== ep: 3237, time 64.59052634239197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3237
goal_identified
=== ep: 3238, time 58.20413279533386, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3238
=== ep: 3239, time 56.85691976547241, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3239
=== ep: 3240, time 57.56331777572632, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3240
goal_identified
=== ep: 3241, time 61.38047218322754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3241
goal_identified
=== ep: 3242, time 64.33747363090515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3242
goal_identified
=== ep: 3243, time 65.30629348754883, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3243
goal_identified
goal_identified
=== ep: 3244, time 67.24929523468018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3244
goal_identified
=== ep: 3245, time 67.14723873138428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3245
goal_identified
=== ep: 3246, time 65.87753820419312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3246
goal_identified
=== ep: 3247, time 63.74479031562805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3247
=== ep: 3248, time 60.85966610908508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3248
=== ep: 3249, time 63.090332984924316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3249
goal_identified
=== ep: 3250, time 58.11735725402832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3250
=== ep: 3251, time 60.50604796409607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3251
=== ep: 3252, time 62.7491352558136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3252
=== ep: 3253, time 67.13525819778442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3253
goal_identified
goal_identified
goal_identified
=== ep: 3254, time 66.3847062587738, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3254
goal_identified
=== ep: 3255, time 66.72184014320374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3255
goal_identified
=== ep: 3256, time 63.887983083724976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3256
=== ep: 3257, time 59.88474178314209, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3257
=== ep: 3258, time 56.6138858795166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3258
goal_identified
=== ep: 3259, time 59.34825420379639, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3259
goal_identified
goal_identified
=== ep: 3260, time 66.01857495307922, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3260
=== ep: 3261, time 65.68393087387085, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3261
goal_identified
=== ep: 3262, time 66.52370381355286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3262
goal_identified
=== ep: 3263, time 63.35957336425781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3263
=== ep: 3264, time 61.699183225631714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3264
=== ep: 3265, time 59.58791399002075, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3265
=== ep: 3266, time 57.823678493499756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3266
=== ep: 3267, time 57.31793713569641, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3267
goal_identified
=== ep: 3268, time 60.55533480644226, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3268
=== ep: 3269, time 60.46547031402588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3269
goal_identified
=== ep: 3270, time 62.770469665527344, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3270
=== ep: 3271, time 65.41151523590088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3271
goal_identified
=== ep: 3272, time 65.80805087089539, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3272
=== ep: 3273, time 66.6402199268341, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3273
goal_identified
=== ep: 3274, time 58.98959732055664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3274
=== ep: 3275, time 56.82882809638977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3275
=== ep: 3276, time 58.51583981513977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3276
goal_identified
goal_identified
=== ep: 3277, time 60.28307914733887, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3277
goal_identified
=== ep: 3278, time 63.45951581001282, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3278
=== ep: 3279, time 66.33290433883667, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3279
=== ep: 3280, time 66.664635181427, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3280
goal_identified
goal_identified
=== ep: 3281, time 62.69422888755798, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3281
goal_identified
=== ep: 3282, time 60.3053503036499, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3282
=== ep: 3283, time 57.849772214889526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3283
goal_identified
=== ep: 3284, time 57.12351417541504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3284
=== ep: 3285, time 64.0235071182251, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3285
=== ep: 3286, time 61.98861360549927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3286
=== ep: 3287, time 64.46892142295837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3287
goal_identified
=== ep: 3288, time 66.34381031990051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3288
=== ep: 3289, time 65.93388819694519, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3289
goal_identified
goal_identified
=== ep: 3290, time 62.73154044151306, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3290
=== ep: 3291, time 58.7262749671936, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3291
goal_identified
=== ep: 3292, time 55.87139964103699, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3292
=== ep: 3293, time 57.12638807296753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3293
goal_identified
=== ep: 3294, time 60.02315068244934, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3294
goal_identified
=== ep: 3295, time 61.71515154838562, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3295
goal_identified
=== ep: 3296, time 65.75988864898682, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3296
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3297, time 69.33971405029297, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2590
=== ep: 3298, time 62.060590982437134, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3298
=== ep: 3299, time 58.75718951225281, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3299
=== ep: 3300, time 58.00089907646179, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3300
goal_identified
=== ep: 3301, time 56.816203355789185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3301
=== ep: 3302, time 59.58906936645508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3302
goal_identified
=== ep: 3303, time 62.683077573776245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3303
goal_identified
=== ep: 3304, time 65.74804472923279, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3304
=== ep: 3305, time 66.39822387695312, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3305
goal_identified
=== ep: 3306, time 64.87488770484924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3306
goal_identified
=== ep: 3307, time 62.76765847206116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3307
goal_identified
=== ep: 3308, time 58.76556611061096, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3308
=== ep: 3309, time 61.59321308135986, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3309
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3310, time 60.51950788497925, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3310
goal_identified
goal_identified
goal_identified
=== ep: 3311, time 64.02249813079834, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3311
=== ep: 3312, time 67.42535066604614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3312
goal_identified
=== ep: 3313, time 66.37271928787231, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3313
=== ep: 3314, time 62.85927414894104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3314
=== ep: 3315, time 60.00129175186157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3315
=== ep: 3316, time 58.97983503341675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3316
goal_identified
=== ep: 3317, time 56.875712633132935, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 59.42784595489502, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3318
goal_identified
=== ep: 3319, time 62.01046347618103, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3319
goal_identified
=== ep: 3320, time 64.47207522392273, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3320
goal_identified
goal_identified
=== ep: 3321, time 66.2943811416626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 33/33)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3321
goal_identified
=== ep: 3322, time 68.61902832984924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3322
goal_identified
=== ep: 3323, time 61.3774778842926, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3323
goal_identified
=== ep: 3324, time 57.52907991409302, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3324
=== ep: 3325, time 57.11315131187439, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3325
=== ep: 3326, time 59.339343309402466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3326
=== ep: 3327, time 62.25125288963318, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3327
goal_identified
goal_identified
=== ep: 3328, time 64.47937202453613, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3328
goal_identified
=== ep: 3329, time 66.55260944366455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3329
goal_identified
goal_identified
=== ep: 3330, time 65.92175436019897, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3330
goal_identified
goal_identified
=== ep: 3331, time 64.90438914299011, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3331
goal_identified
goal_identified
=== ep: 3332, time 61.688827991485596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3332
goal_identified
goal_identified
=== ep: 3333, time 59.52143979072571, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3333
=== ep: 3334, time 64.67193984985352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3334
=== ep: 3335, time 61.634418964385986, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3335
goal_identified
=== ep: 3336, time 65.3407187461853, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3336
goal_identified
=== ep: 3337, time 66.09544706344604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3337
goal_identified
goal_identified
=== ep: 3338, time 64.44530177116394, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3338
=== ep: 3339, time 61.99816107749939, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3339
goal_identified
goal_identified
goal_identified
=== ep: 3340, time 59.026833295822144, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3340
goal_identified
=== ep: 3341, time 57.528748989105225, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3341
goal_identified
=== ep: 3342, time 57.89574837684631, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3342
goal_identified
=== ep: 3343, time 60.43870282173157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3343
=== ep: 3344, time 62.771711349487305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3344
goal_identified
goal_identified
=== ep: 3345, time 66.26140260696411, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3345
goal_identified
=== ep: 3346, time 65.8401370048523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3346
goal_identified
=== ep: 3347, time 69.20822477340698, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3347
=== ep: 3348, time 61.99229669570923, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3348
goal_identified
=== ep: 3349, time 60.75300049781799, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3349
=== ep: 3350, time 57.41276407241821, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3350
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3351, time 57.52418661117554, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2619
goal_identified
goal_identified
=== ep: 3352, time 62.59269332885742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3352
=== ep: 3353, time 66.31300067901611, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3353
goal_identified
=== ep: 3354, time 66.43692827224731, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3354
=== ep: 3355, time 64.76249885559082, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3355
goal_identified
goal_identified
goal_identified
=== ep: 3356, time 60.809062480926514, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3356
=== ep: 3357, time 58.53848695755005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3357
goal_identified
goal_identified
=== ep: 3358, time 59.571568965911865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3358
=== ep: 3359, time 69.72028112411499, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3359
=== ep: 3360, time 66.5307788848877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3360
=== ep: 3361, time 64.19818425178528, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3361
goal_identified
goal_identified
goal_identified
=== ep: 3362, time 61.84501075744629, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3362
=== ep: 3363, time 58.17393136024475, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3363
goal_identified
goal_identified
goal_identified
=== ep: 3364, time 59.73532676696777, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3364
goal_identified
goal_identified
=== ep: 3365, time 62.88399362564087, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3365
=== ep: 3366, time 66.16156959533691, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3366
goal_identified
goal_identified
=== ep: 3367, time 66.73994421958923, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3367
=== ep: 3368, time 64.85930156707764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3368
=== ep: 3369, time 60.26112365722656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3369
goal_identified
=== ep: 3370, time 58.503844261169434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3370
goal_identified
=== ep: 3371, time 68.30103874206543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3371
goal_identified
=== ep: 3372, time 67.45804953575134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3372
goal_identified
=== ep: 3373, time 66.62778544425964, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3373
goal_identified
=== ep: 3374, time 62.179078102111816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3374
goal_identified
goal_identified
=== ep: 3375, time 60.410977363586426, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3375
goal_identified
goal_identified
goal_identified
=== ep: 3376, time 58.015254497528076, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3376
goal_identified
=== ep: 3377, time 61.74292469024658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3377
goal_identified
goal_identified
=== ep: 3378, time 64.64959239959717, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3378
=== ep: 3379, time 66.90460443496704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3379
goal_identified
goal_identified
goal_identified
=== ep: 3380, time 66.33637261390686, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3380
=== ep: 3381, time 63.37879419326782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3381
=== ep: 3382, time 58.933767795562744, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3382
goal_identified
goal_identified
=== ep: 3383, time 65.70047068595886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3383
goal_identified
=== ep: 3384, time 64.41340732574463, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3384
goal_identified
goal_identified
=== ep: 3385, time 66.87597250938416, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3385
=== ep: 3386, time 67.50449538230896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3386
goal_identified
=== ep: 3387, time 66.3819990158081, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3387
goal_identified
=== ep: 3388, time 62.910284757614136, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3388
goal_identified
=== ep: 3389, time 58.7871835231781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3389
goal_identified
goal_identified
goal_identified
=== ep: 3390, time 59.05091857910156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3390
=== ep: 3391, time 63.167200326919556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3392, time 66.90551614761353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2706
=== ep: 3393, time 66.53984642028809, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3393
goal_identified
=== ep: 3394, time 65.1225996017456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3394
goal_identified
=== ep: 3395, time 67.33404064178467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3395
goal_identified
=== ep: 3396, time 57.78810691833496, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3396
goal_identified
goal_identified
=== ep: 3397, time 62.06307649612427, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3397
=== ep: 3398, time 65.57812786102295, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3398
goal_identified
goal_identified
=== ep: 3399, time 66.83231782913208, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3399
=== ep: 3400, time 67.02482175827026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3400
=== ep: 3401, time 63.18321132659912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3401
goal_identified
goal_identified
goal_identified
=== ep: 3402, time 59.49990224838257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3402
goal_identified
=== ep: 3403, time 57.55298709869385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3403
=== ep: 3404, time 62.145418882369995, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3404
=== ep: 3405, time 63.78964710235596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3405
goal_identified
=== ep: 3406, time 67.54449319839478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3406
goal_identified
goal_identified
goal_identified
=== ep: 3407, time 71.39936089515686, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3407
=== ep: 3408, time 59.575623989105225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3408
=== ep: 3409, time 57.958993911743164, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3409
goal_identified
goal_identified
=== ep: 3410, time 58.484206676483154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3410
=== ep: 3411, time 61.59117317199707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3411
goal_identified
=== ep: 3412, time 64.53177118301392, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3412
goal_identified
=== ep: 3413, time 66.82636070251465, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3413
goal_identified
goal_identified
=== ep: 3414, time 66.01140761375427, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3414
=== ep: 3415, time 61.31365609169006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3415
goal_identified
=== ep: 3416, time 57.08242630958557, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3416
goal_identified
=== ep: 3417, time 58.794387340545654, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3417
=== ep: 3418, time 62.453115940093994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3418
goal_identified
=== ep: 3419, time 65.87598967552185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3419
goal_identified
=== ep: 3420, time 69.42139029502869, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3420
=== ep: 3421, time 61.57559370994568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3421
goal_identified
goal_identified
=== ep: 3422, time 58.71218395233154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3422
=== ep: 3423, time 58.90540385246277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3423
goal_identified
goal_identified
=== ep: 3424, time 61.46044397354126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3424
=== ep: 3425, time 63.66815376281738, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3425
=== ep: 3426, time 64.71358823776245, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3426
goal_identified
goal_identified
=== ep: 3427, time 66.55105066299438, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3427
=== ep: 3428, time 64.77121210098267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3428
goal_identified
goal_identified
goal_identified
=== ep: 3429, time 61.62637972831726, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3429
=== ep: 3430, time 57.12466549873352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3430
=== ep: 3431, time 58.41532754898071, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3431
goal_identified
=== ep: 3432, time 61.89677429199219, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3432
=== ep: 3433, time 71.02060604095459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3433
goal_identified
=== ep: 3434, time 64.32523083686829, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3434
=== ep: 3435, time 61.39255928993225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3435
goal_identified
=== ep: 3436, time 58.093159914016724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3436
=== ep: 3437, time 56.786006927490234, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3437
goal_identified
=== ep: 3438, time 60.39221501350403, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3438
goal_identified
=== ep: 3439, time 64.22629356384277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3439
goal_identified
=== ep: 3440, time 66.24106216430664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3440
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3441, time 66.03395819664001, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2714
goal_identified
goal_identified
goal_identified
=== ep: 3442, time 62.657432317733765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3442
goal_identified
=== ep: 3443, time 61.409414529800415, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3443
goal_identified
=== ep: 3444, time 58.20722699165344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3444
goal_identified
=== ep: 3445, time 64.836834192276, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3445
goal_identified
=== ep: 3446, time 66.02238988876343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3446
goal_identified
=== ep: 3447, time 65.77317762374878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 65.29090118408203, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3448
=== ep: 3449, time 62.08280563354492, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3449
goal_identified
=== ep: 3450, time 56.146721839904785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3450
=== ep: 3451, time 58.983622550964355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3451
=== ep: 3452, time 61.8779673576355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3452
goal_identified
goal_identified
=== ep: 3453, time 66.56983804702759, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3453
=== ep: 3454, time 66.28447008132935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3454
=== ep: 3455, time 63.95763826370239, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3455
goal_identified
=== ep: 3456, time 59.95213985443115, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3456
goal_identified
=== ep: 3457, time 57.84225392341614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3457
goal_identified
=== ep: 3458, time 65.61229825019836, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3458
goal_identified
=== ep: 3459, time 63.031731843948364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3459
goal_identified
=== ep: 3460, time 67.1140398979187, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3460
goal_identified
goal_identified
=== ep: 3461, time 67.22460651397705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3461
goal_identified
goal_identified
=== ep: 3462, time 63.1869375705719, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3462
=== ep: 3463, time 60.209670543670654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3463
goal_identified
=== ep: 3464, time 57.70836567878723, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3464
goal_identified
=== ep: 3465, time 60.0775728225708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3465
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3466, time 64.36308693885803, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2727
goal_identified
goal_identified
=== ep: 3467, time 67.555180311203, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3467
goal_identified
=== ep: 3468, time 67.30503416061401, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3468
=== ep: 3469, time 61.89481449127197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3469
goal_identified
=== ep: 3470, time 66.59001207351685, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3470
goal_identified
goal_identified
=== ep: 3471, time 60.959181785583496, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3471
goal_identified
=== ep: 3472, time 65.13263177871704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3472
=== ep: 3473, time 67.27150177955627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3473
goal_identified
=== ep: 3474, time 64.30504631996155, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3474
=== ep: 3475, time 59.43591380119324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3475
goal_identified
=== ep: 3476, time 58.642462491989136, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3476
goal_identified
goal_identified
=== ep: 3477, time 61.47121858596802, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 64.67357683181763, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3478
goal_identified
=== ep: 3479, time 66.83357191085815, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3479
goal_identified
=== ep: 3480, time 65.72440433502197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3480
goal_identified
goal_identified
=== ep: 3481, time 62.38542556762695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3481
goal_identified
=== ep: 3482, time 64.28117156028748, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3482
=== ep: 3483, time 62.228179693222046, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3483
=== ep: 3484, time 66.33870911598206, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3484
goal_identified
goal_identified
=== ep: 3485, time 67.02571368217468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3485
goal_identified
goal_identified
goal_identified
=== ep: 3486, time 65.09193634986877, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2836
goal_identified
goal_identified
=== ep: 3487, time 60.21485733985901, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3487
=== ep: 3488, time 58.15828585624695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3488
=== ep: 3489, time 61.90484857559204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3489
goal_identified
goal_identified
=== ep: 3490, time 66.3943133354187, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3490
goal_identified
=== ep: 3491, time 67.12313055992126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3491
goal_identified
=== ep: 3492, time 66.93961477279663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3492
goal_identified
goal_identified
=== ep: 3493, time 63.608187198638916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3493
goal_identified
=== ep: 3494, time 59.56190514564514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3494
=== ep: 3495, time 66.2078869342804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3495
=== ep: 3496, time 65.82944107055664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3496
=== ep: 3497, time 66.80377125740051, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3497
=== ep: 3498, time 66.31979393959045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3498
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3499, time 61.96890640258789, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2911
=== ep: 3500, time 57.28937578201294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3500
goal_identified
=== ep: 3501, time 58.56434631347656, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3501
goal_identified
=== ep: 3502, time 62.17426919937134, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3502
=== ep: 3503, time 66.12721514701843, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3503
goal_identified
=== ep: 3504, time 66.70437335968018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3504
goal_identified
=== ep: 3505, time 63.05124115943909, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3505
=== ep: 3506, time 59.85272765159607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3506
goal_identified
=== ep: 3507, time 66.13456869125366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3507
goal_identified
=== ep: 3508, time 62.09037113189697, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3508
goal_identified
=== ep: 3509, time 66.03889799118042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3509
goal_identified
=== ep: 3510, time 67.4343650341034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3510
goal_identified
goal_identified
goal_identified
=== ep: 3511, time 63.861021518707275, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3511
goal_identified
goal_identified
=== ep: 3512, time 60.85843062400818, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3512
goal_identified
goal_identified
=== ep: 3513, time 56.89948844909668, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3513
=== ep: 3514, time 59.79819965362549, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3514
=== ep: 3515, time 62.39721131324768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3515
=== ep: 3516, time 66.81198453903198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3516
goal_identified
=== ep: 3517, time 66.90259289741516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3517
goal_identified
=== ep: 3518, time 63.134361028671265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3518
goal_identified
goal_identified
=== ep: 3519, time 58.918747425079346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3519
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3520, time 62.852503538131714, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3007
goal_identified
goal_identified
=== ep: 3521, time 63.379962682724, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3521
goal_identified
=== ep: 3522, time 66.18266940116882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3522
=== ep: 3523, time 65.55371356010437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 35/35)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3523
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3524, time 63.90611910820007, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3351
goal_identified
=== ep: 3525, time 62.91558337211609, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3525
goal_identified
=== ep: 3526, time 59.42029142379761, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3526
goal_identified
=== ep: 3527, time 57.24838304519653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3527
=== ep: 3528, time 59.71889138221741, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3528
goal_identified
=== ep: 3529, time 63.49462056159973, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3529
goal_identified
goal_identified
=== ep: 3530, time 67.39685535430908, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3530
goal_identified
goal_identified
=== ep: 3531, time 65.12279105186462, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3531
=== ep: 3532, time 62.80798292160034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3532
goal_identified
=== ep: 3533, time 58.275407791137695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3533
=== ep: 3534, time 65.10578942298889, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3534
goal_identified
=== ep: 3535, time 62.7890362739563, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3535
goal_identified
goal_identified
=== ep: 3536, time 67.47793841362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3536
=== ep: 3537, time 64.98853993415833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3537
goal_identified
=== ep: 3538, time 62.6333646774292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3538
=== ep: 3539, time 59.01255965232849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3539
goal_identified
=== ep: 3540, time 55.336429834365845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3540
goal_identified
=== ep: 3541, time 58.09370255470276, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3541
goal_identified
goal_identified
goal_identified
=== ep: 3542, time 57.76180601119995, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3542
=== ep: 3543, time 60.033825635910034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3543
goal_identified
=== ep: 3544, time 63.514986991882324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3544
=== ep: 3545, time 66.0844156742096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3545
goal_identified
=== ep: 3546, time 66.35841155052185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3546
=== ep: 3547, time 66.60086107254028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3547
goal_identified
=== ep: 3548, time 58.43465542793274, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3548
goal_identified
=== ep: 3549, time 59.13340473175049, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3549
goal_identified
=== ep: 3550, time 63.488425493240356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3550
goal_identified
=== ep: 3551, time 65.43523716926575, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3551
goal_identified
=== ep: 3552, time 66.49148559570312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3552
goal_identified
goal_identified
goal_identified
=== ep: 3553, time 63.61489486694336, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3553
goal_identified
=== ep: 3554, time 59.68387818336487, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3554
=== ep: 3555, time 57.32889437675476, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3555
=== ep: 3556, time 60.749398946762085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3556
goal_identified
=== ep: 3557, time 63.67652487754822, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3557
=== ep: 3558, time 64.11623167991638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3558
=== ep: 3559, time 65.9769778251648, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3559
goal_identified
=== ep: 3560, time 69.00979423522949, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3560
goal_identified
=== ep: 3561, time 59.39973020553589, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3561
goal_identified
=== ep: 3562, time 57.14262938499451, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3562
=== ep: 3563, time 58.626067876815796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3563
=== ep: 3564, time 61.13263010978699, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3564
=== ep: 3565, time 64.81640434265137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3565
goal_identified
goal_identified
goal_identified
=== ep: 3566, time 66.89179849624634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3566
goal_identified
=== ep: 3567, time 64.30733370780945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3567
goal_identified
goal_identified
=== ep: 3568, time 62.176575660705566, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3568
goal_identified
=== ep: 3569, time 62.79068326950073, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3569
goal_identified
=== ep: 3570, time 59.00020885467529, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3570
goal_identified
=== ep: 3571, time 58.146076679229736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3571
goal_identified
goal_identified
=== ep: 3572, time 59.133238315582275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3572
goal_identified
goal_identified
goal_identified
=== ep: 3573, time 69.68645548820496, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3573
=== ep: 3574, time 66.30852580070496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3574
=== ep: 3575, time 65.8642590045929, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3575
=== ep: 3576, time 64.66213703155518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3576
goal_identified
=== ep: 3577, time 61.34982395172119, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3577
goal_identified
=== ep: 3578, time 57.94274115562439, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3578
=== ep: 3579, time 56.18201756477356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3579
goal_identified
goal_identified
=== ep: 3580, time 59.55235433578491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3580
goal_identified
=== ep: 3581, time 61.97867774963379, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3581
goal_identified
=== ep: 3582, time 64.84455943107605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3582
goal_identified
=== ep: 3583, time 66.37489700317383, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3583
goal_identified
=== ep: 3584, time 64.16784071922302, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3584
goal_identified
=== ep: 3585, time 60.35201144218445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3585
goal_identified
=== ep: 3586, time 63.22842979431152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3586
=== ep: 3587, time 61.15867519378662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3587
=== ep: 3588, time 64.15387415885925, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3588
goal_identified
=== ep: 3589, time 66.1870768070221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3589
goal_identified
=== ep: 3590, time 65.51940941810608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3590
=== ep: 3591, time 61.367093324661255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3591
goal_identified
=== ep: 3592, time 58.06552076339722, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3592
=== ep: 3593, time 56.918954610824585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3593
goal_identified
=== ep: 3594, time 57.15245532989502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3594
=== ep: 3595, time 60.592182874679565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3595
goal_identified
=== ep: 3596, time 64.09744000434875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3596
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3597, time 65.92543339729309, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3441
goal_identified
=== ep: 3598, time 65.25824189186096, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3598
=== ep: 3599, time 68.4210159778595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3599
=== ep: 3600, time 59.535842180252075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3600
goal_identified
goal_identified
goal_identified
=== ep: 3601, time 57.75635027885437, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3601
goal_identified
=== ep: 3602, time 58.35587000846863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3602
goal_identified
goal_identified
=== ep: 3603, time 61.23982381820679, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3603
goal_identified
goal_identified
=== ep: 3604, time 63.8838472366333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3604
goal_identified
goal_identified
=== ep: 3605, time 65.82794404029846, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3605
goal_identified
=== ep: 3606, time 65.70205950737, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3606
goal_identified
goal_identified
=== ep: 3607, time 62.70993518829346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3607
goal_identified
goal_identified
goal_identified
=== ep: 3608, time 58.878926038742065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3608
goal_identified
goal_identified
=== ep: 3609, time 56.1988046169281, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3609
goal_identified
goal_identified
=== ep: 3610, time 58.463515281677246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3610
=== ep: 3611, time 59.17757034301758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3611
=== ep: 3612, time 62.68378019332886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3612
=== ep: 3613, time 69.5854320526123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3613
=== ep: 3614, time 64.39592933654785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3614
goal_identified
=== ep: 3615, time 61.11115574836731, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3615
goal_identified
=== ep: 3616, time 58.333542823791504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3616
goal_identified
goal_identified
=== ep: 3617, time 56.430336475372314, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3617
goal_identified
=== ep: 3618, time 59.04940676689148, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3618
goal_identified
goal_identified
=== ep: 3619, time 62.38582396507263, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3619
=== ep: 3620, time 67.06014132499695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3620
goal_identified
goal_identified
=== ep: 3621, time 66.13400363922119, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3621
goal_identified
goal_identified
=== ep: 3622, time 62.22320342063904, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3622
=== ep: 3623, time 58.50431776046753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3623
=== ep: 3624, time 56.63854193687439, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3624
goal_identified
=== ep: 3625, time 58.88013768196106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3625
goal_identified
=== ep: 3626, time 68.07530665397644, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3626
goal_identified
=== ep: 3627, time 65.43686842918396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3627
=== ep: 3628, time 65.11085963249207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3628
=== ep: 3629, time 63.51326274871826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3629
goal_identified
goal_identified
goal_identified
=== ep: 3630, time 60.144449949264526, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3630
goal_identified
=== ep: 3631, time 56.75171232223511, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3631
goal_identified
=== ep: 3632, time 57.93206286430359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3632
goal_identified
goal_identified
goal_identified
=== ep: 3633, time 58.6793372631073, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3633
=== ep: 3634, time 57.44546723365784, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3634
goal_identified
goal_identified
=== ep: 3635, time 62.21028137207031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3635
goal_identified
=== ep: 3636, time 65.26264572143555, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3636
goal_identified
goal_identified
goal_identified
=== ep: 3637, time 66.41952610015869, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3637
goal_identified
goal_identified
goal_identified
=== ep: 3638, time 63.828269958496094, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3638
goal_identified
=== ep: 3639, time 60.51393747329712, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3640, time 64.15947890281677, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3486
goal_identified
goal_identified
=== ep: 3641, time 59.2825243473053, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3641
goal_identified
=== ep: 3642, time 63.187578439712524, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3642
goal_identified
goal_identified
=== ep: 3643, time 65.82676815986633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3643
=== ep: 3644, time 65.30652046203613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3644
goal_identified
goal_identified
=== ep: 3645, time 62.665507316589355, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3645
=== ep: 3646, time 61.69550633430481, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3646
=== ep: 3647, time 58.558842420578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3647
goal_identified
=== ep: 3648, time 56.9711480140686, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3648
goal_identified
goal_identified
=== ep: 3649, time 59.30998516082764, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3649
=== ep: 3650, time 61.6718213558197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3650
goal_identified
=== ep: 3651, time 65.433509349823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3651
=== ep: 3652, time 66.31072497367859, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3652
=== ep: 3653, time 69.01249074935913, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3653
goal_identified
=== ep: 3654, time 58.60986089706421, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3654
goal_identified
goal_identified
goal_identified
=== ep: 3655, time 55.657318353652954, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3655
goal_identified
=== ep: 3656, time 58.40831637382507, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3656
goal_identified
=== ep: 3657, time 61.5751097202301, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3657
goal_identified
goal_identified
goal_identified
=== ep: 3658, time 65.46080803871155, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3658
goal_identified
=== ep: 3659, time 65.07053661346436, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3659
=== ep: 3660, time 63.93612861633301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3660
goal_identified
=== ep: 3661, time 59.62730431556702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3661
=== ep: 3662, time 59.42453336715698, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3662
goal_identified
=== ep: 3663, time 55.83925819396973, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3663
goal_identified
=== ep: 3664, time 57.32372999191284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3664
=== ep: 3665, time 61.06930708885193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3665
goal_identified
goal_identified
=== ep: 3666, time 64.64003825187683, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3666
goal_identified
=== ep: 3667, time 68.47274231910706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3667
=== ep: 3668, time 61.52111482620239, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3668
goal_identified
goal_identified
=== ep: 3669, time 58.238378286361694, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3669
goal_identified
=== ep: 3670, time 56.07656407356262, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3670
goal_identified
=== ep: 3671, time 59.146321296691895, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3671
goal_identified
=== ep: 3672, time 60.18155264854431, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3672
=== ep: 3673, time 60.026055097579956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3673
=== ep: 3674, time 62.485496282577515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3674
goal_identified
goal_identified
=== ep: 3675, time 66.62609648704529, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3675
goal_identified
goal_identified
=== ep: 3676, time 65.56333470344543, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3676
=== ep: 3677, time 61.91775393486023, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3677
=== ep: 3678, time 63.2946343421936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3678
goal_identified
goal_identified
goal_identified
=== ep: 3679, time 59.06711292266846, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3679
=== ep: 3680, time 56.39100170135498, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3680
=== ep: 3681, time 63.412163496017456, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3681
goal_identified
=== ep: 3682, time 60.85931944847107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3682
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3683, time 63.9777090549469, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3499
goal_identified
=== ep: 3684, time 65.41742515563965, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3684
goal_identified
goal_identified
=== ep: 3685, time 65.51008105278015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3685
goal_identified
=== ep: 3686, time 64.65707588195801, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3686
goal_identified
goal_identified
=== ep: 3687, time 61.07506799697876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3687
goal_identified
=== ep: 3688, time 57.26880955696106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3688
=== ep: 3689, time 59.24505877494812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3689
goal_identified
=== ep: 3690, time 61.99039173126221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3690
=== ep: 3691, time 65.47133278846741, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3691
=== ep: 3692, time 66.71158790588379, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3692
goal_identified
goal_identified
=== ep: 3693, time 64.65585970878601, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3693
goal_identified
=== ep: 3694, time 66.56787252426147, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3694
goal_identified
goal_identified
=== ep: 3695, time 56.32961344718933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3695
goal_identified
=== ep: 3696, time 60.427204608917236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3696
goal_identified
=== ep: 3697, time 63.297045946121216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3697
=== ep: 3698, time 65.4663314819336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3698
goal_identified
=== ep: 3699, time 66.04743981361389, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3699
=== ep: 3700, time 66.24298071861267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3701, time 62.64469003677368, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3520
goal_identified
goal_identified
=== ep: 3702, time 60.01841688156128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3702
=== ep: 3703, time 57.09322929382324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3703
goal_identified
goal_identified
goal_identified
=== ep: 3704, time 58.45368552207947, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3704
=== ep: 3705, time 60.27594208717346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3705
=== ep: 3706, time 63.64255666732788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3706
goal_identified
goal_identified
=== ep: 3707, time 66.08791947364807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3707
goal_identified
goal_identified
=== ep: 3708, time 69.2976622581482, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3708
=== ep: 3709, time 60.00847911834717, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3709
goal_identified
goal_identified
=== ep: 3710, time 58.57741332054138, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3710
=== ep: 3711, time 56.14947557449341, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3711
goal_identified
=== ep: 3712, time 56.55695295333862, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3712
goal_identified
goal_identified
goal_identified
=== ep: 3713, time 56.79022240638733, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3713
goal_identified
=== ep: 3714, time 59.47019839286804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3714
goal_identified
=== ep: 3715, time 62.8363881111145, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3715
=== ep: 3716, time 64.95557117462158, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3716
goal_identified
=== ep: 3717, time 64.90328359603882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3717
goal_identified
goal_identified
=== ep: 3718, time 64.93601274490356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3718
goal_identified
goal_identified
=== ep: 3719, time 63.225433349609375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3719
goal_identified
=== ep: 3720, time 59.68801164627075, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3720
goal_identified
=== ep: 3721, time 55.96632742881775, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3721
goal_identified
=== ep: 3722, time 66.76470303535461, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3722
goal_identified
goal_identified
=== ep: 3723, time 64.1728310585022, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3723
=== ep: 3724, time 65.35503697395325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3724
goal_identified
=== ep: 3725, time 63.350276708602905, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3725
goal_identified
goal_identified
=== ep: 3726, time 60.27575063705444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3726
goal_identified
goal_identified
=== ep: 3727, time 56.83812928199768, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3727
=== ep: 3728, time 57.09865164756775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3728
goal_identified
=== ep: 3729, time 58.93626356124878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3729
goal_identified
goal_identified
=== ep: 3730, time 62.03259325027466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3730
goal_identified
=== ep: 3731, time 64.58724164962769, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3731
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3732, time 64.51412796974182, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3524
goal_identified
goal_identified
goal_identified
=== ep: 3733, time 66.39553737640381, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3733
goal_identified
goal_identified
goal_identified
=== ep: 3734, time 66.83031296730042, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3734
goal_identified
=== ep: 3735, time 68.7273850440979, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3735
goal_identified
goal_identified
=== ep: 3736, time 59.12857699394226, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3736
goal_identified
=== ep: 3737, time 56.89821100234985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3737
=== ep: 3738, time 59.22492456436157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3738
goal_identified
goal_identified
=== ep: 3739, time 61.55006957054138, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3739
goal_identified
goal_identified
=== ep: 3740, time 64.77111840248108, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3740
goal_identified
goal_identified
=== ep: 3741, time 67.54292845726013, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3741
goal_identified
=== ep: 3742, time 65.27277946472168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3742
goal_identified
goal_identified
=== ep: 3743, time 61.49406671524048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3743
goal_identified
goal_identified
=== ep: 3744, time 58.0875358581543, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3744
=== ep: 3745, time 58.50060153007507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3745
=== ep: 3746, time 62.42449355125427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3746
goal_identified
=== ep: 3747, time 65.44239664077759, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 67.09822988510132, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3748
goal_identified
goal_identified
=== ep: 3749, time 66.8954164981842, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3749
goal_identified
=== ep: 3750, time 57.055339097976685, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 58.25061845779419, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3751
goal_identified
=== ep: 3752, time 61.671387672424316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3752
=== ep: 3753, time 61.69258975982666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3753
=== ep: 3754, time 65.5682737827301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3754
goal_identified
goal_identified
=== ep: 3755, time 65.78729295730591, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3755
goal_identified
=== ep: 3756, time 64.96652030944824, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3756
=== ep: 3757, time 59.75587296485901, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3757
goal_identified
goal_identified
goal_identified
=== ep: 3758, time 57.90316987037659, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3758
=== ep: 3759, time 60.05774426460266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3759
goal_identified
goal_identified
=== ep: 3760, time 63.59781336784363, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3760
goal_identified
=== ep: 3761, time 65.97783398628235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3761
=== ep: 3762, time 65.67184162139893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3762
goal_identified
=== ep: 3763, time 67.8875641822815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3763
goal_identified
=== ep: 3764, time 57.252750873565674, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3764
goal_identified
=== ep: 3765, time 60.30273389816284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3765
=== ep: 3766, time 64.49325656890869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3766
goal_identified
=== ep: 3767, time 66.63989043235779, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3767
=== ep: 3768, time 64.23069047927856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3768
=== ep: 3769, time 60.54988741874695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3769
goal_identified
=== ep: 3770, time 58.41874289512634, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3770
goal_identified
goal_identified
=== ep: 3771, time 59.66459798812866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3771
=== ep: 3772, time 60.61191463470459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3772
goal_identified
=== ep: 3773, time 64.64920091629028, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3773
goal_identified
goal_identified
=== ep: 3774, time 66.48112177848816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3774
=== ep: 3775, time 64.23720002174377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3775
goal_identified
goal_identified
=== ep: 3776, time 61.83880019187927, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3776
goal_identified
=== ep: 3777, time 66.0077440738678, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3777
=== ep: 3778, time 57.99660634994507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3778
=== ep: 3779, time 59.31317329406738, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 62.578991174697876, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3780
goal_identified
=== ep: 3781, time 66.18885970115662, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3781
goal_identified
goal_identified
=== ep: 3782, time 66.30600142478943, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3782
=== ep: 3783, time 63.84431314468384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3783
goal_identified
goal_identified
=== ep: 3784, time 59.98272657394409, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3784
goal_identified
=== ep: 3785, time 56.725549936294556, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3785
goal_identified
=== ep: 3786, time 60.54518675804138, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3786
=== ep: 3787, time 62.95124006271362, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3787
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 66.2343487739563, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3597
goal_identified
goal_identified
=== ep: 3789, time 67.41780924797058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3789
goal_identified
=== ep: 3790, time 63.83452129364014, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3790
goal_identified
=== ep: 3791, time 68.46537375450134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3791
goal_identified
=== ep: 3792, time 57.24471473693848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3792
=== ep: 3793, time 61.161139249801636, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3793
goal_identified
=== ep: 3794, time 64.86677980422974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3794
=== ep: 3795, time 66.78354978561401, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3795
goal_identified
=== ep: 3796, time 64.82004714012146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3796
goal_identified
=== ep: 3797, time 61.58782601356506, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3797
=== ep: 3798, time 58.97073745727539, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3798
goal_identified
goal_identified
=== ep: 3799, time 60.57346820831299, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3799
goal_identified
=== ep: 3800, time 65.31080937385559, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3800
=== ep: 3801, time 66.57826471328735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3801
=== ep: 3802, time 65.72538447380066, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3802
goal_identified
goal_identified
=== ep: 3803, time 62.33774948120117, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3803
goal_identified
=== ep: 3804, time 58.845009326934814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3804
goal_identified
=== ep: 3805, time 68.21140098571777, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3805
goal_identified
goal_identified
=== ep: 3806, time 67.26551032066345, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3806
goal_identified
=== ep: 3807, time 66.70846724510193, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3807
=== ep: 3808, time 63.837029457092285, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3808
goal_identified
=== ep: 3809, time 62.40996980667114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3809
goal_identified
=== ep: 3810, time 59.94570875167847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3810
goal_identified
=== ep: 3811, time 58.55647087097168, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3811
=== ep: 3812, time 61.85309338569641, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3812
goal_identified
goal_identified
=== ep: 3813, time 64.70639848709106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3813
=== ep: 3814, time 66.9752881526947, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3814
goal_identified
=== ep: 3815, time 65.51942706108093, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3815
=== ep: 3816, time 60.52608776092529, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3816
=== ep: 3817, time 58.05873656272888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3817
goal_identified
goal_identified
goal_identified
=== ep: 3818, time 61.32767987251282, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3818
goal_identified
goal_identified
goal_identified
=== ep: 3819, time 72.73329305648804, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3819
goal_identified
=== ep: 3820, time 66.22058153152466, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3820
goal_identified
goal_identified
=== ep: 3821, time 61.8948712348938, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3821
goal_identified
goal_identified
=== ep: 3822, time 58.89376759529114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3822
goal_identified
goal_identified
goal_identified
=== ep: 3823, time 57.069176197052, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3823
goal_identified
goal_identified
=== ep: 3824, time 61.08593988418579, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3824
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3825, time 63.106112241744995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3826, time 67.08456087112427, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3683
goal_identified
=== ep: 3827, time 67.22416567802429, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3827
=== ep: 3828, time 67.36950397491455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3828
=== ep: 3829, time 65.33909034729004, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3829
goal_identified
=== ep: 3830, time 62.51401925086975, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3830
goal_identified
goal_identified
=== ep: 3831, time 59.70890164375305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3831
goal_identified
goal_identified
=== ep: 3832, time 67.73944687843323, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3832
goal_identified
goal_identified
=== ep: 3833, time 64.76964855194092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3833
goal_identified
goal_identified
=== ep: 3834, time 68.08597612380981, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3834
goal_identified
=== ep: 3835, time 67.46845817565918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3835
goal_identified
goal_identified
=== ep: 3836, time 63.734071016311646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3836
goal_identified
=== ep: 3837, time 55.12633228302002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3837
=== ep: 3838, time 55.950786113739014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3838
goal_identified
=== ep: 3839, time 55.044846057891846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3839
goal_identified
goal_identified
=== ep: 3840, time 56.077380895614624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3840
goal_identified
=== ep: 3841, time 56.196396350860596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3841
goal_identified
=== ep: 3842, time 55.44794750213623, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3842
goal_identified
goal_identified
=== ep: 3843, time 55.48344039916992, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3843
=== ep: 3844, time 55.76597261428833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3844
=== ep: 3845, time 61.717418909072876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3845
=== ep: 3846, time 55.901450634002686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3846
=== ep: 3847, time 55.07610726356506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3847
=== ep: 3848, time 55.345245122909546, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3848
goal_identified
=== ep: 3849, time 56.20065450668335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3849
goal_identified
goal_identified
goal_identified
=== ep: 3850, time 55.95392656326294, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3850
goal_identified
goal_identified
=== ep: 3851, time 55.6759729385376, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3851
=== ep: 3852, time 56.01404809951782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3852
=== ep: 3853, time 55.932751417160034, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3853
=== ep: 3854, time 55.77501320838928, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3854
goal_identified
=== ep: 3855, time 55.634499311447144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3855
goal_identified
=== ep: 3856, time 55.44889521598816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3856
=== ep: 3857, time 55.73881697654724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3857
=== ep: 3858, time 55.57213592529297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3858
goal_identified
goal_identified
=== ep: 3859, time 61.01060247421265, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3859
goal_identified
=== ep: 3860, time 55.60410141944885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3860
goal_identified
=== ep: 3861, time 56.24250793457031, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3861
goal_identified
goal_identified
=== ep: 3862, time 56.19762349128723, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3862
goal_identified
goal_identified
=== ep: 3863, time 55.639846086502075, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3863
goal_identified
=== ep: 3864, time 55.593464612960815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3864
goal_identified
=== ep: 3865, time 55.957547664642334, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3865
goal_identified
goal_identified
=== ep: 3866, time 56.20489454269409, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3866
goal_identified
=== ep: 3867, time 55.39595699310303, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3867
goal_identified
=== ep: 3868, time 55.499459743499756, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3868
goal_identified
goal_identified
=== ep: 3869, time 55.32502555847168, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3869
goal_identified
=== ep: 3870, time 55.45206356048584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3870
goal_identified
goal_identified
=== ep: 3871, time 56.17926573753357, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3871
goal_identified
=== ep: 3872, time 61.522616147994995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3872
goal_identified
=== ep: 3873, time 55.65104126930237, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3873
goal_identified
goal_identified
=== ep: 3874, time 55.69834041595459, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3874
goal_identified
=== ep: 3875, time 55.61638569831848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3875
goal_identified
=== ep: 3876, time 55.56967210769653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3876
=== ep: 3877, time 55.686644315719604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3877
goal_identified
=== ep: 3878, time 55.64075803756714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3878
=== ep: 3879, time 55.80695343017578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3879
goal_identified
goal_identified
goal_identified
=== ep: 3880, time 55.61773729324341, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3880
=== ep: 3881, time 55.5205397605896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3881
=== ep: 3882, time 55.083024740219116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3882
goal_identified
=== ep: 3883, time 55.005791664123535, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3883
goal_identified
=== ep: 3884, time 55.981117248535156, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3884
=== ep: 3885, time 55.512993574142456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3885
goal_identified
goal_identified
=== ep: 3886, time 61.550376176834106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3886
=== ep: 3887, time 55.113643407821655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3887
goal_identified
=== ep: 3888, time 55.3312304019928, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3888
=== ep: 3889, time 55.99434518814087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3889
goal_identified
=== ep: 3890, time 55.73229670524597, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3890
=== ep: 3891, time 55.65534520149231, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3891
=== ep: 3892, time 55.49649453163147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3892
=== ep: 3893, time 55.257622718811035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3893
goal_identified
=== ep: 3894, time 55.98122024536133, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3894
goal_identified
goal_identified
=== ep: 3895, time 55.11993408203125, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3895
goal_identified
goal_identified
=== ep: 3896, time 55.54281234741211, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3896
goal_identified
goal_identified
=== ep: 3897, time 55.66617226600647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3897
goal_identified
goal_identified
=== ep: 3898, time 56.07643413543701, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3898
goal_identified
goal_identified
=== ep: 3899, time 55.365917682647705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3899
=== ep: 3900, time 61.16181397438049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3900
goal_identified
=== ep: 3901, time 55.74273896217346, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3901
goal_identified
=== ep: 3902, time 55.53218102455139, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3902
=== ep: 3903, time 56.039186239242554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3903
goal_identified
goal_identified
=== ep: 3904, time 55.737276792526245, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3904
goal_identified
goal_identified
=== ep: 3905, time 55.74780821800232, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3905
goal_identified
=== ep: 3906, time 55.705894947052, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3906
goal_identified
=== ep: 3907, time 55.72607111930847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3907
goal_identified
goal_identified
=== ep: 3908, time 55.662399768829346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3908
goal_identified
=== ep: 3909, time 56.139814615249634, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3909
goal_identified
=== ep: 3910, time 55.93303465843201, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3910
goal_identified
goal_identified
=== ep: 3911, time 55.619889974594116, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3911
goal_identified
goal_identified
=== ep: 3912, time 56.02769875526428, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3912
goal_identified
=== ep: 3913, time 55.895057678222656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3913
goal_identified
goal_identified
=== ep: 3914, time 61.307024240493774, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3914
goal_identified
=== ep: 3915, time 55.446749687194824, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3915
goal_identified
goal_identified
=== ep: 3916, time 56.238837003707886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3916
goal_identified
=== ep: 3917, time 55.682422399520874, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3917
=== ep: 3918, time 55.960182189941406, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3918
goal_identified
=== ep: 3919, time 56.05168175697327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3919
goal_identified
=== ep: 3920, time 56.0068416595459, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3920
=== ep: 3921, time 55.85454726219177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3921
goal_identified
goal_identified
=== ep: 3922, time 55.55150318145752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3922
=== ep: 3923, time 55.30762720108032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3923
goal_identified
=== ep: 3924, time 55.802297830581665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3924
goal_identified
goal_identified
=== ep: 3925, time 56.2025830745697, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3925
=== ep: 3926, time 55.5875403881073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3926
goal_identified
=== ep: 3927, time 55.80711269378662, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3927
goal_identified
goal_identified
=== ep: 3928, time 62.0960419178009, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3928
goal_identified
=== ep: 3929, time 56.24972343444824, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3929
goal_identified
=== ep: 3930, time 55.979825496673584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3930
=== ep: 3931, time 55.372493267059326, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3931
=== ep: 3932, time 55.82728433609009, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3932
goal_identified
=== ep: 3933, time 55.773061990737915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3933
goal_identified
=== ep: 3934, time 55.856677293777466, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3934
=== ep: 3935, time 55.19132709503174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3935
=== ep: 3936, time 55.246723890304565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3936
goal_identified
=== ep: 3937, time 55.59983801841736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3937
goal_identified
=== ep: 3938, time 55.31159043312073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3938
goal_identified
=== ep: 3939, time 56.029160022735596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3939
goal_identified
=== ep: 3940, time 55.70463705062866, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3940
goal_identified
=== ep: 3941, time 55.589031457901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3941
=== ep: 3942, time 60.87319040298462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3942
goal_identified
=== ep: 3943, time 55.65896487236023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3943
=== ep: 3944, time 55.81413674354553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3944
goal_identified
=== ep: 3945, time 55.848647117614746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3945
goal_identified
goal_identified
=== ep: 3946, time 56.00801920890808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3946
goal_identified
=== ep: 3947, time 56.07544922828674, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3947
goal_identified
=== ep: 3948, time 55.44126534461975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3948
goal_identified
=== ep: 3949, time 55.56941294670105, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3949
goal_identified
=== ep: 3950, time 55.95172595977783, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3950
goal_identified
=== ep: 3951, time 55.66352725028992, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3951
goal_identified
goal_identified
=== ep: 3952, time 55.884347677230835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3952
goal_identified
=== ep: 3953, time 55.396604776382446, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3953
goal_identified
=== ep: 3954, time 55.71422624588013, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3954
goal_identified
goal_identified
goal_identified
=== ep: 3955, time 55.72000336647034, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3955
goal_identified
goal_identified
=== ep: 3956, time 61.64709949493408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3956
goal_identified
goal_identified
goal_identified
=== ep: 3957, time 55.792956590652466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3957
=== ep: 3958, time 56.128634214401245, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3958
=== ep: 3959, time 55.89238905906677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3959
=== ep: 3960, time 55.55673694610596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3960
=== ep: 3961, time 55.63367199897766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3961
goal_identified
goal_identified
goal_identified
=== ep: 3962, time 55.79819178581238, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3962
=== ep: 3963, time 55.348536014556885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3963
=== ep: 3964, time 55.79483103752136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3964
=== ep: 3965, time 55.48179864883423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3965
goal_identified
goal_identified
=== ep: 3966, time 56.22647452354431, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3966
goal_identified
=== ep: 3967, time 55.79872798919678, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3967
goal_identified
=== ep: 3968, time 56.279948234558105, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3968
=== ep: 3969, time 55.95320987701416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3969
goal_identified
goal_identified
=== ep: 3970, time 61.07156014442444, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3970
=== ep: 3971, time 55.55431818962097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3971
=== ep: 3972, time 55.22570753097534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3972
goal_identified
=== ep: 3973, time 55.77151012420654, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3973
goal_identified
goal_identified
=== ep: 3974, time 55.565308809280396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3974
goal_identified
=== ep: 3975, time 55.48646640777588, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3975
goal_identified
goal_identified
=== ep: 3976, time 55.90291380882263, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3976
=== ep: 3977, time 56.027636766433716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3977
goal_identified
goal_identified
goal_identified
=== ep: 3978, time 56.53819727897644, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3978
=== ep: 3979, time 55.15135455131531, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3979
=== ep: 3980, time 55.338468074798584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3980
goal_identified
=== ep: 3981, time 55.95351481437683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3981
=== ep: 3982, time 55.370004415512085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3982
goal_identified
=== ep: 3983, time 55.39511489868164, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3983
goal_identified
goal_identified
=== ep: 3984, time 61.752073526382446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3984
goal_identified
=== ep: 3985, time 55.697391748428345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3985
goal_identified
=== ep: 3986, time 56.020957231521606, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3986
=== ep: 3987, time 55.72992253303528, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3987
goal_identified
=== ep: 3988, time 55.42343878746033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3988
=== ep: 3989, time 55.73531889915466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3989
=== ep: 3990, time 56.21295976638794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3990
goal_identified
goal_identified
=== ep: 3991, time 55.588730335235596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3991
goal_identified
=== ep: 3992, time 55.778475522994995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 55.7849862575531, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3993
goal_identified
goal_identified
goal_identified
=== ep: 3994, time 55.97925925254822, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3994
=== ep: 3995, time 55.80942440032959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3995
=== ep: 3996, time 55.80154228210449, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3996
=== ep: 3997, time 55.15367889404297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3997
=== ep: 3998, time 61.216309547424316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3998
goal_identified
goal_identified
goal_identified
=== ep: 3999, time 55.800679445266724, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
