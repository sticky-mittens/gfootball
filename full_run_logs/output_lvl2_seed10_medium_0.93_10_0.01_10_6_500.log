==> Playing in 11_vs_11_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 6 layers and 500 hidden units.
=== ep: 0, time 27.717199087142944, eps 0.9, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
goal_identified
goal_identified
=== ep: 1, time 25.561119556427002, eps 0.8561552526261419, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
goal_identified
goal_identified
goal_identified
=== ep: 2, time 25.186277389526367, eps 0.8144488388143276, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
=== ep: 3, time 25.915460109710693, eps 0.774776470806127, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
=== ep: 4, time 26.001586437225342, eps 0.7370389470171057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
=== ep: 5, time 26.565351009368896, eps 0.701141903981193, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
goal_identified
=== ep: 6, time 25.792552947998047, eps 0.6669955803928644, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
=== ep: 7, time 25.943270683288574, eps 0.6345145926571234, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
=== ep: 8, time 25.869296312332153, eps 0.6036177213860398, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
=== ep: 9, time 29.320736169815063, eps 0.5742277083079742, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
=== ep: 10, time 26.246521949768066, eps 0.5462710630816575, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 11, time 25.956977605819702, eps 0.5196778795320575, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
goal_identified
=== ep: 12, time 25.883416891098022, eps 0.49438166084852986, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
=== ep: 13, time 25.589396476745605, eps 0.47031915330815344, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
=== ep: 14, time 25.902703762054443, eps 0.4474301881084772, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 15, time 25.82449507713318, eps 0.42565753091417224, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 16, time 25.694161415100098, eps 0.4049467387413822, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 17, time 26.36674976348877, eps 0.3852460238219053, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
goal_identified
goal_identified
=== ep: 18, time 26.342066287994385, eps 0.3665061241067986, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
=== ep: 19, time 28.23479700088501, eps 0.3486801800855966, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
=== ep: 20, time 26.41449737548828, eps 0.3317236176131267, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
=== ep: 21, time 26.546730518341064, eps 0.31559403645092865, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
=== ep: 22, time 26.440711736679077, eps 0.3002511042445735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
=== ep: 23, time 26.001426219940186, eps 0.2856564556717689, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
goal_identified
=== ep: 24, time 26.510340452194214, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
=== ep: 25, time 26.272382259368896, eps 0.2585678123773109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 26, time 26.503037214279175, eps 0.24600608193757734, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
goal_identified
goal_identified
=== ep: 27, time 25.917375087738037, eps 0.23405699432065646, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
=== ep: 28, time 26.13667106628418, eps 0.22269067058350425, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
goal_identified
goal_identified
=== ep: 29, time 31.465945959091187, eps 0.2118786889963241, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 30, time 26.320611476898193, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
=== ep: 31, time 26.073673009872437, eps 0.191810928470242, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
goal_identified
=== ep: 32, time 26.828351736068726, eps 0.1825049696771952, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 33, time 26.19709277153015, eps 0.17365286785005798, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 34, time 26.29580593109131, eps 0.16523248812340846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
goal_identified
goal_identified
=== ep: 35, time 26.64170527458191, eps 0.15722277516195018, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
goal_identified
goal_identified
=== ep: 36, time 26.30802583694458, eps 0.1496037005112063, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
=== ep: 37, time 26.120224714279175, eps 0.14235621251595124, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
=== ep: 38, time 26.80106782913208, eps 0.13546218868114893, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
goal_identified
=== ep: 39, time 29.36720609664917, eps 0.1289043903562757, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
=== ep: 40, time 26.55345344543457, eps 0.12266641962971482, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
goal_identified
=== ep: 41, time 26.867689847946167, eps 0.116732678325436, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
=== ep: 42, time 26.277522563934326, eps 0.11108832899943073, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 43, time 26.586694478988647, eps 0.10571925783837377, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
=== ep: 44, time 26.308091402053833, eps 0.10061203936773815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
goal_identified
=== ep: 45, time 26.413618087768555, eps 0.09575390288111604, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
=== ep: 46, time 26.80305004119873, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 47, time 26.649174213409424, eps 0.08673687683177911, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 48, time 26.960576057434082, eps 0.08255544000718185, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
=== ep: 49, time 34.457112073898315, eps 0.07857793426293408, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 50, time 25.954062461853027, eps 0.07479441376288502, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
=== ep: 51, time 26.3020236492157, eps 0.0711954177350367, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
goal_identified
=== ep: 52, time 26.941123723983765, eps 0.06777194681468615, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 53, time 26.358157634735107, eps 0.06451544054132621, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 54, time 26.77384042739868, eps 0.06141775595303503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 26.3992440700531, eps 0.05847114722483011, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
=== ep: 56, time 31.77100396156311, eps 0.05566824630007096, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 26.91385245323181, eps 0.05300204446647978, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
goal_identified
=== ep: 58, time 26.71116042137146, eps 0.050465874830710106, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
=== ep: 59, time 30.41322612762451, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
=== ep: 60, time 27.23060154914856, eps 0.045758574462709686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 26.294941902160645, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
=== ep: 62, time 26.524479150772095, eps 0.04149923295180846, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
goal_identified
=== ep: 63, time 26.76466131210327, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
=== ep: 64, time 26.86537003517151, eps 0.03764522137655123, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
=== ep: 65, time 26.533694744110107, eps 0.03585801284071809, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
goal_identified
=== ep: 66, time 26.49078869819641, eps 0.034157967493714775, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
goal_identified
=== ep: 67, time 26.55750346183777, eps 0.03254083433665968, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
=== ep: 68, time 26.15165877342224, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
=== ep: 69, time 29.65974473953247, eps 0.02953932710388308, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
=== ep: 70, time 32.838069915771484, eps 0.028147447696664333, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
=== ep: 71, time 31.411285638809204, eps 0.026823451049161253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
=== ep: 72, time 26.49390196800232, eps 0.025564026480116013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 73, time 26.590316772460938, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
=== ep: 74, time 26.84590983390808, eps 0.02322645029683511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
goal_identified
=== ep: 75, time 26.931508779525757, eps 0.02214245352455219, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
=== ep: 76, time 26.307024478912354, eps 0.02111132389869288, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 26.837782621383667, eps 0.020130483058101077, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
goal_identified
=== ep: 78, time 26.248674154281616, eps 0.019197478389778148, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
=== ep: 79, time 28.888280153274536, eps 0.018309976896072843, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
=== ep: 80, time 26.833433628082275, eps 0.017465759360972027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
goal_identified
=== ep: 81, time 26.87579369544983, eps 0.01666271480090467, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
=== ep: 82, time 26.35572385787964, eps 0.015898835186183367, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
=== ep: 83, time 26.74259901046753, eps 0.015172210419884185, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
=== ep: 84, time 26.382107734680176, eps 0.014481023561609456, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
goal_identified
=== ep: 85, time 26.996880054473877, eps 0.01382354628419033, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
=== ep: 86, time 26.596261262893677, eps 0.013198134551968641, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
goal_identified
goal_identified
=== ep: 87, time 26.941319465637207, eps 0.012603224509851407, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
goal_identified
=== ep: 88, time 26.568593740463257, eps 0.012037328572858524, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 89, time 30.995132446289062, eps 0.011499031706385502, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
=== ep: 90, time 26.79044461250305, eps 0.010986987887879832, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
goal_identified
=== ep: 91, time 26.877830266952515, eps 0.010499916741083536, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
=== ep: 92, time 26.939069509506226, eps 0.010036600334425595, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
=== ep: 93, time 26.624722480773926, eps 0.00959588013555861, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
=== ep: 94, time 26.496994972229004, eps 0.009176654114424539, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
goal_identified
=== ep: 95, time 27.314997911453247, eps 0.00877787398760545, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
goal_identified
=== ep: 96, time 27.24260187149048, eps 0.008398542597069007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
=== ep: 97, time 26.55246376991272, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
goal_identified
=== ep: 98, time 26.725852012634277, eps 0.00769447818076098, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
goal_identified
=== ep: 99, time 30.598792791366577, eps 0.007367984627217855, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 26.99211835861206, eps 0.007057414352177835, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 101, time 27.05081582069397, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
=== ep: 102, time 26.922287702560425, eps 0.006480975162398559, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
goal_identified
goal_identified
=== ep: 103, time 26.52531409263611, eps 0.006213664849431085, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
goal_identified
goal_identified
=== ep: 104, time 26.655596017837524, eps 0.005959391414263934, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
goal_identified
=== ep: 105, time 27.1597900390625, eps 0.005717519040864065, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
goal_identified
goal_identified
=== ep: 106, time 26.90452003479004, eps 0.005487442922312285, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
=== ep: 107, time 27.131251335144043, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
goal_identified
goal_identified
=== ep: 108, time 27.48056173324585, eps 0.005060406267408787, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
goal_identified
=== ep: 109, time 29.663240909576416, eps 0.004862377916986354, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 110, time 26.91330122947693, eps 0.004674007523179196, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 29.387136220932007, eps 0.004494824061885041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 27.45765233039856, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 26.845677375793457, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 32.406381368637085, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
goal_identified
=== ep: 115, time 26.796943426132202, eps 0.0038613199360621906, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
goal_identified
goal_identified
=== ep: 116, time 26.6228609085083, eps 0.003721771716092858, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
goal_identified
goal_identified
goal_identified
=== ep: 117, time 32.134807109832764, eps 0.0035890293431213305, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
goal_identified
=== ep: 118, time 27.010799884796143, eps 0.0034627608920727634, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
=== ep: 119, time 37.47255086898804, eps 0.00334265062604924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
goal_identified
=== ep: 120, time 26.962239027023315, eps 0.0032283982068230565, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 26.728583335876465, eps 0.0031197179438347193, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
=== ep: 122, time 26.66249132156372, eps 0.0030163380798177374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
goal_identified
=== ep: 123, time 26.540437936782837, eps 0.0029180001112638996, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 27.39239811897278, eps 0.002824458142029865, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 26.991759777069092, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
=== ep: 126, time 26.824095249176025, eps 0.0026508379945489875, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
goal_identified
=== ep: 127, time 26.319998741149902, eps 0.0025703256754987464, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
=== ep: 128, time 27.0622878074646, eps 0.0024937399885833667, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
=== ep: 129, time 34.11622905731201, eps 0.0024208894296938593, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
goal_identified
=== ep: 130, time 27.105834007263184, eps 0.0023515918344868374, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
=== ep: 131, time 26.934200048446655, eps 0.002285673922878779, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 132, time 25.84831476211548, eps 0.0022229708657555565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
goal_identified
=== ep: 133, time 26.6323561668396, eps 0.0021633258728137976, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 26.87435507774353, eps 0.0021065898005034594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
goal_identified
=== ep: 135, time 26.929125547409058, eps 0.002052620779091266, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
goal_identified
=== ep: 136, time 26.79591155052185, eps 0.0020012838579124784, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 26.849454164505005, eps 0.0019524506679239415, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
goal_identified
=== ep: 138, time 26.941340684890747, eps 0.001905999100714611, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 31.864446878433228, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
goal_identified
=== ep: 140, time 26.453612565994263, eps 0.0018197818870335101, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 26.805726289749146, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 26.803304433822632, eps 0.0017417693260160481, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 27.2898428440094, eps 0.0017055928090985275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 26.544761180877686, eps 0.0016711806417306348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 26.598124742507935, eps 0.0016384467755694515, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 26.89850687980652, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 30.56186318397522, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
goal_identified
=== ep: 148, time 27.216495037078857, eps 0.0015495162322554856, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
goal_identified
=== ep: 149, time 30.342108964920044, eps 0.0015227160093621863, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
goal_identified
goal_identified
=== ep: 150, time 27.02871036529541, eps 0.0014972228487629025, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 151, time 26.912046670913696, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
goal_identified
=== ep: 152, time 28.63025426864624, eps 0.001449905838663109, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
=== ep: 153, time 26.758718252182007, eps 0.00142796367199102, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
=== ep: 154, time 27.194694995880127, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
goal_identified
goal_identified
goal_identified
=== ep: 155, time 26.447178602218628, eps 0.001387237543977543, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
goal_identified
goal_identified
=== ep: 156, time 27.370352268218994, eps 0.0013683517461028282, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
=== ep: 157, time 26.951069116592407, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 26.86197280883789, eps 0.0013332984428727204, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
goal_identified
goal_identified
=== ep: 159, time 29.588712692260742, eps 0.001317043286000802, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
=== ep: 160, time 27.15204882621765, eps 0.0013015809024843582, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 27.170063495635986, eps 0.0012868726283106018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
goal_identified
goal_identified
=== ep: 162, time 27.671223640441895, eps 0.0012728816851329014, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
=== ep: 163, time 26.936960697174072, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
=== ep: 164, time 26.906140327453613, eps 0.001246913559404956, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
=== ep: 165, time 27.099879264831543, eps 0.0012348714430141991, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
goal_identified
goal_identified
=== ep: 166, time 26.932464838027954, eps 0.0012234166275700486, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
=== ep: 167, time 27.00346326828003, eps 0.001212520470067348, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
goal_identified
=== ep: 168, time 27.697686195373535, eps 0.0012021557244367845, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 30.3264319896698, eps 0.0011922964734155277, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
=== ep: 170, time 27.465603351593018, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 26.808167934417725, eps 0.0011739970445027263, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 172, time 27.042969942092896, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
goal_identified
=== ep: 173, time 27.387146472930908, eps 0.001157439036493735, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
=== ep: 174, time 26.496996641159058, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 175, time 26.889952421188354, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
=== ep: 176, time 26.694193363189697, eps 0.0011355090345108335, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 26.96057438850403, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
goal_identified
=== ep: 178, time 27.27413558959961, eps 0.0011226136449073282, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
=== ep: 179, time 29.863836765289307, eps 0.001116633706881133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 28.76800560951233, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
goal_identified
=== ep: 181, time 27.26218605041504, eps 0.001105534542190287, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
=== ep: 182, time 27.214410305023193, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 26.995211362838745, eps 0.0010954916026690664, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
=== ep: 184, time 30.435872316360474, eps 0.001090834422251547, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
goal_identified
=== ep: 185, time 27.253708124160767, eps 0.0010864043752031938, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
goal_identified
=== ep: 186, time 26.35241985321045, eps 0.0010821903840988777, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
goal_identified
=== ep: 187, time 26.823719024658203, eps 0.0010781819117658682, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
=== ep: 188, time 27.017102479934692, eps 0.0010743689349354123, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 30.034372806549072, eps 0.0010707419191793434, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
=== ep: 190, time 26.897080659866333, eps 0.0010672917950690429, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
=== ep: 191, time 29.129703283309937, eps 0.0010640099354971456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 26.75533938407898, eps 0.0010608881341052777, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 27.048009634017944, eps 0.0010579185847638855, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 194, time 27.43399143218994, eps 0.0010550938620528466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
goal_identified
=== ep: 195, time 26.97253155708313, eps 0.001052406902694051, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
goal_identified
goal_identified
=== ep: 196, time 27.14165949821472, eps 0.001049850987889527, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 27.111528635025024, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 26.950909852981567, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
=== ep: 199, time 29.28210473060608, eps 0.001042907142909185, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
goal_identified
goal_identified
=== ep: 200, time 27.543030738830566, eps 0.001040814536856474, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
goal_identified
=== ep: 201, time 26.76016616821289, eps 0.0010388239884052469, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
goal_identified
=== ep: 202, time 27.27780556678772, eps 0.0010369305201475454, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
goal_identified
=== ep: 203, time 27.45822238922119, eps 0.0010351293974264616, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
goal_identified
=== ep: 204, time 27.41677689552307, eps 0.00103341611649703, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 26.828083992004395, eps 0.0010317863932645186, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 27.09857678413391, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
goal_identified
=== ep: 207, time 27.356735706329346, eps 0.0010287615180101426, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 28.03825330734253, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
goal_identified
goal_identified
goal_identified
=== ep: 209, time 31.49398946762085, eps 0.0010260244976950921, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
=== ep: 210, time 27.10469079017639, eps 0.0010247552679654227, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
goal_identified
goal_identified
=== ep: 211, time 27.235904216766357, eps 0.00102354793930011, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
goal_identified
=== ep: 212, time 27.41441822052002, eps 0.0010223994927486214, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
goal_identified
goal_identified
=== ep: 213, time 26.67414617538452, eps 0.001021307056596379, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 214, time 27.2911319732666, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 215, time 26.99581527709961, eps 0.0010192794220766138, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
goal_identified
=== ep: 216, time 27.41968536376953, eps 0.0010183391535666436, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
=== ep: 217, time 27.20647883415222, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
goal_identified
=== ep: 218, time 27.216742753982544, eps 0.0010165939523622068, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 219, time 33.354289531707764, eps 0.0010157846557556941, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 27.56873345375061, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 27.489381074905396, eps 0.0010142825471585687, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
=== ep: 222, time 28.774134874343872, eps 0.0010135859791140496, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 223, time 29.093730449676514, eps 0.0010129233830939361, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
=== ep: 224, time 26.958577156066895, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
=== ep: 225, time 30.127532958984375, eps 0.001011693560591007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 27.85603165626526, eps 0.0010111232589113477, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
=== ep: 227, time 26.93449378013611, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
=== ep: 228, time 27.27955436706543, eps 0.0010100647408734893, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
goal_identified
=== ep: 229, time 29.99884581565857, eps 0.001009573877668838, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 27.584946155548096, eps 0.001009106954145169, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
goal_identified
=== ep: 231, time 27.821485996246338, eps 0.0010086628027504636, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 27.511057376861572, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
goal_identified
=== ep: 233, time 26.65769648551941, eps 0.0010078384280736842, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
goal_identified
=== ep: 234, time 27.356680154800415, eps 0.001007456143425521, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
goal_identified
=== ep: 235, time 26.978497743606567, eps 0.001007092503019653, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
=== ep: 236, time 27.701965808868408, eps 0.001006746597565654, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 237, time 26.91644525527954, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
=== ep: 238, time 27.52190065383911, eps 0.0010061045739218342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 30.077502727508545, eps 0.0010058068503384884, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
goal_identified
=== ep: 240, time 27.069358587265015, eps 0.001005523646905642, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
goal_identified
goal_identified
=== ep: 241, time 27.394996404647827, eps 0.001005254255467199, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
=== ep: 242, time 27.375304222106934, eps 0.0010049980024042435, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 27.41986870765686, eps 0.0010047542469506416, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
=== ep: 244, time 27.496906518936157, eps 0.0010045223795907931, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
=== ep: 245, time 27.09652876853943, eps 0.001004301820535524, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 27.29371404647827, eps 0.0010040920182723119, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
goal_identified
=== ep: 247, time 26.974140644073486, eps 0.0010038924481862177, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
goal_identified
goal_identified
goal_identified
=== ep: 248, time 27.415865659713745, eps 0.0010037026112480747, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 249, time 44.73246884346008, eps 0.0010035220327666559, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
goal_identified
=== ep: 250, time 27.593383312225342, eps 0.0010033502612016988, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
=== ep: 251, time 27.736311674118042, eps 0.001003186867034819, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
goal_identified
=== ep: 252, time 27.64682626724243, eps 0.001003031441695491, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
goal_identified
=== ep: 253, time 27.555301427841187, eps 0.0010028835965394094, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
goal_identified
=== ep: 254, time 27.25208806991577, eps 0.0010027429618766747, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 255, time 27.524198532104492, eps 0.0010026091860473767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 27.346932411193848, eps 0.0010024819345422614, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
goal_identified
=== ep: 257, time 27.253523588180542, eps 0.0010023608891662839, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 26.960907459259033, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 32.39267015457153, eps 0.0010021362208574892, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 27.039304494857788, eps 0.001002032036136876, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
=== ep: 261, time 27.150226354599, eps 0.0010019329325650452, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
=== ep: 262, time 26.961044311523438, eps 0.0010018386623314465, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 27.294368267059326, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
goal_identified
goal_identified
=== ep: 264, time 27.687623023986816, eps 0.0010016636904766263, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
=== ep: 265, time 27.63303828239441, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 27.53608751296997, eps 0.0010015053693952815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
=== ep: 267, time 27.505051374435425, eps 0.0010014319516635345, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 27.487914323806763, eps 0.0010013621145568167, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 33.391085147857666, eps 0.0010012956834459848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 27.064563512802124, eps 0.0010012324922186594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 27.66472554206848, eps 0.001001172382863857, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 27.34446907043457, eps 0.0010011152050768812, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 27.40004062652588, eps 0.0010010608158834819, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 26.795677423477173, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
=== ep: 275, time 27.50394606590271, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 276, time 27.0982825756073, eps 0.0010009130526924313, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
goal_identified
=== ep: 277, time 27.650858640670776, eps 0.0010008685225871602, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
goal_identified
goal_identified
=== ep: 278, time 27.996875286102295, eps 0.0010008261642407504, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 279, time 32.63025879859924, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
=== ep: 280, time 27.16320013999939, eps 0.0010007475443184742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 27.293771982192993, eps 0.001000711086151851, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 27.264959812164307, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 34.13387751579285, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
goal_identified
=== ep: 284, time 30.327419996261597, eps 0.0010006120375228235, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
goal_identified
goal_identified
goal_identified
=== ep: 285, time 27.284889936447144, eps 0.0010005821881006083, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
=== ep: 286, time 27.797582149505615, eps 0.0010005537944518927, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
=== ep: 287, time 27.621607542037964, eps 0.0010005267855777657, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 27.375211715698242, eps 0.0010005010939419733, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
goal_identified
goal_identified
goal_identified
=== ep: 289, time 31.341455459594727, eps 0.001000476655302044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
goal_identified
=== ep: 290, time 27.705076217651367, eps 0.0010004534085486486, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 27.559937953948975, eps 0.0010004312955527947, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 27.105342149734497, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
goal_identified
=== ep: 293, time 32.98697853088379, eps 0.0010003902523544011, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
goal_identified
=== ep: 294, time 27.85199236869812, eps 0.0010003712195224871, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
goal_identified
goal_identified
=== ep: 295, time 26.781808614730835, eps 0.0010003531149327387, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
goal_identified
=== ep: 296, time 31.442199230194092, eps 0.0010003358933142518, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
goal_identified
goal_identified
=== ep: 297, time 26.884454011917114, eps 0.0010003195116040093, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
goal_identified
goal_identified
=== ep: 298, time 27.426409482955933, eps 0.0010003039288392032, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 30.625113487243652, eps 0.0010002891060548044, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
=== ep: 300, time 27.233917951583862, eps 0.0010002750061861312, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
goal_identified
=== ep: 301, time 27.06290626525879, eps 0.0010002615939761676, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
=== ep: 302, time 27.580869913101196, eps 0.001000248835887403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 27.171456575393677, eps 0.0010002367000179694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
=== ep: 304, time 28.107292652130127, eps 0.0010002251560218723, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
=== ep: 305, time 27.580934524536133, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
=== ep: 306, time 27.97897243499756, eps 0.0010002037295934862, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
=== ep: 307, time 27.287943840026855, eps 0.0010001937935839656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 27.64710283279419, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 30.731801748275757, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
goal_identified
goal_identified
goal_identified
=== ep: 310, time 27.338791370391846, eps 0.0010001667996834991, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
goal_identified
=== ep: 311, time 27.384716510772705, eps 0.001000158664766942, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
goal_identified
=== ep: 312, time 27.323180675506592, eps 0.0010001509265949466, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
=== ep: 313, time 27.372443437576294, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
=== ep: 314, time 29.09809446334839, eps 0.0010001365640304844, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
goal_identified
=== ep: 315, time 27.385929584503174, eps 0.0010001299037241253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
goal_identified
=== ep: 316, time 27.768327236175537, eps 0.0010001235682447402, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
goal_identified
goal_identified
goal_identified
=== ep: 317, time 27.076894283294678, eps 0.0010001175417503308, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 27.420445919036865, eps 0.0010001118091715218, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 31.72445058822632, eps 0.0010001063561738807, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
=== ep: 320, time 26.970741510391235, eps 0.0010001011691220727, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 321, time 27.649425983428955, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
=== ep: 322, time 27.603030920028687, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
goal_identified
=== ep: 323, time 27.154003143310547, eps 0.0010000870770703358, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 34.29459571838379, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 27.542324542999268, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 27.488773584365845, eps 0.0010000749479290019, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
goal_identified
=== ep: 327, time 27.134836196899414, eps 0.001000071292675372, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 27.20434045791626, eps 0.001000067815690565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
=== ep: 329, time 30.817676782608032, eps 0.0010000645082803084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
goal_identified
goal_identified
goal_identified
=== ep: 330, time 27.325077295303345, eps 0.0010000613621743532, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
goal_identified
=== ep: 331, time 27.43862533569336, eps 0.0010000583695057963, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 27.769790172576904, eps 0.0010000555227914069, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 27.439780950546265, eps 0.0010000528149129166, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 27.472518920898438, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 335, time 27.605437517166138, eps 0.0010000477889094373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
=== ep: 336, time 28.17639994621277, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 27.344311237335205, eps 0.001000043241193426, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
goal_identified
=== ep: 338, time 28.157880783081055, eps 0.0010000411322955373, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 339, time 31.51462173461914, eps 0.0010000391262498123, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
goal_identified
=== ep: 340, time 30.691131591796875, eps 0.001000037218040092, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
goal_identified
=== ep: 341, time 27.20041537284851, eps 0.0010000354028948577, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 27.115115880966187, eps 0.0010000336762753012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 27.833860397338867, eps 0.001000032033863974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 344, time 28.01573920249939, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
goal_identified
=== ep: 345, time 27.68753433227539, eps 0.001000028985438768, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 27.62861156463623, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 28.005141973495483, eps 0.0010000262271095755, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
=== ep: 348, time 28.356751680374146, eps 0.0010000249479983478, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
=== ep: 349, time 29.798327445983887, eps 0.0010000237312701107, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 350, time 27.8611843585968, eps 0.00100002257388241, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
goal_identified
=== ep: 351, time 27.240530490875244, eps 0.0010000214729411737, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 27.754263162612915, eps 0.0010000204256934752, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 27.284523487091064, eps 0.0010000194295206493, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
goal_identified
=== ep: 354, time 27.693888425827026, eps 0.0010000184819317455, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
goal_identified
=== ep: 355, time 27.48139524459839, eps 0.001000017580557298, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
goal_identified
=== ep: 356, time 27.09226417541504, eps 0.001000016723143401, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
=== ep: 357, time 27.361079692840576, eps 0.0010000159075460732, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
goal_identified
=== ep: 358, time 27.636576175689697, eps 0.0010000151317258964, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
goal_identified
=== ep: 359, time 35.174723386764526, eps 0.0010000143937429161, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 27.635485649108887, eps 0.0010000136917517905, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
=== ep: 361, time 27.444202423095703, eps 0.001000013023997176, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
goal_identified
=== ep: 362, time 27.209770679473877, eps 0.0010000123888093385, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 27.282085180282593, eps 0.0010000117845999773, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
=== ep: 364, time 28.549641132354736, eps 0.0010000112098582543, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 27.451350688934326, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 27.463018894195557, eps 0.0010000101430991996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
goal_identified
goal_identified
=== ep: 367, time 27.515588760375977, eps 0.0010000096484144142, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
goal_identified
=== ep: 368, time 28.034674167633057, eps 0.0010000091778556905, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
goal_identified
=== ep: 369, time 31.07074213027954, eps 0.0010000087302463867, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
=== ep: 370, time 27.273963928222656, eps 0.001000008304467246, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 27.347804307937622, eps 0.0010000078994535993, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 27.15449857711792, eps 0.0010000075141927012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
=== ep: 373, time 27.46264147758484, eps 0.0010000071477211988, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
=== ep: 374, time 27.391655206680298, eps 0.0010000067991227223, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
=== ep: 375, time 27.243572235107422, eps 0.0010000064675255943, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 376, time 27.478536367416382, eps 0.001000006152100649, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 27.120604276657104, eps 0.0010000058520591598, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 27.441465139389038, eps 0.0010000055666508666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
goal_identified
=== ep: 379, time 31.263186931610107, eps 0.0010000052951621003, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 27.362751960754395, eps 0.0010000050369139975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
=== ep: 381, time 27.137476921081543, eps 0.001000004791260803, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
=== ep: 382, time 28.981895446777344, eps 0.0010000045575882562, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
=== ep: 383, time 27.855860948562622, eps 0.001000004335312054, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
goal_identified
=== ep: 384, time 27.12900710105896, eps 0.0010000041238763903, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 27.593531608581543, eps 0.0010000039227525655, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 27.77180552482605, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 27.71714687347412, eps 0.001000003549453303, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
=== ep: 388, time 27.394014358520508, eps 0.0010000033763444226, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 389, time 31.231502294540405, eps 0.001000003211678162, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 27.351673364639282, eps 0.0010000030550427698, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
goal_identified
goal_identified
=== ep: 391, time 27.137903928756714, eps 0.0010000029060465757, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 27.350617170333862, eps 0.0010000027643170119, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 393, time 27.715149641036987, eps 0.0010000026294996803, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
=== ep: 394, time 27.240878343582153, eps 0.0010000025012574677, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
goal_identified
goal_identified
=== ep: 395, time 27.449203729629517, eps 0.0010000023792697014, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
=== ep: 396, time 27.26441192626953, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
goal_identified
goal_identified
=== ep: 397, time 27.50024724006653, eps 0.0010000021528522535, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
=== ep: 398, time 27.44760012626648, eps 0.00100000204785641, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 34.79287314414978, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
goal_identified
goal_identified
=== ep: 400, time 27.318493366241455, eps 0.0010000018529771066, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
=== ep: 401, time 27.917633295059204, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
goal_identified
=== ep: 402, time 26.886029720306396, eps 0.0010000016766430208, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 26.960686445236206, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 27.432767152786255, eps 0.001000001517089342, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
goal_identified
=== ep: 405, time 27.60101342201233, eps 0.0010000014431000217, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 27.475502729415894, eps 0.001000001372719203, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
goal_identified
goal_identified
=== ep: 407, time 27.15740990638733, eps 0.0010000013057708975, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 27.33913516998291, eps 0.0010000012420876994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
goal_identified
=== ep: 409, time 31.526614665985107, eps 0.0010000011815103674, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 410, time 27.32547950744629, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
goal_identified
goal_identified
=== ep: 411, time 27.144105672836304, eps 0.0010000010690747903, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 27.221068859100342, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 27.721537590026855, eps 0.0010000009673388729, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
goal_identified
=== ep: 414, time 28.030092477798462, eps 0.0010000009201611994, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
=== ep: 415, time 27.463309288024902, eps 0.0010000008752844081, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 27.84845495223999, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
=== ep: 417, time 27.765357494354248, eps 0.001000000791990084, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
goal_identified
=== ep: 418, time 27.175251722335815, eps 0.0010000007533642718, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
goal_identified
=== ep: 419, time 31.16805624961853, eps 0.0010000007166222626, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 27.702139854431152, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 28.092482328414917, eps 0.001000000648426638, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 27.802950143814087, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
goal_identified
goal_identified
goal_identified
=== ep: 423, time 27.598309993743896, eps 0.0010000005867206849, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
=== ep: 424, time 27.49340844154358, eps 0.0010000005581059794, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 425, time 27.738539695739746, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
goal_identified
=== ep: 426, time 27.723217010498047, eps 0.0010000005049951733, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 27.943259716033936, eps 0.001000000480366268, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
goal_identified
=== ep: 428, time 27.62432289123535, eps 0.0010000004569385287, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
=== ep: 429, time 33.39871168136597, eps 0.0010000004346533736, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
=== ep: 430, time 27.85157299041748, eps 0.0010000004134550786, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
=== ep: 431, time 27.855438709259033, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
goal_identified
=== ep: 432, time 27.52571725845337, eps 0.0010000003741096257, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
goal_identified
=== ep: 433, time 27.27099871635437, eps 0.001000000355864084, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
goal_identified
goal_identified
=== ep: 434, time 26.908793449401855, eps 0.0010000003385083878, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
=== ep: 435, time 27.633054971694946, eps 0.001000000321999139, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 27.454294443130493, eps 0.0010000003062950555, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 27.33958387374878, eps 0.0010000002913568694, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
=== ep: 438, time 27.382503032684326, eps 0.0010000002771472273, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
goal_identified
=== ep: 439, time 31.828096628189087, eps 0.0010000002636305976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
=== ep: 440, time 27.68844175338745, eps 0.0010000002507731815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 27.428994178771973, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
=== ep: 442, time 27.76644492149353, eps 0.0010000002269089582, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
goal_identified
=== ep: 443, time 28.169107913970947, eps 0.0010000002158424776, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
goal_identified
=== ep: 444, time 27.32068705558777, eps 0.0010000002053157158, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 27.569156885147095, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
=== ep: 446, time 27.99740433692932, eps 0.001000000185777342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
goal_identified
goal_identified
=== ep: 447, time 27.896220922470093, eps 0.0010000001767168742, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
=== ep: 448, time 27.819851398468018, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
goal_identified
=== ep: 449, time 32.050052642822266, eps 0.0010000001599000403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
=== ep: 450, time 27.151851415634155, eps 0.0010000001521016232, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
goal_identified
=== ep: 451, time 26.87519073486328, eps 0.0010000001446835395, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 28.070462942123413, eps 0.0010000001376272401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 27.416375875473022, eps 0.0010000001309150804, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
=== ep: 454, time 27.783195972442627, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
=== ep: 455, time 27.505942583084106, eps 0.0010000001184568633, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
=== ep: 456, time 27.496390342712402, eps 0.0010000001126796538, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
goal_identified
goal_identified
=== ep: 457, time 27.32032585144043, eps 0.0010000001071842023, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
goal_identified
=== ep: 458, time 27.56085467338562, eps 0.001000000101956767, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 32.981682777404785, eps 0.001000000096984277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
=== ep: 460, time 27.747854471206665, eps 0.001000000092254298, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
=== ep: 461, time 27.671675443649292, eps 0.0010000000877550027, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 30.58328127861023, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 27.417892694473267, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 27.314331769943237, eps 0.0010000000755314307, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
goal_identified
=== ep: 465, time 27.54947543144226, eps 0.0010000000718477194, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 27.749761819839478, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
goal_identified
=== ep: 467, time 27.725425720214844, eps 0.001000000065010505, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 27.704293251037598, eps 0.0010000000618399052, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
goal_identified
goal_identified
=== ep: 469, time 34.21597719192505, eps 0.0010000000588239375, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
=== ep: 470, time 27.92420268058777, eps 0.0010000000559550603, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
=== ep: 471, time 27.366408586502075, eps 0.0010000000532260998, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
=== ep: 472, time 27.525092124938965, eps 0.0010000000506302322, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 473, time 27.08927607536316, eps 0.0010000000481609666, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
goal_identified
=== ep: 474, time 27.791595697402954, eps 0.0010000000458121286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
goal_identified
goal_identified
goal_identified
=== ep: 475, time 27.617615461349487, eps 0.0010000000435778447, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
goal_identified
=== ep: 476, time 30.219902753829956, eps 0.001000000041452528, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
=== ep: 477, time 27.524312019348145, eps 0.0010000000394308644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 27.991025686264038, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 479, time 31.01008367538452, eps 0.0010000000356785216, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 27.377490997314453, eps 0.0010000000339384595, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
=== ep: 481, time 27.57021164894104, eps 0.0010000000322832614, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 35.91905951499939, eps 0.0010000000307087882, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
=== ep: 483, time 27.46369504928589, eps 0.001000000029211103, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
goal_identified
=== ep: 484, time 28.302414178848267, eps 0.0010000000277864607, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
=== ep: 485, time 28.053046703338623, eps 0.0010000000264312988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
=== ep: 486, time 27.477225303649902, eps 0.0010000000251422292, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 37.43272423744202, eps 0.0010000000239160282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
=== ep: 488, time 27.493727207183838, eps 0.00100000002274963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
=== ep: 489, time 32.157522678375244, eps 0.0010000000216401172, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
goal_identified
=== ep: 490, time 27.688690662384033, eps 0.0010000000205847162, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
goal_identified
=== ep: 491, time 27.10028862953186, eps 0.0010000000195807877, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
goal_identified
=== ep: 492, time 27.858863353729248, eps 0.0010000000186258216, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 27.131644010543823, eps 0.0010000000177174295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 27.340266466140747, eps 0.0010000000168533404, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
goal_identified
goal_identified
=== ep: 495, time 27.645907640457153, eps 0.0010000000160313932, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 27.989832878112793, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 28.197339057922363, eps 0.0010000000145058043, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
=== ep: 498, time 27.788270235061646, eps 0.001000000013798348, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 499, time 32.15661334991455, eps 0.0010000000131253947, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
goal_identified
=== ep: 500, time 27.716967344284058, eps 0.0010000000124852615, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 27.27091121673584, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 27.518153429031372, eps 0.0010000000112971319, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 27.349365949630737, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 27.484644174575806, eps 0.0010000000102220676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 27.711228609085083, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
=== ep: 506, time 27.325841903686523, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 27.429497480392456, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 27.46463179588318, eps 0.0010000000083691212, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
goal_identified
=== ep: 509, time 36.23752498626709, eps 0.0010000000079609542, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
goal_identified
=== ep: 510, time 28.283078908920288, eps 0.001000000007572694, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
=== ep: 511, time 27.97443962097168, eps 0.0010000000072033692, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
goal_identified
=== ep: 512, time 27.417267084121704, eps 0.001000000006852057, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
=== ep: 513, time 27.613808631896973, eps 0.001000000006517878, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
goal_identified
=== ep: 514, time 27.54703116416931, eps 0.0010000000061999974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
goal_identified
=== ep: 515, time 27.808351278305054, eps 0.0010000000058976199, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
goal_identified
=== ep: 516, time 27.705454111099243, eps 0.0010000000056099897, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
goal_identified
goal_identified
=== ep: 517, time 28.143235206604004, eps 0.0010000000053363872, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
=== ep: 518, time 27.369896173477173, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
goal_identified
goal_identified
=== ep: 519, time 32.806905031204224, eps 0.001000000004828563, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
=== ep: 520, time 27.963551998138428, eps 0.001000000004593071, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 34.024803161621094, eps 0.0010000000043690644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 27.57936143875122, eps 0.0010000000041559827, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
goal_identified
=== ep: 523, time 27.822206258773804, eps 0.0010000000039532928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
=== ep: 524, time 27.672114372253418, eps 0.0010000000037604885, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
=== ep: 525, time 27.69727921485901, eps 0.0010000000035770874, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 28.352829456329346, eps 0.0010000000034026306, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 27.711989641189575, eps 0.0010000000032366824, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
=== ep: 528, time 34.65015482902527, eps 0.0010000000030788276, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
=== ep: 529, time 32.05014371871948, eps 0.0010000000029286714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
=== ep: 530, time 27.302721738815308, eps 0.0010000000027858384, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
goal_identified
=== ep: 531, time 27.10889768600464, eps 0.0010000000026499714, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
goal_identified
goal_identified
=== ep: 532, time 27.89394998550415, eps 0.0010000000025207308, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
=== ep: 533, time 27.516723155975342, eps 0.0010000000023977934, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
=== ep: 534, time 27.47289752960205, eps 0.0010000000022808515, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
=== ep: 535, time 27.996379137039185, eps 0.0010000000021696133, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
goal_identified
=== ep: 536, time 27.736486434936523, eps 0.0010000000020637999, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 27.655452251434326, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
=== ep: 538, time 27.81792688369751, eps 0.0010000000018674034, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 539, time 34.30632281303406, eps 0.001000000001776329, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
=== ep: 540, time 27.091792345046997, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
=== ep: 541, time 27.49375033378601, eps 0.001000000001607289, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
=== ep: 542, time 27.540223360061646, eps 0.0010000000015289005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
=== ep: 543, time 27.478201627731323, eps 0.0010000000014543352, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
=== ep: 544, time 27.86676335334778, eps 0.0010000000013834064, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 27.753981828689575, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 546, time 27.594937086105347, eps 0.0010000000012517578, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 27.76073694229126, eps 0.001000000001190709, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
=== ep: 548, time 27.384006023406982, eps 0.0010000000011326374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 36.33639359474182, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
goal_identified
=== ep: 550, time 28.11116337776184, eps 0.0010000000010248527, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 29.823619842529297, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
=== ep: 552, time 27.036577939987183, eps 0.001000000000927325, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
=== ep: 553, time 27.713404178619385, eps 0.0010000000008820989, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
goal_identified
=== ep: 554, time 28.170698881149292, eps 0.0010000000008390784, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 27.985369205474854, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
=== ep: 556, time 27.986847400665283, eps 0.0010000000007592295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 27.526063203811646, eps 0.0010000000007222014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 27.504119396209717, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
=== ep: 559, time 34.00444769859314, eps 0.001000000000653475, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 27.515798568725586, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 27.955787658691406, eps 0.0010000000005912885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
=== ep: 562, time 27.26756501197815, eps 0.0010000000005624511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
=== ep: 563, time 27.62454891204834, eps 0.00100000000053502, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 27.73030734062195, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
goal_identified
=== ep: 565, time 28.05374526977539, eps 0.001000000000484106, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 566, time 27.70728087425232, eps 0.001000000000460496, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 27.453707456588745, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 27.849382162094116, eps 0.001000000000416674, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 33.48719334602356, eps 0.0010000000003963527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
=== ep: 570, time 27.485684394836426, eps 0.0010000000003770222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
=== ep: 571, time 27.545856714248657, eps 0.0010000000003586346, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 27.296504974365234, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
goal_identified
=== ep: 573, time 27.841326475143433, eps 0.001000000000324506, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
=== ep: 574, time 27.154865503311157, eps 0.0010000000003086798, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
goal_identified
goal_identified
=== ep: 575, time 27.718860149383545, eps 0.0010000000002936252, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
=== ep: 576, time 28.14653778076172, eps 0.001000000000279305, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
goal_identified
=== ep: 577, time 27.921599864959717, eps 0.0010000000002656831, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 27.830199480056763, eps 0.0010000000002527256, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
=== ep: 579, time 36.66263961791992, eps 0.0010000000002404, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 28.52957558631897, eps 0.0010000000002286756, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
=== ep: 581, time 27.46182656288147, eps 0.0010000000002175229, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
goal_identified
=== ep: 582, time 27.305837869644165, eps 0.0010000000002069142, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 583, time 27.29021382331848, eps 0.0010000000001968228, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
goal_identified
=== ep: 584, time 27.376196146011353, eps 0.0010000000001872237, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 585, time 27.488959789276123, eps 0.0010000000001780928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 27.5562002658844, eps 0.001000000000169407, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 27.519992351531982, eps 0.001000000000161145, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
goal_identified
=== ep: 588, time 27.326643466949463, eps 0.0010000000001532858, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
=== ep: 589, time 33.409099817276, eps 0.00100000000014581, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
=== ep: 590, time 27.85301637649536, eps 0.0010000000001386988, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 28.20762276649475, eps 0.0010000000001319344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 27.76863384246826, eps 0.0010000000001255, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 27.986196041107178, eps 0.0010000000001193791, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
goal_identified
=== ep: 594, time 27.538633823394775, eps 0.001000000000113557, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
goal_identified
goal_identified
goal_identified
=== ep: 595, time 27.643882751464844, eps 0.0010000000001080186, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
=== ep: 596, time 27.22393226623535, eps 0.0010000000001027505, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
=== ep: 597, time 27.41215229034424, eps 0.0010000000000977393, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
goal_identified
=== ep: 598, time 27.37767267227173, eps 0.0010000000000929725, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
=== ep: 599, time 33.44258999824524, eps 0.0010000000000884382, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
=== ep: 600, time 27.77843737602234, eps 0.001000000000084125, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
goal_identified
=== ep: 601, time 27.381547212600708, eps 0.0010000000000800222, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
=== ep: 602, time 27.362309455871582, eps 0.0010000000000761195, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 603, time 27.545774221420288, eps 0.0010000000000724072, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
=== ep: 604, time 27.6408371925354, eps 0.0010000000000688757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 28.145931243896484, eps 0.0010000000000655166, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 606, time 27.799614667892456, eps 0.0010000000000623215, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
=== ep: 607, time 27.794105052947998, eps 0.001000000000059282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
=== ep: 608, time 27.443686723709106, eps 0.0010000000000563907, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
=== ep: 609, time 35.31145119667053, eps 0.0010000000000536405, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
goal_identified
=== ep: 610, time 27.57301139831543, eps 0.0010000000000510245, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 27.95919895172119, eps 0.0010000000000485358, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
=== ep: 612, time 31.82269525527954, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 613, time 27.814756870269775, eps 0.0010000000000439171, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
=== ep: 614, time 27.80738401412964, eps 0.0010000000000417752, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 615, time 27.67283582687378, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
goal_identified
=== ep: 616, time 27.748754739761353, eps 0.0010000000000377999, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
=== ep: 617, time 27.79578137397766, eps 0.0010000000000359563, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
goal_identified
=== ep: 618, time 27.510882139205933, eps 0.0010000000000342027, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 33.444612979888916, eps 0.0010000000000325345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
goal_identified
=== ep: 620, time 27.92659640312195, eps 0.001000000000030948, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
=== ep: 621, time 27.925336122512817, eps 0.0010000000000294385, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
goal_identified
=== ep: 622, time 27.579965114593506, eps 0.0010000000000280028, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
=== ep: 623, time 27.79802441596985, eps 0.0010000000000266371, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
=== ep: 624, time 27.47939109802246, eps 0.001000000000025338, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
goal_identified
=== ep: 625, time 27.53391456604004, eps 0.0010000000000241023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
=== ep: 626, time 27.54549479484558, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
goal_identified
=== ep: 627, time 27.309740781784058, eps 0.0010000000000218085, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
=== ep: 628, time 27.592910051345825, eps 0.001000000000020745, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 35.275030851364136, eps 0.0010000000000197332, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
goal_identified
goal_identified
=== ep: 630, time 27.807811737060547, eps 0.0010000000000187708, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
=== ep: 631, time 27.662208080291748, eps 0.0010000000000178553, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
goal_identified
=== ep: 632, time 27.875184297561646, eps 0.0010000000000169845, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
=== ep: 633, time 27.882676601409912, eps 0.0010000000000161562, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
=== ep: 634, time 28.247348308563232, eps 0.0010000000000153684, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 31.186673402786255, eps 0.0010000000000146188, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
=== ep: 636, time 27.824135780334473, eps 0.0010000000000139058, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
=== ep: 637, time 27.42855930328369, eps 0.0010000000000132275, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 27.407872438430786, eps 0.0010000000000125824, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
goal_identified
=== ep: 639, time 33.18977332115173, eps 0.0010000000000119687, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 640, time 27.78877592086792, eps 0.001000000000011385, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
goal_identified
goal_identified
=== ep: 641, time 27.7264404296875, eps 0.00100000000001083, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
=== ep: 642, time 27.873611450195312, eps 0.0010000000000103017, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
goal_identified
=== ep: 643, time 27.55965232849121, eps 0.0010000000000097993, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 27.733070850372314, eps 0.0010000000000093213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
=== ep: 645, time 27.7648868560791, eps 0.0010000000000088666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
goal_identified
=== ep: 646, time 27.445979833602905, eps 0.0010000000000084342, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 28.10707688331604, eps 0.001000000000008023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
=== ep: 648, time 27.38264751434326, eps 0.0010000000000076317, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
goal_identified
=== ep: 649, time 32.980568170547485, eps 0.0010000000000072594, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 28.039567708969116, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
goal_identified
=== ep: 651, time 27.362390518188477, eps 0.0010000000000065686, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 27.529762029647827, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
goal_identified
goal_identified
=== ep: 653, time 27.52458882331848, eps 0.0010000000000059436, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 27.658510446548462, eps 0.0010000000000056537, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
goal_identified
goal_identified
=== ep: 655, time 28.07468271255493, eps 0.0010000000000053779, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
goal_identified
goal_identified
goal_identified
=== ep: 656, time 27.762792825698853, eps 0.0010000000000051157, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 657, time 27.261621236801147, eps 0.0010000000000048661, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
goal_identified
=== ep: 658, time 27.73650050163269, eps 0.001000000000004629, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
=== ep: 659, time 34.053553342819214, eps 0.0010000000000044032, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
=== ep: 660, time 27.865030527114868, eps 0.0010000000000041883, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 27.758525133132935, eps 0.001000000000003984, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
=== ep: 662, time 27.67891001701355, eps 0.0010000000000037897, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 27.683410167694092, eps 0.001000000000003605, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 27.73514699935913, eps 0.0010000000000034291, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
goal_identified
goal_identified
=== ep: 665, time 27.88279128074646, eps 0.001000000000003262, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
goal_identified
goal_identified
=== ep: 666, time 27.501099586486816, eps 0.0010000000000031028, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
goal_identified
=== ep: 667, time 27.98727297782898, eps 0.0010000000000029514, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
=== ep: 668, time 27.845993518829346, eps 0.0010000000000028075, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 32.25131440162659, eps 0.0010000000000026706, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 27.67907214164734, eps 0.0010000000000025403, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
goal_identified
goal_identified
=== ep: 671, time 27.667442798614502, eps 0.0010000000000024165, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
=== ep: 672, time 27.915905714035034, eps 0.0010000000000022985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
goal_identified
goal_identified
=== ep: 673, time 27.886079788208008, eps 0.0010000000000021864, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 27.59943675994873, eps 0.00100000000000208, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 27.74806308746338, eps 0.0010000000000019785, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
goal_identified
=== ep: 676, time 28.301284790039062, eps 0.001000000000001882, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
=== ep: 677, time 28.01075768470764, eps 0.0010000000000017903, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
=== ep: 678, time 27.573795318603516, eps 0.0010000000000017029, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
=== ep: 679, time 50.99643349647522, eps 0.0010000000000016198, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
goal_identified
=== ep: 680, time 27.34720277786255, eps 0.0010000000000015409, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 27.43222188949585, eps 0.0010000000000014656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 28.115854024887085, eps 0.0010000000000013943, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
goal_identified
=== ep: 683, time 27.97165060043335, eps 0.0010000000000013262, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
=== ep: 684, time 27.517499685287476, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
=== ep: 685, time 27.051661491394043, eps 0.0010000000000012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
goal_identified
=== ep: 686, time 28.0325710773468, eps 0.0010000000000011415, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
goal_identified
=== ep: 687, time 27.97336483001709, eps 0.0010000000000010857, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
goal_identified
=== ep: 688, time 27.771419763565063, eps 0.0010000000000010328, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
=== ep: 689, time 38.002267360687256, eps 0.0010000000000009825, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
=== ep: 690, time 28.299978971481323, eps 0.0010000000000009346, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 27.928991317749023, eps 0.001000000000000889, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
=== ep: 692, time 27.93838667869568, eps 0.0010000000000008457, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 27.608093738555908, eps 0.0010000000000008045, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
=== ep: 694, time 27.837191343307495, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
=== ep: 695, time 27.492414951324463, eps 0.0010000000000007277, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
=== ep: 696, time 27.42764186859131, eps 0.0010000000000006924, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
=== ep: 697, time 27.687244415283203, eps 0.0010000000000006586, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
=== ep: 698, time 27.567652702331543, eps 0.0010000000000006265, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 33.990461587905884, eps 0.001000000000000596, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
=== ep: 700, time 27.903788805007935, eps 0.0010000000000005668, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
goal_identified
=== ep: 701, time 27.369701623916626, eps 0.0010000000000005393, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
=== ep: 702, time 28.108940839767456, eps 0.0010000000000005128, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 28.060466766357422, eps 0.001000000000000488, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
=== ep: 704, time 27.66035747528076, eps 0.001000000000000464, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
=== ep: 705, time 27.789265632629395, eps 0.0010000000000004415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
=== ep: 706, time 27.452986001968384, eps 0.00100000000000042, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
goal_identified
goal_identified
goal_identified
=== ep: 707, time 27.42063283920288, eps 0.0010000000000003994, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
goal_identified
=== ep: 708, time 27.475188970565796, eps 0.00100000000000038, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
goal_identified
goal_identified
=== ep: 709, time 32.439807415008545, eps 0.0010000000000003615, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
=== ep: 710, time 31.992260932922363, eps 0.0010000000000003437, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
=== ep: 711, time 27.62231421470642, eps 0.001000000000000327, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
=== ep: 712, time 27.150020360946655, eps 0.0010000000000003112, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 27.448169231414795, eps 0.001000000000000296, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
=== ep: 714, time 28.04905652999878, eps 0.0010000000000002815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
=== ep: 715, time 28.430888891220093, eps 0.0010000000000002678, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
=== ep: 716, time 27.825785160064697, eps 0.0010000000000002548, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
=== ep: 717, time 27.73620367050171, eps 0.0010000000000002422, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
goal_identified
=== ep: 718, time 27.452227115631104, eps 0.0010000000000002305, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
=== ep: 719, time 33.57198905944824, eps 0.0010000000000002192, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
=== ep: 720, time 27.45916247367859, eps 0.0010000000000002086, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
=== ep: 721, time 32.40341353416443, eps 0.0010000000000001984, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
=== ep: 722, time 27.596861362457275, eps 0.0010000000000001887, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
goal_identified
goal_identified
=== ep: 723, time 28.00356674194336, eps 0.0010000000000001796, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
goal_identified
=== ep: 724, time 27.333916664123535, eps 0.0010000000000001707, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 27.314911127090454, eps 0.0010000000000001624, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
=== ep: 726, time 27.617705583572388, eps 0.0010000000000001544, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
goal_identified
=== ep: 727, time 27.434364795684814, eps 0.001000000000000147, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 27.348155975341797, eps 0.0010000000000001399, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 729, time 33.61426591873169, eps 0.001000000000000133, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
=== ep: 730, time 29.42218780517578, eps 0.0010000000000001264, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
=== ep: 731, time 24.38584303855896, eps 0.0010000000000001204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
=== ep: 732, time 28.904852628707886, eps 0.0010000000000001145, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
goal_identified
=== ep: 733, time 27.532649993896484, eps 0.0010000000000001089, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
goal_identified
goal_identified
=== ep: 734, time 27.741493225097656, eps 0.0010000000000001037, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
goal_identified
goal_identified
=== ep: 735, time 27.595045566558838, eps 0.0010000000000000985, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
=== ep: 736, time 27.44021511077881, eps 0.0010000000000000937, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 737, time 27.63460874557495, eps 0.0010000000000000891, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
goal_identified
goal_identified
=== ep: 738, time 27.8545880317688, eps 0.0010000000000000848, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
=== ep: 739, time 36.774192333221436, eps 0.0010000000000000807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
=== ep: 740, time 28.021031618118286, eps 0.0010000000000000768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 27.435367584228516, eps 0.001000000000000073, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
=== ep: 742, time 27.681257963180542, eps 0.0010000000000000694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
goal_identified
=== ep: 743, time 27.56625270843506, eps 0.001000000000000066, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
goal_identified
goal_identified
=== ep: 744, time 28.012285947799683, eps 0.001000000000000063, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
=== ep: 745, time 27.809748649597168, eps 0.0010000000000000599, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
goal_identified
goal_identified
=== ep: 746, time 27.976847887039185, eps 0.0010000000000000568, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
goal_identified
=== ep: 747, time 27.589056491851807, eps 0.001000000000000054, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 27.82150173187256, eps 0.0010000000000000514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
goal_identified
=== ep: 749, time 34.63942837715149, eps 0.001000000000000049, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
goal_identified
=== ep: 750, time 28.203284740447998, eps 0.0010000000000000466, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 27.427327156066895, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 27.73551893234253, eps 0.001000000000000042, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
=== ep: 753, time 27.539867877960205, eps 0.0010000000000000401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
=== ep: 754, time 27.662111043930054, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
goal_identified
=== ep: 755, time 27.677467584609985, eps 0.0010000000000000362, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 756, time 27.39013385772705, eps 0.0010000000000000345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
=== ep: 757, time 27.509852409362793, eps 0.0010000000000000328, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
=== ep: 758, time 27.439680814743042, eps 0.0010000000000000312, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 32.778767347335815, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
goal_identified
goal_identified
=== ep: 760, time 28.25743818283081, eps 0.0010000000000000282, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
goal_identified
=== ep: 761, time 27.788955211639404, eps 0.001000000000000027, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
goal_identified
=== ep: 762, time 27.22043228149414, eps 0.0010000000000000256, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
goal_identified
goal_identified
goal_identified
=== ep: 763, time 27.956809997558594, eps 0.0010000000000000243, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 28.08251976966858, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
=== ep: 765, time 32.67722725868225, eps 0.001000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
goal_identified
=== ep: 766, time 27.815958976745605, eps 0.0010000000000000208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
goal_identified
goal_identified
=== ep: 767, time 27.723705768585205, eps 0.00100000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
=== ep: 768, time 27.294369220733643, eps 0.0010000000000000189, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
goal_identified
=== ep: 769, time 33.02813744544983, eps 0.001000000000000018, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 770, time 27.819766998291016, eps 0.0010000000000000172, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
=== ep: 771, time 27.68284010887146, eps 0.0010000000000000163, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
goal_identified
goal_identified
=== ep: 772, time 27.59529137611389, eps 0.0010000000000000154, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 27.78096914291382, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
=== ep: 774, time 27.361626386642456, eps 0.0010000000000000141, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
goal_identified
goal_identified
=== ep: 775, time 27.607784509658813, eps 0.0010000000000000132, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
=== ep: 776, time 27.357097625732422, eps 0.0010000000000000126, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
=== ep: 777, time 27.700141191482544, eps 0.0010000000000000122, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
goal_identified
=== ep: 778, time 27.70988893508911, eps 0.0010000000000000115, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
goal_identified
goal_identified
=== ep: 779, time 32.94190049171448, eps 0.0010000000000000109, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
goal_identified
=== ep: 780, time 27.34662938117981, eps 0.0010000000000000104, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
=== ep: 781, time 27.58000111579895, eps 0.00100000000000001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
goal_identified
=== ep: 782, time 31.348170280456543, eps 0.0010000000000000093, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 27.6065936088562, eps 0.001000000000000009, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
goal_identified
=== ep: 784, time 27.474075078964233, eps 0.0010000000000000085, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 27.711665153503418, eps 0.001000000000000008, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
goal_identified
=== ep: 786, time 27.576905965805054, eps 0.0010000000000000076, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
goal_identified
=== ep: 787, time 27.832369565963745, eps 0.0010000000000000074, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
goal_identified
=== ep: 788, time 27.445202827453613, eps 0.001000000000000007, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
goal_identified
=== ep: 789, time 33.09312415122986, eps 0.0010000000000000067, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
=== ep: 790, time 28.207493782043457, eps 0.0010000000000000063, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
goal_identified
=== ep: 791, time 27.720500707626343, eps 0.001000000000000006, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
goal_identified
goal_identified
goal_identified
=== ep: 792, time 27.38612174987793, eps 0.0010000000000000057, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 793, time 27.461954832077026, eps 0.0010000000000000054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
=== ep: 794, time 30.867795705795288, eps 0.0010000000000000052, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 28.119539499282837, eps 0.001000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
=== ep: 796, time 27.595486402511597, eps 0.0010000000000000048, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
=== ep: 797, time 27.746095180511475, eps 0.0010000000000000044, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
goal_identified
=== ep: 798, time 27.51243495941162, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
=== ep: 799, time 33.328782081604004, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
goal_identified
=== ep: 800, time 28.16339087486267, eps 0.001000000000000004, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
=== ep: 801, time 27.97260308265686, eps 0.0010000000000000037, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
goal_identified
=== ep: 802, time 28.04381489753723, eps 0.0010000000000000035, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
goal_identified
goal_identified
goal_identified
=== ep: 803, time 27.424125909805298, eps 0.0010000000000000033, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
goal_identified
=== ep: 804, time 27.589685678482056, eps 0.001000000000000003, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
goal_identified
goal_identified
=== ep: 805, time 27.510368824005127, eps 0.001000000000000003, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
goal_identified
=== ep: 806, time 27.50983428955078, eps 0.0010000000000000028, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
goal_identified
goal_identified
=== ep: 807, time 28.001420497894287, eps 0.0010000000000000026, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
=== ep: 808, time 28.092291355133057, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
goal_identified
goal_identified
=== ep: 809, time 31.043875455856323, eps 0.0010000000000000024, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
goal_identified
=== ep: 810, time 27.58983302116394, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
goal_identified
goal_identified
=== ep: 811, time 27.812804698944092, eps 0.0010000000000000022, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
=== ep: 812, time 27.689947366714478, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
goal_identified
=== ep: 813, time 27.595107316970825, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
=== ep: 814, time 27.964471340179443, eps 0.001000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
=== ep: 815, time 28.20415210723877, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
goal_identified
=== ep: 816, time 27.986414670944214, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 27.60402250289917, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
=== ep: 818, time 27.98599624633789, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
=== ep: 819, time 32.65953612327576, eps 0.0010000000000000015, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
=== ep: 820, time 28.012319564819336, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
=== ep: 821, time 27.68494939804077, eps 0.0010000000000000013, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 27.59530782699585, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 27.120547771453857, eps 0.0010000000000000013, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
=== ep: 824, time 33.47128677368164, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
goal_identified
=== ep: 825, time 28.211256980895996, eps 0.001000000000000001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
=== ep: 826, time 25.520984172821045, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 827, time 27.825558185577393, eps 0.001000000000000001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 828, time 28.022170305252075, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 829, time 32.240297079086304, eps 0.0010000000000000009, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
goal_identified
=== ep: 830, time 27.393476009368896, eps 0.0010000000000000009, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
=== ep: 831, time 27.653131246566772, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
goal_identified
=== ep: 832, time 28.010701417922974, eps 0.0010000000000000009, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
goal_identified
=== ep: 833, time 27.53707218170166, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
=== ep: 834, time 27.588932991027832, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
=== ep: 835, time 27.41077423095703, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
=== ep: 836, time 27.371166467666626, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
=== ep: 837, time 27.787352323532104, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
goal_identified
=== ep: 838, time 27.79597759246826, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
goal_identified
=== ep: 839, time 33.43112564086914, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
=== ep: 840, time 27.645071506500244, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
=== ep: 841, time 27.736165046691895, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
goal_identified
goal_identified
=== ep: 842, time 27.641292572021484, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
=== ep: 843, time 27.367887020111084, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
=== ep: 844, time 27.474594354629517, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
goal_identified
=== ep: 845, time 27.33196210861206, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
goal_identified
goal_identified
=== ep: 846, time 27.783268451690674, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
=== ep: 847, time 27.531103134155273, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
goal_identified
=== ep: 848, time 27.66435217857361, eps 0.0010000000000000005, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
goal_identified
goal_identified
=== ep: 849, time 32.95851516723633, eps 0.0010000000000000005, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
=== ep: 850, time 27.544374465942383, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 27.781555891036987, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
=== ep: 852, time 31.060052394866943, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
goal_identified
goal_identified
=== ep: 853, time 31.108506441116333, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
goal_identified
=== ep: 854, time 27.596819162368774, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
goal_identified
=== ep: 855, time 27.35053825378418, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
=== ep: 856, time 27.661181449890137, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
=== ep: 857, time 27.577677488327026, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
=== ep: 858, time 27.662970781326294, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
goal_identified
goal_identified
goal_identified
=== ep: 859, time 32.51156258583069, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 860, time 27.7108473777771, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
goal_identified
=== ep: 861, time 27.380683660507202, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
goal_identified
goal_identified
=== ep: 862, time 28.001380681991577, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
goal_identified
=== ep: 863, time 27.668989896774292, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
=== ep: 864, time 27.860156297683716, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
=== ep: 865, time 27.36846399307251, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
goal_identified
=== ep: 866, time 27.551728010177612, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
goal_identified
=== ep: 867, time 27.734055519104004, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
=== ep: 868, time 28.25385880470276, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
goal_identified
=== ep: 869, time 31.836586475372314, eps 0.0010000000000000002, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
=== ep: 870, time 27.487573862075806, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 27.335962295532227, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
=== ep: 872, time 27.4847149848938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
goal_identified
=== ep: 873, time 27.271100282669067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
=== ep: 874, time 27.622504949569702, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
=== ep: 875, time 27.211368560791016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
goal_identified
=== ep: 876, time 27.854867935180664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 27.211451530456543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
goal_identified
goal_identified
goal_identified
=== ep: 878, time 27.29449224472046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
goal_identified
goal_identified
=== ep: 879, time 30.365324020385742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
goal_identified
goal_identified
goal_identified
=== ep: 880, time 27.812615394592285, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
=== ep: 881, time 27.63392972946167, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
goal_identified
=== ep: 882, time 27.790364742279053, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
goal_identified
=== ep: 883, time 27.98462414741516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
goal_identified
=== ep: 884, time 27.145845651626587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
=== ep: 885, time 28.27880311012268, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 886, time 27.461585521697998, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
=== ep: 887, time 27.738215446472168, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
=== ep: 888, time 28.219508171081543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
goal_identified
=== ep: 889, time 30.618935346603394, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 27.62890648841858, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
=== ep: 891, time 27.672021627426147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
goal_identified
=== ep: 892, time 27.387125730514526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
=== ep: 893, time 28.054278135299683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
goal_identified
=== ep: 894, time 27.800267457962036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
goal_identified
=== ep: 895, time 28.04046940803528, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
=== ep: 896, time 27.859498262405396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
goal_identified
goal_identified
=== ep: 897, time 27.307852506637573, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
goal_identified
=== ep: 898, time 27.749492168426514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
goal_identified
=== ep: 899, time 30.54003357887268, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
=== ep: 900, time 27.595482349395752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
goal_identified
goal_identified
=== ep: 901, time 28.099404335021973, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
goal_identified
goal_identified
=== ep: 902, time 28.07846164703369, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
goal_identified
=== ep: 903, time 27.895131587982178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
=== ep: 904, time 27.7470543384552, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
=== ep: 905, time 27.063151359558105, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
=== ep: 906, time 27.552545070648193, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
=== ep: 907, time 27.855564832687378, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
goal_identified
=== ep: 908, time 27.71599817276001, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
=== ep: 909, time 31.07276201248169, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
goal_identified
goal_identified
goal_identified
=== ep: 910, time 27.604313135147095, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
=== ep: 911, time 27.41399335861206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
goal_identified
goal_identified
=== ep: 912, time 27.671085834503174, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
=== ep: 913, time 27.594506978988647, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 27.799129009246826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
goal_identified
=== ep: 915, time 27.948960542678833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 915
goal_identified
goal_identified
=== ep: 916, time 27.736157178878784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 28.09367847442627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
=== ep: 918, time 27.592291593551636, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
=== ep: 919, time 30.01992630958557, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
goal_identified
=== ep: 920, time 27.56871199607849, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
goal_identified
goal_identified
=== ep: 921, time 27.3021297454834, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
goal_identified
goal_identified
=== ep: 922, time 27.34075379371643, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 27.463622093200684, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
=== ep: 924, time 27.427144527435303, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
=== ep: 925, time 27.945364952087402, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
=== ep: 926, time 27.421489238739014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
=== ep: 927, time 27.39483880996704, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
goal_identified
=== ep: 928, time 27.37277388572693, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
=== ep: 929, time 29.620696544647217, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
goal_identified
=== ep: 930, time 27.09343957901001, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
goal_identified
=== ep: 931, time 28.210031509399414, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
=== ep: 932, time 28.01322913169861, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
=== ep: 933, time 27.293570280075073, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
goal_identified
=== ep: 934, time 27.601449012756348, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
goal_identified
=== ep: 935, time 27.81964087486267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
goal_identified
=== ep: 936, time 27.332646131515503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
=== ep: 937, time 27.882896423339844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
goal_identified
goal_identified
=== ep: 938, time 27.379650592803955, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
goal_identified
=== ep: 939, time 32.219430923461914, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
goal_identified
=== ep: 940, time 28.0330171585083, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
goal_identified
goal_identified
goal_identified
=== ep: 941, time 27.969139575958252, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
=== ep: 942, time 27.469724893569946, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
goal_identified
=== ep: 943, time 27.321232080459595, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
=== ep: 944, time 27.618685483932495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
goal_identified
goal_identified
=== ep: 945, time 27.71503782272339, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
=== ep: 946, time 27.49774193763733, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
goal_identified
=== ep: 947, time 27.4413845539093, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
=== ep: 948, time 27.592057943344116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
goal_identified
=== ep: 949, time 30.11564064025879, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
goal_identified
goal_identified
=== ep: 950, time 27.862518310546875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
=== ep: 951, time 27.838274002075195, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
=== ep: 952, time 27.78072690963745, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
goal_identified
=== ep: 953, time 26.70243501663208, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
goal_identified
goal_identified
=== ep: 954, time 27.50565767288208, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
goal_identified
=== ep: 955, time 27.6039776802063, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
=== ep: 956, time 27.39477276802063, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
goal_identified
=== ep: 957, time 27.42428183555603, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 28.002774238586426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
goal_identified
goal_identified
goal_identified
=== ep: 959, time 30.769603729248047, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
goal_identified
=== ep: 960, time 27.612327814102173, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
=== ep: 961, time 27.796898365020752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
=== ep: 962, time 25.683792114257812, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
goal_identified
goal_identified
goal_identified
=== ep: 963, time 27.275152683258057, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
goal_identified
=== ep: 964, time 27.514389038085938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
=== ep: 965, time 27.547972440719604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
=== ep: 966, time 27.391947507858276, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 27.81859540939331, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 27.34067940711975, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
=== ep: 969, time 35.93855953216553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
goal_identified
=== ep: 970, time 27.68481993675232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
goal_identified
=== ep: 971, time 28.299487829208374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
=== ep: 972, time 27.993168830871582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
goal_identified
=== ep: 973, time 27.69133949279785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
=== ep: 974, time 27.415964126586914, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
=== ep: 975, time 27.709592580795288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
goal_identified
=== ep: 976, time 27.473994255065918, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
goal_identified
=== ep: 977, time 27.442405223846436, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
=== ep: 978, time 27.64489769935608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
=== ep: 979, time 30.499139070510864, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
goal_identified
=== ep: 980, time 27.567824840545654, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
goal_identified
=== ep: 981, time 27.551729679107666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
=== ep: 982, time 27.63334059715271, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 27.70524525642395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
=== ep: 984, time 27.48507332801819, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
=== ep: 985, time 27.916106700897217, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
goal_identified
goal_identified
=== ep: 986, time 27.860414266586304, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
=== ep: 987, time 34.460837841033936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
=== ep: 988, time 27.851486682891846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
=== ep: 989, time 30.369717597961426, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
=== ep: 990, time 27.71035885810852, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
=== ep: 991, time 27.584192991256714, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
=== ep: 992, time 27.157399892807007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
goal_identified
=== ep: 993, time 27.190659523010254, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
goal_identified
=== ep: 994, time 27.80677604675293, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
=== ep: 995, time 27.427488803863525, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
=== ep: 996, time 28.068602323532104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
goal_identified
goal_identified
=== ep: 997, time 27.848151206970215, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
goal_identified
=== ep: 998, time 27.758567810058594, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
goal_identified
=== ep: 999, time 30.10404944419861, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
goal_identified
=== ep: 1000, time 27.314741373062134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
goal_identified
goal_identified
=== ep: 1001, time 27.197685718536377, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
=== ep: 1002, time 27.606288194656372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1003, time 27.64678430557251, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
=== ep: 1004, time 27.72877264022827, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
=== ep: 1005, time 27.74325180053711, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
goal_identified
=== ep: 1006, time 27.35488748550415, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
=== ep: 1007, time 27.726957321166992, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
goal_identified
=== ep: 1008, time 27.71725606918335, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
goal_identified
=== ep: 1009, time 30.51132869720459, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
goal_identified
=== ep: 1010, time 27.65037202835083, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
=== ep: 1011, time 27.714842796325684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
=== ep: 1012, time 27.82706904411316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
=== ep: 1013, time 28.407882690429688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
goal_identified
goal_identified
=== ep: 1014, time 27.196637868881226, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
=== ep: 1015, time 27.681899547576904, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
=== ep: 1016, time 27.69106364250183, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
=== ep: 1017, time 27.901192665100098, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
goal_identified
=== ep: 1018, time 28.38184404373169, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
=== ep: 1019, time 30.154192447662354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
goal_identified
=== ep: 1020, time 27.19058132171631, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
=== ep: 1021, time 27.69609808921814, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
=== ep: 1022, time 27.78507375717163, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
goal_identified
=== ep: 1023, time 27.339790105819702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
goal_identified
=== ep: 1024, time 27.8945472240448, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
=== ep: 1025, time 27.89894986152649, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
=== ep: 1026, time 27.788777112960815, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
goal_identified
goal_identified
=== ep: 1027, time 27.419739484786987, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
=== ep: 1028, time 27.756508350372314, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
goal_identified
=== ep: 1029, time 29.984145641326904, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
=== ep: 1030, time 27.701266765594482, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
=== ep: 1031, time 27.909937858581543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
goal_identified
=== ep: 1032, time 27.700499057769775, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
goal_identified
=== ep: 1033, time 27.386841773986816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
goal_identified
=== ep: 1034, time 27.81904625892639, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
goal_identified
=== ep: 1035, time 27.121198415756226, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
goal_identified
=== ep: 1036, time 27.83131718635559, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
=== ep: 1037, time 28.175580501556396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
=== ep: 1038, time 27.448121547698975, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
goal_identified
=== ep: 1039, time 29.37365198135376, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
goal_identified
=== ep: 1040, time 27.934312105178833, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
goal_identified
=== ep: 1041, time 27.795549631118774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1041
goal_identified
=== ep: 1042, time 27.611263275146484, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
=== ep: 1043, time 27.42686891555786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
goal_identified
=== ep: 1044, time 27.592976093292236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
=== ep: 1045, time 27.814627170562744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
=== ep: 1046, time 27.94692087173462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
=== ep: 1047, time 27.698583841323853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
goal_identified
=== ep: 1048, time 27.470914363861084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
goal_identified
=== ep: 1049, time 30.48532724380493, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
=== ep: 1050, time 27.485081672668457, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
goal_identified
=== ep: 1051, time 27.533490657806396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
=== ep: 1052, time 27.726569175720215, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
=== ep: 1053, time 27.27750563621521, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
=== ep: 1054, time 27.47243618965149, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
goal_identified
goal_identified
=== ep: 1055, time 27.955079317092896, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
goal_identified
goal_identified
goal_identified
=== ep: 1056, time 27.839126110076904, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
=== ep: 1057, time 27.548932313919067, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
goal_identified
=== ep: 1058, time 27.46304154396057, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
goal_identified
=== ep: 1059, time 30.72168517112732, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
=== ep: 1060, time 27.654836416244507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
goal_identified
=== ep: 1061, time 27.5634822845459, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
goal_identified
=== ep: 1062, time 27.92128324508667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
goal_identified
=== ep: 1063, time 27.66670870780945, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
=== ep: 1064, time 27.45798897743225, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
goal_identified
goal_identified
=== ep: 1065, time 27.59503722190857, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
goal_identified
=== ep: 1066, time 27.583078622817993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
=== ep: 1067, time 27.596188068389893, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
=== ep: 1068, time 27.43811798095703, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
=== ep: 1069, time 30.04445481300354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
goal_identified
goal_identified
goal_identified
=== ep: 1070, time 27.386613845825195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
=== ep: 1071, time 27.553306102752686, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
goal_identified
=== ep: 1072, time 27.313138961791992, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
goal_identified
=== ep: 1073, time 27.857594966888428, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
=== ep: 1074, time 27.316696166992188, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
goal_identified
=== ep: 1075, time 28.010194778442383, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
goal_identified
=== ep: 1076, time 27.337568759918213, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
goal_identified
goal_identified
=== ep: 1077, time 27.64481282234192, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
goal_identified
=== ep: 1078, time 27.464388132095337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
goal_identified
goal_identified
goal_identified
=== ep: 1079, time 30.12298274040222, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
=== ep: 1080, time 27.42736053466797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
=== ep: 1081, time 27.42417812347412, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
=== ep: 1082, time 27.820473194122314, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
=== ep: 1083, time 27.369693994522095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
goal_identified
=== ep: 1084, time 27.061952352523804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
=== ep: 1085, time 27.67909288406372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
goal_identified
=== ep: 1086, time 30.017112731933594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
goal_identified
=== ep: 1087, time 27.411935806274414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
=== ep: 1088, time 27.545328855514526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
goal_identified
goal_identified
=== ep: 1089, time 30.958619117736816, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
=== ep: 1090, time 27.716933488845825, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
=== ep: 1091, time 27.38750958442688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
=== ep: 1092, time 27.113919019699097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
=== ep: 1093, time 28.016712188720703, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
goal_identified
=== ep: 1094, time 27.8210608959198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
goal_identified
=== ep: 1095, time 27.61437702178955, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
goal_identified
=== ep: 1096, time 28.167651653289795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
=== ep: 1097, time 27.781189680099487, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
goal_identified
goal_identified
=== ep: 1098, time 27.361835718154907, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
goal_identified
goal_identified
=== ep: 1099, time 31.175031185150146, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
goal_identified
goal_identified
=== ep: 1100, time 26.860060453414917, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 1101, time 32.04177236557007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
goal_identified
goal_identified
=== ep: 1102, time 27.849658250808716, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
goal_identified
=== ep: 1103, time 27.845993757247925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
goal_identified
=== ep: 1104, time 27.31540083885193, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
=== ep: 1105, time 27.56876230239868, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
=== ep: 1106, time 35.199707984924316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
goal_identified
goal_identified
=== ep: 1107, time 27.621010541915894, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
goal_identified
=== ep: 1108, time 27.627146244049072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
=== ep: 1109, time 33.03955793380737, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
=== ep: 1110, time 27.725658893585205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
=== ep: 1111, time 27.932260036468506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
goal_identified
goal_identified
=== ep: 1112, time 27.557005405426025, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
=== ep: 1113, time 27.66170597076416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
goal_identified
=== ep: 1114, time 27.29182744026184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
goal_identified
goal_identified
=== ep: 1115, time 27.273444414138794, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
=== ep: 1116, time 28.125171661376953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
goal_identified
=== ep: 1117, time 28.07677459716797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
goal_identified
goal_identified
=== ep: 1118, time 27.488535404205322, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
=== ep: 1119, time 30.189504623413086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
=== ep: 1120, time 27.662151098251343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
=== ep: 1121, time 27.59976840019226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
=== ep: 1122, time 27.236603260040283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 27.246670722961426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
=== ep: 1124, time 27.442020893096924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
goal_identified
=== ep: 1125, time 27.489813804626465, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
goal_identified
=== ep: 1126, time 27.312270402908325, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
goal_identified
=== ep: 1127, time 27.525999069213867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
goal_identified
goal_identified
goal_identified
=== ep: 1128, time 27.857628345489502, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
goal_identified
goal_identified
=== ep: 1129, time 30.1803777217865, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
=== ep: 1130, time 27.737422466278076, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
=== ep: 1131, time 33.29447960853577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
=== ep: 1132, time 27.52286386489868, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
goal_identified
goal_identified
=== ep: 1133, time 27.38862705230713, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
=== ep: 1134, time 27.852059364318848, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
=== ep: 1135, time 27.57694101333618, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
goal_identified
=== ep: 1136, time 30.95565962791443, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
=== ep: 1137, time 27.300294160842896, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
=== ep: 1138, time 28.178689002990723, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
=== ep: 1139, time 31.28116011619568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
goal_identified
goal_identified
=== ep: 1140, time 27.462843656539917, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
goal_identified
=== ep: 1141, time 27.64258861541748, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
=== ep: 1142, time 30.832502603530884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
goal_identified
=== ep: 1143, time 27.10001492500305, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
=== ep: 1144, time 26.917176961898804, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
=== ep: 1145, time 26.66366171836853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
goal_identified
goal_identified
=== ep: 1146, time 26.71396040916443, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
goal_identified
=== ep: 1147, time 27.03915309906006, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
goal_identified
=== ep: 1148, time 26.833746433258057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
goal_identified
=== ep: 1149, time 30.16224455833435, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
goal_identified
=== ep: 1150, time 27.224310159683228, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
goal_identified
=== ep: 1151, time 26.404242277145386, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
goal_identified
goal_identified
=== ep: 1152, time 26.997066736221313, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
=== ep: 1153, time 26.861645221710205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
=== ep: 1154, time 26.703789949417114, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
=== ep: 1155, time 26.961636066436768, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
goal_identified
=== ep: 1156, time 33.07853555679321, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
goal_identified
=== ep: 1157, time 26.586723804473877, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
goal_identified
=== ep: 1158, time 27.964756965637207, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
goal_identified
goal_identified
=== ep: 1159, time 28.874568223953247, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
=== ep: 1160, time 26.713980436325073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
=== ep: 1161, time 31.689894914627075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
goal_identified
=== ep: 1162, time 26.93014669418335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
goal_identified
goal_identified
goal_identified
=== ep: 1163, time 26.80658745765686, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
=== ep: 1164, time 26.540512084960938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
=== ep: 1165, time 26.877500295639038, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
goal_identified
goal_identified
=== ep: 1166, time 26.588158130645752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
goal_identified
=== ep: 1167, time 26.8677875995636, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
=== ep: 1168, time 26.85080075263977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
goal_identified
=== ep: 1169, time 28.93715214729309, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
=== ep: 1170, time 26.739266872406006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
goal_identified
=== ep: 1171, time 26.867034196853638, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
goal_identified
=== ep: 1172, time 26.8475284576416, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
goal_identified
goal_identified
=== ep: 1173, time 27.0673246383667, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 26.59313678741455, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
=== ep: 1175, time 26.65454626083374, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
goal_identified
goal_identified
=== ep: 1176, time 27.036916971206665, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
goal_identified
=== ep: 1177, time 26.812415838241577, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
goal_identified
goal_identified
goal_identified
=== ep: 1178, time 26.546894550323486, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
goal_identified
=== ep: 1179, time 29.01535415649414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
=== ep: 1180, time 27.27898335456848, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
=== ep: 1181, time 26.69995880126953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
goal_identified
=== ep: 1182, time 30.293821334838867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
goal_identified
goal_identified
=== ep: 1183, time 26.58511209487915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
=== ep: 1184, time 26.859830617904663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
goal_identified
goal_identified
=== ep: 1185, time 26.55483913421631, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
goal_identified
=== ep: 1186, time 26.95893168449402, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
=== ep: 1187, time 26.923068046569824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
=== ep: 1188, time 27.218640327453613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
=== ep: 1189, time 29.67930793762207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
goal_identified
goal_identified
=== ep: 1190, time 26.96147584915161, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
=== ep: 1191, time 26.860215663909912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
goal_identified
=== ep: 1192, time 27.09361743927002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
=== ep: 1193, time 29.752097368240356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
goal_identified
=== ep: 1194, time 26.813947677612305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 26.844335079193115, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
goal_identified
goal_identified
=== ep: 1196, time 26.345178842544556, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
goal_identified
=== ep: 1197, time 27.14416265487671, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
=== ep: 1198, time 27.0655837059021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
=== ep: 1199, time 30.199039936065674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
goal_identified
goal_identified
=== ep: 1200, time 26.851741075515747, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
=== ep: 1201, time 26.928558349609375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
goal_identified
goal_identified
=== ep: 1202, time 26.782572269439697, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
=== ep: 1203, time 27.157907247543335, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
=== ep: 1204, time 26.584819793701172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
goal_identified
goal_identified
=== ep: 1205, time 27.35084366798401, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
goal_identified
=== ep: 1206, time 26.956780910491943, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
goal_identified
=== ep: 1207, time 26.6603045463562, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
goal_identified
=== ep: 1208, time 26.85337233543396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
goal_identified
=== ep: 1209, time 31.58453679084778, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
=== ep: 1210, time 27.189055919647217, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
=== ep: 1211, time 26.655083179473877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
=== ep: 1212, time 26.77917504310608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
goal_identified
=== ep: 1213, time 26.508694410324097, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
goal_identified
=== ep: 1214, time 30.31076741218567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
=== ep: 1215, time 26.703423738479614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
=== ep: 1216, time 26.888876914978027, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
=== ep: 1217, time 26.79695773124695, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
goal_identified
=== ep: 1218, time 26.773486137390137, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
goal_identified
=== ep: 1219, time 29.45650339126587, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
goal_identified
=== ep: 1220, time 26.928539752960205, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
=== ep: 1221, time 26.635086059570312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
goal_identified
=== ep: 1222, time 27.185519695281982, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
=== ep: 1223, time 26.68864130973816, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 26.832184553146362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
goal_identified
goal_identified
=== ep: 1225, time 26.74333643913269, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
=== ep: 1226, time 26.868950843811035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
=== ep: 1227, time 26.75472855567932, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1228, time 26.720433712005615, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
goal_identified
=== ep: 1229, time 29.514060497283936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
=== ep: 1230, time 30.9641854763031, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
goal_identified
=== ep: 1231, time 26.764174461364746, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
=== ep: 1232, time 26.699136972427368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
goal_identified
=== ep: 1233, time 26.63829517364502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
=== ep: 1234, time 26.638490200042725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
=== ep: 1235, time 26.90459156036377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1235
goal_identified
=== ep: 1236, time 27.175278425216675, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
goal_identified
=== ep: 1237, time 27.239299774169922, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
goal_identified
goal_identified
goal_identified
=== ep: 1238, time 26.282756805419922, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
=== ep: 1239, time 29.761642694473267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
goal_identified
=== ep: 1240, time 27.245848655700684, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
=== ep: 1241, time 27.13915228843689, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
goal_identified
=== ep: 1242, time 26.574580192565918, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
goal_identified
=== ep: 1243, time 27.80110812187195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
=== ep: 1244, time 31.653674602508545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
goal_identified
goal_identified
=== ep: 1245, time 26.945783853530884, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
goal_identified
goal_identified
=== ep: 1246, time 27.03340768814087, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
goal_identified
=== ep: 1247, time 26.883288621902466, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
goal_identified
goal_identified
goal_identified
=== ep: 1248, time 26.916152954101562, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
goal_identified
goal_identified
=== ep: 1249, time 29.69339680671692, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
goal_identified
=== ep: 1250, time 26.739506006240845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
goal_identified
goal_identified
=== ep: 1251, time 29.926923513412476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
goal_identified
=== ep: 1252, time 26.277732133865356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
=== ep: 1253, time 30.337592124938965, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
=== ep: 1254, time 27.204358100891113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
goal_identified
=== ep: 1255, time 26.774617195129395, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
=== ep: 1256, time 26.52544856071472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
goal_identified
=== ep: 1257, time 26.995121717453003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
=== ep: 1258, time 32.09663677215576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
=== ep: 1259, time 32.53578543663025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
=== ep: 1260, time 27.092366933822632, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
goal_identified
goal_identified
=== ep: 1261, time 26.50065541267395, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
goal_identified
=== ep: 1262, time 27.157746076583862, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
=== ep: 1263, time 26.87361240386963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 26.672365427017212, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
goal_identified
=== ep: 1265, time 27.289294958114624, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
=== ep: 1266, time 26.869277000427246, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
=== ep: 1267, time 26.79712200164795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
=== ep: 1268, time 26.879382610321045, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
goal_identified
=== ep: 1269, time 30.102957248687744, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
=== ep: 1270, time 31.41640305519104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
goal_identified
goal_identified
=== ep: 1271, time 26.62682294845581, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
goal_identified
=== ep: 1272, time 26.61382555961609, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
goal_identified
=== ep: 1273, time 26.570959091186523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 26.487988233566284, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
=== ep: 1275, time 26.948559999465942, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
goal_identified
=== ep: 1276, time 26.507827758789062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
goal_identified
=== ep: 1277, time 28.117318868637085, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
goal_identified
=== ep: 1278, time 26.693485975265503, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
goal_identified
=== ep: 1279, time 29.641327381134033, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
=== ep: 1280, time 27.258984565734863, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
=== ep: 1281, time 27.041545867919922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
goal_identified
=== ep: 1282, time 26.66608715057373, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
goal_identified
goal_identified
=== ep: 1283, time 27.553349494934082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
goal_identified
=== ep: 1284, time 26.57264733314514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
=== ep: 1285, time 26.99136519432068, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
=== ep: 1286, time 26.415136098861694, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
=== ep: 1287, time 26.827871561050415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
goal_identified
=== ep: 1288, time 26.755691528320312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
goal_identified
=== ep: 1289, time 29.353981494903564, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
=== ep: 1290, time 26.677462577819824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
=== ep: 1291, time 27.213677883148193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
=== ep: 1292, time 27.159135103225708, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
=== ep: 1293, time 27.11089277267456, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
=== ep: 1294, time 26.645182132720947, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
goal_identified
goal_identified
=== ep: 1295, time 26.63619875907898, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
goal_identified
=== ep: 1296, time 26.93359375, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
goal_identified
=== ep: 1297, time 27.14295983314514, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
=== ep: 1298, time 26.81298828125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 29.47673225402832, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
=== ep: 1300, time 29.798043251037598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
goal_identified
goal_identified
=== ep: 1301, time 26.647700548171997, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
=== ep: 1302, time 26.829793214797974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
=== ep: 1303, time 26.902255535125732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
=== ep: 1304, time 26.581485509872437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
=== ep: 1305, time 26.98302459716797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
goal_identified
goal_identified
=== ep: 1306, time 27.17554259300232, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
goal_identified
=== ep: 1307, time 28.921720504760742, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
=== ep: 1308, time 26.950008630752563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
goal_identified
goal_identified
=== ep: 1309, time 32.414589166641235, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
goal_identified
=== ep: 1310, time 26.664885997772217, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
=== ep: 1311, time 30.50569224357605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
=== ep: 1312, time 29.740517616271973, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
=== ep: 1313, time 27.268938302993774, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
goal_identified
goal_identified
=== ep: 1314, time 26.481661081314087, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
goal_identified
=== ep: 1315, time 26.668567180633545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
=== ep: 1316, time 27.369190454483032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
=== ep: 1317, time 26.67329478263855, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
goal_identified
goal_identified
goal_identified
=== ep: 1318, time 28.084325313568115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
=== ep: 1319, time 29.34722352027893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
goal_identified
=== ep: 1320, time 26.556357860565186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
=== ep: 1321, time 26.666269540786743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
=== ep: 1322, time 30.069355249404907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
goal_identified
=== ep: 1323, time 25.233359813690186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
=== ep: 1324, time 26.57609224319458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
goal_identified
=== ep: 1325, time 27.47803258895874, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
=== ep: 1326, time 30.558127641677856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
goal_identified
goal_identified
=== ep: 1327, time 27.026291608810425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
goal_identified
=== ep: 1328, time 26.331231117248535, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
goal_identified
=== ep: 1329, time 29.324060440063477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
=== ep: 1330, time 26.57581663131714, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
goal_identified
goal_identified
=== ep: 1331, time 26.980594635009766, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
goal_identified
=== ep: 1332, time 27.7179913520813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
=== ep: 1333, time 30.252082109451294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
=== ep: 1334, time 27.321795225143433, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
goal_identified
=== ep: 1335, time 26.820471048355103, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
=== ep: 1336, time 26.532463788986206, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
goal_identified
=== ep: 1337, time 26.581346035003662, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
=== ep: 1338, time 31.98771595954895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
=== ep: 1339, time 28.968878269195557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1340, time 27.04525351524353, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
=== ep: 1341, time 27.032057285308838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
goal_identified
=== ep: 1342, time 26.667641639709473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
=== ep: 1343, time 27.354633808135986, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
goal_identified
=== ep: 1344, time 25.93702507019043, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
goal_identified
=== ep: 1345, time 27.17821455001831, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
goal_identified
=== ep: 1346, time 26.705597400665283, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
goal_identified
goal_identified
=== ep: 1347, time 27.040699243545532, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
=== ep: 1348, time 27.187291145324707, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
=== ep: 1349, time 28.79390597343445, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
goal_identified
=== ep: 1350, time 26.40408706665039, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
=== ep: 1351, time 27.158961296081543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
goal_identified
=== ep: 1352, time 27.4835205078125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
=== ep: 1353, time 27.05252504348755, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
=== ep: 1354, time 26.616013288497925, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
goal_identified
=== ep: 1355, time 27.06549072265625, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
=== ep: 1356, time 26.9203999042511, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
goal_identified
goal_identified
=== ep: 1357, time 26.306308031082153, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
goal_identified
goal_identified
goal_identified
=== ep: 1358, time 27.194363594055176, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
=== ep: 1359, time 29.018765926361084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
=== ep: 1360, time 26.840924978256226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
=== ep: 1361, time 29.673967123031616, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
=== ep: 1362, time 26.929697513580322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
goal_identified
=== ep: 1363, time 27.01502251625061, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
goal_identified
=== ep: 1364, time 27.15404200553894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
=== ep: 1365, time 26.707378387451172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
=== ep: 1366, time 31.70246171951294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
=== ep: 1367, time 32.11713910102844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
goal_identified
=== ep: 1368, time 27.13788914680481, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
=== ep: 1369, time 28.829251766204834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
=== ep: 1370, time 27.02682900428772, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
=== ep: 1371, time 26.859741687774658, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
=== ep: 1372, time 26.367024421691895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
goal_identified
=== ep: 1373, time 26.49592661857605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
goal_identified
goal_identified
=== ep: 1374, time 27.104920387268066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
=== ep: 1375, time 26.95320749282837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
=== ep: 1376, time 26.027666330337524, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
goal_identified
goal_identified
=== ep: 1377, time 27.04753875732422, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
=== ep: 1378, time 26.875502347946167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
=== ep: 1379, time 34.64376711845398, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
=== ep: 1380, time 26.970326900482178, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
=== ep: 1381, time 27.12145495414734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
=== ep: 1382, time 25.65786576271057, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
goal_identified
=== ep: 1383, time 26.6021625995636, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
goal_identified
goal_identified
=== ep: 1384, time 27.7507963180542, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
=== ep: 1385, time 26.89601707458496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
=== ep: 1386, time 26.811798810958862, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
=== ep: 1387, time 30.24544930458069, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
=== ep: 1388, time 27.071784496307373, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
goal_identified
goal_identified
goal_identified
=== ep: 1389, time 28.600913047790527, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
=== ep: 1390, time 26.578397035598755, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
goal_identified
=== ep: 1391, time 26.855655193328857, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
=== ep: 1392, time 27.161628007888794, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
goal_identified
=== ep: 1393, time 27.05106782913208, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
goal_identified
goal_identified
goal_identified
=== ep: 1394, time 26.90553069114685, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
=== ep: 1395, time 26.32627844810486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
goal_identified
=== ep: 1396, time 27.762516021728516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
=== ep: 1397, time 26.889161348342896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
=== ep: 1398, time 26.44147253036499, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
goal_identified
goal_identified
=== ep: 1399, time 28.756235361099243, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
goal_identified
=== ep: 1400, time 26.79994225502014, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
goal_identified
goal_identified
=== ep: 1401, time 26.82824945449829, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
goal_identified
=== ep: 1402, time 26.51905584335327, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
goal_identified
=== ep: 1403, time 26.96253490447998, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
goal_identified
=== ep: 1404, time 26.66939640045166, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
goal_identified
goal_identified
=== ep: 1405, time 26.730159282684326, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
goal_identified
=== ep: 1406, time 27.424022436141968, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
goal_identified
=== ep: 1407, time 26.550101280212402, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
goal_identified
=== ep: 1408, time 26.383847951889038, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
goal_identified
goal_identified
=== ep: 1409, time 28.864042043685913, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
goal_identified
=== ep: 1410, time 26.59063959121704, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
goal_identified
=== ep: 1411, time 29.966426134109497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
goal_identified
=== ep: 1412, time 26.874882459640503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
=== ep: 1413, time 33.057095527648926, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
=== ep: 1414, time 26.87608051300049, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1415, time 27.95829486846924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
=== ep: 1416, time 27.07894515991211, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
goal_identified
goal_identified
=== ep: 1417, time 26.683383226394653, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
=== ep: 1418, time 26.654190063476562, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
=== ep: 1419, time 28.997464656829834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
goal_identified
=== ep: 1420, time 27.370354413986206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 26.60696315765381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
=== ep: 1422, time 27.213828086853027, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
goal_identified
=== ep: 1423, time 27.880926370620728, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 26.602612257003784, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
=== ep: 1425, time 26.789783239364624, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
goal_identified
goal_identified
goal_identified
=== ep: 1426, time 26.720751762390137, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
=== ep: 1427, time 26.899184226989746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
=== ep: 1428, time 27.063496112823486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
=== ep: 1429, time 32.37523126602173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
goal_identified
=== ep: 1430, time 27.24874448776245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
goal_identified
=== ep: 1431, time 26.669468879699707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
goal_identified
=== ep: 1432, time 26.296550273895264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
=== ep: 1433, time 27.24047017097473, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
=== ep: 1434, time 26.093174934387207, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
=== ep: 1435, time 26.628279209136963, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
=== ep: 1436, time 26.715765237808228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
=== ep: 1437, time 26.821351766586304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
goal_identified
=== ep: 1438, time 26.82727813720703, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
=== ep: 1439, time 28.663501739501953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
goal_identified
=== ep: 1440, time 26.84391689300537, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
=== ep: 1441, time 27.254964113235474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
goal_identified
goal_identified
=== ep: 1442, time 26.725341320037842, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
=== ep: 1443, time 31.76769208908081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
=== ep: 1444, time 26.941962718963623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
goal_identified
goal_identified
goal_identified
=== ep: 1445, time 27.109835624694824, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
=== ep: 1446, time 26.578473567962646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
=== ep: 1447, time 26.52393913269043, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
goal_identified
=== ep: 1448, time 26.840502977371216, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
goal_identified
goal_identified
=== ep: 1449, time 28.577391386032104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
=== ep: 1450, time 26.40771245956421, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
goal_identified
=== ep: 1451, time 27.057085514068604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
goal_identified
=== ep: 1452, time 27.01202917098999, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
=== ep: 1453, time 31.0434787273407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
=== ep: 1454, time 27.13702130317688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
goal_identified
=== ep: 1455, time 26.679393529891968, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
=== ep: 1456, time 26.888864755630493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
goal_identified
=== ep: 1457, time 27.157751321792603, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1458, time 26.512053966522217, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
goal_identified
=== ep: 1459, time 28.71507167816162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
goal_identified
=== ep: 1460, time 27.28176498413086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
goal_identified
=== ep: 1461, time 27.076396465301514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
goal_identified
=== ep: 1462, time 26.971160650253296, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
goal_identified
=== ep: 1463, time 26.600993633270264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
=== ep: 1464, time 26.89365267753601, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
goal_identified
goal_identified
=== ep: 1465, time 27.21989369392395, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
=== ep: 1466, time 26.193239212036133, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
=== ep: 1467, time 27.071218252182007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
goal_identified
=== ep: 1468, time 26.808830738067627, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
=== ep: 1469, time 29.22605538368225, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
=== ep: 1470, time 27.06198501586914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
=== ep: 1471, time 26.994083404541016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
=== ep: 1472, time 29.46050477027893, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
=== ep: 1473, time 26.619394063949585, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
goal_identified
=== ep: 1474, time 26.599807024002075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
=== ep: 1475, time 26.79403591156006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
goal_identified
goal_identified
=== ep: 1476, time 26.978304624557495, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
=== ep: 1477, time 26.47026300430298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
goal_identified
=== ep: 1478, time 27.424079656600952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
=== ep: 1479, time 29.50488591194153, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
goal_identified
goal_identified
=== ep: 1480, time 27.38267207145691, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
=== ep: 1481, time 31.945544958114624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
goal_identified
=== ep: 1482, time 29.355491638183594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1482
=== ep: 1483, time 26.874825716018677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
goal_identified
=== ep: 1484, time 26.58158826828003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
goal_identified
goal_identified
=== ep: 1485, time 26.811657190322876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
goal_identified
=== ep: 1486, time 26.320919036865234, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
=== ep: 1487, time 27.098029375076294, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
=== ep: 1488, time 26.873260021209717, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
=== ep: 1489, time 29.52480173110962, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
goal_identified
=== ep: 1490, time 26.71265459060669, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
=== ep: 1491, time 27.156255960464478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
goal_identified
=== ep: 1492, time 27.202523469924927, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
=== ep: 1493, time 26.496164798736572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 26.54454255104065, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
goal_identified
=== ep: 1495, time 26.564289331436157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
goal_identified
=== ep: 1496, time 27.205200910568237, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1496
=== ep: 1497, time 30.474395751953125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
goal_identified
=== ep: 1498, time 26.57642960548401, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
goal_identified
=== ep: 1499, time 29.02505397796631, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
=== ep: 1500, time 26.987374782562256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
=== ep: 1501, time 29.15302348136902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
goal_identified
goal_identified
=== ep: 1502, time 26.826024055480957, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 26.789015293121338, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
goal_identified
=== ep: 1504, time 26.88214898109436, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
goal_identified
=== ep: 1505, time 26.661977767944336, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
goal_identified
goal_identified
=== ep: 1506, time 26.55864691734314, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
=== ep: 1507, time 26.4598069190979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1507
=== ep: 1508, time 26.59562635421753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
=== ep: 1509, time 29.034970998764038, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
goal_identified
goal_identified
=== ep: 1510, time 26.356748342514038, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
=== ep: 1511, time 26.732150316238403, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
goal_identified
goal_identified
=== ep: 1512, time 27.052224159240723, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
goal_identified
goal_identified
=== ep: 1513, time 26.586555004119873, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
goal_identified
goal_identified
=== ep: 1514, time 26.625892162322998, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
=== ep: 1515, time 26.689570665359497, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
=== ep: 1516, time 26.404357194900513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
=== ep: 1517, time 26.79983425140381, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
goal_identified
goal_identified
=== ep: 1518, time 27.151092767715454, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
goal_identified
goal_identified
=== ep: 1519, time 29.052420139312744, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
goal_identified
=== ep: 1520, time 27.238264799118042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
=== ep: 1521, time 26.81145191192627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
goal_identified
=== ep: 1522, time 26.710846662521362, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
=== ep: 1523, time 26.873695135116577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
goal_identified
=== ep: 1524, time 26.58714747428894, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
=== ep: 1525, time 30.637494802474976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
goal_identified
=== ep: 1526, time 27.180436611175537, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
goal_identified
goal_identified
goal_identified
=== ep: 1527, time 26.868131160736084, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1527
=== ep: 1528, time 23.91806435585022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
=== ep: 1529, time 29.02111506462097, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
goal_identified
=== ep: 1530, time 27.02789330482483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
goal_identified
goal_identified
=== ep: 1531, time 27.075117111206055, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
=== ep: 1532, time 27.116259574890137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
=== ep: 1533, time 27.001216411590576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
goal_identified
=== ep: 1534, time 27.186394453048706, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
goal_identified
=== ep: 1535, time 26.958475828170776, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
goal_identified
=== ep: 1536, time 27.188074827194214, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
=== ep: 1537, time 26.86138343811035, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
=== ep: 1538, time 26.877822875976562, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
=== ep: 1539, time 28.889709949493408, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
goal_identified
=== ep: 1540, time 26.9853458404541, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
goal_identified
=== ep: 1541, time 26.700576305389404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
=== ep: 1542, time 26.596909999847412, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
goal_identified
=== ep: 1543, time 26.359161138534546, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
=== ep: 1544, time 27.081095457077026, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 27.017687797546387, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
goal_identified
=== ep: 1546, time 32.99368953704834, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1547, time 26.69180154800415, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
=== ep: 1548, time 27.087055206298828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
goal_identified
goal_identified
=== ep: 1549, time 28.351157426834106, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
=== ep: 1550, time 27.20490860939026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
goal_identified
=== ep: 1551, time 26.647173404693604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
=== ep: 1552, time 26.47898578643799, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
=== ep: 1553, time 26.073473930358887, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
=== ep: 1554, time 26.430885791778564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
goal_identified
goal_identified
=== ep: 1555, time 26.89318299293518, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
=== ep: 1556, time 27.015634298324585, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
=== ep: 1557, time 26.957942247390747, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
goal_identified
=== ep: 1558, time 26.877814531326294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
goal_identified
goal_identified
=== ep: 1559, time 29.32273030281067, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
goal_identified
=== ep: 1560, time 26.79345202445984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
goal_identified
=== ep: 1561, time 26.92324447631836, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
goal_identified
goal_identified
=== ep: 1562, time 27.142543077468872, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
goal_identified
goal_identified
goal_identified
=== ep: 1563, time 26.81288743019104, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
goal_identified
goal_identified
goal_identified
=== ep: 1564, time 26.885675191879272, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
goal_identified
goal_identified
=== ep: 1565, time 26.759084939956665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
goal_identified
goal_identified
=== ep: 1566, time 26.692132711410522, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
=== ep: 1567, time 26.8265323638916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
=== ep: 1568, time 26.85171127319336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
=== ep: 1569, time 28.965076208114624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
goal_identified
=== ep: 1570, time 27.13008999824524, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
goal_identified
goal_identified
=== ep: 1571, time 26.91393280029297, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
goal_identified
=== ep: 1572, time 26.976842641830444, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
goal_identified
=== ep: 1573, time 26.767462730407715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
=== ep: 1574, time 26.797122955322266, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
goal_identified
=== ep: 1575, time 26.742915391921997, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
=== ep: 1576, time 26.25047469139099, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
=== ep: 1577, time 26.89898681640625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
goal_identified
=== ep: 1578, time 26.91475772857666, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
goal_identified
=== ep: 1579, time 29.17469596862793, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1579
=== ep: 1580, time 26.847960710525513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
=== ep: 1581, time 26.981504440307617, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
=== ep: 1582, time 27.0270893573761, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
goal_identified
goal_identified
=== ep: 1583, time 27.237279653549194, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 26.925398588180542, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1584
goal_identified
goal_identified
goal_identified
=== ep: 1585, time 27.154844760894775, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
goal_identified
=== ep: 1586, time 26.556970357894897, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
goal_identified
=== ep: 1587, time 26.300352096557617, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
goal_identified
=== ep: 1588, time 26.90409755706787, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
=== ep: 1589, time 28.761141061782837, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
goal_identified
goal_identified
=== ep: 1590, time 26.74243998527527, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
goal_identified
=== ep: 1591, time 29.421632528305054, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
=== ep: 1592, time 26.884912967681885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
goal_identified
=== ep: 1593, time 27.041706800460815, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
goal_identified
goal_identified
=== ep: 1594, time 27.106966257095337, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
goal_identified
goal_identified
=== ep: 1595, time 26.719413995742798, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
=== ep: 1596, time 31.658795833587646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
goal_identified
=== ep: 1597, time 26.707253217697144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
goal_identified
=== ep: 1598, time 26.757941722869873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
goal_identified
=== ep: 1599, time 28.75825595855713, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
goal_identified
goal_identified
=== ep: 1600, time 27.124770402908325, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1600
goal_identified
=== ep: 1601, time 26.77522301673889, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
=== ep: 1602, time 27.22582459449768, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
=== ep: 1603, time 27.019721508026123, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
=== ep: 1604, time 26.72341799736023, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
=== ep: 1605, time 27.62995743751526, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
=== ep: 1606, time 26.912787675857544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
=== ep: 1607, time 27.2277193069458, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
=== ep: 1608, time 26.885847330093384, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
=== ep: 1609, time 28.69587230682373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
goal_identified
=== ep: 1610, time 30.4829363822937, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
=== ep: 1611, time 25.214040517807007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
goal_identified
=== ep: 1612, time 26.655211925506592, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
=== ep: 1613, time 27.974838972091675, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
=== ep: 1614, time 28.406896591186523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
=== ep: 1615, time 26.952893018722534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
goal_identified
=== ep: 1616, time 26.69724178314209, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
=== ep: 1617, time 29.707985877990723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
=== ep: 1618, time 26.685707330703735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
goal_identified
=== ep: 1619, time 28.696484565734863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
goal_identified
goal_identified
=== ep: 1620, time 26.6229145526886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
=== ep: 1621, time 27.017919063568115, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
=== ep: 1622, time 26.628589630126953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
=== ep: 1623, time 26.98001456260681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
=== ep: 1624, time 26.70893621444702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1625, time 26.80003333091736, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
=== ep: 1626, time 26.71900248527527, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
goal_identified
goal_identified
=== ep: 1627, time 26.93731665611267, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
goal_identified
=== ep: 1628, time 26.615840911865234, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 28.60883855819702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
goal_identified
=== ep: 1630, time 26.838301181793213, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
goal_identified
=== ep: 1631, time 26.364994049072266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
goal_identified
=== ep: 1632, time 27.122979164123535, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
=== ep: 1633, time 26.950140953063965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
goal_identified
=== ep: 1634, time 27.068179607391357, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
=== ep: 1635, time 26.99497652053833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
goal_identified
goal_identified
=== ep: 1636, time 26.50010085105896, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
goal_identified
goal_identified
=== ep: 1637, time 26.717146396636963, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
goal_identified
=== ep: 1638, time 27.25916075706482, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
=== ep: 1639, time 29.38873863220215, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
goal_identified
goal_identified
=== ep: 1640, time 26.711622714996338, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
=== ep: 1641, time 26.760063886642456, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
goal_identified
=== ep: 1642, time 26.933451175689697, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
=== ep: 1643, time 27.03929305076599, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
goal_identified
=== ep: 1644, time 29.449892044067383, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
=== ep: 1645, time 27.086397647857666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
goal_identified
=== ep: 1646, time 26.389609575271606, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
=== ep: 1647, time 26.9081027507782, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
=== ep: 1648, time 30.606062412261963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 29.068299293518066, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
goal_identified
=== ep: 1650, time 26.746714115142822, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
goal_identified
=== ep: 1651, time 26.925475120544434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
=== ep: 1652, time 26.617952585220337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
=== ep: 1653, time 27.28366231918335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
=== ep: 1654, time 30.699036598205566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
goal_identified
=== ep: 1655, time 30.54877257347107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
=== ep: 1656, time 26.99295425415039, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
=== ep: 1657, time 26.98245096206665, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
=== ep: 1658, time 26.76869821548462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
goal_identified
goal_identified
=== ep: 1659, time 28.738094329833984, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
goal_identified
=== ep: 1660, time 27.20456838607788, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
=== ep: 1661, time 26.844308137893677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
=== ep: 1662, time 26.903464317321777, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
=== ep: 1663, time 26.855876445770264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
=== ep: 1664, time 27.237160682678223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
goal_identified
=== ep: 1665, time 26.87872552871704, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
goal_identified
goal_identified
=== ep: 1666, time 26.972549200057983, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
=== ep: 1667, time 26.564239978790283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
=== ep: 1668, time 27.02665662765503, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
goal_identified
=== ep: 1669, time 29.217933177947998, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
goal_identified
goal_identified
=== ep: 1670, time 26.718870639801025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
goal_identified
goal_identified
=== ep: 1671, time 26.768216371536255, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
goal_identified
=== ep: 1672, time 26.51310896873474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
=== ep: 1673, time 27.53662085533142, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
=== ep: 1674, time 27.111148595809937, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 27.283916234970093, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
=== ep: 1676, time 26.747263193130493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1676
=== ep: 1677, time 26.895397424697876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
goal_identified
=== ep: 1678, time 26.909289121627808, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
=== ep: 1679, time 29.51311159133911, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
=== ep: 1680, time 27.05905771255493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
goal_identified
goal_identified
=== ep: 1681, time 26.709064245224, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
goal_identified
=== ep: 1682, time 26.940152883529663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
=== ep: 1683, time 34.938600301742554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
=== ep: 1684, time 26.768451929092407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
goal_identified
goal_identified
goal_identified
=== ep: 1685, time 26.692236185073853, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
=== ep: 1686, time 26.491321086883545, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
=== ep: 1687, time 27.043647527694702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
=== ep: 1688, time 27.01888155937195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
=== ep: 1689, time 28.818177461624146, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
goal_identified
goal_identified
=== ep: 1690, time 27.2372305393219, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
goal_identified
=== ep: 1691, time 31.978620290756226, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
goal_identified
=== ep: 1692, time 26.655986070632935, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
=== ep: 1693, time 27.053746700286865, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
=== ep: 1694, time 27.583361625671387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
goal_identified
goal_identified
=== ep: 1695, time 26.910738706588745, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 26.535756587982178, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
=== ep: 1697, time 27.0101056098938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
goal_identified
=== ep: 1698, time 26.540332317352295, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
goal_identified
=== ep: 1699, time 29.362449407577515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 26.881049394607544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
=== ep: 1701, time 26.905812740325928, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
=== ep: 1702, time 26.861304998397827, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1702
goal_identified
=== ep: 1703, time 26.88874578475952, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1703
goal_identified
=== ep: 1704, time 26.661876678466797, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1704
=== ep: 1705, time 24.101067781448364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 31/31)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1705
goal_identified
=== ep: 1706, time 26.91244077682495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1706
goal_identified
=== ep: 1707, time 26.779114961624146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1707
=== ep: 1708, time 26.980177879333496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1708
=== ep: 1709, time 28.56988024711609, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1709
=== ep: 1710, time 26.627712726593018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1710
goal_identified
=== ep: 1711, time 30.485999822616577, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1711
goal_identified
=== ep: 1712, time 27.060410976409912, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1712
=== ep: 1713, time 27.358155488967896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1713
goal_identified
=== ep: 1714, time 26.99561595916748, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1714
=== ep: 1715, time 27.521463632583618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1715
goal_identified
=== ep: 1716, time 26.895907640457153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1716
goal_identified
=== ep: 1717, time 26.377599954605103, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1717
goal_identified
goal_identified
=== ep: 1718, time 26.73181653022766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1718
goal_identified
=== ep: 1719, time 28.70080852508545, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1719
goal_identified
goal_identified
goal_identified
=== ep: 1720, time 27.12279200553894, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1720
=== ep: 1721, time 27.21322011947632, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1721
goal_identified
=== ep: 1722, time 26.982433557510376, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1722
=== ep: 1723, time 26.98155689239502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1723
goal_identified
goal_identified
=== ep: 1724, time 26.934017419815063, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1724
goal_identified
=== ep: 1725, time 26.75212526321411, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1725
=== ep: 1726, time 26.71994376182556, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1726
goal_identified
=== ep: 1727, time 26.728997468948364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1727
goal_identified
goal_identified
=== ep: 1728, time 27.404938220977783, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1728
goal_identified
=== ep: 1729, time 28.473853826522827, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1729
=== ep: 1730, time 27.215923309326172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1730
goal_identified
=== ep: 1731, time 27.074700832366943, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1731
=== ep: 1732, time 31.41690444946289, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1732
=== ep: 1733, time 27.276541709899902, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1733
goal_identified
goal_identified
=== ep: 1734, time 27.04087495803833, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1734
=== ep: 1735, time 30.237684726715088, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1735
=== ep: 1736, time 26.781436920166016, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1736
=== ep: 1737, time 26.614378213882446, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1737
=== ep: 1738, time 26.743515491485596, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1738
goal_identified
goal_identified
=== ep: 1739, time 28.87358522415161, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1739
goal_identified
goal_identified
=== ep: 1740, time 27.315771102905273, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1740
=== ep: 1741, time 26.69736337661743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1741
goal_identified
goal_identified
=== ep: 1742, time 27.010494709014893, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1742
goal_identified
goal_identified
=== ep: 1743, time 26.740313291549683, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1743
goal_identified
=== ep: 1744, time 26.820646286010742, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1744
goal_identified
=== ep: 1745, time 27.06286311149597, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1745
=== ep: 1746, time 27.169455766677856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1746
=== ep: 1747, time 26.686126947402954, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1747
=== ep: 1748, time 26.863288402557373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1748
=== ep: 1749, time 31.572027444839478, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1749
=== ep: 1750, time 26.30944585800171, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1750
goal_identified
goal_identified
=== ep: 1751, time 27.023699522018433, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1751
=== ep: 1752, time 26.88196325302124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1752
goal_identified
goal_identified
=== ep: 1753, time 27.130845069885254, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1753
goal_identified
=== ep: 1754, time 27.00960683822632, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1754
goal_identified
=== ep: 1755, time 26.915143966674805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1755
=== ep: 1756, time 27.058506965637207, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1756
goal_identified
=== ep: 1757, time 26.948155403137207, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1757
goal_identified
=== ep: 1758, time 26.80357265472412, eps 0.001, sum reward: 1, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1758
goal_identified
=== ep: 1759, time 29.223609685897827, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1759
goal_identified
goal_identified
=== ep: 1760, time 26.56925940513611, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1760
goal_identified
goal_identified
goal_identified
=== ep: 1761, time 27.12783455848694, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1761
goal_identified
goal_identified
=== ep: 1762, time 26.93642830848694, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1762
goal_identified
=== ep: 1763, time 27.470850229263306, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1763
goal_identified
=== ep: 1764, time 27.31841254234314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1764
goal_identified
=== ep: 1765, time 26.61383819580078, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1765
=== ep: 1766, time 26.782158613204956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1766
=== ep: 1767, time 34.33117079734802, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1767
=== ep: 1768, time 25.245418071746826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1768
=== ep: 1769, time 29.038830518722534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1769
goal_identified
goal_identified
=== ep: 1770, time 26.88204288482666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1770
=== ep: 1771, time 26.11895442008972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1771
goal_identified
=== ep: 1772, time 27.02454710006714, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1772
=== ep: 1773, time 26.752169370651245, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1773
goal_identified
=== ep: 1774, time 26.42893886566162, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1774
goal_identified
goal_identified
goal_identified
=== ep: 1775, time 26.360703706741333, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1775
=== ep: 1776, time 26.85754084587097, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1776
=== ep: 1777, time 31.091137647628784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1777
goal_identified
=== ep: 1778, time 26.563278198242188, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1778
goal_identified
goal_identified
=== ep: 1779, time 28.6022732257843, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1779
goal_identified
=== ep: 1780, time 27.189768314361572, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1780
=== ep: 1781, time 26.644752502441406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1781
=== ep: 1782, time 26.589677572250366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1782
=== ep: 1783, time 26.799602031707764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1783
=== ep: 1784, time 26.665468454360962, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1784
goal_identified
=== ep: 1785, time 26.74636697769165, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1785
=== ep: 1786, time 27.124024391174316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1786
goal_identified
=== ep: 1787, time 26.760950088500977, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1787
=== ep: 1788, time 26.988789796829224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 28.557422637939453, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
goal_identified
=== ep: 1790, time 28.37916374206543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1790
goal_identified
goal_identified
=== ep: 1791, time 26.662472009658813, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1791
=== ep: 1792, time 27.248964548110962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1792
=== ep: 1793, time 26.78773021697998, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1793
goal_identified
=== ep: 1794, time 26.753970861434937, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1794
goal_identified
=== ep: 1795, time 26.950055360794067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1795
=== ep: 1796, time 26.780540943145752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1796
=== ep: 1797, time 27.043318510055542, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1797
=== ep: 1798, time 27.45282506942749, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1798
goal_identified
=== ep: 1799, time 28.59890627861023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1799
goal_identified
=== ep: 1800, time 26.851887226104736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 26.673630952835083, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 26.80788779258728, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1802
=== ep: 1803, time 26.958781480789185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1803
=== ep: 1804, time 27.21956968307495, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1804
=== ep: 1805, time 27.315061807632446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1805
=== ep: 1806, time 26.714109659194946, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1806
goal_identified
=== ep: 1807, time 27.116137266159058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1807
=== ep: 1808, time 32.89914560317993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 29.189122438430786, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1809
=== ep: 1810, time 27.26231098175049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1810
=== ep: 1811, time 27.173625230789185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1811
=== ep: 1812, time 26.983428239822388, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1812
goal_identified
goal_identified
=== ep: 1813, time 27.058763027191162, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1813
goal_identified
=== ep: 1814, time 27.05243444442749, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1814
=== ep: 1815, time 27.324211597442627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1815
goal_identified
goal_identified
goal_identified
=== ep: 1816, time 26.485551118850708, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1816
=== ep: 1817, time 26.590742111206055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1817
=== ep: 1818, time 26.613338947296143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1818
=== ep: 1819, time 28.91335964202881, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1819
=== ep: 1820, time 26.820228099822998, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1820
goal_identified
=== ep: 1821, time 26.667075157165527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1821
goal_identified
=== ep: 1822, time 26.417057752609253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1822
goal_identified
=== ep: 1823, time 27.30200219154358, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1823
=== ep: 1824, time 26.99565291404724, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1824
goal_identified
=== ep: 1825, time 26.842307806015015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1825
=== ep: 1826, time 31.678174257278442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1826
goal_identified
goal_identified
=== ep: 1827, time 26.712738275527954, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1827
goal_identified
=== ep: 1828, time 27.141173362731934, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1828
goal_identified
goal_identified
=== ep: 1829, time 28.229445934295654, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1829
goal_identified
goal_identified
=== ep: 1830, time 26.94097638130188, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1830
goal_identified
=== ep: 1831, time 31.05641508102417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1831
=== ep: 1832, time 26.7193386554718, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1832
goal_identified
=== ep: 1833, time 26.876821041107178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1833
=== ep: 1834, time 27.100022792816162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1834
=== ep: 1835, time 35.751713037490845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1835
goal_identified
goal_identified
=== ep: 1836, time 26.393372535705566, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1836
=== ep: 1837, time 27.209735870361328, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1837
=== ep: 1838, time 26.600244998931885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1838
=== ep: 1839, time 28.854999780654907, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1839
goal_identified
=== ep: 1840, time 27.198838710784912, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1840
goal_identified
goal_identified
=== ep: 1841, time 26.96261215209961, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1841
goal_identified
=== ep: 1842, time 27.1208016872406, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1842
=== ep: 1843, time 27.330714464187622, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1843
=== ep: 1844, time 26.815392017364502, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1844
goal_identified
=== ep: 1845, time 26.710734128952026, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1845
=== ep: 1846, time 26.872267246246338, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1846
goal_identified
=== ep: 1847, time 27.5620756149292, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1847
goal_identified
goal_identified
=== ep: 1848, time 26.648523807525635, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1848
goal_identified
goal_identified
=== ep: 1849, time 28.402822494506836, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1849
goal_identified
=== ep: 1850, time 31.08911371231079, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1850
=== ep: 1851, time 26.973454475402832, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1851
=== ep: 1852, time 27.264419555664062, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1852
=== ep: 1853, time 27.215117931365967, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1853
goal_identified
=== ep: 1854, time 26.573440074920654, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1854
goal_identified
goal_identified
=== ep: 1855, time 27.199382543563843, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1855
goal_identified
=== ep: 1856, time 27.054752111434937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1856
goal_identified
=== ep: 1857, time 27.074748516082764, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1857
goal_identified
=== ep: 1858, time 26.544539213180542, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1858
=== ep: 1859, time 29.86057186126709, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1859
goal_identified
=== ep: 1860, time 27.337624073028564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1860
goal_identified
goal_identified
=== ep: 1861, time 26.923292636871338, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1861
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 26.8808012008667, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
=== ep: 1863, time 26.996070623397827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1863
goal_identified
=== ep: 1864, time 26.964548587799072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1864
=== ep: 1865, time 27.463471174240112, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1865
goal_identified
goal_identified
=== ep: 1866, time 27.11321520805359, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1866
goal_identified
goal_identified
=== ep: 1867, time 26.62755060195923, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1867
goal_identified
=== ep: 1868, time 26.75642204284668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1868
=== ep: 1869, time 32.71547722816467, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1869
goal_identified
=== ep: 1870, time 27.429229736328125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1870
=== ep: 1871, time 31.71266269683838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1871
goal_identified
=== ep: 1872, time 26.983120679855347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1872
goal_identified
=== ep: 1873, time 26.84963846206665, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1873
goal_identified
=== ep: 1874, time 27.183305025100708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1874
=== ep: 1875, time 27.187403678894043, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1875
=== ep: 1876, time 26.780173540115356, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1876
goal_identified
=== ep: 1877, time 26.742208003997803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1877
goal_identified
=== ep: 1878, time 26.66862988471985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1878
=== ep: 1879, time 33.7391631603241, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1879
=== ep: 1880, time 26.907068014144897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1880
goal_identified
goal_identified
goal_identified
=== ep: 1881, time 29.089194297790527, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1881
=== ep: 1882, time 26.843215465545654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1882
=== ep: 1883, time 27.047571659088135, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1883
goal_identified
=== ep: 1884, time 26.70160150527954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1884
goal_identified
=== ep: 1885, time 27.121994495391846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1885
goal_identified
=== ep: 1886, time 26.530561447143555, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1886
=== ep: 1887, time 26.12996554374695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1887
goal_identified
goal_identified
=== ep: 1888, time 26.900797128677368, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1888
=== ep: 1889, time 33.88966250419617, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1889
goal_identified
=== ep: 1890, time 26.562446355819702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1890
goal_identified
goal_identified
=== ep: 1891, time 27.231536865234375, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1891
goal_identified
=== ep: 1892, time 27.44836688041687, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1892
=== ep: 1893, time 27.241530895233154, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1893
=== ep: 1894, time 26.659285068511963, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1894
=== ep: 1895, time 27.100339889526367, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1895
=== ep: 1896, time 26.883482694625854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1896
=== ep: 1897, time 26.753697156906128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1897
goal_identified
goal_identified
goal_identified
=== ep: 1898, time 26.65352153778076, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1898
=== ep: 1899, time 28.977460145950317, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1899
=== ep: 1900, time 26.970372915267944, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1900
goal_identified
goal_identified
=== ep: 1901, time 26.72773838043213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1901
goal_identified
=== ep: 1902, time 26.843904972076416, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1902
goal_identified
=== ep: 1903, time 26.65175986289978, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1903
goal_identified
=== ep: 1904, time 26.758379220962524, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1904
goal_identified
=== ep: 1905, time 26.42647123336792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1905
=== ep: 1906, time 26.97168755531311, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1906
goal_identified
goal_identified
goal_identified
=== ep: 1907, time 26.401498079299927, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1907
goal_identified
goal_identified
=== ep: 1908, time 26.626608848571777, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1908
=== ep: 1909, time 28.957391500473022, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1909
=== ep: 1910, time 27.42993140220642, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1910
=== ep: 1911, time 27.073339462280273, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1911
goal_identified
goal_identified
=== ep: 1912, time 27.22366499900818, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1912
=== ep: 1913, time 26.534831523895264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1913
goal_identified
=== ep: 1914, time 26.832876205444336, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1914
goal_identified
=== ep: 1915, time 26.700978755950928, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1915
=== ep: 1916, time 26.815927982330322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1916
goal_identified
goal_identified
=== ep: 1917, time 26.404807567596436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1917
=== ep: 1918, time 26.5453097820282, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1918
=== ep: 1919, time 29.272813320159912, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1919
goal_identified
=== ep: 1920, time 26.576515436172485, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1920
=== ep: 1921, time 26.507468700408936, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1921
=== ep: 1922, time 27.16632843017578, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1922
goal_identified
=== ep: 1923, time 26.99386239051819, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1923
goal_identified
=== ep: 1924, time 26.551251888275146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1924
=== ep: 1925, time 27.33181595802307, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1925
=== ep: 1926, time 27.15034055709839, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1926
goal_identified
goal_identified
=== ep: 1927, time 26.81170415878296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1927
=== ep: 1928, time 26.629576444625854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1928
goal_identified
=== ep: 1929, time 29.028812646865845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1929
goal_identified
=== ep: 1930, time 27.396856546401978, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1930
goal_identified
=== ep: 1931, time 26.599018812179565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1931
=== ep: 1932, time 26.914144039154053, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1932
goal_identified
=== ep: 1933, time 26.565841674804688, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1933
goal_identified
=== ep: 1934, time 26.395875453948975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1934
=== ep: 1935, time 26.59996199607849, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1935
=== ep: 1936, time 26.9794282913208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1936
goal_identified
=== ep: 1937, time 26.338432788848877, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1937
goal_identified
=== ep: 1938, time 26.81962013244629, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1938
goal_identified
goal_identified
=== ep: 1939, time 28.873795747756958, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1939
goal_identified
=== ep: 1940, time 26.952069759368896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1940
=== ep: 1941, time 26.821784496307373, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1941
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1942, time 26.407588481903076, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1942
goal_identified
=== ep: 1943, time 27.137924432754517, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1943
=== ep: 1944, time 32.9942364692688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1944
goal_identified
=== ep: 1945, time 26.73888850212097, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1945
=== ep: 1946, time 27.11468267440796, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1946
goal_identified
=== ep: 1947, time 27.300023317337036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1947
goal_identified
=== ep: 1948, time 27.412012338638306, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1948
goal_identified
goal_identified
=== ep: 1949, time 28.89699387550354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1949
goal_identified
goal_identified
=== ep: 1950, time 26.483576774597168, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1950
goal_identified
goal_identified
=== ep: 1951, time 26.74464201927185, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1951
=== ep: 1952, time 29.06967520713806, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1952
goal_identified
goal_identified
=== ep: 1953, time 26.750145196914673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1953
=== ep: 1954, time 30.62353515625, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1954
goal_identified
=== ep: 1955, time 27.24688959121704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1955
goal_identified
goal_identified
goal_identified
=== ep: 1956, time 27.2003436088562, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1956
goal_identified
=== ep: 1957, time 26.870027542114258, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1957
=== ep: 1958, time 26.91424036026001, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1958
goal_identified
goal_identified
=== ep: 1959, time 28.848886013031006, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1959
=== ep: 1960, time 26.5937340259552, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1960
goal_identified
=== ep: 1961, time 26.58207106590271, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1961
=== ep: 1962, time 26.423309564590454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1962
goal_identified
=== ep: 1963, time 27.25587272644043, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1963
goal_identified
=== ep: 1964, time 26.795263290405273, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1964
goal_identified
=== ep: 1965, time 31.93978762626648, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1965
=== ep: 1966, time 27.324894189834595, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1966
=== ep: 1967, time 26.75887894630432, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1967
goal_identified
goal_identified
=== ep: 1968, time 26.98878049850464, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1968
=== ep: 1969, time 28.016757488250732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1969
=== ep: 1970, time 26.874143600463867, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1970
=== ep: 1971, time 26.73167109489441, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1971
goal_identified
=== ep: 1972, time 26.773593187332153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1972
goal_identified
=== ep: 1973, time 26.977598428726196, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1973
=== ep: 1974, time 26.719181776046753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1974
=== ep: 1975, time 26.59493660926819, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1975
=== ep: 1976, time 26.89966917037964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1976
=== ep: 1977, time 27.304720878601074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1977
goal_identified
goal_identified
=== ep: 1978, time 27.072896480560303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1978
goal_identified
=== ep: 1979, time 28.93778657913208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1979
=== ep: 1980, time 26.998878002166748, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1980
goal_identified
goal_identified
=== ep: 1981, time 26.961294651031494, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1981
goal_identified
goal_identified
=== ep: 1982, time 26.969191312789917, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1982
=== ep: 1983, time 26.738040924072266, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1983
goal_identified
=== ep: 1984, time 27.23225212097168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1984
goal_identified
=== ep: 1985, time 26.835482358932495, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1985
=== ep: 1986, time 26.589275360107422, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1986
goal_identified
=== ep: 1987, time 26.847158908843994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1987
=== ep: 1988, time 27.230271339416504, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1988
=== ep: 1989, time 28.916118621826172, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1989
goal_identified
=== ep: 1990, time 26.86100482940674, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1990
=== ep: 1991, time 30.690682649612427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1991
goal_identified
=== ep: 1992, time 26.479485273361206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1992
=== ep: 1993, time 27.215277433395386, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1993
goal_identified
=== ep: 1994, time 26.984400510787964, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1994
goal_identified
=== ep: 1995, time 27.162328243255615, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1995
goal_identified
=== ep: 1996, time 27.301952838897705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1996
goal_identified
=== ep: 1997, time 26.997349977493286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1997
=== ep: 1998, time 26.98973321914673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1998
goal_identified
=== ep: 1999, time 28.956159114837646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1999
goal_identified
=== ep: 2000, time 27.85330104827881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2000
=== ep: 2001, time 26.544578790664673, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2001
goal_identified
=== ep: 2002, time 26.593748569488525, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2002
=== ep: 2003, time 26.289010524749756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2003
=== ep: 2004, time 26.705758094787598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2004
=== ep: 2005, time 30.6519775390625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2005
=== ep: 2006, time 26.57515287399292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2006
goal_identified
=== ep: 2007, time 27.144904375076294, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2007
=== ep: 2008, time 27.529948949813843, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2008
=== ep: 2009, time 29.746098041534424, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2009
goal_identified
=== ep: 2010, time 27.01730751991272, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2010
=== ep: 2011, time 26.617858409881592, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2011
goal_identified
=== ep: 2012, time 26.77125859260559, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2012
goal_identified
=== ep: 2013, time 26.897275924682617, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2013
=== ep: 2014, time 26.82560133934021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2014
=== ep: 2015, time 32.784143924713135, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2015
=== ep: 2016, time 26.595375776290894, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2016
goal_identified
=== ep: 2017, time 26.725587368011475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2017
goal_identified
=== ep: 2018, time 27.159523963928223, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2018
=== ep: 2019, time 28.80582547187805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2019
goal_identified
goal_identified
goal_identified
=== ep: 2020, time 26.873036861419678, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2020
=== ep: 2021, time 26.86879324913025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2021
goal_identified
goal_identified
=== ep: 2022, time 26.860172033309937, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2022
=== ep: 2023, time 27.223488330841064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2023
goal_identified
=== ep: 2024, time 26.38762092590332, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2024
=== ep: 2025, time 26.936519861221313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2025
=== ep: 2026, time 27.032574892044067, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2026
goal_identified
=== ep: 2027, time 26.786858797073364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2027
=== ep: 2028, time 26.860121965408325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2028
=== ep: 2029, time 37.97405028343201, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2029
=== ep: 2030, time 26.906556367874146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2030
=== ep: 2031, time 26.897904872894287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2031
=== ep: 2032, time 27.20870041847229, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2032
goal_identified
=== ep: 2033, time 26.60581088066101, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2033
goal_identified
=== ep: 2034, time 26.699565649032593, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2034
=== ep: 2035, time 26.378435850143433, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2035
goal_identified
=== ep: 2036, time 26.385804653167725, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2036
goal_identified
goal_identified
=== ep: 2037, time 27.103691577911377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2037
=== ep: 2038, time 26.730133533477783, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2038
goal_identified
goal_identified
=== ep: 2039, time 29.189568042755127, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2039
goal_identified
goal_identified
=== ep: 2040, time 26.692241430282593, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2040
goal_identified
=== ep: 2041, time 26.724844694137573, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2041
=== ep: 2042, time 26.499614477157593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2042
goal_identified
=== ep: 2043, time 26.775936603546143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2043
goal_identified
=== ep: 2044, time 26.959410667419434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2044
=== ep: 2045, time 27.087611436843872, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2045
goal_identified
goal_identified
=== ep: 2046, time 26.612797021865845, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2046
=== ep: 2047, time 26.549514770507812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2047
=== ep: 2048, time 34.13729953765869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2048
goal_identified
=== ep: 2049, time 28.65522575378418, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2049
goal_identified
=== ep: 2050, time 26.892560720443726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2050
=== ep: 2051, time 27.217920541763306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2051
goal_identified
=== ep: 2052, time 27.132421731948853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2052
=== ep: 2053, time 26.924382209777832, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2053
=== ep: 2054, time 27.169891119003296, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2054
goal_identified
=== ep: 2055, time 26.481305360794067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2055
=== ep: 2056, time 26.8258957862854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2056
goal_identified
=== ep: 2057, time 26.327635049819946, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2057
=== ep: 2058, time 27.01494550704956, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2058
goal_identified
goal_identified
=== ep: 2059, time 28.89555048942566, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2059
goal_identified
=== ep: 2060, time 26.9874324798584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2060
goal_identified
=== ep: 2061, time 26.6863911151886, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2061
goal_identified
goal_identified
goal_identified
=== ep: 2062, time 26.80212426185608, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2062
goal_identified
=== ep: 2063, time 26.88939642906189, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2063
=== ep: 2064, time 27.383216857910156, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2064
=== ep: 2065, time 27.10991334915161, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2065
goal_identified
goal_identified
=== ep: 2066, time 27.615678548812866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2066
goal_identified
=== ep: 2067, time 26.519965171813965, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2067
goal_identified
=== ep: 2068, time 26.793737649917603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2068
=== ep: 2069, time 37.27586054801941, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2069
=== ep: 2070, time 25.61932373046875, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2070
=== ep: 2071, time 26.560274362564087, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2071
goal_identified
goal_identified
=== ep: 2072, time 27.18298316001892, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2072
goal_identified
=== ep: 2073, time 27.065953731536865, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2073
goal_identified
=== ep: 2074, time 26.589745044708252, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2074
=== ep: 2075, time 27.19122314453125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2075
=== ep: 2076, time 27.214932441711426, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2076
goal_identified
=== ep: 2077, time 27.01845073699951, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2077
=== ep: 2078, time 25.778199195861816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2078
goal_identified
=== ep: 2079, time 28.85605001449585, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2079
=== ep: 2080, time 26.987168312072754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2080
goal_identified
goal_identified
=== ep: 2081, time 27.09957480430603, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2081
goal_identified
=== ep: 2082, time 27.307916402816772, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2082
=== ep: 2083, time 26.679407119750977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2083
goal_identified
goal_identified
=== ep: 2084, time 26.514971017837524, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2084
goal_identified
=== ep: 2085, time 26.765092134475708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2085
goal_identified
=== ep: 2086, time 26.8416965007782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2086
goal_identified
goal_identified
goal_identified
=== ep: 2087, time 26.6109881401062, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2087
goal_identified
=== ep: 2088, time 26.36221194267273, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2088
goal_identified
=== ep: 2089, time 34.09878611564636, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2089
=== ep: 2090, time 30.73732280731201, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2090
goal_identified
=== ep: 2091, time 27.099643230438232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2091
=== ep: 2092, time 26.830943822860718, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2092
=== ep: 2093, time 26.67588186264038, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2093
=== ep: 2094, time 26.651750326156616, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2094
goal_identified
=== ep: 2095, time 26.95578122138977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2095
=== ep: 2096, time 27.053723096847534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2096
=== ep: 2097, time 27.224383115768433, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2097
goal_identified
=== ep: 2098, time 26.960575103759766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2098
goal_identified
goal_identified
=== ep: 2099, time 28.857424020767212, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2099
goal_identified
goal_identified
=== ep: 2100, time 26.977027416229248, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2100
=== ep: 2101, time 27.194721937179565, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2101
=== ep: 2102, time 27.54785442352295, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2102
=== ep: 2103, time 26.805901765823364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2103
=== ep: 2104, time 26.598361253738403, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2104
goal_identified
=== ep: 2105, time 26.78875494003296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2105
=== ep: 2106, time 28.384992599487305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2106
=== ep: 2107, time 30.880306720733643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2107
goal_identified
=== ep: 2108, time 27.247947454452515, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2108
=== ep: 2109, time 28.9194495677948, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2109
=== ep: 2110, time 27.361708164215088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2110
goal_identified
=== ep: 2111, time 26.883763074874878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2111
goal_identified
=== ep: 2112, time 26.439013242721558, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2112
goal_identified
goal_identified
=== ep: 2113, time 26.791292905807495, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2113
=== ep: 2114, time 26.912846565246582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2114
=== ep: 2115, time 26.469208240509033, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2115
goal_identified
=== ep: 2116, time 26.99079990386963, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2116
goal_identified
=== ep: 2117, time 26.748228788375854, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2117
=== ep: 2118, time 27.282047986984253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2118
goal_identified
goal_identified
=== ep: 2119, time 28.67000436782837, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2119
=== ep: 2120, time 26.949833631515503, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2120
goal_identified
=== ep: 2121, time 26.693453073501587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2121
goal_identified
=== ep: 2122, time 26.79302167892456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2122
=== ep: 2123, time 27.09610891342163, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2123
goal_identified
goal_identified
=== ep: 2124, time 27.05728816986084, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2124
=== ep: 2125, time 26.642569303512573, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2125
goal_identified
=== ep: 2126, time 29.597820281982422, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 27.141340017318726, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2127
=== ep: 2128, time 27.015878915786743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2128
goal_identified
goal_identified
goal_identified
=== ep: 2129, time 28.830254554748535, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2129
goal_identified
goal_identified
=== ep: 2130, time 26.437001705169678, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2130
goal_identified
=== ep: 2131, time 29.82766318321228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2131
=== ep: 2132, time 26.95987582206726, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2132
goal_identified
=== ep: 2133, time 26.931622982025146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2133
goal_identified
=== ep: 2134, time 27.230385780334473, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2134
=== ep: 2135, time 26.81269598007202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2135
goal_identified
=== ep: 2136, time 26.890913009643555, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2136
goal_identified
=== ep: 2137, time 27.072455406188965, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2137
goal_identified
goal_identified
=== ep: 2138, time 26.947757720947266, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2138
goal_identified
goal_identified
=== ep: 2139, time 28.80658745765686, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2139
goal_identified
=== ep: 2140, time 26.58126187324524, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2140
goal_identified
goal_identified
=== ep: 2141, time 26.525375843048096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2141
=== ep: 2142, time 26.64028024673462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2142
goal_identified
=== ep: 2143, time 26.818790197372437, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2143
goal_identified
goal_identified
goal_identified
=== ep: 2144, time 26.7247576713562, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2144
goal_identified
=== ep: 2145, time 26.90155553817749, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2145
=== ep: 2146, time 26.76543426513672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2146
=== ep: 2147, time 26.891653537750244, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2147
goal_identified
=== ep: 2148, time 26.856629133224487, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2148
goal_identified
=== ep: 2149, time 27.80620813369751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2149
goal_identified
=== ep: 2150, time 27.02378225326538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2150
goal_identified
=== ep: 2151, time 26.968815803527832, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2151
goal_identified
=== ep: 2152, time 24.53640842437744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2152
goal_identified
=== ep: 2153, time 27.369155406951904, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2154, time 26.945251941680908, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2154
goal_identified
=== ep: 2155, time 27.007018566131592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2155
=== ep: 2156, time 26.834402084350586, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2156
goal_identified
goal_identified
=== ep: 2157, time 26.768062353134155, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2157
goal_identified
goal_identified
=== ep: 2158, time 26.659874439239502, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2158
=== ep: 2159, time 30.880407571792603, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2159
=== ep: 2160, time 27.136933088302612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2160
goal_identified
=== ep: 2161, time 26.966286182403564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2161
goal_identified
=== ep: 2162, time 27.126624584197998, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2162
=== ep: 2163, time 26.94605040550232, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2163
=== ep: 2164, time 26.98849368095398, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2164
goal_identified
=== ep: 2165, time 31.53775191307068, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2165
goal_identified
goal_identified
=== ep: 2166, time 26.816192865371704, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2166
goal_identified
=== ep: 2167, time 27.11256194114685, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2167
goal_identified
goal_identified
=== ep: 2168, time 26.818851709365845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2168
=== ep: 2169, time 34.648815393447876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2169
=== ep: 2170, time 26.979691982269287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2170
goal_identified
=== ep: 2171, time 31.88465690612793, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2171
=== ep: 2172, time 27.1904079914093, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2172
goal_identified
=== ep: 2173, time 26.320138216018677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2173
goal_identified
goal_identified
=== ep: 2174, time 28.687904596328735, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2174
goal_identified
=== ep: 2175, time 27.06230092048645, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2175
goal_identified
goal_identified
=== ep: 2176, time 26.67185068130493, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2176
goal_identified
=== ep: 2177, time 27.114021062850952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2177
=== ep: 2178, time 26.727911949157715, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2178
=== ep: 2179, time 29.049569129943848, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2179
goal_identified
=== ep: 2180, time 26.636875867843628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2180
goal_identified
goal_identified
goal_identified
=== ep: 2181, time 26.608753204345703, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2181
goal_identified
=== ep: 2182, time 26.956490516662598, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2182
=== ep: 2183, time 27.180094242095947, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2183
=== ep: 2184, time 28.714665412902832, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2184
goal_identified
goal_identified
=== ep: 2185, time 26.939025402069092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2185
goal_identified
=== ep: 2186, time 27.180936574935913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2186
goal_identified
goal_identified
=== ep: 2187, time 26.44940972328186, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2187
=== ep: 2188, time 27.304812908172607, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2188
goal_identified
=== ep: 2189, time 28.700973749160767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2189
goal_identified
=== ep: 2190, time 27.12196373939514, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2190
=== ep: 2191, time 27.04608130455017, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2191
goal_identified
goal_identified
=== ep: 2192, time 26.822604656219482, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2192
goal_identified
=== ep: 2193, time 26.725820541381836, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2193
goal_identified
goal_identified
goal_identified
=== ep: 2194, time 26.89626431465149, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2194
=== ep: 2195, time 26.426817655563354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2195
=== ep: 2196, time 27.029290914535522, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2196
=== ep: 2197, time 26.979658842086792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2197
goal_identified
=== ep: 2198, time 26.256855964660645, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2198
goal_identified
=== ep: 2199, time 28.461061477661133, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2199
goal_identified
=== ep: 2200, time 27.10576105117798, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2200
=== ep: 2201, time 26.98021936416626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2201
goal_identified
=== ep: 2202, time 26.77723217010498, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2202
=== ep: 2203, time 26.715986967086792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2203
goal_identified
=== ep: 2204, time 26.833611726760864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2204
=== ep: 2205, time 26.816052675247192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2205
=== ep: 2206, time 26.857622861862183, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2206
=== ep: 2207, time 27.041548490524292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2207
goal_identified
=== ep: 2208, time 26.545592069625854, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2208
goal_identified
=== ep: 2209, time 28.57927942276001, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2209
goal_identified
goal_identified
=== ep: 2210, time 26.817530870437622, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2210
goal_identified
=== ep: 2211, time 27.834376096725464, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2211
=== ep: 2212, time 27.134605884552002, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2212
goal_identified
goal_identified
=== ep: 2213, time 26.358350038528442, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2213
=== ep: 2214, time 26.99724817276001, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2214
goal_identified
=== ep: 2215, time 26.797621488571167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2215
goal_identified
=== ep: 2216, time 27.919564962387085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2216
goal_identified
=== ep: 2217, time 27.035032510757446, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2217
=== ep: 2218, time 27.266535997390747, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2218
=== ep: 2219, time 32.28643250465393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2219
goal_identified
goal_identified
=== ep: 2220, time 30.57785701751709, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2220
goal_identified
=== ep: 2221, time 26.914193630218506, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2221
=== ep: 2222, time 26.770527601242065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2222
=== ep: 2223, time 27.142692804336548, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2223
=== ep: 2224, time 26.8924298286438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2224
goal_identified
goal_identified
=== ep: 2225, time 26.753918647766113, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2225
=== ep: 2226, time 27.284045696258545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2226
=== ep: 2227, time 27.16231656074524, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2227
=== ep: 2228, time 26.74345326423645, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2228
goal_identified
=== ep: 2229, time 28.895782947540283, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2229
=== ep: 2230, time 27.2643301486969, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2230
goal_identified
=== ep: 2231, time 26.97854208946228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2231
goal_identified
goal_identified
goal_identified
=== ep: 2232, time 26.910133361816406, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2232
=== ep: 2233, time 27.07692575454712, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2233
=== ep: 2234, time 29.292991876602173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2234
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2235, time 26.952289581298828, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
goal_identified
=== ep: 2236, time 26.824929237365723, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2236
=== ep: 2237, time 26.931659698486328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2237
=== ep: 2238, time 27.172632694244385, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2238
=== ep: 2239, time 28.863969087600708, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2239
=== ep: 2240, time 27.724177598953247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2240
=== ep: 2241, time 27.075785160064697, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2241
=== ep: 2242, time 27.728670597076416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2242
goal_identified
goal_identified
=== ep: 2243, time 26.91611909866333, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2243
goal_identified
goal_identified
=== ep: 2244, time 26.873971462249756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2244
=== ep: 2245, time 27.46921133995056, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2245
goal_identified
=== ep: 2246, time 29.738420009613037, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2246
=== ep: 2247, time 26.78429126739502, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2247
=== ep: 2248, time 27.14075756072998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2248
=== ep: 2249, time 28.850994110107422, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2249
goal_identified
goal_identified
=== ep: 2250, time 26.61425518989563, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2250
goal_identified
=== ep: 2251, time 27.28575110435486, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2251
=== ep: 2252, time 30.01371145248413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2252
goal_identified
=== ep: 2253, time 26.66608500480652, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2253
=== ep: 2254, time 26.965051889419556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2254
=== ep: 2255, time 27.34784984588623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2255
=== ep: 2256, time 27.012986183166504, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2256
=== ep: 2257, time 27.056303024291992, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2257
=== ep: 2258, time 26.897888898849487, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2258
=== ep: 2259, time 28.904412984848022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2259
goal_identified
=== ep: 2260, time 27.08286190032959, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2260
goal_identified
=== ep: 2261, time 27.72586941719055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2261
=== ep: 2262, time 27.35382843017578, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2262
goal_identified
=== ep: 2263, time 26.867388248443604, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2263
=== ep: 2264, time 27.085505485534668, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2264
goal_identified
=== ep: 2265, time 26.603891372680664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2265
goal_identified
=== ep: 2266, time 26.960344791412354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2266
goal_identified
=== ep: 2267, time 26.86223602294922, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2267
goal_identified
=== ep: 2268, time 26.70998764038086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2268
goal_identified
goal_identified
=== ep: 2269, time 28.838643550872803, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2269
goal_identified
goal_identified
=== ep: 2270, time 27.218393802642822, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2270
=== ep: 2271, time 27.301478147506714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2271
=== ep: 2272, time 26.692310333251953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2272
goal_identified
goal_identified
=== ep: 2273, time 26.97537398338318, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2273
=== ep: 2274, time 26.94012689590454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2274
goal_identified
=== ep: 2275, time 27.39206576347351, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 26.791996002197266, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2276
=== ep: 2277, time 26.98959732055664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2277
goal_identified
goal_identified
goal_identified
=== ep: 2278, time 26.974029779434204, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2278
=== ep: 2279, time 28.99161124229431, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2279
goal_identified
=== ep: 2280, time 27.268141269683838, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2280
goal_identified
=== ep: 2281, time 30.07997965812683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2281
=== ep: 2282, time 29.175256729125977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2282
goal_identified
=== ep: 2283, time 27.387950897216797, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2283
=== ep: 2284, time 29.669838666915894, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2284
=== ep: 2285, time 27.01082944869995, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2285
=== ep: 2286, time 27.02019238471985, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2286
goal_identified
=== ep: 2287, time 27.320125579833984, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2287
goal_identified
=== ep: 2288, time 27.148030996322632, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2288
goal_identified
=== ep: 2289, time 28.457351684570312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2289
=== ep: 2290, time 27.287745475769043, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2290
goal_identified
=== ep: 2291, time 27.057989358901978, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2291
goal_identified
goal_identified
=== ep: 2292, time 27.052021265029907, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2292
goal_identified
goal_identified
=== ep: 2293, time 26.59794592857361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2293
goal_identified
=== ep: 2294, time 26.588889360427856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2294
=== ep: 2295, time 26.77692437171936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2295
goal_identified
=== ep: 2296, time 27.145807027816772, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2296
=== ep: 2297, time 26.974050760269165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2297
goal_identified
=== ep: 2298, time 26.812248468399048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2298
=== ep: 2299, time 29.075687646865845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 26.912686109542847, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2300
goal_identified
=== ep: 2301, time 27.23183846473694, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2301
=== ep: 2302, time 27.080762147903442, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2302
=== ep: 2303, time 28.906529426574707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2303
=== ep: 2304, time 27.352638959884644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2304
goal_identified
=== ep: 2305, time 26.803871154785156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2305
=== ep: 2306, time 26.683605432510376, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2306
goal_identified
=== ep: 2307, time 26.82502555847168, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2307
=== ep: 2308, time 26.795968770980835, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2308
=== ep: 2309, time 29.32179832458496, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2309
=== ep: 2310, time 27.225247859954834, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2310
=== ep: 2311, time 27.223853588104248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2311
goal_identified
goal_identified
=== ep: 2312, time 26.445727586746216, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2312
=== ep: 2313, time 27.345134973526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2313
goal_identified
=== ep: 2314, time 26.698391437530518, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2314
goal_identified
=== ep: 2315, time 26.894360065460205, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2315
goal_identified
=== ep: 2316, time 29.438589572906494, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2316
=== ep: 2317, time 26.837530851364136, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2317
=== ep: 2318, time 26.671317100524902, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2318
goal_identified
goal_identified
=== ep: 2319, time 29.44524884223938, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2319
=== ep: 2320, time 26.460842609405518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2320
goal_identified
=== ep: 2321, time 27.25151491165161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2321
goal_identified
=== ep: 2322, time 27.102354764938354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2322
goal_identified
goal_identified
goal_identified
=== ep: 2323, time 27.00958800315857, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2323
=== ep: 2324, time 26.73534321784973, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2324
=== ep: 2325, time 27.377267360687256, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2325
=== ep: 2326, time 27.15471649169922, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2326
=== ep: 2327, time 26.9472713470459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2327
goal_identified
=== ep: 2328, time 26.524656534194946, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2328
=== ep: 2329, time 33.72074627876282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2329
goal_identified
=== ep: 2330, time 27.029828548431396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2330
goal_identified
goal_identified
=== ep: 2331, time 26.63226008415222, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2331
=== ep: 2332, time 26.944950342178345, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2332
=== ep: 2333, time 27.203871488571167, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2333
=== ep: 2334, time 27.177461862564087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2334
goal_identified
=== ep: 2335, time 26.52989673614502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2335
=== ep: 2336, time 27.13337755203247, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2336
goal_identified
=== ep: 2337, time 26.952228784561157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2337
=== ep: 2338, time 26.556053638458252, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2338
=== ep: 2339, time 31.514145612716675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2339
=== ep: 2340, time 26.84471321105957, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2340
goal_identified
=== ep: 2341, time 27.091896533966064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2341
goal_identified
=== ep: 2342, time 27.109053373336792, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2342
goal_identified
goal_identified
=== ep: 2343, time 27.041218280792236, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2343
=== ep: 2344, time 26.81986713409424, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2344
=== ep: 2345, time 26.620683431625366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2345
=== ep: 2346, time 27.174837112426758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2346
goal_identified
=== ep: 2347, time 26.650763988494873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2347
goal_identified
=== ep: 2348, time 26.874410152435303, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2348
goal_identified
goal_identified
=== ep: 2349, time 28.823126792907715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2349
goal_identified
goal_identified
goal_identified
=== ep: 2350, time 27.386478900909424, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2350
=== ep: 2351, time 26.72748851776123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2351
=== ep: 2352, time 27.281075477600098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2352
=== ep: 2353, time 26.805503129959106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2353
goal_identified
=== ep: 2354, time 27.03448486328125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2354
goal_identified
=== ep: 2355, time 27.014176607131958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2355
goal_identified
=== ep: 2356, time 27.017377376556396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2356
goal_identified
=== ep: 2357, time 26.669055938720703, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2357
=== ep: 2358, time 26.74796772003174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2358
=== ep: 2359, time 29.806615114212036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2359
goal_identified
=== ep: 2360, time 27.318253755569458, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2360
goal_identified
goal_identified
=== ep: 2361, time 26.652504682540894, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2361
goal_identified
goal_identified
=== ep: 2362, time 26.93640160560608, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2362
goal_identified
=== ep: 2363, time 26.992074966430664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2363
goal_identified
=== ep: 2364, time 26.592873573303223, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2364
=== ep: 2365, time 26.823314428329468, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2365
=== ep: 2366, time 26.906105518341064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2366
=== ep: 2367, time 26.939876556396484, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2367
goal_identified
=== ep: 2368, time 26.900671243667603, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2368
goal_identified
goal_identified
=== ep: 2369, time 29.35863947868347, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2369
goal_identified
goal_identified
=== ep: 2370, time 26.819418907165527, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2370
goal_identified
=== ep: 2371, time 27.33067798614502, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2371
=== ep: 2372, time 27.160457372665405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2372
=== ep: 2373, time 27.09842848777771, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2373
=== ep: 2374, time 31.072608470916748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2374
goal_identified
=== ep: 2375, time 27.121042013168335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2375
=== ep: 2376, time 26.411640405654907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2376
=== ep: 2377, time 27.25153875350952, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2377
goal_identified
=== ep: 2378, time 27.03939175605774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2378
=== ep: 2379, time 28.93960738182068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2379
=== ep: 2380, time 26.791691541671753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2380
goal_identified
=== ep: 2381, time 26.79769206047058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2381
goal_identified
goal_identified
=== ep: 2382, time 26.93035650253296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2382
=== ep: 2383, time 26.88600730895996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2383
goal_identified
=== ep: 2384, time 26.83042621612549, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2384
=== ep: 2385, time 33.47514867782593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2385
goal_identified
goal_identified
=== ep: 2386, time 26.620997667312622, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2386
=== ep: 2387, time 27.084954261779785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2387
goal_identified
=== ep: 2388, time 27.106560945510864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2388
goal_identified
=== ep: 2389, time 29.144652843475342, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2389
goal_identified
=== ep: 2390, time 26.89577579498291, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2390
=== ep: 2391, time 26.882154941558838, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2391
goal_identified
goal_identified
=== ep: 2392, time 26.507845640182495, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2392
=== ep: 2393, time 26.474684238433838, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2393
=== ep: 2394, time 26.714488744735718, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2394
goal_identified
=== ep: 2395, time 27.00715708732605, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2395
goal_identified
goal_identified
goal_identified
=== ep: 2396, time 26.82771921157837, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2396
goal_identified
=== ep: 2397, time 26.93350911140442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2397
=== ep: 2398, time 26.729063749313354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2398
=== ep: 2399, time 28.652533531188965, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2399
=== ep: 2400, time 26.730798482894897, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2400
goal_identified
goal_identified
=== ep: 2401, time 26.56555962562561, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2401
goal_identified
=== ep: 2402, time 26.71062207221985, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2402
=== ep: 2403, time 30.217260360717773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2403
goal_identified
goal_identified
=== ep: 2404, time 26.73625659942627, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2404
goal_identified
goal_identified
goal_identified
=== ep: 2405, time 26.66871452331543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2405
goal_identified
goal_identified
=== ep: 2406, time 26.750701189041138, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2406
goal_identified
goal_identified
=== ep: 2407, time 26.50646948814392, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2407
goal_identified
=== ep: 2408, time 27.21890425682068, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2408
=== ep: 2409, time 28.853914260864258, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2409
goal_identified
=== ep: 2410, time 27.13004422187805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2410
goal_identified
=== ep: 2411, time 26.992823123931885, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2411
=== ep: 2412, time 31.76286792755127, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2412
goal_identified
=== ep: 2413, time 26.440647840499878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2413
goal_identified
goal_identified
=== ep: 2414, time 27.185149669647217, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2414
goal_identified
goal_identified
goal_identified
=== ep: 2415, time 26.561530828475952, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2415
=== ep: 2416, time 26.740259885787964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2416
=== ep: 2417, time 30.928841590881348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2417
goal_identified
=== ep: 2418, time 27.17555284500122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2418
=== ep: 2419, time 33.07636046409607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2419
goal_identified
=== ep: 2420, time 28.77530837059021, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2420
=== ep: 2421, time 29.76351809501648, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2421
=== ep: 2422, time 27.1148099899292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2422
goal_identified
=== ep: 2423, time 26.98107075691223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2423
=== ep: 2424, time 29.682971954345703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2424
goal_identified
=== ep: 2425, time 26.782434463500977, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2425
=== ep: 2426, time 27.003302812576294, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2426
=== ep: 2427, time 26.929399490356445, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2427
=== ep: 2428, time 27.084322929382324, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2428
goal_identified
=== ep: 2429, time 28.69157576560974, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2429
=== ep: 2430, time 26.84014654159546, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2430
=== ep: 2431, time 26.900615692138672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2431
goal_identified
=== ep: 2432, time 26.665090560913086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2432
goal_identified
goal_identified
=== ep: 2433, time 26.412569284439087, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2433
=== ep: 2434, time 27.18562912940979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2434
goal_identified
=== ep: 2435, time 26.95470142364502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2435
goal_identified
goal_identified
=== ep: 2436, time 27.124465703964233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2436
goal_identified
=== ep: 2437, time 27.029372453689575, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2437
=== ep: 2438, time 26.67476463317871, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2438
goal_identified
=== ep: 2439, time 29.140893697738647, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2439
=== ep: 2440, time 27.03204846382141, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2440
goal_identified
=== ep: 2441, time 26.67992925643921, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2441
goal_identified
=== ep: 2442, time 27.027358055114746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2442
goal_identified
=== ep: 2443, time 27.059537410736084, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2443
goal_identified
goal_identified
goal_identified
=== ep: 2444, time 26.703253746032715, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2444
goal_identified
=== ep: 2445, time 26.889775037765503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2445
=== ep: 2446, time 26.93498206138611, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2446
=== ep: 2447, time 27.05173659324646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2447
goal_identified
=== ep: 2448, time 27.210901021957397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2448
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2449, time 28.63205051422119, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
goal_identified
=== ep: 2450, time 27.19999361038208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2450
goal_identified
goal_identified
=== ep: 2451, time 27.09520196914673, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2451
=== ep: 2452, time 27.147990465164185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2452
goal_identified
=== ep: 2453, time 26.82580280303955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2453
goal_identified
=== ep: 2454, time 26.936450719833374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2454
=== ep: 2455, time 26.639658451080322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2455
goal_identified
=== ep: 2456, time 27.12194275856018, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2456
goal_identified
=== ep: 2457, time 26.522745609283447, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2457
=== ep: 2458, time 27.057647943496704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2458
goal_identified
=== ep: 2459, time 28.99724054336548, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2459
=== ep: 2460, time 26.790754318237305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2460
=== ep: 2461, time 27.099286794662476, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2461
goal_identified
=== ep: 2462, time 26.831596851348877, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2462
goal_identified
=== ep: 2463, time 26.68708062171936, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2463
=== ep: 2464, time 29.854970455169678, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2464
goal_identified
=== ep: 2465, time 26.638224363327026, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2465
=== ep: 2466, time 26.889835119247437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2466
goal_identified
=== ep: 2467, time 27.399780750274658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2467
=== ep: 2468, time 27.44396996498108, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2468
=== ep: 2469, time 28.78966474533081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2469
=== ep: 2470, time 26.95845365524292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2470
goal_identified
goal_identified
=== ep: 2471, time 27.38547372817993, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2471
=== ep: 2472, time 27.245137453079224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2472
goal_identified
=== ep: 2473, time 26.734696865081787, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2473
goal_identified
=== ep: 2474, time 27.02383279800415, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2474
goal_identified
=== ep: 2475, time 27.601880311965942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2475
goal_identified
goal_identified
=== ep: 2476, time 26.982049226760864, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2476
goal_identified
goal_identified
=== ep: 2477, time 26.864366054534912, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2477
goal_identified
goal_identified
goal_identified
=== ep: 2478, time 26.57454204559326, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2478
goal_identified
=== ep: 2479, time 28.72983956336975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2479
=== ep: 2480, time 26.75335669517517, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2480
=== ep: 2481, time 27.38341498374939, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2481
goal_identified
goal_identified
=== ep: 2482, time 26.822155714035034, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2482
=== ep: 2483, time 27.130518198013306, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2483
=== ep: 2484, time 32.558146476745605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2484
=== ep: 2485, time 26.925894737243652, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2485
=== ep: 2486, time 27.21779775619507, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2486
=== ep: 2487, time 33.68916082382202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2487
goal_identified
=== ep: 2488, time 26.411760568618774, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2488
=== ep: 2489, time 28.863271951675415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2489
goal_identified
goal_identified
=== ep: 2490, time 26.586560487747192, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2490
=== ep: 2491, time 26.62879991531372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2491
=== ep: 2492, time 27.256778955459595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2492
goal_identified
goal_identified
=== ep: 2493, time 26.681687116622925, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2493
goal_identified
=== ep: 2494, time 27.25759243965149, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2494
goal_identified
goal_identified
=== ep: 2495, time 26.9616801738739, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2495
goal_identified
=== ep: 2496, time 27.147934198379517, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2496
=== ep: 2497, time 27.164573907852173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2497
goal_identified
=== ep: 2498, time 27.219202280044556, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2498
goal_identified
goal_identified
=== ep: 2499, time 29.3825786113739, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2499
goal_identified
=== ep: 2500, time 26.47933340072632, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
== current size of memory is eps 11 > 10.0 and we are deleting ep 2500
goal_identified
goal_identified
=== ep: 2501, time 26.558573007583618, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2501
=== ep: 2502, time 26.827032327651978, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2502
=== ep: 2503, time 27.275514125823975, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2503
goal_identified
goal_identified
=== ep: 2504, time 26.80130434036255, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2504
goal_identified
goal_identified
=== ep: 2505, time 27.388509511947632, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2505
=== ep: 2506, time 27.31294846534729, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2506
=== ep: 2507, time 26.655150890350342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2507
=== ep: 2508, time 26.57876753807068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2508
=== ep: 2509, time 28.798360347747803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2509
goal_identified
=== ep: 2510, time 26.418118953704834, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2510
=== ep: 2511, time 26.554694414138794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2511
goal_identified
=== ep: 2512, time 28.156384706497192, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2512
goal_identified
=== ep: 2513, time 26.558777570724487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2513
=== ep: 2514, time 26.895808458328247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2514
=== ep: 2515, time 26.88976740837097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2515
=== ep: 2516, time 26.69084095954895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2516
goal_identified
goal_identified
=== ep: 2517, time 26.835964679718018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2517
=== ep: 2518, time 27.202104568481445, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2518
=== ep: 2519, time 28.586575984954834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2519
goal_identified
=== ep: 2520, time 26.85362720489502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2520
=== ep: 2521, time 31.645103454589844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2521
goal_identified
goal_identified
=== ep: 2522, time 26.83626890182495, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2522
goal_identified
=== ep: 2523, time 26.462636709213257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2523
=== ep: 2524, time 26.945489406585693, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2524
goal_identified
goal_identified
goal_identified
=== ep: 2525, time 26.536211252212524, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2525
goal_identified
=== ep: 2526, time 26.99942135810852, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2526
goal_identified
goal_identified
goal_identified
=== ep: 2527, time 26.942737579345703, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2527
goal_identified
=== ep: 2528, time 26.691073179244995, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2528
goal_identified
=== ep: 2529, time 33.94993185997009, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2529
goal_identified
goal_identified
=== ep: 2530, time 26.801719188690186, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2530
=== ep: 2531, time 26.773858070373535, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2531
=== ep: 2532, time 26.62248206138611, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2532
goal_identified
=== ep: 2533, time 26.842921495437622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2533
=== ep: 2534, time 27.178435564041138, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2534
goal_identified
=== ep: 2535, time 26.735801935195923, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2535
=== ep: 2536, time 27.059629917144775, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2536
goal_identified
=== ep: 2537, time 30.367857456207275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2537
=== ep: 2538, time 27.28073215484619, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2538
goal_identified
=== ep: 2539, time 29.021231412887573, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2539
goal_identified
goal_identified
=== ep: 2540, time 27.242223501205444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2540
=== ep: 2541, time 26.763332843780518, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2541
goal_identified
goal_identified
=== ep: 2542, time 26.735267400741577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2542
=== ep: 2543, time 30.67615556716919, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2543
=== ep: 2544, time 27.34879755973816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2544
=== ep: 2545, time 26.916280269622803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2545
goal_identified
=== ep: 2546, time 26.940659284591675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2546
=== ep: 2547, time 30.397122383117676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2547
goal_identified
=== ep: 2548, time 26.96746015548706, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2548
=== ep: 2549, time 28.830328941345215, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2549
=== ep: 2550, time 27.212462186813354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2550
goal_identified
goal_identified
=== ep: 2551, time 27.26381754875183, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2551
=== ep: 2552, time 29.520954608917236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2552
=== ep: 2553, time 26.928900480270386, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2553
goal_identified
=== ep: 2554, time 26.940186023712158, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2554
=== ep: 2555, time 27.127698183059692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2555
=== ep: 2556, time 26.945326566696167, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2556
=== ep: 2557, time 26.455211877822876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2557
=== ep: 2558, time 27.280832767486572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2558
=== ep: 2559, time 29.177693843841553, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2559
=== ep: 2560, time 26.938599824905396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2560
goal_identified
goal_identified
=== ep: 2561, time 28.374410152435303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2561
=== ep: 2562, time 27.088064908981323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2562
=== ep: 2563, time 26.97415518760681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2563
=== ep: 2564, time 30.630698204040527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2564
goal_identified
=== ep: 2565, time 26.75759220123291, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2565
=== ep: 2566, time 31.35050129890442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2566
goal_identified
=== ep: 2567, time 26.770066738128662, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2567
=== ep: 2568, time 26.71239709854126, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2568
goal_identified
goal_identified
=== ep: 2569, time 28.95075011253357, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2569
=== ep: 2570, time 26.358140230178833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2570
=== ep: 2571, time 26.679572820663452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2571
goal_identified
=== ep: 2572, time 26.87603259086609, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2572
goal_identified
=== ep: 2573, time 26.538920640945435, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2573
goal_identified
=== ep: 2574, time 26.86045551300049, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2574
=== ep: 2575, time 27.569910287857056, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2575
=== ep: 2576, time 27.1650652885437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2576
=== ep: 2577, time 27.103281021118164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2577
goal_identified
=== ep: 2578, time 26.72528386116028, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2578
goal_identified
=== ep: 2579, time 29.16622829437256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2579
goal_identified
=== ep: 2580, time 26.82184600830078, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2580
goal_identified
=== ep: 2581, time 27.04529094696045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2581
goal_identified
=== ep: 2582, time 26.369934797286987, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2582
goal_identified
=== ep: 2583, time 26.566620111465454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2583
=== ep: 2584, time 26.77220869064331, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2584
=== ep: 2585, time 31.18622589111328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2585
goal_identified
=== ep: 2586, time 26.66866135597229, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2586
goal_identified
=== ep: 2587, time 26.782195806503296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2587
goal_identified
=== ep: 2588, time 26.93087387084961, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2588
goal_identified
=== ep: 2589, time 28.744306564331055, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2589
goal_identified
=== ep: 2590, time 26.86656665802002, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2590
=== ep: 2591, time 27.14777946472168, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2591
=== ep: 2592, time 27.045740604400635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2592
goal_identified
=== ep: 2593, time 26.76734733581543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2593
goal_identified
=== ep: 2594, time 27.148464918136597, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2594
goal_identified
=== ep: 2595, time 26.945484161376953, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2595
goal_identified
=== ep: 2596, time 26.603222608566284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2596
=== ep: 2597, time 26.791014194488525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2597
=== ep: 2598, time 32.777673959732056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2598
=== ep: 2599, time 28.6122465133667, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2599
=== ep: 2600, time 27.020665645599365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2600
goal_identified
goal_identified
=== ep: 2601, time 26.91002869606018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2601
goal_identified
goal_identified
=== ep: 2602, time 27.031826734542847, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2602
goal_identified
=== ep: 2603, time 27.45760154724121, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2603
=== ep: 2604, time 26.822960376739502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2604
=== ep: 2605, time 27.03288173675537, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2605
=== ep: 2606, time 24.44886541366577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2606
=== ep: 2607, time 26.83510446548462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2607
goal_identified
=== ep: 2608, time 31.861796617507935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2608
=== ep: 2609, time 28.963909149169922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2609
goal_identified
goal_identified
=== ep: 2610, time 26.928643941879272, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2610
=== ep: 2611, time 26.97501230239868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2611
goal_identified
=== ep: 2612, time 26.883936882019043, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2612
goal_identified
=== ep: 2613, time 26.424344062805176, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2613
=== ep: 2614, time 26.656702041625977, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2614
goal_identified
goal_identified
=== ep: 2615, time 26.765007495880127, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2615
goal_identified
=== ep: 2616, time 26.834465742111206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2616
=== ep: 2617, time 26.678940534591675, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2617
goal_identified
=== ep: 2618, time 26.988842248916626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2618
goal_identified
goal_identified
=== ep: 2619, time 29.677668571472168, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2619
=== ep: 2620, time 29.220471143722534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2620
goal_identified
goal_identified
goal_identified
=== ep: 2621, time 27.20010018348694, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2621
goal_identified
=== ep: 2622, time 26.629183769226074, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2622
goal_identified
=== ep: 2623, time 26.985503911972046, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2623
goal_identified
goal_identified
=== ep: 2624, time 26.834067583084106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2624
=== ep: 2625, time 26.74190330505371, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2625
goal_identified
=== ep: 2626, time 26.784180164337158, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2626
=== ep: 2627, time 26.69306755065918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2627
=== ep: 2628, time 27.0478777885437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2628
=== ep: 2629, time 28.552560091018677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2629
=== ep: 2630, time 26.896777153015137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2630
goal_identified
=== ep: 2631, time 26.796405792236328, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2631
=== ep: 2632, time 27.22474718093872, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2632
goal_identified
goal_identified
=== ep: 2633, time 27.3184916973114, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2633
=== ep: 2634, time 27.03760838508606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2634
goal_identified
goal_identified
=== ep: 2635, time 26.951560974121094, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2635
goal_identified
=== ep: 2636, time 27.108349084854126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2636
goal_identified
goal_identified
goal_identified
=== ep: 2637, time 26.392940044403076, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2637
=== ep: 2638, time 26.815317392349243, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2638
goal_identified
=== ep: 2639, time 28.83644127845764, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2639
=== ep: 2640, time 27.372334718704224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2640
=== ep: 2641, time 26.53160786628723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2641
goal_identified
=== ep: 2642, time 27.22521209716797, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2642
=== ep: 2643, time 27.06280016899109, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2643
=== ep: 2644, time 27.408161401748657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2644
=== ep: 2645, time 26.493640899658203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2645
=== ep: 2646, time 26.990232467651367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2646
=== ep: 2647, time 27.187835216522217, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2647
goal_identified
goal_identified
=== ep: 2648, time 26.827029943466187, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2648
goal_identified
goal_identified
=== ep: 2649, time 28.996909618377686, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2649
goal_identified
=== ep: 2650, time 26.663350105285645, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2650
=== ep: 2651, time 26.794809341430664, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2651
goal_identified
goal_identified
=== ep: 2652, time 26.624686002731323, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2652
goal_identified
goal_identified
goal_identified
=== ep: 2653, time 26.749998807907104, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2653
=== ep: 2654, time 33.18563675880432, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2654
goal_identified
=== ep: 2655, time 26.932369470596313, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2655
goal_identified
=== ep: 2656, time 26.551326274871826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2656
=== ep: 2657, time 30.497761726379395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2657
goal_identified
goal_identified
=== ep: 2658, time 26.452119827270508, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2658
goal_identified
goal_identified
=== ep: 2659, time 29.169681787490845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2659
goal_identified
=== ep: 2660, time 26.766184091567993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2660
=== ep: 2661, time 26.475319385528564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2661
goal_identified
=== ep: 2662, time 26.751664638519287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2662
goal_identified
goal_identified
=== ep: 2663, time 26.94307518005371, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2663
goal_identified
=== ep: 2664, time 26.672438144683838, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2664
=== ep: 2665, time 30.55024266242981, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2665
goal_identified
goal_identified
=== ep: 2666, time 26.46889853477478, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2666
=== ep: 2667, time 26.749022722244263, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2667
=== ep: 2668, time 26.690849781036377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2668
goal_identified
=== ep: 2669, time 28.915886640548706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2669
goal_identified
goal_identified
=== ep: 2670, time 27.30568814277649, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2670
goal_identified
goal_identified
=== ep: 2671, time 27.46787428855896, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2671
goal_identified
=== ep: 2672, time 26.443625688552856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2672
goal_identified
=== ep: 2673, time 27.458059549331665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2673
goal_identified
=== ep: 2674, time 27.16744089126587, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2674
goal_identified
=== ep: 2675, time 27.274585247039795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2675
goal_identified
=== ep: 2676, time 26.97735357284546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2676
goal_identified
goal_identified
=== ep: 2677, time 26.40780520439148, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2677
=== ep: 2678, time 27.199768543243408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2678
goal_identified
goal_identified
=== ep: 2679, time 29.313323259353638, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2679
goal_identified
goal_identified
=== ep: 2680, time 27.147772312164307, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2680
goal_identified
=== ep: 2681, time 27.046327352523804, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2681
=== ep: 2682, time 26.76844024658203, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2682
=== ep: 2683, time 27.040870428085327, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2684, time 26.55171036720276, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2684
=== ep: 2685, time 27.017186403274536, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2685
=== ep: 2686, time 26.766894340515137, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2686
goal_identified
goal_identified
=== ep: 2687, time 27.32481288909912, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2687
goal_identified
goal_identified
=== ep: 2688, time 27.238383769989014, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2688
goal_identified
goal_identified
=== ep: 2689, time 28.82004976272583, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2689
=== ep: 2690, time 26.656935691833496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2690
=== ep: 2691, time 27.129245042800903, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2691
goal_identified
goal_identified
=== ep: 2692, time 26.63434147834778, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2692
goal_identified
=== ep: 2693, time 27.031761646270752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2693
goal_identified
goal_identified
goal_identified
=== ep: 2694, time 26.724722862243652, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2694
=== ep: 2695, time 26.692744493484497, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2695
=== ep: 2696, time 26.592753648757935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2696
goal_identified
=== ep: 2697, time 26.567965507507324, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2697
goal_identified
goal_identified
goal_identified
=== ep: 2698, time 26.578161001205444, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2698
goal_identified
=== ep: 2699, time 28.870092391967773, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2699
=== ep: 2700, time 31.356106281280518, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2700
goal_identified
=== ep: 2701, time 26.99983811378479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2701
goal_identified
goal_identified
goal_identified
=== ep: 2702, time 26.538460731506348, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2702
goal_identified
goal_identified
=== ep: 2703, time 26.94028115272522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2703
=== ep: 2704, time 26.52842903137207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2704
=== ep: 2705, time 26.82731056213379, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2705
=== ep: 2706, time 26.658230781555176, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2706
goal_identified
goal_identified
=== ep: 2707, time 26.86426854133606, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2707
goal_identified
goal_identified
=== ep: 2708, time 26.996567249298096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2708
goal_identified
=== ep: 2709, time 28.857216596603394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2709
goal_identified
goal_identified
goal_identified
=== ep: 2710, time 26.934937238693237, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2710
=== ep: 2711, time 26.749126434326172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2711
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2712, time 26.455861806869507, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
=== ep: 2713, time 26.96530055999756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2713
=== ep: 2714, time 26.792741298675537, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2714
goal_identified
goal_identified
=== ep: 2715, time 27.135777235031128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2715
goal_identified
goal_identified
=== ep: 2716, time 26.630028009414673, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2716
goal_identified
goal_identified
=== ep: 2717, time 26.75548243522644, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2717
goal_identified
goal_identified
=== ep: 2718, time 26.72128915786743, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2718
=== ep: 2719, time 28.633769035339355, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2719
=== ep: 2720, time 27.00563359260559, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2720
goal_identified
=== ep: 2721, time 26.518285989761353, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2721
=== ep: 2722, time 26.991514921188354, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2722
goal_identified
=== ep: 2723, time 26.810483932495117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2723
goal_identified
=== ep: 2724, time 27.04517436027527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2724
=== ep: 2725, time 26.745922803878784, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2725
goal_identified
goal_identified
=== ep: 2726, time 26.81780219078064, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2726
goal_identified
goal_identified
=== ep: 2727, time 26.745736598968506, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2727
goal_identified
goal_identified
=== ep: 2728, time 26.759539365768433, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2728
goal_identified
=== ep: 2729, time 28.559603929519653, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2729
=== ep: 2730, time 26.710006713867188, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2730
=== ep: 2731, time 26.9353768825531, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2731
goal_identified
goal_identified
=== ep: 2732, time 26.90431046485901, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2732
=== ep: 2733, time 27.607415199279785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2733
goal_identified
goal_identified
=== ep: 2734, time 27.432230710983276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2734
goal_identified
goal_identified
=== ep: 2735, time 27.004594564437866, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2735
=== ep: 2736, time 26.785043954849243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2736
=== ep: 2737, time 27.018600702285767, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2737
=== ep: 2738, time 31.668736934661865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2738
goal_identified
=== ep: 2739, time 28.406834602355957, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2739
=== ep: 2740, time 26.59561777114868, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2740
goal_identified
=== ep: 2741, time 27.42560648918152, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2741
=== ep: 2742, time 26.808516025543213, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 26.77469801902771, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2743
=== ep: 2744, time 27.384214401245117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2744
=== ep: 2745, time 27.343453884124756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2745
goal_identified
goal_identified
=== ep: 2746, time 26.528151035308838, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2746
=== ep: 2747, time 27.18076992034912, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2747
=== ep: 2748, time 26.91565251350403, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2748
=== ep: 2749, time 28.743309020996094, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2749
=== ep: 2750, time 27.059882879257202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 27.001922607421875, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2751
=== ep: 2752, time 26.960657596588135, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2752
goal_identified
=== ep: 2753, time 26.840212106704712, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2753
goal_identified
=== ep: 2754, time 27.873857736587524, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2754
goal_identified
=== ep: 2755, time 27.09150719642639, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2755
goal_identified
=== ep: 2756, time 26.905911445617676, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2756
goal_identified
=== ep: 2757, time 27.13845729827881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2757
goal_identified
goal_identified
goal_identified
=== ep: 2758, time 27.07925772666931, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2758
=== ep: 2759, time 29.055944681167603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2759
goal_identified
=== ep: 2760, time 26.58838152885437, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2760
goal_identified
goal_identified
=== ep: 2761, time 26.7508442401886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2761
goal_identified
=== ep: 2762, time 26.803440809249878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2762
=== ep: 2763, time 26.998263359069824, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2763
=== ep: 2764, time 26.947771310806274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2764
=== ep: 2765, time 36.21847105026245, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2765
=== ep: 2766, time 26.96122169494629, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2766
goal_identified
=== ep: 2767, time 26.739265203475952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2767
goal_identified
=== ep: 2768, time 26.556020259857178, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2768
goal_identified
goal_identified
=== ep: 2769, time 28.877774477005005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2769
=== ep: 2770, time 26.188632249832153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2770
goal_identified
=== ep: 2771, time 27.05522847175598, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2771
=== ep: 2772, time 33.75526142120361, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2772
=== ep: 2773, time 27.1176016330719, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2773
goal_identified
=== ep: 2774, time 26.61123752593994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2774
=== ep: 2775, time 27.22832417488098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2775
goal_identified
goal_identified
=== ep: 2776, time 26.976272106170654, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2776
=== ep: 2777, time 27.177936553955078, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2777
goal_identified
=== ep: 2778, time 26.743607759475708, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2778
goal_identified
=== ep: 2779, time 29.01781463623047, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2779
=== ep: 2780, time 27.07822561264038, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2780
goal_identified
=== ep: 2781, time 26.850993394851685, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2781
goal_identified
=== ep: 2782, time 27.137439250946045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2782
=== ep: 2783, time 27.172552347183228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2783
=== ep: 2784, time 26.631475687026978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2784
=== ep: 2785, time 27.325981855392456, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2785
goal_identified
=== ep: 2786, time 26.756265878677368, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2786
goal_identified
goal_identified
=== ep: 2787, time 26.8231463432312, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2787
goal_identified
=== ep: 2788, time 26.590550899505615, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2788
=== ep: 2789, time 29.054096937179565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2789
=== ep: 2790, time 26.589659452438354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2790
goal_identified
=== ep: 2791, time 26.989723920822144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2791
=== ep: 2792, time 27.6282217502594, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2792
goal_identified
goal_identified
=== ep: 2793, time 26.842726945877075, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2793
goal_identified
=== ep: 2794, time 27.21104621887207, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2794
=== ep: 2795, time 27.313079833984375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2795
=== ep: 2796, time 27.148710250854492, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2796
=== ep: 2797, time 27.141536474227905, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2797
goal_identified
goal_identified
=== ep: 2798, time 26.615461826324463, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2798
goal_identified
=== ep: 2799, time 28.772098541259766, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2799
=== ep: 2800, time 26.908148050308228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2800
=== ep: 2801, time 27.35382914543152, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2801
=== ep: 2802, time 26.493566513061523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2802
=== ep: 2803, time 26.7773277759552, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2803
=== ep: 2804, time 27.035141706466675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2804
goal_identified
=== ep: 2805, time 26.95604658126831, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2805
=== ep: 2806, time 27.11662268638611, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2806
=== ep: 2807, time 26.726869106292725, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2807
goal_identified
=== ep: 2808, time 26.631121397018433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2808
goal_identified
=== ep: 2809, time 28.601762533187866, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2809
goal_identified
=== ep: 2810, time 26.95527172088623, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2810
goal_identified
goal_identified
=== ep: 2811, time 26.847163677215576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2811
goal_identified
=== ep: 2812, time 26.7973792552948, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2812
=== ep: 2813, time 26.709572076797485, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2813
=== ep: 2814, time 26.57817840576172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2814
=== ep: 2815, time 31.28213667869568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2815
goal_identified
=== ep: 2816, time 26.48223328590393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2816
goal_identified
=== ep: 2817, time 27.435436010360718, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2817
=== ep: 2818, time 26.95715570449829, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2818
goal_identified
=== ep: 2819, time 29.279049158096313, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2819
goal_identified
goal_identified
=== ep: 2820, time 27.3659245967865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2820
=== ep: 2821, time 29.804848194122314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2821
goal_identified
=== ep: 2822, time 27.326348781585693, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2822
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2823, time 26.98888874053955, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1547
=== ep: 2824, time 30.69351363182068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2824
=== ep: 2825, time 26.85164499282837, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2825
goal_identified
=== ep: 2826, time 28.846012830734253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2826
goal_identified
goal_identified
=== ep: 2827, time 26.659764766693115, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2827
=== ep: 2828, time 31.74975895881653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2828
goal_identified
=== ep: 2829, time 29.374186515808105, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2829
=== ep: 2830, time 26.848963022232056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2830
goal_identified
=== ep: 2831, time 26.507753372192383, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2831
goal_identified
=== ep: 2832, time 26.5989933013916, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2832
=== ep: 2833, time 27.069514274597168, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2833
goal_identified
=== ep: 2834, time 26.32801651954651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2834
=== ep: 2835, time 26.854389905929565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2835
=== ep: 2836, time 27.27328610420227, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2836
goal_identified
=== ep: 2837, time 26.987249612808228, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2837
goal_identified
=== ep: 2838, time 26.969335794448853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2838
goal_identified
goal_identified
=== ep: 2839, time 28.7576744556427, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2839
goal_identified
goal_identified
=== ep: 2840, time 26.294753551483154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 27.077677965164185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2841
=== ep: 2842, time 26.57088279724121, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2842
goal_identified
goal_identified
=== ep: 2843, time 28.792364597320557, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 26.74332594871521, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2844
goal_identified
=== ep: 2845, time 26.763842582702637, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 26.239278078079224, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2846
goal_identified
goal_identified
=== ep: 2847, time 27.03700089454651, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2847
goal_identified
=== ep: 2848, time 27.223934412002563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2848
goal_identified
=== ep: 2849, time 30.44508194923401, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2849
goal_identified
=== ep: 2850, time 26.713027000427246, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2850
goal_identified
=== ep: 2851, time 26.887685298919678, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2851
=== ep: 2852, time 27.226590156555176, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2852
=== ep: 2853, time 26.976462602615356, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2853
goal_identified
goal_identified
=== ep: 2854, time 26.861934185028076, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2854
=== ep: 2855, time 26.835339784622192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2855
goal_identified
goal_identified
=== ep: 2856, time 26.718318223953247, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2856
=== ep: 2857, time 30.570767879486084, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2857
=== ep: 2858, time 26.75277352333069, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2858
goal_identified
=== ep: 2859, time 28.941474437713623, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2859
=== ep: 2860, time 27.05507755279541, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2860
goal_identified
=== ep: 2861, time 32.37153720855713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2861
goal_identified
goal_identified
goal_identified
=== ep: 2862, time 26.874796628952026, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2862
goal_identified
=== ep: 2863, time 27.25098204612732, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2863
goal_identified
goal_identified
=== ep: 2864, time 26.689597606658936, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2864
=== ep: 2865, time 27.07105588912964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2865
goal_identified
=== ep: 2866, time 27.25873613357544, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2866
goal_identified
=== ep: 2867, time 27.234564542770386, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2867
=== ep: 2868, time 26.506162643432617, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2868
=== ep: 2869, time 32.02471566200256, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2869
goal_identified
=== ep: 2870, time 27.46897029876709, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2870
goal_identified
goal_identified
=== ep: 2871, time 26.41048526763916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2871
goal_identified
goal_identified
=== ep: 2872, time 26.618733167648315, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2872
=== ep: 2873, time 26.846532344818115, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2873
goal_identified
=== ep: 2874, time 27.009127140045166, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2874
goal_identified
=== ep: 2875, time 26.83137035369873, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2875
=== ep: 2876, time 27.059086322784424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2876
=== ep: 2877, time 26.817670822143555, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2877
goal_identified
goal_identified
goal_identified
=== ep: 2878, time 26.56098771095276, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2878
=== ep: 2879, time 28.804262161254883, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2879
goal_identified
=== ep: 2880, time 31.34650182723999, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2880
=== ep: 2881, time 24.06615161895752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2881
=== ep: 2882, time 27.133219480514526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2882
goal_identified
=== ep: 2883, time 26.762609243392944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2883
goal_identified
goal_identified
=== ep: 2884, time 26.668187141418457, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2884
=== ep: 2885, time 26.61957836151123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2885
goal_identified
=== ep: 2886, time 26.434558391571045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2886
=== ep: 2887, time 27.20819401741028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2887
goal_identified
=== ep: 2888, time 27.04717779159546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2888
goal_identified
=== ep: 2889, time 28.922785997390747, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2889
goal_identified
goal_identified
goal_identified
=== ep: 2890, time 26.984965085983276, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2890
=== ep: 2891, time 26.774561166763306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2891
=== ep: 2892, time 26.955376386642456, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2892
goal_identified
=== ep: 2893, time 26.92474627494812, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2893
goal_identified
=== ep: 2894, time 26.797670125961304, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2894
=== ep: 2895, time 26.57706379890442, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2895
=== ep: 2896, time 27.20890736579895, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2896
=== ep: 2897, time 30.716915607452393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2897
=== ep: 2898, time 27.153207302093506, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2898
goal_identified
=== ep: 2899, time 28.877958059310913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2899
goal_identified
=== ep: 2900, time 26.992931365966797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2900
goal_identified
goal_identified
=== ep: 2901, time 27.260792016983032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2901
goal_identified
=== ep: 2902, time 29.374191761016846, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2902
goal_identified
goal_identified
=== ep: 2903, time 26.606025218963623, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2903
goal_identified
goal_identified
=== ep: 2904, time 26.834977626800537, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2904
=== ep: 2905, time 26.864230632781982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2905
goal_identified
=== ep: 2906, time 26.859803676605225, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2906
=== ep: 2907, time 26.942304849624634, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2907
=== ep: 2908, time 26.40565514564514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2908
=== ep: 2909, time 28.88061833381653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2909
goal_identified
goal_identified
=== ep: 2910, time 26.7186598777771, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2910
goal_identified
=== ep: 2911, time 26.5886652469635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2911
goal_identified
=== ep: 2912, time 26.935174465179443, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2912
goal_identified
goal_identified
goal_identified
=== ep: 2913, time 26.685295343399048, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2913
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2914, time 26.459958791732788, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
goal_identified
=== ep: 2915, time 26.58969235420227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2915
=== ep: 2916, time 27.06014609336853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2916
goal_identified
goal_identified
=== ep: 2917, time 27.503201007843018, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2917
=== ep: 2918, time 26.605499744415283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2918
goal_identified
goal_identified
=== ep: 2919, time 29.00752305984497, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2919
=== ep: 2920, time 32.28999853134155, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2920
goal_identified
goal_identified
goal_identified
=== ep: 2921, time 27.14783263206482, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2921
=== ep: 2922, time 27.34210205078125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2922
=== ep: 2923, time 27.188716173171997, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2923
goal_identified
=== ep: 2924, time 26.63046932220459, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2924
goal_identified
=== ep: 2925, time 27.449512481689453, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2925
=== ep: 2926, time 26.631720304489136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2926
=== ep: 2927, time 26.647183895111084, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2927
=== ep: 2928, time 26.4841628074646, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 28.644023656845093, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2929
=== ep: 2930, time 27.008429765701294, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2930
goal_identified
goal_identified
=== ep: 2931, time 27.050339460372925, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2931
goal_identified
goal_identified
=== ep: 2932, time 26.832923412322998, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2932
=== ep: 2933, time 26.765483856201172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2933
=== ep: 2934, time 27.063173294067383, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2934
=== ep: 2935, time 27.137186288833618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2935
goal_identified
=== ep: 2936, time 27.18237280845642, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2936
goal_identified
goal_identified
=== ep: 2937, time 26.67185378074646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2937
goal_identified
=== ep: 2938, time 26.905030012130737, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2938
=== ep: 2939, time 30.987021684646606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2939
goal_identified
goal_identified
=== ep: 2940, time 27.202775478363037, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2940
goal_identified
goal_identified
goal_identified
=== ep: 2941, time 26.407387733459473, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2941
=== ep: 2942, time 27.045167922973633, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2942
=== ep: 2943, time 26.433966159820557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2943
goal_identified
goal_identified
=== ep: 2944, time 27.129899978637695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2944
goal_identified
goal_identified
=== ep: 2945, time 27.141876697540283, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2945
=== ep: 2946, time 30.416350603103638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2946
=== ep: 2947, time 27.55807375907898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2947
=== ep: 2948, time 27.359412670135498, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2948
goal_identified
=== ep: 2949, time 31.046388864517212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2949
=== ep: 2950, time 26.83493661880493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2950
goal_identified
=== ep: 2951, time 27.214877367019653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2951
goal_identified
=== ep: 2952, time 26.837443351745605, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2952
=== ep: 2953, time 26.948373556137085, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2953
goal_identified
=== ep: 2954, time 26.808353662490845, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2954
goal_identified
goal_identified
=== ep: 2955, time 26.738008737564087, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2955
goal_identified
=== ep: 2956, time 30.92813491821289, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2956
goal_identified
=== ep: 2957, time 26.21150016784668, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2957
=== ep: 2958, time 27.090288877487183, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2958
=== ep: 2959, time 28.813446044921875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2959
=== ep: 2960, time 27.20315670967102, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2960
goal_identified
=== ep: 2961, time 26.3036687374115, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2961
=== ep: 2962, time 29.373359441757202, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2962
=== ep: 2963, time 26.482413053512573, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2963
=== ep: 2964, time 26.8777916431427, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2964
=== ep: 2965, time 26.566187143325806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2965
goal_identified
=== ep: 2966, time 27.111417531967163, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2966
goal_identified
=== ep: 2967, time 27.04721212387085, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2967
=== ep: 2968, time 27.33491325378418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2968
=== ep: 2969, time 28.957308053970337, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2969
=== ep: 2970, time 26.73834776878357, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2970
=== ep: 2971, time 27.20685601234436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2971
=== ep: 2972, time 27.478333234786987, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2972
=== ep: 2973, time 27.093209743499756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2973
=== ep: 2974, time 26.737367630004883, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2974
=== ep: 2975, time 26.816179990768433, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2975
=== ep: 2976, time 27.15428113937378, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2976
goal_identified
=== ep: 2977, time 25.89927315711975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2977
=== ep: 2978, time 27.24895215034485, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2978
=== ep: 2979, time 28.78146457672119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2979
goal_identified
=== ep: 2980, time 27.00128698348999, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2980
=== ep: 2981, time 31.8469877243042, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2981
=== ep: 2982, time 26.925452947616577, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2982
=== ep: 2983, time 32.08593559265137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2983
=== ep: 2984, time 27.010390520095825, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2984
=== ep: 2985, time 26.393377542495728, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2985
=== ep: 2986, time 27.000236749649048, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2986
=== ep: 2987, time 26.99419331550598, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2987
goal_identified
=== ep: 2988, time 26.794954538345337, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2988
goal_identified
=== ep: 2989, time 28.58417844772339, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2989
goal_identified
goal_identified
=== ep: 2990, time 26.603153944015503, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2990
goal_identified
goal_identified
=== ep: 2991, time 27.513042211532593, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2991
=== ep: 2992, time 26.89007329940796, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2992
goal_identified
=== ep: 2993, time 27.230029582977295, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2993
goal_identified
=== ep: 2994, time 26.78614091873169, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2994
goal_identified
=== ep: 2995, time 26.975594758987427, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2995
goal_identified
=== ep: 2996, time 26.782063961029053, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2996
goal_identified
=== ep: 2997, time 26.962618589401245, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2997
goal_identified
=== ep: 2998, time 26.851834058761597, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2998
=== ep: 2999, time 29.650490522384644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2999
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3000, time 26.666005611419678, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1789
goal_identified
=== ep: 3001, time 26.821833610534668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3001
goal_identified
=== ep: 3002, time 26.868128538131714, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3002
goal_identified
=== ep: 3003, time 26.625389099121094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3003
=== ep: 3004, time 26.880219221115112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3004
goal_identified
=== ep: 3005, time 26.394431829452515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3005
=== ep: 3006, time 27.010151863098145, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3006
=== ep: 3007, time 31.516569137573242, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3007
=== ep: 3008, time 26.773519039154053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3008
goal_identified
goal_identified
=== ep: 3009, time 28.894362926483154, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3009
goal_identified
=== ep: 3010, time 26.88175678253174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3010
=== ep: 3011, time 27.191701889038086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3011
goal_identified
=== ep: 3012, time 26.78483557701111, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3012
goal_identified
=== ep: 3013, time 27.020592212677002, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3013
=== ep: 3014, time 27.18114709854126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3014
=== ep: 3015, time 27.384501457214355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3015
goal_identified
=== ep: 3016, time 26.820393800735474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3016
goal_identified
=== ep: 3017, time 26.737616539001465, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3017
goal_identified
goal_identified
=== ep: 3018, time 26.8882999420166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3018
goal_identified
=== ep: 3019, time 28.846699237823486, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3019
goal_identified
goal_identified
=== ep: 3020, time 26.52777647972107, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3020
=== ep: 3021, time 27.033668994903564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3021
goal_identified
=== ep: 3022, time 27.30618453025818, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3022
=== ep: 3023, time 26.862962007522583, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3023
goal_identified
=== ep: 3024, time 27.194030284881592, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3024
goal_identified
goal_identified
=== ep: 3025, time 26.754234552383423, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3025
=== ep: 3026, time 26.84424376487732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3026
goal_identified
=== ep: 3027, time 26.5724880695343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3027
goal_identified
=== ep: 3028, time 27.20584464073181, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3028
=== ep: 3029, time 28.086853504180908, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3029
=== ep: 3030, time 31.614357709884644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3030
=== ep: 3031, time 30.227277040481567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3031
goal_identified
goal_identified
goal_identified
=== ep: 3032, time 26.75815987586975, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3032
goal_identified
=== ep: 3033, time 26.896517992019653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3033
goal_identified
=== ep: 3034, time 30.160269737243652, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3034
goal_identified
goal_identified
=== ep: 3035, time 26.67823028564453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3035
=== ep: 3036, time 26.909400463104248, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3036
goal_identified
goal_identified
=== ep: 3037, time 26.868607997894287, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3037
=== ep: 3038, time 26.88544726371765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3038
=== ep: 3039, time 31.519530534744263, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3039
goal_identified
=== ep: 3040, time 27.082688570022583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3040
=== ep: 3041, time 26.795090675354004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3041
goal_identified
=== ep: 3042, time 26.19126796722412, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3042
=== ep: 3043, time 26.9600191116333, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3043
=== ep: 3044, time 27.41077446937561, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3044
=== ep: 3045, time 26.927192449569702, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3045
=== ep: 3046, time 27.2019522190094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3046
goal_identified
=== ep: 3047, time 26.61035180091858, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3047
=== ep: 3048, time 28.829136610031128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3048
=== ep: 3049, time 28.959650993347168, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3049
goal_identified
=== ep: 3050, time 27.146740674972534, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3050
goal_identified
goal_identified
=== ep: 3051, time 26.782084226608276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3051
=== ep: 3052, time 27.187366724014282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3052
=== ep: 3053, time 27.467569828033447, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3053
=== ep: 3054, time 27.104446411132812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3054
=== ep: 3055, time 33.91696381568909, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3055
=== ep: 3056, time 26.86528253555298, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3056
goal_identified
goal_identified
=== ep: 3057, time 26.902628898620605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3057
goal_identified
=== ep: 3058, time 28.48982834815979, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3058
goal_identified
=== ep: 3059, time 29.207860946655273, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3059
=== ep: 3060, time 26.59274959564209, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3060
goal_identified
=== ep: 3061, time 27.554688453674316, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3061
=== ep: 3062, time 26.870628118515015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3062
=== ep: 3063, time 27.274991512298584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3063
goal_identified
=== ep: 3064, time 26.837902784347534, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3064
=== ep: 3065, time 34.45477604866028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3065
=== ep: 3066, time 25.92051410675049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3066
goal_identified
goal_identified
=== ep: 3067, time 26.72159481048584, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3067
goal_identified
=== ep: 3068, time 26.93302083015442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3068
=== ep: 3069, time 28.943304777145386, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3069
=== ep: 3070, time 26.688619136810303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3070
=== ep: 3071, time 30.767586708068848, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3071
=== ep: 3072, time 26.67627716064453, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3072
=== ep: 3073, time 26.844180583953857, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3073
=== ep: 3074, time 27.340024709701538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3074
goal_identified
=== ep: 3075, time 26.579689264297485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3075
=== ep: 3076, time 27.016165018081665, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3076
=== ep: 3077, time 26.989938020706177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3077
goal_identified
goal_identified
=== ep: 3078, time 26.86820387840271, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3078
=== ep: 3079, time 29.19833993911743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3079
goal_identified
goal_identified
=== ep: 3080, time 26.564702033996582, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3080
=== ep: 3081, time 26.95650577545166, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3081
=== ep: 3082, time 27.199934244155884, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3082
goal_identified
=== ep: 3083, time 26.667576551437378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3083
goal_identified
goal_identified
=== ep: 3084, time 26.92237377166748, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3084
goal_identified
goal_identified
=== ep: 3085, time 26.86789345741272, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3085
=== ep: 3086, time 26.80307674407959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3086
goal_identified
goal_identified
=== ep: 3087, time 26.707735538482666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3087
goal_identified
=== ep: 3088, time 26.73691201210022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3088
=== ep: 3089, time 33.789125204086304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3089
=== ep: 3090, time 26.817224264144897, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3090
=== ep: 3091, time 30.508652687072754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3091
=== ep: 3092, time 26.796667098999023, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3092
=== ep: 3093, time 27.463838815689087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3093
goal_identified
=== ep: 3094, time 27.150845766067505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3094
goal_identified
=== ep: 3095, time 26.99265694618225, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3095
=== ep: 3096, time 26.861326217651367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3096
goal_identified
=== ep: 3097, time 26.438222408294678, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 30.321013689041138, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3098
goal_identified
=== ep: 3099, time 29.20711064338684, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3099
goal_identified
=== ep: 3100, time 26.87105631828308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3100
goal_identified
=== ep: 3101, time 27.346965312957764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3101
goal_identified
goal_identified
=== ep: 3102, time 26.920482397079468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3102
=== ep: 3103, time 26.923703908920288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3103
goal_identified
=== ep: 3104, time 26.73059320449829, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3104
=== ep: 3105, time 26.333738327026367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3105
goal_identified
=== ep: 3106, time 26.523498058319092, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3106
goal_identified
goal_identified
=== ep: 3107, time 26.955760955810547, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3107
goal_identified
goal_identified
=== ep: 3108, time 27.16513729095459, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3108
goal_identified
=== ep: 3109, time 28.767353773117065, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3109
=== ep: 3110, time 26.940792083740234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3110
=== ep: 3111, time 26.722639799118042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3111
goal_identified
=== ep: 3112, time 27.162135362625122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3112
goal_identified
=== ep: 3113, time 26.89478611946106, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3113
=== ep: 3114, time 26.473182678222656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3114
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3115, time 26.72650957107544, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3115
=== ep: 3116, time 26.51643466949463, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3116
goal_identified
goal_identified
=== ep: 3117, time 27.261653661727905, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3117
=== ep: 3118, time 26.804160833358765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3118
=== ep: 3119, time 29.255908250808716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3119
=== ep: 3120, time 26.593918323516846, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3120
goal_identified
=== ep: 3121, time 26.885631322860718, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3121
goal_identified
=== ep: 3122, time 26.762598276138306, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3122
goal_identified
goal_identified
=== ep: 3123, time 27.04385256767273, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3123
=== ep: 3124, time 32.433106422424316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3124
goal_identified
=== ep: 3125, time 26.739657878875732, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3125
goal_identified
=== ep: 3126, time 26.815454721450806, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3126
=== ep: 3127, time 26.494916439056396, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3127
=== ep: 3128, time 27.014623641967773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3128
=== ep: 3129, time 28.654953241348267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3129
goal_identified
=== ep: 3130, time 27.15650177001953, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3130
=== ep: 3131, time 26.468854665756226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3131
=== ep: 3132, time 27.28277325630188, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3132
goal_identified
=== ep: 3133, time 26.740741968154907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3133
goal_identified
goal_identified
=== ep: 3134, time 27.15975785255432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3134
=== ep: 3135, time 26.848562955856323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3135
=== ep: 3136, time 29.191485166549683, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3136
=== ep: 3137, time 26.4818913936615, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3137
=== ep: 3138, time 27.381045818328857, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3138
goal_identified
=== ep: 3139, time 28.88083529472351, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3139
goal_identified
=== ep: 3140, time 26.789831161499023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3140
goal_identified
=== ep: 3141, time 26.847244262695312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3141
=== ep: 3142, time 26.873937368392944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3142
=== ep: 3143, time 26.81216287612915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3143
=== ep: 3144, time 26.54938268661499, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3144
goal_identified
=== ep: 3145, time 27.081666231155396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3145
goal_identified
=== ep: 3146, time 28.43474054336548, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3146
goal_identified
=== ep: 3147, time 26.65489673614502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3147
=== ep: 3148, time 26.947455167770386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3148
goal_identified
=== ep: 3149, time 28.619482278823853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3149
goal_identified
=== ep: 3150, time 26.729243755340576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3150
goal_identified
=== ep: 3151, time 27.012555599212646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3151
=== ep: 3152, time 26.926643133163452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3152
goal_identified
=== ep: 3153, time 26.673365354537964, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3153
goal_identified
=== ep: 3154, time 26.823891639709473, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3154
goal_identified
goal_identified
goal_identified
=== ep: 3155, time 26.595265865325928, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3155
=== ep: 3156, time 26.56556987762451, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3156
=== ep: 3157, time 27.116804361343384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3157
goal_identified
=== ep: 3158, time 26.428594827651978, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3158
goal_identified
=== ep: 3159, time 29.34492540359497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3159
=== ep: 3160, time 27.113022804260254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3160
=== ep: 3161, time 27.14568853378296, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3161
goal_identified
=== ep: 3162, time 27.12600302696228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3162
goal_identified
=== ep: 3163, time 26.43391251564026, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3163
goal_identified
=== ep: 3164, time 26.860968589782715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3164
goal_identified
=== ep: 3165, time 24.65184187889099, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 17/17)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3165
goal_identified
=== ep: 3166, time 27.18148422241211, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3166
=== ep: 3167, time 27.086087226867676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3167
goal_identified
=== ep: 3168, time 27.090606212615967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3168
=== ep: 3169, time 28.70353364944458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3169
=== ep: 3170, time 27.000786781311035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3170
=== ep: 3171, time 26.61428213119507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3171
=== ep: 3172, time 26.702671766281128, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3172
=== ep: 3173, time 27.03477907180786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3173
goal_identified
=== ep: 3174, time 29.351462841033936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3174
goal_identified
=== ep: 3175, time 26.91215944290161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3175
=== ep: 3176, time 26.65508246421814, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3176
=== ep: 3177, time 27.111284494400024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3177
=== ep: 3178, time 29.825748682022095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3178
=== ep: 3179, time 28.603751182556152, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3179
goal_identified
goal_identified
=== ep: 3180, time 31.370466947555542, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3180
goal_identified
goal_identified
=== ep: 3181, time 27.890480518341064, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3181
goal_identified
=== ep: 3182, time 27.175941944122314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3182
goal_identified
goal_identified
=== ep: 3183, time 27.045918464660645, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3183
=== ep: 3184, time 26.597347021102905, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3184
=== ep: 3185, time 26.615394830703735, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3185
=== ep: 3186, time 27.1187162399292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3186
goal_identified
=== ep: 3187, time 27.01423192024231, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3187
goal_identified
=== ep: 3188, time 27.105274200439453, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3188
goal_identified
goal_identified
=== ep: 3189, time 29.1629741191864, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3189
goal_identified
=== ep: 3190, time 26.478456020355225, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3190
goal_identified
goal_identified
=== ep: 3191, time 26.981376886367798, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3191
goal_identified
=== ep: 3192, time 27.122689247131348, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3192
goal_identified
goal_identified
=== ep: 3193, time 26.626973152160645, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3193
goal_identified
=== ep: 3194, time 26.898817777633667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3194
goal_identified
=== ep: 3195, time 26.539403676986694, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3195
=== ep: 3196, time 24.709567546844482, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3196
goal_identified
=== ep: 3197, time 30.024725198745728, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3197
=== ep: 3198, time 26.542587757110596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 29.463898420333862, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3199
goal_identified
=== ep: 3200, time 26.627706289291382, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3200
goal_identified
=== ep: 3201, time 27.065388202667236, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3201
=== ep: 3202, time 27.06837248802185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3202
=== ep: 3203, time 27.112241506576538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3203
goal_identified
=== ep: 3204, time 26.643661975860596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3204
goal_identified
=== ep: 3205, time 26.616291046142578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3205
=== ep: 3206, time 27.021013736724854, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3206
=== ep: 3207, time 26.731791496276855, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3207
=== ep: 3208, time 26.56039261817932, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3208
goal_identified
goal_identified
=== ep: 3209, time 28.775694370269775, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3209
goal_identified
goal_identified
=== ep: 3210, time 28.81465196609497, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3210
goal_identified
=== ep: 3211, time 27.451160669326782, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3211
goal_identified
goal_identified
goal_identified
=== ep: 3212, time 26.490426301956177, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3212
=== ep: 3213, time 26.822818517684937, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3213
goal_identified
=== ep: 3214, time 26.766735315322876, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3214
goal_identified
=== ep: 3215, time 27.053983688354492, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3215
goal_identified
=== ep: 3216, time 26.86051106452942, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3216
goal_identified
=== ep: 3217, time 28.913095474243164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3217
goal_identified
=== ep: 3218, time 26.66509199142456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3218
goal_identified
=== ep: 3219, time 29.310944318771362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3219
=== ep: 3220, time 31.80112600326538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3220
=== ep: 3221, time 27.073286771774292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3221
goal_identified
goal_identified
=== ep: 3222, time 28.48437809944153, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3222
goal_identified
goal_identified
goal_identified
=== ep: 3223, time 27.024428367614746, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3223
=== ep: 3224, time 26.733782052993774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3224
goal_identified
=== ep: 3225, time 26.6949725151062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3225
=== ep: 3226, time 27.102726936340332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3226
=== ep: 3227, time 26.719995737075806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3227
goal_identified
goal_identified
goal_identified
=== ep: 3228, time 26.83353567123413, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3228
goal_identified
=== ep: 3229, time 28.86975383758545, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3229
goal_identified
=== ep: 3230, time 26.95353627204895, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3230
=== ep: 3231, time 26.637872219085693, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3231
goal_identified
=== ep: 3232, time 27.029695749282837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3232
goal_identified
=== ep: 3233, time 26.86570143699646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3233
goal_identified
=== ep: 3234, time 26.95243501663208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3234
=== ep: 3235, time 27.169732570648193, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3235
=== ep: 3236, time 27.357116222381592, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3236
goal_identified
=== ep: 3237, time 29.853148937225342, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3237
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3238, time 27.196471214294434, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 135/135)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1862
=== ep: 3239, time 29.1875479221344, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3239
goal_identified
=== ep: 3240, time 27.118651628494263, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3240
goal_identified
=== ep: 3241, time 27.236489057540894, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3241
=== ep: 3242, time 27.23616862297058, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3242
goal_identified
=== ep: 3243, time 29.284757137298584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3243
=== ep: 3244, time 27.591799020767212, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3244
goal_identified
=== ep: 3245, time 26.817896366119385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3245
goal_identified
=== ep: 3246, time 26.456056356430054, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3246
goal_identified
=== ep: 3247, time 26.94996404647827, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3247
=== ep: 3248, time 26.956592082977295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3248
=== ep: 3249, time 29.510465145111084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3249
=== ep: 3250, time 26.996276378631592, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3250
goal_identified
=== ep: 3251, time 28.614750862121582, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3251
=== ep: 3252, time 26.586531400680542, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3252
goal_identified
goal_identified
=== ep: 3253, time 27.089998245239258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3253
=== ep: 3254, time 27.00135087966919, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3254
=== ep: 3255, time 26.90065908432007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3255
goal_identified
=== ep: 3256, time 27.27128505706787, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3256
goal_identified
=== ep: 3257, time 26.780778646469116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3257
=== ep: 3258, time 27.17593502998352, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3258
goal_identified
=== ep: 3259, time 28.97927165031433, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3259
goal_identified
=== ep: 3260, time 27.203747510910034, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3260
=== ep: 3261, time 26.71495795249939, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3261
goal_identified
=== ep: 3262, time 27.394920587539673, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3262
=== ep: 3263, time 27.174437761306763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3263
=== ep: 3264, time 26.94398832321167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3264
=== ep: 3265, time 26.756343126296997, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3265
goal_identified
=== ep: 3266, time 26.8426296710968, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3266
goal_identified
=== ep: 3267, time 26.801224946975708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3267
=== ep: 3268, time 26.74173641204834, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3268
goal_identified
=== ep: 3269, time 32.69029641151428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3269
=== ep: 3270, time 27.66424298286438, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3270
goal_identified
=== ep: 3271, time 27.006189107894897, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3271
=== ep: 3272, time 27.122185230255127, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3272
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3273, time 26.756211280822754, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2235
goal_identified
goal_identified
goal_identified
=== ep: 3274, time 26.976722240447998, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3274
=== ep: 3275, time 27.11971163749695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3275
goal_identified
=== ep: 3276, time 27.08096432685852, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3276
goal_identified
goal_identified
=== ep: 3277, time 26.945069789886475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3277
=== ep: 3278, time 34.578404664993286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3278
goal_identified
=== ep: 3279, time 28.75243091583252, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3279
goal_identified
goal_identified
=== ep: 3280, time 27.12037491798401, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3280
goal_identified
goal_identified
=== ep: 3281, time 26.859783172607422, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3281
goal_identified
goal_identified
=== ep: 3282, time 26.850937604904175, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3282
goal_identified
goal_identified
=== ep: 3283, time 27.153779983520508, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3283
=== ep: 3284, time 32.79290795326233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3284
=== ep: 3285, time 27.2017560005188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3285
goal_identified
=== ep: 3286, time 27.194420337677002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3286
goal_identified
=== ep: 3287, time 26.726623058319092, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3287
goal_identified
=== ep: 3288, time 26.930683851242065, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3288
=== ep: 3289, time 28.765015125274658, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3289
goal_identified
goal_identified
=== ep: 3290, time 26.80275249481201, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3290
goal_identified
=== ep: 3291, time 26.799587726593018, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3291
=== ep: 3292, time 27.308845043182373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3292
=== ep: 3293, time 27.011870622634888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3293
=== ep: 3294, time 32.15610361099243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3294
=== ep: 3295, time 26.87790536880493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3295
goal_identified
goal_identified
=== ep: 3296, time 26.872563362121582, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3296
goal_identified
goal_identified
=== ep: 3297, time 27.04122495651245, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3297
goal_identified
=== ep: 3298, time 26.982569694519043, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3298
=== ep: 3299, time 29.553521633148193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3299
=== ep: 3300, time 27.035070180892944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3300
goal_identified
goal_identified
goal_identified
=== ep: 3301, time 26.884536504745483, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3301
goal_identified
=== ep: 3302, time 26.993258476257324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3302
goal_identified
=== ep: 3303, time 27.57563543319702, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3303
goal_identified
=== ep: 3304, time 27.070258140563965, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3304
goal_identified
=== ep: 3305, time 26.80467438697815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3305
=== ep: 3306, time 27.049084424972534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3306
goal_identified
goal_identified
=== ep: 3307, time 26.652483224868774, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3307
=== ep: 3308, time 27.06801700592041, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3308
goal_identified
goal_identified
=== ep: 3309, time 28.772974729537964, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3309
=== ep: 3310, time 27.233729362487793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3310
=== ep: 3311, time 26.859477043151855, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3311
goal_identified
=== ep: 3312, time 27.06727886199951, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3312
goal_identified
=== ep: 3313, time 27.056471824645996, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3313
goal_identified
=== ep: 3314, time 31.56666874885559, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3314
goal_identified
=== ep: 3315, time 26.70587706565857, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3315
goal_identified
=== ep: 3316, time 26.769036293029785, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3316
goal_identified
=== ep: 3317, time 26.964121341705322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3317
goal_identified
goal_identified
=== ep: 3318, time 26.772908687591553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3318
goal_identified
goal_identified
=== ep: 3319, time 29.21503233909607, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3319
=== ep: 3320, time 26.923938035964966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3320
=== ep: 3321, time 27.578848600387573, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3321
goal_identified
=== ep: 3322, time 27.128519773483276, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3322
goal_identified
=== ep: 3323, time 27.552473306655884, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3323
=== ep: 3324, time 26.787381410598755, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3324
=== ep: 3325, time 27.07438635826111, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3325
=== ep: 3326, time 27.3227596282959, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3326
=== ep: 3327, time 27.077033042907715, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3327
=== ep: 3328, time 26.69544744491577, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3328
goal_identified
goal_identified
=== ep: 3329, time 28.988444805145264, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3329
goal_identified
=== ep: 3330, time 27.181045055389404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3330
goal_identified
=== ep: 3331, time 26.66256856918335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3331
=== ep: 3332, time 26.777117252349854, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3332
goal_identified
goal_identified
=== ep: 3333, time 26.5580632686615, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3333
goal_identified
=== ep: 3334, time 27.25133180618286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3334
=== ep: 3335, time 26.75929880142212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3335
=== ep: 3336, time 26.8385272026062, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3336
goal_identified
=== ep: 3337, time 27.314650297164917, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3337
=== ep: 3338, time 27.474825859069824, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3338
=== ep: 3339, time 28.985912799835205, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3339
=== ep: 3340, time 26.958619356155396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3340
=== ep: 3341, time 26.72665810585022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3341
goal_identified
=== ep: 3342, time 26.92954397201538, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3342
=== ep: 3343, time 31.67950701713562, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3344, time 26.619758367538452, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2449
goal_identified
goal_identified
=== ep: 3345, time 27.04338502883911, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3345
=== ep: 3346, time 27.187913179397583, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3346
=== ep: 3347, time 27.28476095199585, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3347
goal_identified
goal_identified
=== ep: 3348, time 27.24119210243225, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3348
=== ep: 3349, time 29.18211340904236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3349
goal_identified
=== ep: 3350, time 27.891989707946777, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3350
goal_identified
goal_identified
=== ep: 3351, time 26.517550945281982, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3351
goal_identified
goal_identified
=== ep: 3352, time 26.6081759929657, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3352
=== ep: 3353, time 26.921241283416748, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3353
=== ep: 3354, time 26.940768480300903, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3354
goal_identified
=== ep: 3355, time 26.904090404510498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3355
=== ep: 3356, time 27.237860441207886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3356
goal_identified
goal_identified
=== ep: 3357, time 26.837563514709473, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3357
goal_identified
=== ep: 3358, time 26.872421503067017, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3358
goal_identified
goal_identified
goal_identified
=== ep: 3359, time 29.179335594177246, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3359
goal_identified
=== ep: 3360, time 27.169973134994507, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3360
=== ep: 3361, time 27.106718063354492, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3361
goal_identified
goal_identified
=== ep: 3362, time 26.27011251449585, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3362
goal_identified
=== ep: 3363, time 27.0704345703125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3363
=== ep: 3364, time 27.192475080490112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3364
goal_identified
goal_identified
=== ep: 3365, time 26.806490421295166, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3365
goal_identified
goal_identified
goal_identified
=== ep: 3366, time 26.913074493408203, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3366
=== ep: 3367, time 26.96223211288452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3367
goal_identified
=== ep: 3368, time 26.699294090270996, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3368
goal_identified
=== ep: 3369, time 28.88944911956787, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3369
=== ep: 3370, time 27.676769256591797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3370
=== ep: 3371, time 27.271512746810913, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3371
goal_identified
=== ep: 3372, time 26.45325231552124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3372
goal_identified
=== ep: 3373, time 26.960259199142456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3373
goal_identified
=== ep: 3374, time 26.953975200653076, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3374
=== ep: 3375, time 26.03202247619629, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3375
goal_identified
=== ep: 3376, time 26.749021768569946, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3376
=== ep: 3377, time 26.810994625091553, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3377
goal_identified
goal_identified
goal_identified
=== ep: 3378, time 26.509127855300903, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3378
=== ep: 3379, time 36.69691443443298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3379
goal_identified
=== ep: 3380, time 27.33197259902954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3380
=== ep: 3381, time 30.082360982894897, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3381
=== ep: 3382, time 26.749674320220947, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3382
=== ep: 3383, time 26.92941951751709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3383
=== ep: 3384, time 27.12717580795288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3384
=== ep: 3385, time 26.85942029953003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3385
=== ep: 3386, time 27.281867027282715, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3386
goal_identified
=== ep: 3387, time 26.493814945220947, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3387
goal_identified
=== ep: 3388, time 26.830400705337524, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3388
=== ep: 3389, time 29.03388285636902, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3389
goal_identified
=== ep: 3390, time 26.82349729537964, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3390
=== ep: 3391, time 27.091044425964355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3391
goal_identified
goal_identified
=== ep: 3392, time 26.950188398361206, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3392
=== ep: 3393, time 26.963493585586548, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3393
goal_identified
goal_identified
goal_identified
=== ep: 3394, time 26.670852184295654, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3394
goal_identified
=== ep: 3395, time 26.799869537353516, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3395
goal_identified
=== ep: 3396, time 26.62775754928589, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3396
goal_identified
=== ep: 3397, time 26.807615995407104, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3397
=== ep: 3398, time 26.928624391555786, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3398
=== ep: 3399, time 28.99446415901184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3399
goal_identified
=== ep: 3400, time 26.753090381622314, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3400
goal_identified
=== ep: 3401, time 30.25739550590515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3401
=== ep: 3402, time 26.835376262664795, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3402
=== ep: 3403, time 26.68726372718811, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3403
=== ep: 3404, time 26.697269201278687, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3404
=== ep: 3405, time 26.776103973388672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3405
=== ep: 3406, time 27.8279709815979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3406
goal_identified
goal_identified
=== ep: 3407, time 27.054142475128174, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3407
=== ep: 3408, time 30.416401386260986, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3408
=== ep: 3409, time 34.4724178314209, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3409
=== ep: 3410, time 32.87703776359558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3410
=== ep: 3411, time 26.904961109161377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3411
=== ep: 3412, time 31.10450315475464, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3412
=== ep: 3413, time 26.839488983154297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3413
goal_identified
=== ep: 3414, time 26.739821672439575, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3414
=== ep: 3415, time 26.735944747924805, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3415
=== ep: 3416, time 27.287440299987793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3416
goal_identified
=== ep: 3417, time 26.814204692840576, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3417
=== ep: 3418, time 26.878820419311523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3418
=== ep: 3419, time 33.01946234703064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3419
goal_identified
=== ep: 3420, time 26.926883220672607, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3420
=== ep: 3421, time 26.78193950653076, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3421
=== ep: 3422, time 26.593502521514893, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3422
=== ep: 3423, time 26.872771978378296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3423
=== ep: 3424, time 27.39607810974121, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3424
goal_identified
=== ep: 3425, time 26.702202320098877, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3425
goal_identified
goal_identified
=== ep: 3426, time 26.914085865020752, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3426
goal_identified
=== ep: 3427, time 26.840730905532837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3427
goal_identified
=== ep: 3428, time 27.069196224212646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3428
goal_identified
=== ep: 3429, time 29.00020933151245, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3429
=== ep: 3430, time 27.08534550666809, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3430
goal_identified
=== ep: 3431, time 26.900400638580322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3431
goal_identified
goal_identified
goal_identified
=== ep: 3432, time 26.85088086128235, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3432
=== ep: 3433, time 26.945833921432495, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3433
=== ep: 3434, time 26.833984375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3434
goal_identified
goal_identified
=== ep: 3435, time 27.17022156715393, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3435
goal_identified
=== ep: 3436, time 26.82166361808777, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3436
goal_identified
=== ep: 3437, time 24.911783456802368, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3437
=== ep: 3438, time 27.052172422409058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3438
goal_identified
goal_identified
=== ep: 3439, time 29.21350598335266, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3439
goal_identified
=== ep: 3440, time 26.906878232955933, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3440
goal_identified
=== ep: 3441, time 26.63880968093872, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3441
goal_identified
=== ep: 3442, time 27.218185663223267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3442
goal_identified
=== ep: 3443, time 26.928673028945923, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3443
=== ep: 3444, time 26.865929126739502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3444
=== ep: 3445, time 26.913278102874756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3445
goal_identified
goal_identified
=== ep: 3446, time 26.728926181793213, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3446
goal_identified
goal_identified
=== ep: 3447, time 26.87521457672119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 26.600985765457153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3448
=== ep: 3449, time 29.489314794540405, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3449
=== ep: 3450, time 28.609591722488403, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3450
goal_identified
goal_identified
=== ep: 3451, time 26.68542242050171, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3451
=== ep: 3452, time 26.56988549232483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3452
goal_identified
=== ep: 3453, time 26.648635625839233, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3453
goal_identified
goal_identified
=== ep: 3454, time 26.953709602355957, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3454
=== ep: 3455, time 26.954784393310547, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3455
=== ep: 3456, time 26.859760999679565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3456
goal_identified
=== ep: 3457, time 26.858190774917603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3457
goal_identified
goal_identified
=== ep: 3458, time 27.056713342666626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3458
goal_identified
=== ep: 3459, time 29.441739320755005, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3459
=== ep: 3460, time 26.98086190223694, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3460
=== ep: 3461, time 26.947128772735596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3461
goal_identified
goal_identified
=== ep: 3462, time 26.835030794143677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3462
goal_identified
=== ep: 3463, time 26.881266832351685, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3463
=== ep: 3464, time 26.537494659423828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3464
goal_identified
=== ep: 3465, time 27.010645627975464, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3465
=== ep: 3466, time 27.08625292778015, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3466
=== ep: 3467, time 27.00269389152527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3467
=== ep: 3468, time 26.759923934936523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3468
=== ep: 3469, time 33.81114935874939, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3469
goal_identified
=== ep: 3470, time 26.5579936504364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3470
goal_identified
=== ep: 3471, time 26.794438362121582, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3471
=== ep: 3472, time 27.876981735229492, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3472
goal_identified
=== ep: 3473, time 29.7018780708313, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3473
=== ep: 3474, time 27.101341724395752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3474
goal_identified
=== ep: 3475, time 26.600548267364502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3475
=== ep: 3476, time 26.99455690383911, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3476
=== ep: 3477, time 26.727434158325195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 27.32959794998169, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3478
=== ep: 3479, time 28.942819595336914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3479
=== ep: 3480, time 26.55919098854065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3480
=== ep: 3481, time 27.190300941467285, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3481
goal_identified
=== ep: 3482, time 26.90349006652832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3482
=== ep: 3483, time 27.786238431930542, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3483
=== ep: 3484, time 26.77910017967224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3484
goal_identified
=== ep: 3485, time 31.739282608032227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3485
goal_identified
=== ep: 3486, time 26.6336030960083, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3486
=== ep: 3487, time 27.006348133087158, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3487
goal_identified
goal_identified
=== ep: 3488, time 26.85323739051819, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3488
=== ep: 3489, time 29.042433977127075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3489
goal_identified
=== ep: 3490, time 27.222536087036133, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3490
=== ep: 3491, time 26.99071431159973, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3491
=== ep: 3492, time 26.70384120941162, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3492
=== ep: 3493, time 27.158353567123413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3493
=== ep: 3494, time 26.99466896057129, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3494
=== ep: 3495, time 26.476149797439575, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3495
goal_identified
goal_identified
goal_identified
=== ep: 3496, time 27.011225938796997, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3496
=== ep: 3497, time 27.091607570648193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3497
goal_identified
=== ep: 3498, time 26.896195650100708, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3498
goal_identified
=== ep: 3499, time 34.512826442718506, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3499
=== ep: 3500, time 26.826545238494873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3500
goal_identified
=== ep: 3501, time 26.781405210494995, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3501
goal_identified
goal_identified
=== ep: 3502, time 27.030271530151367, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3502
=== ep: 3503, time 32.035823345184326, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3503
=== ep: 3504, time 27.035823822021484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3504
=== ep: 3505, time 26.864320516586304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3505
=== ep: 3506, time 28.684983253479004, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3506
goal_identified
=== ep: 3507, time 26.902263641357422, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3507
=== ep: 3508, time 26.93550705909729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3508
=== ep: 3509, time 29.163200616836548, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3509
=== ep: 3510, time 27.16365361213684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3510
=== ep: 3511, time 27.31524395942688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3511
goal_identified
=== ep: 3512, time 26.85897731781006, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3512
goal_identified
goal_identified
goal_identified
=== ep: 3513, time 26.823762893676758, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3513
=== ep: 3514, time 26.871444702148438, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3514
=== ep: 3515, time 27.079745531082153, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3515
goal_identified
goal_identified
goal_identified
=== ep: 3516, time 27.034823656082153, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3516
=== ep: 3517, time 26.490360021591187, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3517
goal_identified
=== ep: 3518, time 26.980574369430542, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3518
=== ep: 3519, time 28.718050956726074, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3519
=== ep: 3520, time 30.328104734420776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3520
goal_identified
=== ep: 3521, time 26.569138526916504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3521
goal_identified
=== ep: 3522, time 27.32411789894104, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3522
goal_identified
=== ep: 3523, time 26.900516748428345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3523
=== ep: 3524, time 27.027939796447754, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3524
=== ep: 3525, time 26.722611904144287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3525
goal_identified
=== ep: 3526, time 26.797722339630127, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3526
=== ep: 3527, time 26.94608449935913, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3527
goal_identified
goal_identified
goal_identified
=== ep: 3528, time 26.629942178726196, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3528
=== ep: 3529, time 28.891152143478394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3529
goal_identified
=== ep: 3530, time 27.148369789123535, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3530
goal_identified
=== ep: 3531, time 26.990216493606567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3531
goal_identified
goal_identified
=== ep: 3532, time 26.656144618988037, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3532
goal_identified
goal_identified
=== ep: 3533, time 27.425541400909424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3533
=== ep: 3534, time 32.37752151489258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3534
goal_identified
goal_identified
=== ep: 3535, time 27.204282999038696, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3535
=== ep: 3536, time 27.07720637321472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3536
goal_identified
goal_identified
=== ep: 3537, time 26.683194160461426, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3537
goal_identified
=== ep: 3538, time 26.707756757736206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3538
=== ep: 3539, time 28.73185706138611, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3539
goal_identified
=== ep: 3540, time 26.697214603424072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3540
goal_identified
=== ep: 3541, time 27.00035333633423, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3541
goal_identified
goal_identified
=== ep: 3542, time 26.622498035430908, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3542
=== ep: 3543, time 32.6001181602478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3543
goal_identified
=== ep: 3544, time 26.582797050476074, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3544
goal_identified
=== ep: 3545, time 26.92011570930481, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3545
=== ep: 3546, time 26.84469723701477, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3546
=== ep: 3547, time 26.90602135658264, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3547
=== ep: 3548, time 27.41996669769287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3548
goal_identified
=== ep: 3549, time 29.21077299118042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3549
goal_identified
goal_identified
=== ep: 3550, time 26.879409313201904, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3550
=== ep: 3551, time 26.961490869522095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3551
=== ep: 3552, time 26.97531509399414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3552
=== ep: 3553, time 26.715959310531616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3553
=== ep: 3554, time 26.933155298233032, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3554
goal_identified
=== ep: 3555, time 26.835118055343628, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3555
goal_identified
=== ep: 3556, time 26.755333423614502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3556
=== ep: 3557, time 26.836929321289062, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3557
goal_identified
=== ep: 3558, time 27.10610032081604, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3558
=== ep: 3559, time 28.76948571205139, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3559
goal_identified
=== ep: 3560, time 26.837873220443726, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3560
goal_identified
=== ep: 3561, time 26.979899644851685, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3561
goal_identified
=== ep: 3562, time 28.109010457992554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3562
goal_identified
=== ep: 3563, time 26.75718402862549, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3563
goal_identified
goal_identified
=== ep: 3564, time 26.896553993225098, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3564
=== ep: 3565, time 27.080079555511475, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3565
goal_identified
goal_identified
=== ep: 3566, time 26.659812211990356, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3566
goal_identified
goal_identified
=== ep: 3567, time 26.836004495620728, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3567
=== ep: 3568, time 27.211820363998413, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3568
=== ep: 3569, time 29.03088903427124, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3569
goal_identified
goal_identified
goal_identified
=== ep: 3570, time 26.725582122802734, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3570
=== ep: 3571, time 25.90462613105774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3571
goal_identified
=== ep: 3572, time 26.709686994552612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3572
=== ep: 3573, time 27.127288103103638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3573
goal_identified
=== ep: 3574, time 26.48972749710083, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3574
goal_identified
goal_identified
=== ep: 3575, time 26.565964460372925, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3575
goal_identified
=== ep: 3576, time 27.35744023323059, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3576
goal_identified
goal_identified
=== ep: 3577, time 26.919885396957397, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3577
=== ep: 3578, time 26.878594160079956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3578
=== ep: 3579, time 29.110883235931396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3579
goal_identified
=== ep: 3580, time 26.87484645843506, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3580
goal_identified
=== ep: 3581, time 26.848634243011475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3581
=== ep: 3582, time 26.81910276412964, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3582
=== ep: 3583, time 26.583202123641968, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3583
=== ep: 3584, time 26.99848461151123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3584
goal_identified
=== ep: 3585, time 26.892588138580322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3585
goal_identified
=== ep: 3586, time 27.143285036087036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3586
=== ep: 3587, time 27.127312421798706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3587
goal_identified
=== ep: 3588, time 26.8871328830719, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3588
=== ep: 3589, time 29.39241051673889, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3589
=== ep: 3590, time 26.536588668823242, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3590
=== ep: 3591, time 28.937047719955444, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 26.867886066436768, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3592
goal_identified
goal_identified
goal_identified
=== ep: 3593, time 26.65180015563965, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3593
goal_identified
=== ep: 3594, time 27.783538103103638, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3594
goal_identified
goal_identified
goal_identified
=== ep: 3595, time 26.98125910758972, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3595
goal_identified
=== ep: 3596, time 27.10511875152588, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3596
=== ep: 3597, time 26.881192445755005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3597
goal_identified
=== ep: 3598, time 29.972205877304077, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3598
=== ep: 3599, time 28.970005750656128, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3599
goal_identified
goal_identified
=== ep: 3600, time 26.94411325454712, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3600
goal_identified
goal_identified
=== ep: 3601, time 26.958978414535522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3601
goal_identified
=== ep: 3602, time 27.42280340194702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3602
=== ep: 3603, time 27.50045156478882, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3603
goal_identified
=== ep: 3604, time 26.94257616996765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3604
=== ep: 3605, time 26.964158296585083, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3605
=== ep: 3606, time 27.057299613952637, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3606
goal_identified
goal_identified
=== ep: 3607, time 26.799825429916382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3607
goal_identified
=== ep: 3608, time 26.86661672592163, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3608
goal_identified
=== ep: 3609, time 28.576684951782227, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3609
goal_identified
goal_identified
=== ep: 3610, time 27.399526596069336, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3610
=== ep: 3611, time 26.966441869735718, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3611
=== ep: 3612, time 26.63138508796692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3612
goal_identified
=== ep: 3613, time 26.77782917022705, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3613
=== ep: 3614, time 27.082590341567993, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3614
=== ep: 3615, time 26.845826148986816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3615
=== ep: 3616, time 27.02816367149353, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3616
=== ep: 3617, time 26.528913259506226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3617
=== ep: 3618, time 28.81244707107544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3618
=== ep: 3619, time 30.18743395805359, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3619
goal_identified
=== ep: 3620, time 27.024799585342407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3620
goal_identified
goal_identified
goal_identified
=== ep: 3621, time 26.95217728614807, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3621
=== ep: 3622, time 27.092416286468506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3622
goal_identified
=== ep: 3623, time 26.809104204177856, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3623
=== ep: 3624, time 26.823275327682495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3624
goal_identified
=== ep: 3625, time 26.737807035446167, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3625
=== ep: 3626, time 27.08912968635559, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3626
=== ep: 3627, time 26.942676067352295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3627
goal_identified
=== ep: 3628, time 26.980263233184814, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3628
goal_identified
goal_identified
=== ep: 3629, time 29.37980890274048, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3629
goal_identified
=== ep: 3630, time 26.7153217792511, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3630
=== ep: 3631, time 26.815877676010132, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3631
goal_identified
=== ep: 3632, time 26.77578043937683, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3632
=== ep: 3633, time 26.84657621383667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3633
=== ep: 3634, time 26.918769121170044, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3634
=== ep: 3635, time 27.03835701942444, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3635
goal_identified
=== ep: 3636, time 26.511207103729248, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3636
goal_identified
=== ep: 3637, time 27.037469625473022, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3637
=== ep: 3638, time 26.812870979309082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3638
=== ep: 3639, time 27.96570372581482, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3639
=== ep: 3640, time 26.96481966972351, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3640
=== ep: 3641, time 26.913402795791626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3641
goal_identified
goal_identified
=== ep: 3642, time 26.79836392402649, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3642
=== ep: 3643, time 26.639492750167847, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3643
goal_identified
=== ep: 3644, time 26.84512424468994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3644
goal_identified
=== ep: 3645, time 27.103086233139038, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3645
=== ep: 3646, time 27.081926345825195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3646
goal_identified
=== ep: 3647, time 26.887818574905396, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3647
goal_identified
goal_identified
=== ep: 3648, time 27.232422351837158, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3648
goal_identified
=== ep: 3649, time 28.811729669570923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3649
=== ep: 3650, time 26.950687885284424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3650
=== ep: 3651, time 26.940099000930786, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 27.027645111083984, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3652
goal_identified
goal_identified
=== ep: 3653, time 26.740341424942017, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3653
=== ep: 3654, time 25.33072543144226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3654
=== ep: 3655, time 27.19166922569275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3655
=== ep: 3656, time 26.70686984062195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3656
=== ep: 3657, time 27.034709930419922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3657
=== ep: 3658, time 26.714617490768433, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3658
goal_identified
goal_identified
goal_identified
=== ep: 3659, time 29.068195819854736, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3659
goal_identified
=== ep: 3660, time 27.20598030090332, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3660
=== ep: 3661, time 26.862152099609375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3661
goal_identified
goal_identified
=== ep: 3662, time 26.614173412322998, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3662
=== ep: 3663, time 26.26856017112732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3663
=== ep: 3664, time 27.289469480514526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3664
goal_identified
=== ep: 3665, time 26.914032459259033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3665
goal_identified
=== ep: 3666, time 27.203967094421387, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3666
goal_identified
=== ep: 3667, time 27.396365642547607, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3667
goal_identified
=== ep: 3668, time 27.08792018890381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3668
=== ep: 3669, time 29.08547282218933, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3669
=== ep: 3670, time 26.952769994735718, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3670
=== ep: 3671, time 30.699591875076294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3671
goal_identified
=== ep: 3672, time 27.513739824295044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3672
goal_identified
goal_identified
goal_identified
=== ep: 3673, time 27.14931893348694, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3673
=== ep: 3674, time 27.221563577651978, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3674
=== ep: 3675, time 27.06987476348877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3675
goal_identified
=== ep: 3676, time 27.073814392089844, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3676
goal_identified
=== ep: 3677, time 26.789386749267578, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3677
goal_identified
goal_identified
=== ep: 3678, time 26.578104257583618, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3678
goal_identified
goal_identified
goal_identified
=== ep: 3679, time 29.237189292907715, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3679
goal_identified
goal_identified
=== ep: 3680, time 27.25504970550537, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3680
goal_identified
goal_identified
=== ep: 3681, time 26.841383457183838, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3681
=== ep: 3682, time 32.02896046638489, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3682
=== ep: 3683, time 27.16657328605652, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3684, time 26.681100845336914, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2712
=== ep: 3685, time 27.050569772720337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3685
=== ep: 3686, time 26.83862066268921, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3686
=== ep: 3687, time 26.839519262313843, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3687
goal_identified
goal_identified
=== ep: 3688, time 27.390652418136597, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3688
goal_identified
goal_identified
=== ep: 3689, time 28.967833757400513, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3689
=== ep: 3690, time 26.97244429588318, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3690
=== ep: 3691, time 27.024921894073486, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3691
goal_identified
=== ep: 3692, time 26.524741411209106, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3692
goal_identified
goal_identified
=== ep: 3693, time 27.090478897094727, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3693
=== ep: 3694, time 26.64661741256714, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3694
=== ep: 3695, time 26.587573528289795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3695
=== ep: 3696, time 32.05097007751465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3696
goal_identified
=== ep: 3697, time 28.839085578918457, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3697
goal_identified
=== ep: 3698, time 27.191495418548584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3698
=== ep: 3699, time 29.38612961769104, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3699
=== ep: 3700, time 28.693023920059204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3700
=== ep: 3701, time 26.75642681121826, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3701
=== ep: 3702, time 28.05166530609131, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3702
=== ep: 3703, time 26.960105895996094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3703
=== ep: 3704, time 27.09498167037964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3704
goal_identified
goal_identified
=== ep: 3705, time 26.59469175338745, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3705
=== ep: 3706, time 27.03051209449768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3706
=== ep: 3707, time 27.051217794418335, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3707
goal_identified
=== ep: 3708, time 26.905564785003662, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3708
goal_identified
goal_identified
=== ep: 3709, time 28.999083995819092, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3709
=== ep: 3710, time 27.214810609817505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3710
goal_identified
goal_identified
=== ep: 3711, time 26.870151042938232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3711
=== ep: 3712, time 26.914021253585815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3712
=== ep: 3713, time 26.932668447494507, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3713
goal_identified
=== ep: 3714, time 26.824390411376953, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3714
goal_identified
=== ep: 3715, time 26.709636211395264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3715
=== ep: 3716, time 26.74995183944702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3716
goal_identified
=== ep: 3717, time 27.08621859550476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3717
goal_identified
=== ep: 3718, time 26.91694402694702, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3718
goal_identified
=== ep: 3719, time 28.791588068008423, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3719
goal_identified
=== ep: 3720, time 27.396201372146606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3720
=== ep: 3721, time 26.61043643951416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3721
goal_identified
=== ep: 3722, time 26.752708673477173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3722
goal_identified
goal_identified
=== ep: 3723, time 26.69584846496582, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3723
=== ep: 3724, time 27.083974361419678, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3724
goal_identified
=== ep: 3725, time 26.828598022460938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3725
=== ep: 3726, time 26.621222972869873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3726
goal_identified
=== ep: 3727, time 30.05958604812622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3727
=== ep: 3728, time 27.099352598190308, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3728
=== ep: 3729, time 29.358444690704346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3729
=== ep: 3730, time 26.950430154800415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3730
goal_identified
=== ep: 3731, time 27.422341108322144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3731
=== ep: 3732, time 27.25942897796631, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3732
=== ep: 3733, time 27.01254391670227, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3733
goal_identified
=== ep: 3734, time 27.14496612548828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3734
goal_identified
=== ep: 3735, time 26.712318658828735, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3735
=== ep: 3736, time 27.395618438720703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3736
=== ep: 3737, time 27.763570308685303, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3737
=== ep: 3738, time 32.11199140548706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3738
goal_identified
=== ep: 3739, time 28.929586172103882, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3739
goal_identified
=== ep: 3740, time 27.014639616012573, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3740
goal_identified
goal_identified
=== ep: 3741, time 26.666016817092896, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3741
=== ep: 3742, time 26.58122444152832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3742
=== ep: 3743, time 26.70164465904236, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3743
goal_identified
=== ep: 3744, time 26.600167512893677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3744
goal_identified
=== ep: 3745, time 26.982439041137695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3745
goal_identified
=== ep: 3746, time 26.868643045425415, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3746
=== ep: 3747, time 31.470722198486328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 26.95123291015625, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3748
goal_identified
=== ep: 3749, time 28.866188526153564, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3749
=== ep: 3750, time 27.154174089431763, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 26.92135238647461, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3751
goal_identified
=== ep: 3752, time 26.579456090927124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3752
goal_identified
goal_identified
=== ep: 3753, time 27.425346851348877, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3753
goal_identified
=== ep: 3754, time 26.836490631103516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3754
goal_identified
=== ep: 3755, time 26.94720196723938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3755
goal_identified
=== ep: 3756, time 26.95313000679016, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3756
=== ep: 3757, time 26.9593825340271, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3757
goal_identified
=== ep: 3758, time 27.09201955795288, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3758
goal_identified
=== ep: 3759, time 29.12476348876953, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3759
goal_identified
=== ep: 3760, time 26.937891721725464, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3760
goal_identified
=== ep: 3761, time 26.754422664642334, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3761
=== ep: 3762, time 26.667264699935913, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3762
goal_identified
=== ep: 3763, time 26.958499431610107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3763
goal_identified
goal_identified
=== ep: 3764, time 26.803892374038696, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3764
goal_identified
=== ep: 3765, time 26.81020951271057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3765
=== ep: 3766, time 26.837438821792603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3766
goal_identified
=== ep: 3767, time 27.1595299243927, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3767
goal_identified
goal_identified
=== ep: 3768, time 27.04939603805542, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3768
=== ep: 3769, time 29.299455642700195, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3769
goal_identified
=== ep: 3770, time 26.823443174362183, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3770
=== ep: 3771, time 27.31690526008606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3771
=== ep: 3772, time 26.73571801185608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3772
=== ep: 3773, time 26.646422386169434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3773
goal_identified
goal_identified
=== ep: 3774, time 26.65868330001831, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3774
=== ep: 3775, time 26.817574977874756, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3775
=== ep: 3776, time 27.07813549041748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3776
goal_identified
goal_identified
=== ep: 3777, time 26.805304050445557, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3777
goal_identified
=== ep: 3778, time 27.18083357810974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3778
goal_identified
=== ep: 3779, time 36.77569770812988, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 26.49314045906067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3780
=== ep: 3781, time 29.35866689682007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3781
=== ep: 3782, time 26.87307596206665, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3782
goal_identified
goal_identified
=== ep: 3783, time 26.56281018257141, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3783
goal_identified
=== ep: 3784, time 26.895912170410156, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3784
goal_identified
=== ep: 3785, time 26.33865237236023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3785
goal_identified
=== ep: 3786, time 26.80226469039917, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3786
=== ep: 3787, time 26.961495637893677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3787
goal_identified
=== ep: 3788, time 26.82484722137451, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3788
goal_identified
goal_identified
=== ep: 3789, time 28.875778675079346, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3789
=== ep: 3790, time 27.026787519454956, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3790
goal_identified
=== ep: 3791, time 27.069658279418945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3791
=== ep: 3792, time 27.004209280014038, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3792
goal_identified
=== ep: 3793, time 26.70586609840393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3793
=== ep: 3794, time 26.805657863616943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3794
goal_identified
=== ep: 3795, time 27.039975881576538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3795
=== ep: 3796, time 27.098734378814697, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3796
=== ep: 3797, time 26.596871614456177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3797
goal_identified
=== ep: 3798, time 26.48794436454773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3798
=== ep: 3799, time 29.311416387557983, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3799
goal_identified
=== ep: 3800, time 27.222112894058228, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3800
=== ep: 3801, time 26.963669776916504, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3801
goal_identified
=== ep: 3802, time 26.967092990875244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3802
goal_identified
=== ep: 3803, time 26.94175386428833, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3803
goal_identified
=== ep: 3804, time 27.498806476593018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3804
goal_identified
=== ep: 3805, time 26.606225967407227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3805
=== ep: 3806, time 26.997063636779785, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3806
=== ep: 3807, time 27.481284618377686, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3807
=== ep: 3808, time 26.922905445098877, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3808
=== ep: 3809, time 29.137999773025513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3809
goal_identified
=== ep: 3810, time 26.826146364212036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3810
goal_identified
=== ep: 3811, time 26.91547417640686, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3811
=== ep: 3812, time 26.75624108314514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3812
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3813, time 27.060215950012207, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3813
goal_identified
=== ep: 3814, time 26.458828449249268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3814
goal_identified
goal_identified
=== ep: 3815, time 26.72807765007019, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3815
goal_identified
=== ep: 3816, time 27.119947910308838, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3816
goal_identified
=== ep: 3817, time 26.881844520568848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3817
goal_identified
goal_identified
=== ep: 3818, time 27.28251886367798, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3818
goal_identified
=== ep: 3819, time 28.678342580795288, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3819
goal_identified
=== ep: 3820, time 27.1814124584198, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3820
=== ep: 3821, time 32.13095426559448, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3821
goal_identified
goal_identified
=== ep: 3822, time 30.86577558517456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3822
goal_identified
goal_identified
=== ep: 3823, time 26.833706378936768, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3823
goal_identified
=== ep: 3824, time 27.17454743385315, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3824
goal_identified
=== ep: 3825, time 27.414644479751587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3825
goal_identified
=== ep: 3826, time 27.285595655441284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3826
=== ep: 3827, time 27.29064393043518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3827
goal_identified
goal_identified
=== ep: 3828, time 27.10877537727356, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3828
=== ep: 3829, time 29.46875834465027, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3829
=== ep: 3830, time 27.02941918373108, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3830
goal_identified
=== ep: 3831, time 27.08885359764099, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3831
=== ep: 3832, time 32.58556413650513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3832
=== ep: 3833, time 26.735093355178833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3833
=== ep: 3834, time 27.370559692382812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3834
goal_identified
=== ep: 3835, time 27.023714780807495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3835
=== ep: 3836, time 33.96758151054382, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3836
=== ep: 3837, time 26.790140867233276, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3837
goal_identified
goal_identified
=== ep: 3838, time 27.018906593322754, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3838
=== ep: 3839, time 29.079360246658325, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3839
goal_identified
goal_identified
=== ep: 3840, time 26.699988842010498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3840
=== ep: 3841, time 27.02165174484253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3841
goal_identified
goal_identified
=== ep: 3842, time 27.065434217453003, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3842
=== ep: 3843, time 27.409484148025513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3843
=== ep: 3844, time 30.40140438079834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3844
goal_identified
=== ep: 3845, time 26.83414387702942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3845
=== ep: 3846, time 26.826955318450928, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3846
=== ep: 3847, time 26.803057193756104, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3847
=== ep: 3848, time 26.906738758087158, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3848
goal_identified
goal_identified
=== ep: 3849, time 29.0914363861084, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3849
=== ep: 3850, time 26.722890615463257, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3850
=== ep: 3851, time 26.005977630615234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 2/2)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3851
=== ep: 3852, time 26.899166584014893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3852
=== ep: 3853, time 27.208973169326782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3853
goal_identified
=== ep: 3854, time 26.913068056106567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3854
=== ep: 3855, time 26.799954414367676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3855
goal_identified
goal_identified
=== ep: 3856, time 26.498851537704468, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3856
goal_identified
=== ep: 3857, time 26.542487144470215, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3857
=== ep: 3858, time 26.909088373184204, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3858
goal_identified
=== ep: 3859, time 28.97227168083191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3859
=== ep: 3860, time 32.4874746799469, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3860
=== ep: 3861, time 26.713052988052368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3861
goal_identified
=== ep: 3862, time 26.43579387664795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3862
=== ep: 3863, time 26.975568056106567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3863
=== ep: 3864, time 26.853301763534546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3864
=== ep: 3865, time 26.995975017547607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3865
=== ep: 3866, time 27.045389652252197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3866
=== ep: 3867, time 26.445146322250366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3867
=== ep: 3868, time 26.87326455116272, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3868
=== ep: 3869, time 28.947578191757202, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3869
goal_identified
=== ep: 3870, time 26.788464546203613, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3870
goal_identified
=== ep: 3871, time 26.718436002731323, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3871
=== ep: 3872, time 26.954987287521362, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3872
goal_identified
=== ep: 3873, time 27.112873077392578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3873
=== ep: 3874, time 26.884254455566406, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3874
=== ep: 3875, time 26.325920343399048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3875
=== ep: 3876, time 26.881134033203125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3876
=== ep: 3877, time 27.399852752685547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3877
=== ep: 3878, time 26.86335563659668, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3878
=== ep: 3879, time 28.917983055114746, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3879
=== ep: 3880, time 26.85139274597168, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3880
goal_identified
=== ep: 3881, time 26.872166395187378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3881
=== ep: 3882, time 26.839754581451416, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3882
=== ep: 3883, time 26.95413637161255, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3883
goal_identified
goal_identified
goal_identified
=== ep: 3884, time 26.89624261856079, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3884
goal_identified
=== ep: 3885, time 27.159440755844116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3885
=== ep: 3886, time 26.888408184051514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3886
=== ep: 3887, time 32.087886095047, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3887
=== ep: 3888, time 27.477581024169922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3888
goal_identified
=== ep: 3889, time 29.01015615463257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3889
goal_identified
goal_identified
goal_identified
=== ep: 3890, time 26.534642457962036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3890
goal_identified
goal_identified
=== ep: 3891, time 26.926374435424805, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3891
goal_identified
=== ep: 3892, time 28.685343503952026, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3892
=== ep: 3893, time 26.553749799728394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3893
goal_identified
=== ep: 3894, time 26.959977626800537, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3894
goal_identified
goal_identified
=== ep: 3895, time 26.320345163345337, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3895
=== ep: 3896, time 27.201768398284912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3896
goal_identified
=== ep: 3897, time 27.328506231307983, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3897
=== ep: 3898, time 26.793540000915527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3898
goal_identified
goal_identified
=== ep: 3899, time 28.706467151641846, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3899
=== ep: 3900, time 27.198991060256958, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3900
goal_identified
goal_identified
=== ep: 3901, time 26.864287614822388, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3901
=== ep: 3902, time 27.040109634399414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3902
goal_identified
goal_identified
=== ep: 3903, time 26.688908100128174, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3903
=== ep: 3904, time 26.903746604919434, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3904
=== ep: 3905, time 27.247806310653687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3905
=== ep: 3906, time 26.406946182250977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3906
goal_identified
goal_identified
goal_identified
=== ep: 3907, time 27.076531171798706, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3907
goal_identified
goal_identified
=== ep: 3908, time 26.831639051437378, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3908
=== ep: 3909, time 28.552104949951172, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3909
=== ep: 3910, time 27.11730432510376, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3910
=== ep: 3911, time 27.199981451034546, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3911
goal_identified
=== ep: 3912, time 26.64741802215576, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3912
=== ep: 3913, time 27.33307695388794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3913
goal_identified
=== ep: 3914, time 26.7077956199646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3914
goal_identified
=== ep: 3915, time 26.720242023468018, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3915
goal_identified
=== ep: 3916, time 27.178942680358887, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3916
=== ep: 3917, time 26.761778116226196, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3917
=== ep: 3918, time 27.049968242645264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3918
goal_identified
=== ep: 3919, time 28.690287113189697, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3919
=== ep: 3920, time 34.13361430168152, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3920
=== ep: 3921, time 27.60416054725647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3921
goal_identified
=== ep: 3922, time 26.9211266040802, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3922
goal_identified
goal_identified
=== ep: 3923, time 28.593900203704834, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3923
=== ep: 3924, time 26.996342420578003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3924
=== ep: 3925, time 26.85078454017639, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3925
=== ep: 3926, time 26.75751757621765, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3926
=== ep: 3927, time 26.805169582366943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3927
=== ep: 3928, time 27.163172721862793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3928
goal_identified
goal_identified
=== ep: 3929, time 28.635526657104492, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3929
goal_identified
=== ep: 3930, time 26.657211303710938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3930
=== ep: 3931, time 27.192975521087646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3931
goal_identified
=== ep: 3932, time 26.931183099746704, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3932
=== ep: 3933, time 26.904550075531006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3933
goal_identified
=== ep: 3934, time 26.71226954460144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3934
goal_identified
=== ep: 3935, time 26.39011311531067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3935
=== ep: 3936, time 27.34875202178955, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3936
goal_identified
goal_identified
=== ep: 3937, time 27.193383932113647, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3937
goal_identified
=== ep: 3938, time 26.831965923309326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3938
goal_identified
=== ep: 3939, time 29.019582748413086, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3939
goal_identified
=== ep: 3940, time 30.216053247451782, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3940
=== ep: 3941, time 27.28917407989502, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3941
=== ep: 3942, time 27.101593017578125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3942
=== ep: 3943, time 26.70540976524353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3943
goal_identified
=== ep: 3944, time 27.199692010879517, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3944
goal_identified
goal_identified
=== ep: 3945, time 26.817485332489014, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3945
=== ep: 3946, time 27.39222025871277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3946
goal_identified
=== ep: 3947, time 27.149859189987183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3947
=== ep: 3948, time 32.617209672927856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3948
=== ep: 3949, time 28.617355585098267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3949
=== ep: 3950, time 27.294761180877686, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3950
goal_identified
goal_identified
=== ep: 3951, time 26.757651567459106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3951
=== ep: 3952, time 27.11885380744934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3952
goal_identified
=== ep: 3953, time 27.140060424804688, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3953
=== ep: 3954, time 27.072139263153076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3954
=== ep: 3955, time 26.429391622543335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3955
goal_identified
=== ep: 3956, time 26.79188895225525, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3956
=== ep: 3957, time 24.107211112976074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3957
goal_identified
goal_identified
=== ep: 3958, time 26.646304845809937, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3958
=== ep: 3959, time 29.049818515777588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3959
goal_identified
=== ep: 3960, time 27.029991149902344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3960
goal_identified
=== ep: 3961, time 26.787402868270874, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3961
goal_identified
=== ep: 3962, time 26.90848684310913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3962
goal_identified
goal_identified
=== ep: 3963, time 27.34409213066101, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3963
goal_identified
goal_identified
=== ep: 3964, time 30.0812771320343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3964
goal_identified
goal_identified
=== ep: 3965, time 26.457267999649048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3965
=== ep: 3966, time 26.80716323852539, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3966
goal_identified
goal_identified
=== ep: 3967, time 27.43574070930481, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3967
=== ep: 3968, time 26.82649302482605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3968
=== ep: 3969, time 29.176489114761353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3969
goal_identified
=== ep: 3970, time 27.03424620628357, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3970
=== ep: 3971, time 26.5249924659729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3971
goal_identified
goal_identified
=== ep: 3972, time 27.25232434272766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3972
=== ep: 3973, time 27.403240442276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3973
goal_identified
=== ep: 3974, time 26.709322690963745, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3974
=== ep: 3975, time 27.29893398284912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3975
goal_identified
goal_identified
=== ep: 3976, time 26.80168604850769, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3976
goal_identified
goal_identified
=== ep: 3977, time 26.74867844581604, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3977
=== ep: 3978, time 26.84758710861206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3978
=== ep: 3979, time 33.50708532333374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3979
goal_identified
=== ep: 3980, time 26.882863998413086, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3980
=== ep: 3981, time 27.22870969772339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3981
goal_identified
goal_identified
=== ep: 3982, time 26.650126218795776, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3982
goal_identified
goal_identified
=== ep: 3983, time 26.73376774787903, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3983
=== ep: 3984, time 27.331233263015747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3984
goal_identified
=== ep: 3985, time 27.20961904525757, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3985
=== ep: 3986, time 34.70976662635803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3986
=== ep: 3987, time 26.907187938690186, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3987
=== ep: 3988, time 29.689802408218384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3988
goal_identified
=== ep: 3989, time 28.890561819076538, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3989
=== ep: 3990, time 26.691853046417236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3990
=== ep: 3991, time 26.747968673706055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3991
=== ep: 3992, time 27.38703966140747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3992
=== ep: 3993, time 26.935477018356323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3993
goal_identified
=== ep: 3994, time 27.345401525497437, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3994
=== ep: 3995, time 27.28461265563965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3995
goal_identified
=== ep: 3996, time 30.44294238090515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3996
=== ep: 3997, time 26.935110330581665, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3997
=== ep: 3998, time 27.326151132583618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3998
=== ep: 3999, time 28.766804456710815, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
