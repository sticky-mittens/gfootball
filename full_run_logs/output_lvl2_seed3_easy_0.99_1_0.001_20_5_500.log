==> Playing in 11_vs_11_easy_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
goal_identified
=== ep: 0, time 27.813454151153564, eps 0.9, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
goal_identified
=== ep: 1, time 27.09794592857361, eps 0.8561552526261419, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
=== ep: 2, time 27.55151081085205, eps 0.8144488388143276, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
goal_identified
goal_identified
=== ep: 3, time 27.763336420059204, eps 0.774776470806127, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
=== ep: 4, time 31.031760215759277, eps 0.7370389470171057, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
goal_identified
goal_identified
=== ep: 5, time 30.154234647750854, eps 0.701141903981193, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 6, time 31.56751251220703, eps 0.6669955803928644, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
goal_identified
goal_identified
=== ep: 7, time 31.549104928970337, eps 0.6345145926571234, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
goal_identified
=== ep: 8, time 29.011847019195557, eps 0.6036177213860398, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
goal_identified
=== ep: 9, time 31.20988941192627, eps 0.5742277083079742, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
goal_identified
goal_identified
=== ep: 10, time 31.59917140007019, eps 0.5462710630816575, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
goal_identified
goal_identified
=== ep: 11, time 31.461172342300415, eps 0.5196778795320575, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
=== ep: 12, time 32.68036985397339, eps 0.49438166084852986, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
goal_identified
goal_identified
=== ep: 13, time 33.18248462677002, eps 0.47031915330815344, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
goal_identified
=== ep: 14, time 30.821658611297607, eps 0.4474301881084772, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
=== ep: 15, time 33.957908153533936, eps 0.42565753091417224, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
=== ep: 16, time 34.60408282279968, eps 0.4049467387413822, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
=== ep: 17, time 39.373581886291504, eps 0.3852460238219053, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
goal_identified
goal_identified
goal_identified
=== ep: 18, time 41.63641095161438, eps 0.3665061241067986, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 19, time 39.50320887565613, eps 0.3486801800855966, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
goal_identified
goal_identified
goal_identified
=== ep: 20, time 36.52681016921997, eps 0.3317236176131267, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
goal_identified
=== ep: 21, time 38.971702098846436, eps 0.31559403645092865, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
=== ep: 22, time 41.77027082443237, eps 0.3002511042445735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
goal_identified
=== ep: 23, time 42.47806739807129, eps 0.2856564556717689, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
=== ep: 24, time 39.23179507255554, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 25, time 37.711313247680664, eps 0.2585678123773109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
=== ep: 26, time 42.53118181228638, eps 0.24600608193757734, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
=== ep: 27, time 41.90370559692383, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
=== ep: 28, time 47.463457107543945, eps 0.22269067058350425, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
goal_identified
=== ep: 29, time 48.23882174491882, eps 0.2118786889963241, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
goal_identified
=== ep: 30, time 48.466397762298584, eps 0.2015940139734384, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
=== ep: 31, time 48.32469463348389, eps 0.191810928470242, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
goal_identified
=== ep: 32, time 46.501333475112915, eps 0.1825049696771952, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
=== ep: 33, time 40.33728241920471, eps 0.17365286785005798, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 34, time 46.601542472839355, eps 0.16523248812340846, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
goal_identified
goal_identified
=== ep: 35, time 44.06087112426758, eps 0.15722277516195018, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
goal_identified
goal_identified
=== ep: 36, time 45.756593227386475, eps 0.1496037005112063, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
=== ep: 37, time 43.52708435058594, eps 0.14235621251595124, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
goal_identified
=== ep: 38, time 47.081196308135986, eps 0.13546218868114893, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 39, time 46.74552941322327, eps 0.1289043903562757, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
=== ep: 40, time 45.6268949508667, eps 0.12266641962971482, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
goal_identified
goal_identified
=== ep: 41, time 50.92443132400513, eps 0.116732678325436, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
=== ep: 42, time 46.7748646736145, eps 0.11108832899943073, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
=== ep: 43, time 49.56727695465088, eps 0.10571925783837377, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
goal_identified
goal_identified
=== ep: 44, time 50.02669596672058, eps 0.10061203936773815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
=== ep: 45, time 54.43379354476929, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
goal_identified
=== ep: 46, time 52.7086980342865, eps 0.09113270050680057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
goal_identified
goal_identified
goal_identified
=== ep: 47, time 52.840837717056274, eps 0.08673687683177911, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
goal_identified
goal_identified
=== ep: 48, time 55.52246594429016, eps 0.08255544000718185, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 49, time 61.12751817703247, eps 0.07857793426293408, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
=== ep: 50, time 63.39093089103699, eps 0.07479441376288502, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
goal_identified
=== ep: 51, time 50.43168878555298, eps 0.0711954177350367, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
=== ep: 52, time 50.720062494277954, eps 0.06777194681468615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
=== ep: 53, time 44.952956676483154, eps 0.06451544054132621, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
=== ep: 54, time 52.40363693237305, eps 0.06141775595303503, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 52.457661390304565, eps 0.05847114722483011, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 51.82402968406677, eps 0.05566824630007096, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
=== ep: 57, time 55.32747006416321, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
goal_identified
=== ep: 58, time 62.15320348739624, eps 0.050465874830710106, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
goal_identified
goal_identified
=== ep: 59, time 61.4291205406189, eps 0.04805339564764071, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
goal_identified
goal_identified
=== ep: 60, time 48.40543246269226, eps 0.045758574462709686, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
=== ep: 61, time 55.70109677314758, eps 0.043575673027635695, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
=== ep: 62, time 48.38402557373047, eps 0.04149923295180846, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
=== ep: 63, time 56.97852659225464, eps 0.03952406205346913, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
=== ep: 64, time 52.17522382736206, eps 0.03764522137655123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
goal_identified
goal_identified
=== ep: 65, time 59.68206262588501, eps 0.03585801284071809, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
goal_identified
=== ep: 66, time 61.81299638748169, eps 0.034157967493714775, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
=== ep: 67, time 75.3329029083252, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 68, time 73.26722860336304, eps 0.031002569694333147, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
=== ep: 69, time 68.52267861366272, eps 0.02953932710388308, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 70, time 61.184253215789795, eps 0.028147447696664333, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
=== ep: 71, time 72.99266481399536, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 67.22436594963074, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
goal_identified
goal_identified
=== ep: 73, time 79.12713074684143, eps 0.02436602477210106, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
=== ep: 74, time 82.16967153549194, eps 0.02322645029683511, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
=== ep: 75, time 94.52955174446106, eps 0.02214245352455219, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
=== ep: 76, time 89.01169538497925, eps 0.02111132389869288, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
goal_identified
=== ep: 77, time 81.92417120933533, eps 0.020130483058101077, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
=== ep: 78, time 85.59429359436035, eps 0.019197478389778148, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
=== ep: 79, time 93.27908706665039, eps 0.018309976896072843, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 80, time 103.24631571769714, eps 0.017465759360972027, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
=== ep: 81, time 100.01851439476013, eps 0.01666271480090467, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
=== ep: 82, time 93.18410849571228, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
goal_identified
goal_identified
=== ep: 83, time 97.75753784179688, eps 0.015172210419884185, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
=== ep: 84, time 91.28307175636292, eps 0.014481023561609456, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
=== ep: 85, time 100.83495378494263, eps 0.01382354628419033, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
=== ep: 86, time 111.66950869560242, eps 0.013198134551968641, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
=== ep: 87, time 106.11958289146423, eps 0.012603224509851407, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
goal_identified
goal_identified
=== ep: 88, time 91.11882472038269, eps 0.012037328572858524, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
goal_identified
=== ep: 89, time 105.99927711486816, eps 0.011499031706385502, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
goal_identified
=== ep: 90, time 103.73718857765198, eps 0.010986987887879832, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
goal_identified
=== ep: 91, time 111.87443995475769, eps 0.010499916741083536, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
=== ep: 92, time 115.5913188457489, eps 0.010036600334425595, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
goal_identified
goal_identified
=== ep: 93, time 100.99014854431152, eps 0.00959588013555861, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
=== ep: 94, time 90.61439418792725, eps 0.009176654114424539, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
goal_identified
=== ep: 95, time 101.94249176979065, eps 0.00877787398760545, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
=== ep: 96, time 113.84565877914429, eps 0.008398542597069007, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
goal_identified
=== ep: 97, time 110.52380537986755, eps 0.008037711416753971, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
goal_identified
=== ep: 98, time 105.14756107330322, eps 0.00769447818076098, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
=== ep: 99, time 98.96473479270935, eps 0.007367984627217855, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 95.74668312072754, eps 0.007057414352177835, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
goal_identified
goal_identified
goal_identified
=== ep: 101, time 117.02016162872314, eps 0.006761990768184489, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
=== ep: 102, time 112.1365966796875, eps 0.006480975162398559, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
goal_identified
=== ep: 103, time 104.65546131134033, eps 0.006213664849431085, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
goal_identified
=== ep: 104, time 100.97783875465393, eps 0.005959391414263934, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
=== ep: 105, time 104.36363506317139, eps 0.005717519040864065, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
goal_identified
=== ep: 106, time 114.96333575248718, eps 0.005487442922312285, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
=== ep: 107, time 108.43545866012573, eps 0.005268587748470919, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
goal_identified
goal_identified
=== ep: 108, time 103.3621575832367, eps 0.005060406267408787, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
=== ep: 109, time 105.66406583786011, eps 0.004862377916986354, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
goal_identified
goal_identified
=== ep: 110, time 103.9073121547699, eps 0.004674007523179196, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
=== ep: 111, time 123.23356246948242, eps 0.004494824061885041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
=== ep: 112, time 110.7291431427002, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
goal_identified
goal_identified
=== ep: 113, time 102.20592474937439, eps 0.0041622475806460035, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
goal_identified
goal_identified
=== ep: 114, time 101.50740122795105, eps 0.0040080229462666735, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
goal_identified
=== ep: 115, time 114.8446433544159, eps 0.0038613199360621906, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 116, time 124.72757482528687, eps 0.003721771716092858, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
=== ep: 117, time 112.95949125289917, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
=== ep: 118, time 118.45861172676086, eps 0.0034627608920727634, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
=== ep: 119, time 111.02325367927551, eps 0.00334265062604924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 27/27)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
goal_identified
=== ep: 120, time 121.31366777420044, eps 0.0032283982068230565, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 129.22852277755737, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 106.70251107215881, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 123, time 98.07974743843079, eps 0.0029180001112638996, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
=== ep: 124, time 119.23685669898987, eps 0.002824458142029865, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 129.59641575813293, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
=== ep: 126, time 116.44669103622437, eps 0.0026508379945489875, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
=== ep: 127, time 114.31412243843079, eps 0.0025703256754987464, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
goal_identified
=== ep: 128, time 116.71074938774109, eps 0.0024937399885833667, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
goal_identified
=== ep: 129, time 130.5785939693451, eps 0.0024208894296938593, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
goal_identified
goal_identified
=== ep: 130, time 112.4246301651001, eps 0.0023515918344868374, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
goal_identified
goal_identified
=== ep: 131, time 107.22981071472168, eps 0.002285673922878779, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
goal_identified
goal_identified
goal_identified
=== ep: 132, time 124.51759147644043, eps 0.0022229708657555565, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
=== ep: 133, time 136.65154337882996, eps 0.0021633258728137976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
goal_identified
goal_identified
goal_identified
=== ep: 134, time 124.39335584640503, eps 0.0021065898005034594, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
=== ep: 135, time 136.09588360786438, eps 0.002052620779091266, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
goal_identified
goal_identified
=== ep: 136, time 124.38913488388062, eps 0.0020012838579124784, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 145.2405195236206, eps 0.0019524506679239415, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
goal_identified
=== ep: 138, time 124.63414740562439, eps 0.001905999100714611, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 139, time 127.67581558227539, eps 0.001861813003170924, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
goal_identified
goal_identified
=== ep: 140, time 144.2754933834076, eps 0.0018197818870335101, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
=== ep: 141, time 134.06339621543884, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
goal_identified
goal_identified
=== ep: 142, time 131.03328776359558, eps 0.0017417693260160481, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
goal_identified
=== ep: 143, time 135.99894428253174, eps 0.0017055928090985275, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 144, time 138.90138983726501, eps 0.0016711806417306348, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
=== ep: 145, time 141.1032531261444, eps 0.0016384467755694515, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 143.16402864456177, eps 0.0016073093588992661, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
goal_identified
goal_identified
=== ep: 147, time 128.5728633403778, eps 0.0015776905319596466, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
goal_identified
=== ep: 148, time 158.71069073677063, eps 0.0015495162322554856, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 149, time 135.26612210273743, eps 0.0015227160093621863, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
goal_identified
goal_identified
=== ep: 150, time 150.6298487186432, eps 0.0014972228487629025, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 143.30986046791077, eps 0.0014729730042773413, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
=== ep: 152, time 152.1802954673767, eps 0.001449905838663109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 142.50008845329285, eps 0.00142796367199102, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
=== ep: 154, time 136.01813912391663, eps 0.0014070916374152305, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
=== ep: 155, time 154.0224769115448, eps 0.001387237543977543, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 156, time 145.73641633987427, eps 0.0013683517461028282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
goal_identified
=== ep: 157, time 134.91216921806335, eps 0.0013503870194592265, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 135.91812801361084, eps 0.0013332984428727204, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
=== ep: 159, time 154.58972668647766, eps 0.001317043286000802, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
goal_identified
goal_identified
goal_identified
=== ep: 160, time 139.33593583106995, eps 0.0013015809024843582, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 135.3142991065979, eps 0.0012868726283106018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
=== ep: 162, time 136.64839792251587, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
=== ep: 163, time 160.0474283695221, eps 0.0012595730883057546, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
=== ep: 164, time 137.78474807739258, eps 0.001246913559404956, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
=== ep: 165, time 138.59257626533508, eps 0.0012348714430141991, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
=== ep: 166, time 141.41060304641724, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
=== ep: 167, time 147.7076416015625, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
goal_identified
=== ep: 168, time 129.13457918167114, eps 0.0012021557244367845, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 124.18039655685425, eps 0.0011922964734155277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
goal_identified
goal_identified
=== ep: 170, time 149.6965868473053, eps 0.001182918063740569, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
goal_identified
goal_identified
=== ep: 171, time 151.57316088676453, eps 0.0011739970445027263, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
goal_identified
goal_identified
goal_identified
=== ep: 172, time 143.30649328231812, eps 0.0011655111085071537, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
=== ep: 173, time 137.2771019935608, eps 0.001157439036493735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
=== ep: 174, time 172.2628824710846, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
goal_identified
goal_identified
=== ep: 175, time 148.2600612640381, eps 0.0011424567312790603, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
=== ep: 176, time 148.44431567192078, eps 0.0011355090345108335, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
goal_identified
goal_identified
=== ep: 177, time 158.96526718139648, eps 0.0011289001809123877, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
=== ep: 178, time 161.32851481437683, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
=== ep: 179, time 151.7374165058136, eps 0.001116633706881133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 180, time 139.4984154701233, eps 0.001110945413873925, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
goal_identified
goal_identified
=== ep: 181, time 158.92472577095032, eps 0.001105534542190287, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
=== ep: 182, time 136.58871603012085, eps 0.0011003875618326132, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 183, time 147.20306658744812, eps 0.0010954916026690664, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
=== ep: 184, time 148.24749159812927, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
=== ep: 185, time 150.02080941200256, eps 0.0010864043752031938, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 186, time 139.01876759529114, eps 0.0010821903840988777, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
=== ep: 187, time 139.19214367866516, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
goal_identified
=== ep: 188, time 163.72080731391907, eps 0.0010743689349354123, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
=== ep: 189, time 151.42563438415527, eps 0.0010707419191793434, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
goal_identified
goal_identified
goal_identified
=== ep: 190, time 131.71348190307617, eps 0.0010672917950690429, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
goal_identified
goal_identified
=== ep: 191, time 149.2574372291565, eps 0.0010640099354971456, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 138.758225440979, eps 0.0010608881341052777, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 130.93882179260254, eps 0.0010579185847638855, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
=== ep: 194, time 134.63430738449097, eps 0.0010550938620528466, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
goal_identified
=== ep: 195, time 148.24591159820557, eps 0.001052406902694051, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
=== ep: 196, time 132.62453436851501, eps 0.001049850987889527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
=== ep: 197, time 149.10567355155945, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
=== ep: 198, time 137.18443608283997, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
=== ep: 199, time 157.22760391235352, eps 0.001042907142909185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
=== ep: 200, time 131.68486785888672, eps 0.001040814536856474, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
=== ep: 201, time 143.61875128746033, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
goal_identified
=== ep: 202, time 136.20420122146606, eps 0.0010369305201475454, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
goal_identified
goal_identified
goal_identified
=== ep: 203, time 159.76230001449585, eps 0.0010351293974264616, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
goal_identified
=== ep: 204, time 150.66347694396973, eps 0.00103341611649703, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
goal_identified
=== ep: 205, time 135.88035202026367, eps 0.0010317863932645186, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
=== ep: 206, time 164.16635489463806, eps 0.0010302361525719613, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
=== ep: 207, time 153.2551772594452, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
=== ep: 208, time 142.49218583106995, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
=== ep: 209, time 151.93782258033752, eps 0.0010260244976950921, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
goal_identified
goal_identified
=== ep: 210, time 149.93754124641418, eps 0.0010247552679654227, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
goal_identified
goal_identified
goal_identified
=== ep: 211, time 152.7866566181183, eps 0.00102354793930011, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
goal_identified
=== ep: 212, time 131.4153754711151, eps 0.0010223994927486214, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
goal_identified
=== ep: 213, time 168.13751077651978, eps 0.001021307056596379, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
=== ep: 214, time 143.8100209236145, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
=== ep: 215, time 140.18278765678406, eps 0.0010192794220766138, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
goal_identified
=== ep: 216, time 144.3468849658966, eps 0.0010183391535666436, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
goal_identified
=== ep: 217, time 156.1728549003601, eps 0.0010174447424930286, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
=== ep: 218, time 145.14740467071533, eps 0.0010165939523622068, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
=== ep: 219, time 149.18508219718933, eps 0.0010157846557556941, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 160.81171655654907, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 138.12620544433594, eps 0.0010142825471585687, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
goal_identified
=== ep: 222, time 125.88623404502869, eps 0.0010135859791140496, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
goal_identified
goal_identified
=== ep: 223, time 152.33793020248413, eps 0.0010129233830939361, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
=== ep: 224, time 147.17967891693115, eps 0.0010122931022630473, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
goal_identified
=== ep: 225, time 143.87975192070007, eps 0.001011693560591007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
goal_identified
=== ep: 226, time 135.08349442481995, eps 0.0010111232589113477, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
goal_identified
goal_identified
goal_identified
=== ep: 227, time 147.79187965393066, eps 0.0010105807711728136, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
=== ep: 228, time 139.4824616909027, eps 0.0010100647408734893, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 145.05061221122742, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
goal_identified
goal_identified
=== ep: 230, time 144.0939326286316, eps 0.001009106954145169, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
=== ep: 231, time 140.52650594711304, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
goal_identified
=== ep: 232, time 126.47533893585205, eps 0.0010082403128748867, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
=== ep: 233, time 154.30798077583313, eps 0.0010078384280736842, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 234, time 124.69331789016724, eps 0.001007456143425521, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 235, time 117.30618405342102, eps 0.001007092503019653, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
goal_identified
goal_identified
goal_identified
=== ep: 236, time 112.39128637313843, eps 0.001006746597565654, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
=== ep: 237, time 118.30130243301392, eps 0.001006417562119715, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
goal_identified
goal_identified
=== ep: 238, time 104.30112051963806, eps 0.0010061045739218342, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
goal_identified
goal_identified
=== ep: 239, time 109.68934226036072, eps 0.0010058068503384884, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
=== ep: 240, time 109.51921510696411, eps 0.001005523646905642, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
goal_identified
=== ep: 241, time 128.45726561546326, eps 0.001005254255467199, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
=== ep: 242, time 110.98074531555176, eps 0.0010049980024042435, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
goal_identified
goal_identified
=== ep: 243, time 119.23054528236389, eps 0.0010047542469506416, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 244, time 109.75172352790833, eps 0.0010045223795907931, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
=== ep: 245, time 119.57478332519531, eps 0.001004301820535524, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 246, time 106.60419344902039, eps 0.0010040920182723119, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
goal_identified
goal_identified
=== ep: 247, time 95.53980159759521, eps 0.0010038924481862177, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
=== ep: 248, time 102.04392766952515, eps 0.0010037026112480747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 249, time 103.60808253288269, eps 0.0010035220327666559, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
goal_identified
goal_identified
=== ep: 250, time 104.72472071647644, eps 0.0010033502612016988, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 251, time 93.22272753715515, eps 0.001003186867034819, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
=== ep: 252, time 113.15298199653625, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
goal_identified
goal_identified
=== ep: 253, time 110.48681211471558, eps 0.0010028835965394094, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
goal_identified
goal_identified
goal_identified
=== ep: 254, time 112.23264026641846, eps 0.0010027429618766747, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 255, time 97.96270871162415, eps 0.0010026091860473767, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
goal_identified
=== ep: 256, time 100.57552814483643, eps 0.0010024819345422614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
goal_identified
goal_identified
=== ep: 257, time 104.74808979034424, eps 0.0010023608891662839, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
goal_identified
goal_identified
=== ep: 258, time 100.7585597038269, eps 0.001002245747242954, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
goal_identified
goal_identified
goal_identified
=== ep: 259, time 110.5104672908783, eps 0.0010021362208574892, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
goal_identified
goal_identified
goal_identified
=== ep: 260, time 105.53679299354553, eps 0.001002032036136876, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
goal_identified
=== ep: 261, time 108.07873725891113, eps 0.0010019329325650452, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 107.64472341537476, eps 0.0010018386623314465, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
goal_identified
goal_identified
goal_identified
=== ep: 263, time 115.11569094657898, eps 0.0010017489897113931, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
goal_identified
goal_identified
=== ep: 264, time 110.28555989265442, eps 0.0010016636904766263, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
goal_identified
goal_identified
=== ep: 265, time 108.58950400352478, eps 0.0010015825513346283, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
goal_identified
goal_identified
=== ep: 266, time 111.15138030052185, eps 0.0010015053693952815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
=== ep: 267, time 116.30633401870728, eps 0.0010014319516635345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
goal_identified
=== ep: 268, time 114.15615510940552, eps 0.0010013621145568167, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
=== ep: 269, time 110.35025191307068, eps 0.0010012956834459848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 270, time 91.06709289550781, eps 0.0010012324922186594, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
goal_identified
=== ep: 271, time 103.640371799469, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 105.72180652618408, eps 0.0010011152050768812, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
=== ep: 273, time 96.6839849948883, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
=== ep: 274, time 109.37303447723389, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 275, time 97.28027534484863, eps 0.0010009598659050213, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
=== ep: 276, time 98.87260365486145, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
goal_identified
=== ep: 277, time 99.36220145225525, eps 0.0010008685225871602, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
goal_identified
goal_identified
goal_identified
=== ep: 278, time 95.2102952003479, eps 0.0010008261642407504, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
goal_identified
goal_identified
goal_identified
=== ep: 279, time 104.6078896522522, eps 0.001000785871735272, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
goal_identified
goal_identified
=== ep: 280, time 103.37178683280945, eps 0.0010007475443184742, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
goal_identified
=== ep: 281, time 94.03327441215515, eps 0.001000711086151851, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
goal_identified
goal_identified
=== ep: 282, time 99.10735154151917, eps 0.0010006764060709957, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
goal_identified
goal_identified
goal_identified
=== ep: 283, time 102.7674617767334, eps 0.001000643417357642, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
goal_identified
goal_identified
goal_identified
=== ep: 284, time 100.26556611061096, eps 0.0010006120375228235, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
=== ep: 285, time 84.41732215881348, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 104.3089108467102, eps 0.0010005537944518927, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
goal_identified
goal_identified
goal_identified
=== ep: 287, time 91.52374720573425, eps 0.0010005267855777657, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 97.40862107276917, eps 0.0010005010939419733, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
goal_identified
goal_identified
goal_identified
=== ep: 289, time 102.06425023078918, eps 0.001000476655302044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
=== ep: 290, time 101.79093074798584, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
goal_identified
=== ep: 291, time 103.66280460357666, eps 0.0010004312955527947, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 292, time 97.27066659927368, eps 0.0010004102610204745, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
goal_identified
goal_identified
=== ep: 293, time 95.29714846611023, eps 0.0010003902523544011, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
=== ep: 294, time 105.59277009963989, eps 0.0010003712195224871, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
goal_identified
goal_identified
=== ep: 295, time 105.79704093933105, eps 0.0010003531149327387, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
goal_identified
=== ep: 296, time 106.18367624282837, eps 0.0010003358933142518, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
goal_identified
=== ep: 297, time 108.36857557296753, eps 0.0010003195116040093, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
goal_identified
goal_identified
goal_identified
=== ep: 298, time 96.01872372627258, eps 0.0010003039288392032, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
goal_identified
goal_identified
=== ep: 299, time 114.26949000358582, eps 0.0010002891060548044, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
goal_identified
goal_identified
=== ep: 300, time 111.6646375656128, eps 0.0010002750061861312, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
goal_identified
goal_identified
=== ep: 301, time 96.0691864490509, eps 0.0010002615939761676, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
goal_identified
goal_identified
=== ep: 302, time 100.29777431488037, eps 0.001000248835887403, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
=== ep: 303, time 101.43139481544495, eps 0.0010002367000179694, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
=== ep: 304, time 118.25031065940857, eps 0.0010002251560218723, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
goal_identified
goal_identified
goal_identified
=== ep: 305, time 99.30840110778809, eps 0.0010002141750331084, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
goal_identified
=== ep: 306, time 108.88128185272217, eps 0.0010002037295934862, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
goal_identified
=== ep: 307, time 104.0319995880127, eps 0.0010001937935839656, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
goal_identified
=== ep: 308, time 109.6951277256012, eps 0.0010001843421593476, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
goal_identified
goal_identified
=== ep: 309, time 110.10847496986389, eps 0.0010001753516861473, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
goal_identified
=== ep: 310, time 118.40533542633057, eps 0.0010001667996834991, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
goal_identified
=== ep: 311, time 88.29942178726196, eps 0.001000158664766942, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
goal_identified
goal_identified
=== ep: 312, time 101.71045279502869, eps 0.0010001509265949466, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
goal_identified
goal_identified
goal_identified
=== ep: 313, time 100.96701836585999, eps 0.001000143565818053, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
goal_identified
goal_identified
=== ep: 314, time 107.0610556602478, eps 0.0010001365640304844, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
goal_identified
goal_identified
=== ep: 315, time 98.73083066940308, eps 0.0010001299037241253, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
goal_identified
=== ep: 316, time 104.51303672790527, eps 0.0010001235682447402, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 317, time 99.18097877502441, eps 0.0010001175417503308, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 99.9408221244812, eps 0.0010001118091715218, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 113.88509106636047, eps 0.0010001063561738807, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
goal_identified
=== ep: 320, time 112.7985486984253, eps 0.0010001011691220727, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
goal_identified
goal_identified
goal_identified
=== ep: 321, time 99.0959701538086, eps 0.0010000962350457665, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
=== ep: 322, time 101.46667504310608, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
goal_identified
goal_identified
goal_identified
=== ep: 323, time 111.12430143356323, eps 0.0010000870770703358, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
goal_identified
goal_identified
=== ep: 324, time 99.12434554100037, eps 0.0010000828302715028, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 325, time 89.27895665168762, eps 0.0010000787905914928, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
=== ep: 326, time 107.85292530059814, eps 0.0010000749479290019, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 327, time 101.47899508476257, eps 0.001000071292675372, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 328, time 111.18309211730957, eps 0.001000067815690565, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
goal_identified
=== ep: 329, time 112.65881752967834, eps 0.0010000645082803084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
=== ep: 330, time 101.58056044578552, eps 0.0010000613621743532, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
goal_identified
=== ep: 331, time 114.68363857269287, eps 0.0010000583695057963, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
goal_identified
=== ep: 332, time 101.80426216125488, eps 0.0010000555227914069, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
goal_identified
goal_identified
=== ep: 333, time 93.8509030342102, eps 0.0010000528149129166, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 334, time 105.34649705886841, eps 0.0010000502390992187, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
=== ep: 335, time 103.82922220230103, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
=== ep: 336, time 96.11981105804443, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 337, time 105.26657629013062, eps 0.001000043241193426, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
=== ep: 338, time 107.83412170410156, eps 0.0010000411322955373, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 339, time 108.08624243736267, eps 0.0010000391262498123, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
goal_identified
goal_identified
goal_identified
=== ep: 340, time 84.90184998512268, eps 0.001000037218040092, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
=== ep: 341, time 88.29976153373718, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
goal_identified
goal_identified
goal_identified
=== ep: 342, time 102.61091446876526, eps 0.0010000336762753012, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
goal_identified
goal_identified
=== ep: 343, time 105.48191237449646, eps 0.001000032033863974, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
goal_identified
goal_identified
=== ep: 344, time 95.96150946617126, eps 0.0010000304715539925, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
=== ep: 345, time 100.72544884681702, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
goal_identified
=== ep: 346, time 91.77574825286865, eps 0.001000027571802238, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
goal_identified
goal_identified
goal_identified
=== ep: 347, time 97.86651468276978, eps 0.0010000262271095755, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
goal_identified
goal_identified
=== ep: 348, time 98.76651191711426, eps 0.0010000249479983478, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
=== ep: 349, time 100.4556770324707, eps 0.0010000237312701107, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 350, time 97.76245713233948, eps 0.00100002257388241, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 351, time 99.52226614952087, eps 0.0010000214729411737, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 352, time 99.29964280128479, eps 0.0010000204256934752, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
goal_identified
goal_identified
=== ep: 353, time 99.58795833587646, eps 0.0010000194295206493, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 354, time 86.23175382614136, eps 0.0010000184819317455, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
goal_identified
goal_identified
=== ep: 355, time 101.18206977844238, eps 0.001000017580557298, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 356, time 97.87923073768616, eps 0.001000016723143401, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
=== ep: 357, time 88.1857328414917, eps 0.0010000159075460732, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
=== ep: 358, time 91.5956699848175, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
goal_identified
goal_identified
goal_identified
=== ep: 359, time 97.9344117641449, eps 0.0010000143937429161, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 360, time 98.76710438728333, eps 0.0010000136917517905, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
goal_identified
=== ep: 361, time 95.78215622901917, eps 0.001000013023997176, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 99.20408058166504, eps 0.0010000123888093385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
goal_identified
goal_identified
goal_identified
=== ep: 363, time 102.54200768470764, eps 0.0010000117845999773, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
=== ep: 364, time 92.09045886993408, eps 0.0010000112098582543, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
goal_identified
goal_identified
goal_identified
=== ep: 365, time 85.89011716842651, eps 0.001000010663147016, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
goal_identified
goal_identified
goal_identified
=== ep: 366, time 102.1498692035675, eps 0.0010000101430991996, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
goal_identified
goal_identified
=== ep: 367, time 99.86312651634216, eps 0.0010000096484144142, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 97.94413471221924, eps 0.0010000091778556905, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 99.90355134010315, eps 0.0010000087302463867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 370, time 100.67132639884949, eps 0.001000008304467246, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
=== ep: 371, time 102.43598628044128, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
goal_identified
goal_identified
=== ep: 372, time 86.46112060546875, eps 0.0010000075141927012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
=== ep: 373, time 108.45610737800598, eps 0.0010000071477211988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
=== ep: 374, time 96.07722091674805, eps 0.0010000067991227223, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
goal_identified
goal_identified
=== ep: 375, time 107.32150363922119, eps 0.0010000064675255943, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
goal_identified
=== ep: 376, time 98.19062304496765, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 377, time 85.02024483680725, eps 0.0010000058520591598, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
goal_identified
=== ep: 378, time 97.09617614746094, eps 0.0010000055666508666, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
=== ep: 379, time 110.70947599411011, eps 0.0010000052951621003, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
=== ep: 380, time 107.72447609901428, eps 0.0010000050369139975, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
=== ep: 381, time 106.64360308647156, eps 0.001000004791260803, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
goal_identified
=== ep: 382, time 91.2649872303009, eps 0.0010000045575882562, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 101.05701160430908, eps 0.001000004335312054, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
goal_identified
goal_identified
=== ep: 384, time 95.17030668258667, eps 0.0010000041238763903, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 94.48521685600281, eps 0.0010000039227525655, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
goal_identified
goal_identified
goal_identified
=== ep: 386, time 106.84108185768127, eps 0.0010000037314376652, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 387, time 103.60476112365723, eps 0.001000003549453303, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
goal_identified
goal_identified
=== ep: 388, time 101.38508701324463, eps 0.0010000033763444226, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 106.74668860435486, eps 0.001000003211678162, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
goal_identified
=== ep: 390, time 96.75713682174683, eps 0.0010000030550427698, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 107.37088322639465, eps 0.0010000029060465757, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
goal_identified
goal_identified
=== ep: 392, time 100.09323954582214, eps 0.0010000027643170119, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
goal_identified
goal_identified
goal_identified
=== ep: 393, time 97.78418588638306, eps 0.0010000026294996803, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
goal_identified
goal_identified
=== ep: 394, time 103.47137880325317, eps 0.0010000025012574677, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
goal_identified
=== ep: 395, time 82.40098905563354, eps 0.0010000023792697014, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
goal_identified
goal_identified
=== ep: 396, time 80.0737533569336, eps 0.0010000022632313489, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
goal_identified
goal_identified
=== ep: 397, time 78.43109464645386, eps 0.0010000021528522535, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 80.70301079750061, eps 0.00100000204785641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
goal_identified
goal_identified
goal_identified
=== ep: 399, time 85.30081224441528, eps 0.0010000019479812744, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 79.48958086967468, eps 0.0010000018529771066, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
goal_identified
=== ep: 401, time 73.13756966590881, eps 0.0010000017626063467, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
goal_identified
goal_identified
goal_identified
=== ep: 402, time 73.42987179756165, eps 0.0010000016766430208, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
goal_identified
=== ep: 403, time 79.61827564239502, eps 0.0010000015948721758, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
goal_identified
goal_identified
goal_identified
=== ep: 404, time 76.1098837852478, eps 0.001000001517089342, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
goal_identified
=== ep: 405, time 72.08104109764099, eps 0.0010000014431000217, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
goal_identified
goal_identified
goal_identified
=== ep: 406, time 79.53608393669128, eps 0.001000001372719203, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
goal_identified
=== ep: 407, time 83.48305082321167, eps 0.0010000013057708975, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
goal_identified
goal_identified
goal_identified
=== ep: 408, time 80.51937627792358, eps 0.0010000012420876994, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
goal_identified
goal_identified
=== ep: 409, time 75.60607123374939, eps 0.0010000011815103674, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
goal_identified
goal_identified
goal_identified
=== ep: 410, time 74.48722219467163, eps 0.001000001123887427, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
=== ep: 411, time 79.8416817188263, eps 0.0010000010690747903, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
goal_identified
goal_identified
=== ep: 412, time 75.98451328277588, eps 0.0010000010169353975, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
goal_identified
=== ep: 413, time 67.41876649856567, eps 0.0010000009673388729, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
goal_identified
=== ep: 414, time 76.2497787475586, eps 0.0010000009201611994, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
goal_identified
goal_identified
=== ep: 415, time 79.74469137191772, eps 0.0010000008752844081, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
goal_identified
=== ep: 416, time 83.41436576843262, eps 0.0010000008325962838, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
goal_identified
=== ep: 417, time 81.66291785240173, eps 0.001000000791990084, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
goal_identified
=== ep: 418, time 78.98505973815918, eps 0.0010000007533642718, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
goal_identified
goal_identified
goal_identified
=== ep: 419, time 78.97072887420654, eps 0.0010000007166222626, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
=== ep: 420, time 81.65415906906128, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
goal_identified
goal_identified
=== ep: 421, time 77.29825901985168, eps 0.001000000648426638, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
=== ep: 422, time 72.23216438293457, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
goal_identified
=== ep: 423, time 76.96312594413757, eps 0.0010000005867206849, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
goal_identified
goal_identified
goal_identified
=== ep: 424, time 77.084881067276, eps 0.0010000005581059794, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
=== ep: 425, time 74.10115361213684, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
=== ep: 426, time 78.54895377159119, eps 0.0010000005049951733, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
goal_identified
goal_identified
goal_identified
=== ep: 427, time 69.72417759895325, eps 0.001000000480366268, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 428, time 85.15468335151672, eps 0.0010000004569385287, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
=== ep: 429, time 82.92632675170898, eps 0.0010000004346533736, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 430, time 85.5118191242218, eps 0.0010000004134550786, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
goal_identified
goal_identified
=== ep: 431, time 81.59735584259033, eps 0.0010000003932906364, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 432, time 82.10669827461243, eps 0.0010000003741096257, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
goal_identified
goal_identified
goal_identified
=== ep: 433, time 85.42416572570801, eps 0.001000000355864084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
goal_identified
goal_identified
goal_identified
=== ep: 434, time 87.15210127830505, eps 0.0010000003385083878, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
goal_identified
goal_identified
=== ep: 435, time 79.11725425720215, eps 0.001000000321999139, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
=== ep: 436, time 77.62668800354004, eps 0.0010000003062950555, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
goal_identified
goal_identified
=== ep: 437, time 75.55282473564148, eps 0.0010000002913568694, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
goal_identified
goal_identified
=== ep: 438, time 84.03271722793579, eps 0.0010000002771472273, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
goal_identified
=== ep: 439, time 78.97026515007019, eps 0.0010000002636305976, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
goal_identified
goal_identified
=== ep: 440, time 75.10155415534973, eps 0.0010000002507731815, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 441, time 72.65086340904236, eps 0.0010000002385428292, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 442, time 89.18755316734314, eps 0.0010000002269089582, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
goal_identified
=== ep: 443, time 83.46036553382874, eps 0.0010000002158424776, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 444, time 81.98324370384216, eps 0.0010000002053157158, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 445, time 84.83356976509094, eps 0.0010000001953023503, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
goal_identified
goal_identified
=== ep: 446, time 83.10863137245178, eps 0.001000000185777342, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 447, time 76.95384669303894, eps 0.0010000001767168742, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
goal_identified
=== ep: 448, time 90.12780618667603, eps 0.0010000001680982905, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
goal_identified
goal_identified
=== ep: 449, time 87.79039025306702, eps 0.0010000001599000403, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 89.67693185806274, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
=== ep: 451, time 88.64363503456116, eps 0.0010000001446835395, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
goal_identified
goal_identified
=== ep: 452, time 87.75672578811646, eps 0.0010000001376272401, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
goal_identified
goal_identified
goal_identified
=== ep: 453, time 78.05334115028381, eps 0.0010000001309150804, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
goal_identified
goal_identified
=== ep: 454, time 84.24804067611694, eps 0.0010000001245302765, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
goal_identified
goal_identified
=== ep: 455, time 81.83566617965698, eps 0.0010000001184568633, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
goal_identified
goal_identified
=== ep: 456, time 83.01084566116333, eps 0.0010000001126796538, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
=== ep: 457, time 84.48329520225525, eps 0.0010000001071842023, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 458, time 88.89672946929932, eps 0.001000000101956767, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
goal_identified
goal_identified
goal_identified
=== ep: 459, time 78.79656934738159, eps 0.001000000096984277, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 460, time 67.80168271064758, eps 0.001000000092254298, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 461, time 80.87090563774109, eps 0.0010000000877550027, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 462, time 83.52785444259644, eps 0.0010000000834751407, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
goal_identified
goal_identified
=== ep: 463, time 69.14485216140747, eps 0.00100000007940401, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 464, time 65.20108318328857, eps 0.0010000000755314307, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
goal_identified
goal_identified
goal_identified
=== ep: 465, time 75.14388966560364, eps 0.0010000000718477194, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
goal_identified
goal_identified
goal_identified
=== ep: 466, time 87.4308648109436, eps 0.0010000000683436647, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
=== ep: 467, time 87.51908850669861, eps 0.001000000065010505, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
goal_identified
goal_identified
=== ep: 468, time 81.87672638893127, eps 0.0010000000618399052, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
goal_identified
goal_identified
goal_identified
=== ep: 469, time 73.1178138256073, eps 0.0010000000588239375, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 470, time 78.33048939704895, eps 0.0010000000559550603, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
goal_identified
goal_identified
goal_identified
=== ep: 471, time 81.02271866798401, eps 0.0010000000532260998, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
goal_identified
=== ep: 472, time 80.36180520057678, eps 0.0010000000506302322, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
goal_identified
goal_identified
goal_identified
=== ep: 473, time 72.8518853187561, eps 0.0010000000481609666, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 474, time 75.13936614990234, eps 0.0010000000458121286, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
goal_identified
goal_identified
=== ep: 475, time 77.48265957832336, eps 0.0010000000435778447, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 476, time 83.5226035118103, eps 0.001000000041452528, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
goal_identified
goal_identified
goal_identified
=== ep: 477, time 80.05752396583557, eps 0.0010000000394308644, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 478, time 81.38930487632751, eps 0.0010000000375077985, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
goal_identified
goal_identified
=== ep: 479, time 79.34387874603271, eps 0.0010000000356785216, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 480, time 87.13012504577637, eps 0.0010000000339384595, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
goal_identified
goal_identified
goal_identified
=== ep: 481, time 89.09673357009888, eps 0.0010000000322832614, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
goal_identified
=== ep: 482, time 85.11320543289185, eps 0.0010000000307087882, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
goal_identified
goal_identified
=== ep: 483, time 73.63100481033325, eps 0.001000000029211103, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
goal_identified
goal_identified
=== ep: 484, time 73.12207841873169, eps 0.0010000000277864607, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 485, time 79.09903168678284, eps 0.0010000000264312988, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 486, time 83.27841687202454, eps 0.0010000000251422292, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 487, time 84.41499376296997, eps 0.0010000000239160282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 488, time 81.91258430480957, eps 0.00100000002274963, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 489, time 78.61556243896484, eps 0.0010000000216401172, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 490, time 84.82665348052979, eps 0.0010000000205847162, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
goal_identified
goal_identified
=== ep: 491, time 84.99468779563904, eps 0.0010000000195807877, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
goal_identified
goal_identified
=== ep: 492, time 92.44901776313782, eps 0.0010000000186258216, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
goal_identified
goal_identified
goal_identified
=== ep: 493, time 91.5591893196106, eps 0.0010000000177174295, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
goal_identified
goal_identified
goal_identified
=== ep: 494, time 91.37223386764526, eps 0.0010000000168533404, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
goal_identified
=== ep: 495, time 80.97634148597717, eps 0.0010000000160313932, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 496, time 84.97201132774353, eps 0.001000000015249533, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
goal_identified
goal_identified
goal_identified
=== ep: 497, time 72.83488035202026, eps 0.0010000000145058043, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
goal_identified
goal_identified
=== ep: 498, time 77.08565092086792, eps 0.001000000013798348, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
goal_identified
=== ep: 499, time 86.98565697669983, eps 0.0010000000131253947, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
goal_identified
goal_identified
=== ep: 500, time 83.65008974075317, eps 0.0010000000124852615, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
goal_identified
=== ep: 501, time 78.16106009483337, eps 0.0010000000118763482, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
goal_identified
goal_identified
=== ep: 502, time 78.48589587211609, eps 0.0010000000112971319, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
goal_identified
goal_identified
=== ep: 503, time 77.39831829071045, eps 0.0010000000107461642, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
goal_identified
goal_identified
goal_identified
=== ep: 504, time 87.98704409599304, eps 0.0010000000102220676, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
=== ep: 505, time 87.72490811347961, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 506, time 91.34156370162964, eps 0.0010000000092493092, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
goal_identified
=== ep: 507, time 86.46137833595276, eps 0.0010000000087982152, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
goal_identified
=== ep: 508, time 87.51430702209473, eps 0.0010000000083691212, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
goal_identified
goal_identified
goal_identified
=== ep: 509, time 78.77585959434509, eps 0.0010000000079609542, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
goal_identified
=== ep: 510, time 85.41854190826416, eps 0.001000000007572694, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 511, time 89.71330237388611, eps 0.0010000000072033692, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
goal_identified
goal_identified
goal_identified
=== ep: 512, time 88.09826254844666, eps 0.001000000006852057, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 513, time 85.69340538978577, eps 0.001000000006517878, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
goal_identified
goal_identified
goal_identified
=== ep: 514, time 71.6230878829956, eps 0.0010000000061999974, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
goal_identified
goal_identified
goal_identified
=== ep: 515, time 80.0103120803833, eps 0.0010000000058976199, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
goal_identified
goal_identified
=== ep: 516, time 86.12284207344055, eps 0.0010000000056099897, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 517, time 81.30034351348877, eps 0.0010000000053363872, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
goal_identified
goal_identified
goal_identified
=== ep: 518, time 78.36755418777466, eps 0.0010000000050761286, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
goal_identified
goal_identified
goal_identified
=== ep: 519, time 76.25086331367493, eps 0.001000000004828563, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 86.63631844520569, eps 0.001000000004593071, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
=== ep: 521, time 91.18314385414124, eps 0.0010000000043690644, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
goal_identified
goal_identified
goal_identified
=== ep: 522, time 91.12022948265076, eps 0.0010000000041559827, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 523, time 89.16964030265808, eps 0.0010000000039532928, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
goal_identified
=== ep: 524, time 88.14319634437561, eps 0.0010000000037604885, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
goal_identified
goal_identified
=== ep: 525, time 77.53452324867249, eps 0.0010000000035770874, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 526, time 78.41052508354187, eps 0.0010000000034026306, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
=== ep: 527, time 79.38547873497009, eps 0.0010000000032366824, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 84.95466828346252, eps 0.0010000000030788276, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 529, time 85.36267924308777, eps 0.0010000000029286714, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
goal_identified
goal_identified
goal_identified
=== ep: 530, time 81.76707005500793, eps 0.0010000000027858384, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 531, time 72.98802947998047, eps 0.0010000000026499714, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
goal_identified
goal_identified
=== ep: 532, time 79.27668523788452, eps 0.0010000000025207308, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
goal_identified
=== ep: 533, time 85.42060828208923, eps 0.0010000000023977934, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
goal_identified
goal_identified
=== ep: 534, time 87.08777928352356, eps 0.0010000000022808515, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 535, time 87.07164096832275, eps 0.0010000000021696133, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 536, time 86.70510935783386, eps 0.0010000000020637999, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
goal_identified
goal_identified
goal_identified
=== ep: 537, time 81.4895076751709, eps 0.0010000000019631471, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
=== ep: 538, time 80.73088836669922, eps 0.0010000000018674034, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 37/37)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 539, time 85.6166000366211, eps 0.001000000001776329, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
goal_identified
goal_identified
goal_identified
=== ep: 540, time 81.58392596244812, eps 0.0010000000016896964, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
goal_identified
goal_identified
=== ep: 541, time 86.35469079017639, eps 0.001000000001607289, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
goal_identified
goal_identified
goal_identified
=== ep: 542, time 90.38665056228638, eps 0.0010000000015289005, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
goal_identified
goal_identified
goal_identified
=== ep: 543, time 90.15499758720398, eps 0.0010000000014543352, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 544, time 88.97165083885193, eps 0.0010000000013834064, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
goal_identified
goal_identified
goal_identified
=== ep: 545, time 92.19305849075317, eps 0.001000000001315937, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 90.43035888671875, eps 0.0010000000012517578, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
goal_identified
goal_identified
=== ep: 547, time 86.99018931388855, eps 0.001000000001190709, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
goal_identified
goal_identified
=== ep: 548, time 76.59182572364807, eps 0.0010000000011326374, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
goal_identified
goal_identified
=== ep: 549, time 72.75292325019836, eps 0.001000000001077398, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
goal_identified
goal_identified
=== ep: 550, time 87.43796277046204, eps 0.0010000000010248527, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 85.22837376594543, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 77.83570265769958, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
goal_identified
=== ep: 553, time 83.03192806243896, eps 0.0010000000008820989, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
goal_identified
goal_identified
goal_identified
=== ep: 554, time 77.36540508270264, eps 0.0010000000008390784, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
goal_identified
goal_identified
goal_identified
=== ep: 555, time 71.89667820930481, eps 0.001000000000798156, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
goal_identified
goal_identified
=== ep: 556, time 83.07309699058533, eps 0.0010000000007592295, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
goal_identified
goal_identified
goal_identified
=== ep: 557, time 84.91292214393616, eps 0.0010000000007222014, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 558, time 77.55347561836243, eps 0.0010000000006869794, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
goal_identified
=== ep: 559, time 76.61905360221863, eps 0.001000000000653475, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 560, time 84.7291419506073, eps 0.0010000000006216046, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 561, time 84.05702185630798, eps 0.0010000000005912885, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
goal_identified
goal_identified
goal_identified
=== ep: 562, time 86.5510778427124, eps 0.0010000000005624511, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
goal_identified
goal_identified
=== ep: 563, time 88.57361030578613, eps 0.00100000000053502, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 564, time 89.9855260848999, eps 0.001000000000508927, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
goal_identified
goal_identified
goal_identified
=== ep: 565, time 84.69437074661255, eps 0.001000000000484106, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 566, time 74.85150814056396, eps 0.001000000000460496, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
goal_identified
goal_identified
goal_identified
=== ep: 567, time 78.73557686805725, eps 0.0010000000004380374, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
goal_identified
=== ep: 568, time 79.70990705490112, eps 0.001000000000416674, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
goal_identified
goal_identified
=== ep: 569, time 78.246661901474, eps 0.0010000000003963527, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 79.39115858078003, eps 0.0010000000003770222, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
goal_identified
goal_identified
goal_identified
=== ep: 571, time 82.41567897796631, eps 0.0010000000003586346, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
goal_identified
goal_identified
=== ep: 572, time 77.8925850391388, eps 0.0010000000003411438, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
goal_identified
goal_identified
=== ep: 573, time 73.39752411842346, eps 0.001000000000324506, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
goal_identified
goal_identified
goal_identified
=== ep: 574, time 81.17023253440857, eps 0.0010000000003086798, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 575, time 89.05707311630249, eps 0.0010000000002936252, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 576, time 91.68604254722595, eps 0.001000000000279305, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 577, time 88.827152967453, eps 0.0010000000002656831, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 578, time 86.03799295425415, eps 0.0010000000002527256, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
goal_identified
goal_identified
goal_identified
=== ep: 579, time 83.73119258880615, eps 0.0010000000002404, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
goal_identified
goal_identified
goal_identified
=== ep: 580, time 84.3706533908844, eps 0.0010000000002286756, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
goal_identified
=== ep: 581, time 84.28851294517517, eps 0.0010000000002175229, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
goal_identified
goal_identified
=== ep: 582, time 80.99145126342773, eps 0.0010000000002069142, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 583, time 83.12408638000488, eps 0.0010000000001968228, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 584, time 79.6992540359497, eps 0.0010000000001872237, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 585, time 82.43325138092041, eps 0.0010000000001780928, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
goal_identified
goal_identified
goal_identified
=== ep: 586, time 83.74956464767456, eps 0.001000000000169407, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
goal_identified
goal_identified
=== ep: 587, time 80.83970737457275, eps 0.001000000000161145, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
goal_identified
=== ep: 588, time 79.8348503112793, eps 0.0010000000001532858, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
goal_identified
=== ep: 589, time 71.20174169540405, eps 0.00100000000014581, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
goal_identified
goal_identified
=== ep: 590, time 79.10039138793945, eps 0.0010000000001386988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
goal_identified
goal_identified
goal_identified
=== ep: 591, time 81.49084639549255, eps 0.0010000000001319344, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 592, time 84.22016549110413, eps 0.0010000000001255, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
goal_identified
goal_identified
=== ep: 593, time 88.49499917030334, eps 0.0010000000001193791, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
goal_identified
goal_identified
=== ep: 594, time 90.35704255104065, eps 0.001000000000113557, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
goal_identified
goal_identified
goal_identified
=== ep: 595, time 87.79461526870728, eps 0.0010000000001080186, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
goal_identified
goal_identified
goal_identified
=== ep: 596, time 79.80640196800232, eps 0.0010000000001027505, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
goal_identified
goal_identified
=== ep: 597, time 81.22136354446411, eps 0.0010000000000977393, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
goal_identified
goal_identified
=== ep: 598, time 70.05066704750061, eps 0.0010000000000929725, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
goal_identified
goal_identified
=== ep: 599, time 76.30715489387512, eps 0.0010000000000884382, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
goal_identified
goal_identified
=== ep: 600, time 83.8792896270752, eps 0.001000000000084125, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
goal_identified
goal_identified
=== ep: 601, time 84.18013525009155, eps 0.0010000000000800222, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
goal_identified
goal_identified
goal_identified
=== ep: 602, time 83.28590297698975, eps 0.0010000000000761195, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
goal_identified
goal_identified
goal_identified
=== ep: 603, time 82.41819834709167, eps 0.0010000000000724072, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
goal_identified
goal_identified
goal_identified
=== ep: 604, time 75.67720174789429, eps 0.0010000000000688757, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
goal_identified
goal_identified
goal_identified
=== ep: 605, time 75.0536253452301, eps 0.0010000000000655166, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
goal_identified
goal_identified
=== ep: 606, time 83.362473487854, eps 0.0010000000000623215, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
goal_identified
goal_identified
goal_identified
=== ep: 607, time 85.53597927093506, eps 0.001000000000059282, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 608, time 89.7450704574585, eps 0.0010000000000563907, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
goal_identified
=== ep: 609, time 92.3516366481781, eps 0.0010000000000536405, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
=== ep: 610, time 90.56788682937622, eps 0.0010000000000510245, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
goal_identified
=== ep: 611, time 92.468674659729, eps 0.0010000000000485358, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
goal_identified
=== ep: 612, time 84.46941757202148, eps 0.0010000000000461688, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 82.15882086753845, eps 0.0010000000000439171, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 614, time 74.8434419631958, eps 0.0010000000000417752, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 615, time 80.56886577606201, eps 0.0010000000000397378, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 615
goal_identified
goal_identified
goal_identified
=== ep: 616, time 81.94811820983887, eps 0.0010000000000377999, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
goal_identified
goal_identified
=== ep: 617, time 83.88916230201721, eps 0.0010000000000359563, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 618, time 86.55564522743225, eps 0.0010000000000342027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
=== ep: 619, time 93.04877781867981, eps 0.0010000000000325345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
goal_identified
=== ep: 620, time 86.62764167785645, eps 0.001000000000030948, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 621, time 88.05405640602112, eps 0.0010000000000294385, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 622, time 86.39963936805725, eps 0.0010000000000280028, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 623, time 84.57312607765198, eps 0.0010000000000266371, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
goal_identified
goal_identified
=== ep: 624, time 77.07425856590271, eps 0.001000000000025338, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
goal_identified
=== ep: 625, time 87.53358626365662, eps 0.0010000000000241023, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
goal_identified
=== ep: 626, time 83.04801821708679, eps 0.0010000000000229268, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
goal_identified
goal_identified
goal_identified
=== ep: 627, time 85.9752824306488, eps 0.0010000000000218085, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
goal_identified
=== ep: 628, time 86.61294531822205, eps 0.001000000000020745, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
goal_identified
goal_identified
=== ep: 629, time 89.51410031318665, eps 0.0010000000000197332, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
goal_identified
goal_identified
=== ep: 630, time 92.06470990180969, eps 0.0010000000000187708, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
goal_identified
goal_identified
=== ep: 631, time 95.22591304779053, eps 0.0010000000000178553, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
goal_identified
goal_identified
goal_identified
=== ep: 632, time 90.85369372367859, eps 0.0010000000000169845, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
=== ep: 633, time 95.41807436943054, eps 0.0010000000000161562, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
goal_identified
=== ep: 634, time 93.66937184333801, eps 0.0010000000000153684, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
goal_identified
goal_identified
goal_identified
=== ep: 635, time 99.02824211120605, eps 0.0010000000000146188, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
goal_identified
goal_identified
goal_identified
=== ep: 636, time 94.73661351203918, eps 0.0010000000000139058, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 637, time 95.95859289169312, eps 0.0010000000000132275, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 638, time 97.76736736297607, eps 0.0010000000000125824, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
goal_identified
goal_identified
goal_identified
=== ep: 639, time 94.11349201202393, eps 0.0010000000000119687, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
=== ep: 640, time 96.59884691238403, eps 0.001000000000011385, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
goal_identified
goal_identified
=== ep: 641, time 97.21135306358337, eps 0.00100000000001083, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
=== ep: 642, time 94.11443948745728, eps 0.0010000000000103017, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 642
goal_identified
goal_identified
goal_identified
=== ep: 643, time 91.36660242080688, eps 0.0010000000000097993, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
goal_identified
=== ep: 644, time 93.87925004959106, eps 0.0010000000000093213, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
goal_identified
=== ep: 645, time 90.74641060829163, eps 0.0010000000000088666, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 646, time 87.14200115203857, eps 0.0010000000000084342, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
goal_identified
=== ep: 647, time 91.18604969978333, eps 0.001000000000008023, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
goal_identified
=== ep: 648, time 88.79542660713196, eps 0.0010000000000076317, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
goal_identified
=== ep: 649, time 81.17054176330566, eps 0.0010000000000072594, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 650, time 92.28711295127869, eps 0.0010000000000069055, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 651, time 68.06369829177856, eps 0.0010000000000065686, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
goal_identified
goal_identified
goal_identified
=== ep: 652, time 85.04287528991699, eps 0.0010000000000062483, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
goal_identified
goal_identified
=== ep: 653, time 87.35249924659729, eps 0.0010000000000059436, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 86.70256280899048, eps 0.0010000000000056537, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 655, time 92.725257396698, eps 0.0010000000000053779, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 656, time 95.7801148891449, eps 0.0010000000000051157, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
goal_identified
goal_identified
goal_identified
=== ep: 657, time 90.3528950214386, eps 0.0010000000000048661, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 658, time 96.8358793258667, eps 0.001000000000004629, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 659, time 97.18868565559387, eps 0.0010000000000044032, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 660, time 97.33615326881409, eps 0.0010000000000041883, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
goal_identified
goal_identified
goal_identified
=== ep: 661, time 93.00735545158386, eps 0.001000000000003984, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 95.41862773895264, eps 0.0010000000000037897, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
goal_identified
goal_identified
goal_identified
=== ep: 663, time 93.89288187026978, eps 0.001000000000003605, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 664, time 95.88103485107422, eps 0.0010000000000034291, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
goal_identified
=== ep: 665, time 94.7977774143219, eps 0.001000000000003262, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
goal_identified
goal_identified
=== ep: 666, time 86.18914580345154, eps 0.0010000000000031028, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 88.13902187347412, eps 0.0010000000000029514, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 668, time 92.62425446510315, eps 0.0010000000000028075, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
goal_identified
goal_identified
goal_identified
=== ep: 669, time 87.6805510520935, eps 0.0010000000000026706, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 96.24029278755188, eps 0.0010000000000025403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
goal_identified
goal_identified
=== ep: 671, time 94.6062126159668, eps 0.0010000000000024165, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
goal_identified
goal_identified
goal_identified
=== ep: 672, time 93.15922284126282, eps 0.0010000000000022985, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
goal_identified
goal_identified
=== ep: 673, time 95.97136068344116, eps 0.0010000000000021864, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
goal_identified
goal_identified
=== ep: 674, time 96.00386548042297, eps 0.00100000000000208, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
goal_identified
goal_identified
goal_identified
=== ep: 675, time 98.0668375492096, eps 0.0010000000000019785, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
goal_identified
goal_identified
=== ep: 676, time 97.07414603233337, eps 0.001000000000001882, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 677, time 96.31432461738586, eps 0.0010000000000017903, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
goal_identified
goal_identified
=== ep: 678, time 95.01664185523987, eps 0.0010000000000017029, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
goal_identified
goal_identified
goal_identified
=== ep: 679, time 82.63670587539673, eps 0.0010000000000016198, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
goal_identified
goal_identified
goal_identified
=== ep: 680, time 93.17152547836304, eps 0.0010000000000015409, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
=== ep: 681, time 80.6866626739502, eps 0.0010000000000014656, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
goal_identified
goal_identified
goal_identified
=== ep: 682, time 89.32805275917053, eps 0.0010000000000013943, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
goal_identified
goal_identified
=== ep: 683, time 90.92434525489807, eps 0.0010000000000013262, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
goal_identified
=== ep: 684, time 87.28158736228943, eps 0.0010000000000012616, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
goal_identified
goal_identified
=== ep: 685, time 91.43325400352478, eps 0.0010000000000012, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 686, time 91.857417345047, eps 0.0010000000000011415, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 687, time 98.15016436576843, eps 0.0010000000000010857, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
goal_identified
=== ep: 688, time 89.95932340621948, eps 0.0010000000000010328, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
goal_identified
=== ep: 689, time 93.49567222595215, eps 0.0010000000000009825, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
goal_identified
=== ep: 690, time 95.41127824783325, eps 0.0010000000000009346, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
goal_identified
goal_identified
goal_identified
=== ep: 691, time 89.85733079910278, eps 0.001000000000000889, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 692, time 78.87771677970886, eps 0.0010000000000008457, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 88.14345622062683, eps 0.0010000000000008045, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 694, time 82.56989645957947, eps 0.0010000000000007653, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
goal_identified
goal_identified
=== ep: 695, time 86.60644340515137, eps 0.0010000000000007277, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
goal_identified
=== ep: 696, time 86.18475461006165, eps 0.0010000000000006924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
goal_identified
goal_identified
goal_identified
=== ep: 697, time 91.90372800827026, eps 0.0010000000000006586, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
goal_identified
goal_identified
goal_identified
=== ep: 698, time 94.64776110649109, eps 0.0010000000000006265, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
goal_identified
goal_identified
=== ep: 699, time 90.8471839427948, eps 0.001000000000000596, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 700, time 92.22651839256287, eps 0.0010000000000005668, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 701, time 96.60570669174194, eps 0.0010000000000005393, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 702, time 94.44486594200134, eps 0.0010000000000005128, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 702
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 703, time 98.9597704410553, eps 0.001000000000000488, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
goal_identified
goal_identified
=== ep: 704, time 101.4441032409668, eps 0.001000000000000464, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 705, time 97.89129567146301, eps 0.0010000000000004415, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
goal_identified
=== ep: 706, time 92.41747069358826, eps 0.00100000000000042, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 707, time 94.59164071083069, eps 0.0010000000000003994, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
goal_identified
goal_identified
=== ep: 708, time 93.52204775810242, eps 0.00100000000000038, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 709, time 91.02893781661987, eps 0.0010000000000003615, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
goal_identified
goal_identified
goal_identified
=== ep: 710, time 81.19487452507019, eps 0.0010000000000003437, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
goal_identified
=== ep: 711, time 88.95256400108337, eps 0.001000000000000327, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
goal_identified
goal_identified
goal_identified
=== ep: 712, time 72.94497847557068, eps 0.0010000000000003112, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 97.26211261749268, eps 0.001000000000000296, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
goal_identified
goal_identified
=== ep: 714, time 92.36867690086365, eps 0.0010000000000002815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
goal_identified
goal_identified
=== ep: 715, time 89.56806492805481, eps 0.0010000000000002678, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
goal_identified
goal_identified
goal_identified
=== ep: 716, time 87.30406880378723, eps 0.0010000000000002548, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
goal_identified
=== ep: 717, time 86.02504277229309, eps 0.0010000000000002422, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 718, time 83.45464563369751, eps 0.0010000000000002305, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 719, time 88.848308801651, eps 0.0010000000000002192, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 720, time 89.02243304252625, eps 0.0010000000000002086, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 721, time 91.36851334571838, eps 0.0010000000000001984, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 722, time 93.60500049591064, eps 0.0010000000000001887, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
goal_identified
=== ep: 723, time 85.63910794258118, eps 0.0010000000000001796, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
goal_identified
goal_identified
=== ep: 724, time 84.14308714866638, eps 0.0010000000000001707, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
goal_identified
goal_identified
goal_identified
=== ep: 725, time 82.37983131408691, eps 0.0010000000000001624, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
goal_identified
=== ep: 726, time 85.99078917503357, eps 0.0010000000000001544, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
goal_identified
goal_identified
goal_identified
=== ep: 727, time 89.90142059326172, eps 0.001000000000000147, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
goal_identified
goal_identified
=== ep: 728, time 94.46465516090393, eps 0.0010000000000001399, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
goal_identified
goal_identified
=== ep: 729, time 94.41411542892456, eps 0.001000000000000133, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
goal_identified
goal_identified
=== ep: 730, time 89.82686305046082, eps 0.0010000000000001264, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 731, time 88.44379901885986, eps 0.0010000000000001204, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
goal_identified
=== ep: 732, time 83.80728721618652, eps 0.0010000000000001145, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
goal_identified
goal_identified
=== ep: 733, time 88.78400754928589, eps 0.0010000000000001089, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
goal_identified
goal_identified
goal_identified
=== ep: 734, time 93.94656467437744, eps 0.0010000000000001037, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
goal_identified
goal_identified
=== ep: 735, time 85.93197679519653, eps 0.0010000000000000985, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
goal_identified
goal_identified
=== ep: 736, time 95.67016077041626, eps 0.0010000000000000937, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
goal_identified
goal_identified
=== ep: 737, time 95.94159388542175, eps 0.0010000000000000891, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
goal_identified
goal_identified
goal_identified
=== ep: 738, time 96.53254580497742, eps 0.0010000000000000848, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
goal_identified
goal_identified
=== ep: 739, time 100.27208113670349, eps 0.0010000000000000807, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
=== ep: 740, time 100.97415447235107, eps 0.0010000000000000768, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
goal_identified
goal_identified
=== ep: 741, time 96.56980514526367, eps 0.001000000000000073, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 95.45533967018127, eps 0.0010000000000000694, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 743, time 88.1907377243042, eps 0.001000000000000066, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
goal_identified
goal_identified
goal_identified
=== ep: 744, time 88.78477168083191, eps 0.001000000000000063, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
goal_identified
=== ep: 745, time 66.37631750106812, eps 0.0010000000000000599, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
=== ep: 746, time 90.68757605552673, eps 0.0010000000000000568, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
goal_identified
goal_identified
=== ep: 747, time 85.45952916145325, eps 0.001000000000000054, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
goal_identified
goal_identified
goal_identified
=== ep: 748, time 83.95190668106079, eps 0.0010000000000000514, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 749, time 94.64340949058533, eps 0.001000000000000049, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
goal_identified
goal_identified
goal_identified
=== ep: 750, time 92.505704164505, eps 0.0010000000000000466, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 751, time 93.20049381256104, eps 0.0010000000000000443, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
goal_identified
goal_identified
goal_identified
=== ep: 752, time 91.01085305213928, eps 0.001000000000000042, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 753, time 83.90802383422852, eps 0.0010000000000000401, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 754, time 86.58010506629944, eps 0.0010000000000000382, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 755, time 86.11174249649048, eps 0.0010000000000000362, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
goal_identified
goal_identified
goal_identified
=== ep: 756, time 83.21857953071594, eps 0.0010000000000000345, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
goal_identified
goal_identified
=== ep: 757, time 90.68277716636658, eps 0.0010000000000000328, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
goal_identified
goal_identified
goal_identified
=== ep: 758, time 89.9391839504242, eps 0.0010000000000000312, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
goal_identified
goal_identified
=== ep: 759, time 94.84921789169312, eps 0.0010000000000000297, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
goal_identified
goal_identified
goal_identified
=== ep: 760, time 94.4954195022583, eps 0.0010000000000000282, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 761, time 94.25086188316345, eps 0.001000000000000027, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 762, time 93.90606832504272, eps 0.0010000000000000256, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
=== ep: 763, time 99.80130505561829, eps 0.0010000000000000243, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
=== ep: 764, time 94.77729034423828, eps 0.0010000000000000232, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 765, time 95.0871262550354, eps 0.001000000000000022, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
goal_identified
goal_identified
goal_identified
=== ep: 766, time 92.20252442359924, eps 0.0010000000000000208, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
goal_identified
goal_identified
goal_identified
=== ep: 767, time 90.91205978393555, eps 0.00100000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
goal_identified
goal_identified
goal_identified
=== ep: 768, time 94.37171602249146, eps 0.0010000000000000189, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
goal_identified
goal_identified
=== ep: 769, time 94.62655425071716, eps 0.001000000000000018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 770, time 93.77312994003296, eps 0.0010000000000000172, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
goal_identified
=== ep: 771, time 90.99524998664856, eps 0.0010000000000000163, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
goal_identified
goal_identified
goal_identified
=== ep: 772, time 96.222971200943, eps 0.0010000000000000154, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
goal_identified
goal_identified
goal_identified
=== ep: 773, time 90.8364486694336, eps 0.0010000000000000148, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
goal_identified
goal_identified
=== ep: 774, time 93.98179411888123, eps 0.0010000000000000141, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
goal_identified
goal_identified
goal_identified
=== ep: 775, time 93.72942614555359, eps 0.0010000000000000132, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 776, time 83.76700067520142, eps 0.0010000000000000126, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
goal_identified
goal_identified
=== ep: 777, time 85.17054557800293, eps 0.0010000000000000122, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 778, time 77.06041979789734, eps 0.0010000000000000115, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
goal_identified
goal_identified
=== ep: 779, time 88.0672357082367, eps 0.0010000000000000109, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
goal_identified
goal_identified
goal_identified
=== ep: 780, time 88.67366194725037, eps 0.0010000000000000104, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
goal_identified
goal_identified
goal_identified
=== ep: 781, time 95.45003366470337, eps 0.00100000000000001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 782, time 93.78419852256775, eps 0.0010000000000000093, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
goal_identified
goal_identified
goal_identified
=== ep: 783, time 95.42070865631104, eps 0.001000000000000009, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
goal_identified
goal_identified
goal_identified
=== ep: 784, time 89.23358631134033, eps 0.0010000000000000085, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
goal_identified
goal_identified
=== ep: 785, time 96.35553789138794, eps 0.001000000000000008, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
goal_identified
goal_identified
goal_identified
=== ep: 786, time 91.34129214286804, eps 0.0010000000000000076, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
goal_identified
goal_identified
goal_identified
=== ep: 787, time 88.97335124015808, eps 0.0010000000000000074, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
goal_identified
goal_identified
goal_identified
=== ep: 788, time 87.59057354927063, eps 0.001000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
goal_identified
goal_identified
goal_identified
=== ep: 789, time 92.03566193580627, eps 0.0010000000000000067, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
goal_identified
goal_identified
=== ep: 790, time 90.51416897773743, eps 0.0010000000000000063, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 791, time 92.23270750045776, eps 0.001000000000000006, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
goal_identified
goal_identified
=== ep: 792, time 87.68841600418091, eps 0.0010000000000000057, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
goal_identified
goal_identified
goal_identified
=== ep: 793, time 91.09266209602356, eps 0.0010000000000000054, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
goal_identified
=== ep: 794, time 85.41711282730103, eps 0.0010000000000000052, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
goal_identified
goal_identified
=== ep: 795, time 95.2939670085907, eps 0.001000000000000005, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
=== ep: 796, time 87.6876368522644, eps 0.0010000000000000048, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
goal_identified
goal_identified
goal_identified
=== ep: 797, time 87.31102204322815, eps 0.0010000000000000044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
=== ep: 798, time 84.1017861366272, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
goal_identified
goal_identified
=== ep: 799, time 84.28539943695068, eps 0.0010000000000000041, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
goal_identified
goal_identified
=== ep: 800, time 86.19726634025574, eps 0.001000000000000004, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
goal_identified
goal_identified
=== ep: 801, time 86.71380662918091, eps 0.0010000000000000037, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
=== ep: 802, time 92.05264830589294, eps 0.0010000000000000035, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
goal_identified
goal_identified
=== ep: 803, time 88.98553395271301, eps 0.0010000000000000033, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 804, time 62.12640881538391, eps 0.001000000000000003, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
goal_identified
goal_identified
=== ep: 805, time 85.6500027179718, eps 0.001000000000000003, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
goal_identified
=== ep: 806, time 83.80666828155518, eps 0.0010000000000000028, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
goal_identified
goal_identified
=== ep: 807, time 90.93086409568787, eps 0.0010000000000000026, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
goal_identified
goal_identified
=== ep: 808, time 95.7603874206543, eps 0.0010000000000000026, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
goal_identified
goal_identified
goal_identified
=== ep: 809, time 93.00018429756165, eps 0.0010000000000000024, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
goal_identified
goal_identified
goal_identified
=== ep: 810, time 91.25081706047058, eps 0.0010000000000000024, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
goal_identified
goal_identified
goal_identified
=== ep: 811, time 90.1033296585083, eps 0.0010000000000000022, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
goal_identified
=== ep: 812, time 94.39175581932068, eps 0.0010000000000000022, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
goal_identified
goal_identified
=== ep: 813, time 90.54776692390442, eps 0.001000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
goal_identified
goal_identified
goal_identified
=== ep: 814, time 92.65678405761719, eps 0.001000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 815, time 88.07244324684143, eps 0.0010000000000000018, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 816, time 90.01197981834412, eps 0.0010000000000000018, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 817, time 89.36073589324951, eps 0.0010000000000000018, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
goal_identified
goal_identified
goal_identified
=== ep: 818, time 94.76882719993591, eps 0.0010000000000000015, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
goal_identified
goal_identified
goal_identified
=== ep: 819, time 86.62229132652283, eps 0.0010000000000000015, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
goal_identified
goal_identified
goal_identified
=== ep: 820, time 87.8543152809143, eps 0.0010000000000000013, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
goal_identified
goal_identified
=== ep: 821, time 89.05136179924011, eps 0.0010000000000000013, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
goal_identified
goal_identified
=== ep: 822, time 88.93362092971802, eps 0.0010000000000000013, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
goal_identified
=== ep: 823, time 90.07875633239746, eps 0.0010000000000000013, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 824, time 95.63046026229858, eps 0.001000000000000001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 825, time 94.45674109458923, eps 0.001000000000000001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
goal_identified
goal_identified
=== ep: 826, time 97.5439932346344, eps 0.001000000000000001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
goal_identified
=== ep: 827, time 102.26415801048279, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
=== ep: 828, time 96.05256795883179, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
goal_identified
=== ep: 829, time 96.19549083709717, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
=== ep: 830, time 90.74441027641296, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
goal_identified
=== ep: 831, time 88.57028889656067, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
goal_identified
goal_identified
=== ep: 832, time 85.15053534507751, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
goal_identified
goal_identified
=== ep: 833, time 83.30959343910217, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
goal_identified
goal_identified
goal_identified
=== ep: 834, time 91.88596725463867, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 835, time 90.02871751785278, eps 0.0010000000000000007, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
goal_identified
goal_identified
goal_identified
=== ep: 836, time 94.28752589225769, eps 0.0010000000000000007, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
goal_identified
goal_identified
goal_identified
=== ep: 837, time 95.52975249290466, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
goal_identified
goal_identified
=== ep: 838, time 92.61338973045349, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
=== ep: 839, time 86.99565100669861, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 840, time 90.26581001281738, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 841, time 90.80518078804016, eps 0.0010000000000000005, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 842, time 86.9993908405304, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
goal_identified
goal_identified
goal_identified
=== ep: 843, time 87.77549886703491, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
goal_identified
goal_identified
goal_identified
=== ep: 844, time 91.61462140083313, eps 0.0010000000000000005, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
goal_identified
goal_identified
goal_identified
=== ep: 845, time 86.67209076881409, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
goal_identified
goal_identified
goal_identified
=== ep: 846, time 93.82263255119324, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
goal_identified
=== ep: 847, time 85.84366655349731, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
goal_identified
goal_identified
goal_identified
=== ep: 848, time 88.15304160118103, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
goal_identified
goal_identified
=== ep: 849, time 80.41258955001831, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 850, time 85.92141652107239, eps 0.0010000000000000002, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
goal_identified
=== ep: 851, time 87.96204996109009, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
goal_identified
goal_identified
goal_identified
=== ep: 852, time 91.88572883605957, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 853, time 94.25527453422546, eps 0.0010000000000000002, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
goal_identified
goal_identified
goal_identified
=== ep: 854, time 88.22602868080139, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
goal_identified
goal_identified
goal_identified
=== ep: 855, time 86.84127187728882, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 856, time 81.94635105133057, eps 0.0010000000000000002, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
goal_identified
=== ep: 857, time 82.06953358650208, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 858, time 83.46574759483337, eps 0.0010000000000000002, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
goal_identified
goal_identified
=== ep: 859, time 92.90253710746765, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
goal_identified
=== ep: 860, time 93.59676361083984, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 861, time 95.38584899902344, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
goal_identified
goal_identified
goal_identified
=== ep: 862, time 94.5031533241272, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
goal_identified
goal_identified
=== ep: 863, time 88.59787917137146, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
goal_identified
goal_identified
goal_identified
=== ep: 864, time 74.19169998168945, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
goal_identified
goal_identified
goal_identified
=== ep: 865, time 79.81429266929626, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
goal_identified
goal_identified
=== ep: 866, time 80.38810324668884, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
goal_identified
=== ep: 867, time 82.68394756317139, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 868, time 90.04420566558838, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 869, time 81.24609971046448, eps 0.0010000000000000002, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
goal_identified
goal_identified
=== ep: 870, time 75.68545532226562, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 86.51745414733887, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
goal_identified
goal_identified
=== ep: 872, time 88.89590382575989, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
goal_identified
=== ep: 873, time 92.17446327209473, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
goal_identified
goal_identified
=== ep: 874, time 91.63553667068481, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
goal_identified
goal_identified
=== ep: 875, time 85.35814714431763, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 876, time 73.82473301887512, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
goal_identified
goal_identified
goal_identified
=== ep: 877, time 83.45626425743103, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
goal_identified
goal_identified
=== ep: 878, time 76.28916382789612, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 879, time 93.54112315177917, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
goal_identified
goal_identified
=== ep: 880, time 88.7513313293457, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 881, time 86.7006208896637, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
goal_identified
goal_identified
goal_identified
=== ep: 882, time 86.25575017929077, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
goal_identified
goal_identified
=== ep: 883, time 87.22279500961304, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 884, time 81.50371551513672, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
goal_identified
goal_identified
=== ep: 885, time 87.58081865310669, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 886, time 86.93962788581848, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 887, time 86.85203242301941, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
goal_identified
goal_identified
=== ep: 888, time 88.0989100933075, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 889, time 88.80299663543701, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 890, time 82.80686378479004, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 891, time 86.2935938835144, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
goal_identified
goal_identified
=== ep: 892, time 72.40310263633728, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
goal_identified
goal_identified
=== ep: 893, time 81.88659071922302, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
goal_identified
goal_identified
=== ep: 894, time 90.33525705337524, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
goal_identified
=== ep: 895, time 90.91329383850098, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
goal_identified
=== ep: 896, time 81.82164549827576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
goal_identified
goal_identified
=== ep: 897, time 88.46450638771057, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 898, time 79.99427390098572, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 898
goal_identified
goal_identified
=== ep: 899, time 71.05220937728882, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
goal_identified
goal_identified
goal_identified
=== ep: 900, time 85.33467054367065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
goal_identified
goal_identified
=== ep: 901, time 88.12694835662842, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
goal_identified
goal_identified
goal_identified
=== ep: 902, time 80.54577302932739, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 903, time 80.71770095825195, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 904, time 83.15279936790466, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
goal_identified
goal_identified
goal_identified
=== ep: 905, time 91.5544183254242, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 906, time 94.70050120353699, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
goal_identified
goal_identified
goal_identified
=== ep: 907, time 95.27446985244751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 908, time 97.57310223579407, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
goal_identified
goal_identified
goal_identified
=== ep: 909, time 90.28313398361206, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
goal_identified
goal_identified
goal_identified
=== ep: 910, time 79.81807351112366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 911, time 76.5409517288208, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
goal_identified
goal_identified
goal_identified
=== ep: 912, time 75.57548785209656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
goal_identified
=== ep: 913, time 89.43199396133423, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
goal_identified
goal_identified
=== ep: 914, time 87.89531826972961, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
goal_identified
goal_identified
=== ep: 915, time 82.84235787391663, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
goal_identified
goal_identified
goal_identified
=== ep: 916, time 81.01539540290833, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
goal_identified
goal_identified
goal_identified
=== ep: 917, time 89.2606828212738, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
goal_identified
goal_identified
goal_identified
=== ep: 918, time 85.43672299385071, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
goal_identified
goal_identified
goal_identified
=== ep: 919, time 85.61858010292053, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
goal_identified
goal_identified
=== ep: 920, time 86.85989451408386, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
goal_identified
goal_identified
=== ep: 921, time 85.3280737400055, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
goal_identified
goal_identified
goal_identified
=== ep: 922, time 83.96193480491638, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
goal_identified
goal_identified
goal_identified
=== ep: 923, time 86.42414736747742, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
=== ep: 924, time 74.0064103603363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
goal_identified
goal_identified
goal_identified
=== ep: 925, time 81.35019588470459, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 926, time 86.63251042366028, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 927, time 91.14456486701965, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 928, time 91.76655697822571, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
goal_identified
goal_identified
=== ep: 929, time 91.80429267883301, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
goal_identified
goal_identified
goal_identified
=== ep: 930, time 81.5515501499176, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
goal_identified
goal_identified
goal_identified
=== ep: 931, time 76.32324647903442, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
goal_identified
goal_identified
=== ep: 932, time 82.39126110076904, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 933, time 84.6371910572052, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
=== ep: 934, time 81.09718585014343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
goal_identified
goal_identified
=== ep: 935, time 86.59691667556763, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
goal_identified
goal_identified
goal_identified
=== ep: 936, time 85.45836043357849, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
goal_identified
=== ep: 937, time 83.29401421546936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
goal_identified
goal_identified
=== ep: 938, time 82.96794724464417, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
goal_identified
goal_identified
=== ep: 939, time 87.57713174819946, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
goal_identified
goal_identified
=== ep: 940, time 78.69706678390503, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
goal_identified
goal_identified
=== ep: 941, time 78.89873933792114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
goal_identified
goal_identified
=== ep: 942, time 89.99234867095947, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
goal_identified
goal_identified
goal_identified
=== ep: 943, time 84.11284375190735, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
goal_identified
goal_identified
goal_identified
=== ep: 944, time 77.39419960975647, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
goal_identified
=== ep: 945, time 84.89884901046753, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 946, time 87.28521871566772, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
goal_identified
goal_identified
goal_identified
=== ep: 947, time 84.13526654243469, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
goal_identified
=== ep: 948, time 86.47860312461853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
goal_identified
=== ep: 949, time 88.50712847709656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
goal_identified
goal_identified
=== ep: 950, time 79.90573501586914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 951, time 68.4277834892273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
goal_identified
goal_identified
=== ep: 952, time 87.11670660972595, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
goal_identified
goal_identified
=== ep: 953, time 87.00641179084778, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 954, time 90.27122235298157, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
goal_identified
goal_identified
=== ep: 955, time 83.04290270805359, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
goal_identified
goal_identified
=== ep: 956, time 84.64711999893188, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
goal_identified
goal_identified
=== ep: 957, time 80.66959857940674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 958, time 85.4689428806305, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 959, time 90.54378342628479, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 960, time 84.88780117034912, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
goal_identified
goal_identified
goal_identified
=== ep: 961, time 69.56909561157227, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
goal_identified
=== ep: 962, time 82.78513312339783, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
goal_identified
goal_identified
=== ep: 963, time 82.50124311447144, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 964, time 91.20953226089478, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
goal_identified
=== ep: 965, time 91.13698172569275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 966, time 92.95182824134827, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
goal_identified
goal_identified
=== ep: 967, time 89.54069542884827, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
goal_identified
goal_identified
=== ep: 968, time 92.79896807670593, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
goal_identified
=== ep: 969, time 85.99152660369873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 970, time 88.30846095085144, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
goal_identified
=== ep: 971, time 76.07781314849854, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
goal_identified
goal_identified
goal_identified
=== ep: 972, time 83.6660041809082, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
=== ep: 973, time 89.65405225753784, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
goal_identified
goal_identified
=== ep: 974, time 94.26546478271484, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
goal_identified
goal_identified
goal_identified
=== ep: 975, time 95.9619996547699, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
goal_identified
goal_identified
goal_identified
=== ep: 976, time 93.41554379463196, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
goal_identified
goal_identified
=== ep: 977, time 77.97425389289856, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 978, time 79.74814105033875, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
goal_identified
=== ep: 979, time 82.14664006233215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
goal_identified
goal_identified
=== ep: 980, time 90.11458778381348, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
goal_identified
goal_identified
=== ep: 981, time 94.00752282142639, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
goal_identified
goal_identified
goal_identified
=== ep: 982, time 94.17317962646484, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 983, time 89.03131675720215, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
goal_identified
=== ep: 984, time 82.90905451774597, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
goal_identified
goal_identified
goal_identified
=== ep: 985, time 65.74210858345032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
goal_identified
=== ep: 986, time 92.93082666397095, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
goal_identified
goal_identified
=== ep: 987, time 93.31163597106934, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
=== ep: 988, time 91.30462694168091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
goal_identified
goal_identified
=== ep: 989, time 91.16128444671631, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 990, time 66.41837930679321, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 991, time 82.5983316898346, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
goal_identified
goal_identified
goal_identified
=== ep: 992, time 87.03377771377563, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 993, time 91.16145730018616, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
goal_identified
goal_identified
=== ep: 994, time 90.80464458465576, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
goal_identified
goal_identified
goal_identified
=== ep: 995, time 89.50494313240051, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 996, time 83.98656153678894, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
goal_identified
goal_identified
goal_identified
=== ep: 997, time 76.35493063926697, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
goal_identified
goal_identified
=== ep: 998, time 80.56467723846436, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
goal_identified
=== ep: 999, time 85.80168223381042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
goal_identified
goal_identified
goal_identified
=== ep: 1000, time 92.34144592285156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
=== ep: 1001, time 89.8750741481781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
goal_identified
goal_identified
=== ep: 1002, time 89.63217663764954, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
goal_identified
=== ep: 1003, time 75.18598127365112, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1004, time 79.75233125686646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
=== ep: 1005, time 79.87528157234192, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
goal_identified
goal_identified
goal_identified
=== ep: 1006, time 90.65783786773682, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
goal_identified
goal_identified
=== ep: 1007, time 90.34579181671143, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
goal_identified
goal_identified
=== ep: 1008, time 89.50009083747864, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
goal_identified
goal_identified
=== ep: 1009, time 86.41620016098022, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
goal_identified
goal_identified
goal_identified
=== ep: 1010, time 75.68198895454407, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
goal_identified
=== ep: 1011, time 81.05962228775024, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1012, time 75.07373380661011, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
goal_identified
goal_identified
goal_identified
=== ep: 1013, time 89.77854037284851, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
goal_identified
goal_identified
goal_identified
=== ep: 1014, time 91.22686171531677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
goal_identified
goal_identified
=== ep: 1015, time 87.09984374046326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
goal_identified
goal_identified
=== ep: 1016, time 66.16746783256531, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
goal_identified
goal_identified
goal_identified
=== ep: 1017, time 68.66160178184509, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
goal_identified
goal_identified
goal_identified
=== ep: 1018, time 88.72725510597229, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1019, time 88.86518430709839, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1020, time 88.19181942939758, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1021, time 88.28939867019653, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1022, time 83.90578007698059, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
goal_identified
goal_identified
=== ep: 1023, time 66.96795344352722, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
goal_identified
goal_identified
goal_identified
=== ep: 1024, time 88.50790929794312, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1025, time 94.41028380393982, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
goal_identified
goal_identified
goal_identified
=== ep: 1026, time 95.06057739257812, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
goal_identified
goal_identified
goal_identified
=== ep: 1027, time 90.2640733718872, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1028, time 83.24785780906677, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
goal_identified
goal_identified
goal_identified
=== ep: 1029, time 83.82798266410828, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
goal_identified
goal_identified
=== ep: 1030, time 78.76308274269104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1031, time 88.82922267913818, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1032, time 90.09898114204407, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
goal_identified
=== ep: 1033, time 91.90175700187683, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
goal_identified
=== ep: 1034, time 95.04738807678223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
goal_identified
=== ep: 1035, time 92.44303560256958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
goal_identified
goal_identified
goal_identified
=== ep: 1036, time 85.7963457107544, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
goal_identified
goal_identified
goal_identified
=== ep: 1037, time 82.47967481613159, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1038, time 80.35316300392151, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
goal_identified
goal_identified
goal_identified
=== ep: 1039, time 86.21435809135437, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
goal_identified
goal_identified
=== ep: 1040, time 88.29182410240173, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1041, time 87.83594131469727, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
goal_identified
goal_identified
goal_identified
=== ep: 1042, time 84.53425598144531, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1043, time 85.20440220832825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1044, time 84.12778782844543, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
goal_identified
=== ep: 1045, time 85.92403364181519, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1046, time 89.45852971076965, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
goal_identified
goal_identified
=== ep: 1047, time 88.9723470211029, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
goal_identified
goal_identified
goal_identified
=== ep: 1048, time 91.48276853561401, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
goal_identified
goal_identified
goal_identified
=== ep: 1049, time 89.27895379066467, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1050, time 91.5125162601471, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
goal_identified
goal_identified
goal_identified
=== ep: 1051, time 93.55471849441528, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
goal_identified
goal_identified
goal_identified
=== ep: 1052, time 88.46218776702881, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
goal_identified
goal_identified
=== ep: 1053, time 86.18368601799011, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
goal_identified
goal_identified
=== ep: 1054, time 78.09454417228699, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
goal_identified
goal_identified
=== ep: 1055, time 85.56217002868652, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1056, time 87.83062601089478, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1057, time 90.74701070785522, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
goal_identified
=== ep: 1058, time 90.8743793964386, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1059, time 92.78014326095581, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1060, time 90.13207268714905, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1061, time 91.10465621948242, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
goal_identified
goal_identified
=== ep: 1062, time 86.03542828559875, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
goal_identified
goal_identified
=== ep: 1063, time 77.05870199203491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
goal_identified
goal_identified
=== ep: 1064, time 83.50139093399048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
goal_identified
goal_identified
=== ep: 1065, time 78.42424869537354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
goal_identified
goal_identified
goal_identified
=== ep: 1066, time 91.69117569923401, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1067, time 88.2925398349762, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
goal_identified
goal_identified
=== ep: 1068, time 83.37613558769226, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1069, time 83.5061411857605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
goal_identified
goal_identified
goal_identified
=== ep: 1070, time 90.18005132675171, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1071, time 84.43491077423096, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1072, time 86.87656426429749, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1073, time 77.1357204914093, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
goal_identified
goal_identified
=== ep: 1074, time 87.78050208091736, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
goal_identified
=== ep: 1075, time 84.54911804199219, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1076, time 92.38230562210083, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1072
goal_identified
goal_identified
goal_identified
=== ep: 1077, time 93.01889944076538, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
goal_identified
goal_identified
goal_identified
=== ep: 1078, time 89.72847247123718, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
goal_identified
goal_identified
=== ep: 1079, time 89.7307665348053, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
goal_identified
goal_identified
=== ep: 1080, time 92.39242482185364, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
goal_identified
goal_identified
goal_identified
=== ep: 1081, time 91.49261093139648, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
goal_identified
goal_identified
=== ep: 1082, time 88.19553017616272, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
goal_identified
goal_identified
goal_identified
=== ep: 1083, time 77.48897075653076, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1084, time 83.55098962783813, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
goal_identified
goal_identified
goal_identified
=== ep: 1085, time 88.69018316268921, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
goal_identified
goal_identified
goal_identified
=== ep: 1086, time 93.66418814659119, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
goal_identified
=== ep: 1087, time 95.82942843437195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
goal_identified
goal_identified
goal_identified
=== ep: 1088, time 97.29987573623657, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
goal_identified
goal_identified
goal_identified
=== ep: 1089, time 88.67343616485596, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
goal_identified
goal_identified
goal_identified
=== ep: 1090, time 85.94279074668884, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1091, time 62.27047920227051, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
=== ep: 1092, time 86.86948800086975, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1093, time 91.30347084999084, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1094, time 90.56809496879578, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
goal_identified
goal_identified
goal_identified
=== ep: 1095, time 88.59379005432129, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1096, time 87.38956475257874, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
goal_identified
goal_identified
goal_identified
=== ep: 1097, time 81.50058913230896, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1098, time 86.70430254936218, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
goal_identified
goal_identified
goal_identified
=== ep: 1099, time 87.52393460273743, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1100, time 86.91569495201111, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
goal_identified
=== ep: 1101, time 87.91858148574829, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
goal_identified
goal_identified
goal_identified
=== ep: 1102, time 88.67713379859924, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
goal_identified
goal_identified
goal_identified
=== ep: 1103, time 87.90958166122437, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1104, time 90.97942209243774, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1105, time 87.45709490776062, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1106, time 86.11849570274353, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
goal_identified
=== ep: 1107, time 83.80085396766663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1108, time 79.22071623802185, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1109, time 87.43676233291626, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
goal_identified
goal_identified
=== ep: 1110, time 87.7675678730011, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
goal_identified
goal_identified
goal_identified
=== ep: 1111, time 90.11206316947937, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
goal_identified
goal_identified
goal_identified
=== ep: 1112, time 91.05270624160767, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1113, time 90.36782336235046, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
goal_identified
goal_identified
goal_identified
=== ep: 1114, time 94.56949806213379, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
goal_identified
goal_identified
=== ep: 1115, time 91.51402974128723, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1116, time 82.71870923042297, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
goal_identified
goal_identified
=== ep: 1117, time 84.84544634819031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1118, time 79.01913571357727, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
goal_identified
goal_identified
goal_identified
=== ep: 1119, time 87.39024901390076, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
goal_identified
goal_identified
=== ep: 1120, time 87.26279640197754, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
goal_identified
goal_identified
=== ep: 1121, time 92.17090082168579, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1122, time 90.95624327659607, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
goal_identified
goal_identified
=== ep: 1123, time 92.5287024974823, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1124, time 91.05790758132935, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1125, time 78.82712483406067, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1126, time 78.66014862060547, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
=== ep: 1127, time 84.86181116104126, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
goal_identified
goal_identified
goal_identified
=== ep: 1128, time 87.44088959693909, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
goal_identified
goal_identified
=== ep: 1129, time 93.2331337928772, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
goal_identified
goal_identified
=== ep: 1130, time 94.57699203491211, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
goal_identified
goal_identified
=== ep: 1131, time 90.89568614959717, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
goal_identified
goal_identified
goal_identified
=== ep: 1132, time 80.5766191482544, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
goal_identified
goal_identified
=== ep: 1133, time 78.8337414264679, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
goal_identified
goal_identified
goal_identified
=== ep: 1134, time 80.61205387115479, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
goal_identified
goal_identified
=== ep: 1135, time 90.7359778881073, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
goal_identified
goal_identified
goal_identified
=== ep: 1136, time 92.48638129234314, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1137, time 89.91972804069519, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
goal_identified
goal_identified
goal_identified
=== ep: 1138, time 86.37813997268677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
goal_identified
goal_identified
=== ep: 1139, time 83.34286761283875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
goal_identified
goal_identified
goal_identified
=== ep: 1140, time 74.21909284591675, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1141, time 87.46330952644348, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1142, time 85.95247292518616, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
goal_identified
=== ep: 1143, time 94.73947143554688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1144, time 91.08681654930115, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
goal_identified
goal_identified
=== ep: 1145, time 86.18726658821106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
goal_identified
goal_identified
=== ep: 1146, time 82.66667008399963, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1147, time 73.32397103309631, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
goal_identified
goal_identified
=== ep: 1148, time 87.66391396522522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
goal_identified
goal_identified
goal_identified
=== ep: 1149, time 87.25423669815063, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
goal_identified
goal_identified
goal_identified
=== ep: 1150, time 93.23876428604126, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
goal_identified
goal_identified
goal_identified
=== ep: 1151, time 95.58067512512207, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1152, time 90.87989354133606, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
=== ep: 1153, time 76.47442436218262, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
goal_identified
goal_identified
=== ep: 1154, time 70.24161148071289, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
goal_identified
=== ep: 1155, time 90.16214752197266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1156, time 89.92568945884705, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
goal_identified
goal_identified
=== ep: 1157, time 89.70102643966675, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
=== ep: 1158, time 83.73962998390198, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
goal_identified
goal_identified
goal_identified
=== ep: 1159, time 77.47281527519226, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
goal_identified
goal_identified
goal_identified
=== ep: 1160, time 74.99019622802734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1161, time 89.01689982414246, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
goal_identified
goal_identified
=== ep: 1162, time 90.23060512542725, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
goal_identified
goal_identified
=== ep: 1163, time 86.48472356796265, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
goal_identified
goal_identified
goal_identified
=== ep: 1164, time 68.64568424224854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
goal_identified
=== ep: 1165, time 82.66639161109924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
goal_identified
goal_identified
goal_identified
=== ep: 1166, time 81.46399164199829, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
goal_identified
goal_identified
goal_identified
=== ep: 1167, time 87.18878245353699, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1168, time 87.67775678634644, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
goal_identified
goal_identified
=== ep: 1169, time 87.90010356903076, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1170, time 78.25511956214905, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
goal_identified
goal_identified
goal_identified
=== ep: 1171, time 73.89748334884644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1172, time 84.59609818458557, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1173, time 87.2647032737732, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1174, time 83.04347586631775, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
=== ep: 1175, time 82.51181197166443, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1176, time 82.96554660797119, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1177, time 77.92912316322327, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1178, time 77.48151135444641, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1179, time 86.7109124660492, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
goal_identified
goal_identified
goal_identified
=== ep: 1180, time 78.47790384292603, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
goal_identified
goal_identified
goal_identified
=== ep: 1181, time 75.4574978351593, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
goal_identified
goal_identified
=== ep: 1182, time 81.6492567062378, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
goal_identified
=== ep: 1183, time 85.18430852890015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
goal_identified
=== ep: 1184, time 77.99170589447021, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
goal_identified
goal_identified
goal_identified
=== ep: 1185, time 79.6504864692688, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
goal_identified
goal_identified
goal_identified
=== ep: 1186, time 81.38670229911804, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
goal_identified
goal_identified
goal_identified
=== ep: 1187, time 78.92342042922974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
goal_identified
goal_identified
=== ep: 1188, time 80.93184804916382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
goal_identified
goal_identified
=== ep: 1189, time 81.30836272239685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
goal_identified
goal_identified
goal_identified
=== ep: 1190, time 87.51369190216064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
goal_identified
=== ep: 1191, time 84.0831823348999, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1192, time 81.11630153656006, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
goal_identified
goal_identified
goal_identified
=== ep: 1193, time 74.43678998947144, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
goal_identified
goal_identified
=== ep: 1194, time 79.14990329742432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
goal_identified
goal_identified
=== ep: 1195, time 80.31407761573792, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
goal_identified
goal_identified
goal_identified
=== ep: 1196, time 82.35715460777283, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1197, time 87.24039697647095, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
goal_identified
goal_identified
goal_identified
=== ep: 1198, time 85.01586580276489, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
goal_identified
goal_identified
=== ep: 1199, time 84.25908827781677, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
goal_identified
=== ep: 1200, time 74.17722821235657, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1201, time 75.99011421203613, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
goal_identified
goal_identified
goal_identified
=== ep: 1202, time 81.72426652908325, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
goal_identified
goal_identified
goal_identified
=== ep: 1203, time 80.91626453399658, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1204, time 89.58009958267212, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1205, time 86.95079159736633, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
goal_identified
=== ep: 1206, time 83.5634663105011, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1207, time 79.3268449306488, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1208, time 74.84964346885681, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
goal_identified
goal_identified
goal_identified
=== ep: 1209, time 83.05065155029297, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1210, time 83.53035044670105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
goal_identified
goal_identified
=== ep: 1211, time 87.36438155174255, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1212, time 86.11702466011047, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
goal_identified
goal_identified
=== ep: 1213, time 67.5475811958313, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
goal_identified
goal_identified
goal_identified
=== ep: 1214, time 76.43503880500793, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
goal_identified
goal_identified
=== ep: 1215, time 87.5214409828186, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
goal_identified
goal_identified
=== ep: 1216, time 86.87505149841309, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
goal_identified
=== ep: 1217, time 85.55168056488037, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1218, time 79.52151703834534, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
goal_identified
=== ep: 1219, time 74.67773151397705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
goal_identified
goal_identified
goal_identified
=== ep: 1220, time 80.74786758422852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
goal_identified
goal_identified
=== ep: 1221, time 89.50521039962769, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
goal_identified
goal_identified
goal_identified
=== ep: 1222, time 89.65817189216614, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
goal_identified
goal_identified
=== ep: 1223, time 82.42970299720764, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 59.671369791030884, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1225, time 82.80166506767273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1226, time 85.70185112953186, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
goal_identified
=== ep: 1227, time 85.00611162185669, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
goal_identified
=== ep: 1228, time 72.6556887626648, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1229, time 76.23074388504028, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1230, time 81.25942993164062, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
=== ep: 1231, time 79.7624900341034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1232, time 77.83231449127197, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1233, time 78.6753442287445, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1234, time 84.49098324775696, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
goal_identified
goal_identified
goal_identified
=== ep: 1235, time 83.63661742210388, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
goal_identified
goal_identified
=== ep: 1236, time 80.59336853027344, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1236
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1237, time 63.682764291763306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1238, time 78.69044995307922, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1239, time 83.60627746582031, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
goal_identified
goal_identified
goal_identified
=== ep: 1240, time 87.4076042175293, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1241, time 82.67050647735596, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1242, time 75.45967292785645, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
goal_identified
goal_identified
=== ep: 1243, time 79.60104393959045, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
goal_identified
goal_identified
goal_identified
=== ep: 1244, time 81.20136332511902, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1245, time 82.86324882507324, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1246, time 69.46101379394531, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1247, time 77.04784727096558, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
goal_identified
goal_identified
=== ep: 1248, time 82.58575224876404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
=== ep: 1249, time 86.76251864433289, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
goal_identified
goal_identified
=== ep: 1250, time 83.17025947570801, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
goal_identified
goal_identified
goal_identified
=== ep: 1251, time 77.9236249923706, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
goal_identified
=== ep: 1252, time 77.71827936172485, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
goal_identified
goal_identified
goal_identified
=== ep: 1253, time 79.34677863121033, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
goal_identified
goal_identified
goal_identified
=== ep: 1254, time 83.80514979362488, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
goal_identified
goal_identified
=== ep: 1255, time 82.71121263504028, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
goal_identified
goal_identified
=== ep: 1256, time 79.62310075759888, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
goal_identified
goal_identified
goal_identified
=== ep: 1257, time 72.38158392906189, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1258, time 74.81027626991272, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
goal_identified
=== ep: 1259, time 78.4729835987091, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
goal_identified
goal_identified
goal_identified
=== ep: 1260, time 83.50772786140442, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
goal_identified
goal_identified
goal_identified
=== ep: 1261, time 79.98209238052368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1262, time 73.2580668926239, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
goal_identified
goal_identified
goal_identified
=== ep: 1263, time 81.46574783325195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1264, time 85.19989848136902, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
goal_identified
goal_identified
=== ep: 1265, time 78.31965065002441, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
goal_identified
goal_identified
goal_identified
=== ep: 1266, time 66.76609969139099, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
goal_identified
goal_identified
goal_identified
=== ep: 1267, time 83.41960859298706, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1268, time 84.72809600830078, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
goal_identified
goal_identified
=== ep: 1269, time 77.39674282073975, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1270, time 67.84984135627747, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1271, time 83.04461073875427, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
goal_identified
goal_identified
goal_identified
=== ep: 1272, time 71.49571490287781, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
goal_identified
goal_identified
=== ep: 1273, time 76.46443247795105, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 84.40574049949646, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1275, time 83.48196315765381, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1276, time 69.00361609458923, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
goal_identified
goal_identified
goal_identified
=== ep: 1277, time 75.87216901779175, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1278, time 84.85948491096497, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
goal_identified
goal_identified
goal_identified
=== ep: 1279, time 82.32264733314514, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1280, time 67.65461421012878, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
goal_identified
goal_identified
=== ep: 1281, time 71.20158004760742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
goal_identified
goal_identified
goal_identified
=== ep: 1282, time 80.04612135887146, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1283, time 81.06240391731262, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1284, time 76.23498773574829, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
goal_identified
goal_identified
=== ep: 1285, time 73.52075219154358, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1286, time 80.3041627407074, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
goal_identified
goal_identified
goal_identified
=== ep: 1287, time 76.39186477661133, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1288, time 79.89761853218079, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1289, time 74.52489638328552, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1290, time 79.89971137046814, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
goal_identified
goal_identified
=== ep: 1291, time 73.20436501502991, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1292, time 72.63277387619019, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
goal_identified
=== ep: 1293, time 80.74165272712708, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1293
goal_identified
goal_identified
goal_identified
=== ep: 1294, time 80.83298325538635, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
goal_identified
goal_identified
goal_identified
=== ep: 1295, time 73.94471549987793, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
goal_identified
goal_identified
goal_identified
=== ep: 1296, time 68.9211699962616, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
goal_identified
goal_identified
goal_identified
=== ep: 1297, time 79.31161713600159, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
=== ep: 1298, time 82.35180354118347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
goal_identified
=== ep: 1299, time 78.56993508338928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
goal_identified
goal_identified
=== ep: 1300, time 68.23719048500061, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1301, time 72.81639575958252, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1302, time 80.315505027771, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1303, time 79.67599606513977, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1304, time 76.39358806610107, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1305, time 75.28197526931763, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
goal_identified
goal_identified
goal_identified
=== ep: 1306, time 75.65983247756958, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1307, time 73.16172814369202, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1308, time 75.15759873390198, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
goal_identified
goal_identified
goal_identified
=== ep: 1309, time 83.97455358505249, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1310, time 74.82867479324341, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
goal_identified
goal_identified
=== ep: 1311, time 74.76200866699219, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1312, time 79.05766081809998, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1313, time 78.41180372238159, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
goal_identified
goal_identified
goal_identified
=== ep: 1314, time 78.11883783340454, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1315, time 78.77957940101624, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
goal_identified
goal_identified
goal_identified
=== ep: 1316, time 75.98252010345459, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1317, time 80.97369027137756, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
goal_identified
goal_identified
goal_identified
=== ep: 1318, time 78.55963683128357, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
goal_identified
goal_identified
=== ep: 1319, time 74.22043800354004, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
goal_identified
goal_identified
=== ep: 1320, time 81.0157516002655, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
goal_identified
goal_identified
goal_identified
=== ep: 1321, time 81.61672449111938, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1322, time 78.33652925491333, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
goal_identified
goal_identified
goal_identified
=== ep: 1323, time 73.41519355773926, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
goal_identified
=== ep: 1324, time 80.51882362365723, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
goal_identified
goal_identified
=== ep: 1325, time 82.53286480903625, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1326, time 75.13100409507751, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
goal_identified
goal_identified
=== ep: 1327, time 72.42107963562012, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1328, time 82.10284066200256, eps 0.001, sum reward: 6, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
goal_identified
goal_identified
=== ep: 1329, time 82.10262203216553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
goal_identified
goal_identified
=== ep: 1330, time 66.51638555526733, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
goal_identified
goal_identified
goal_identified
=== ep: 1331, time 81.97804880142212, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
goal_identified
=== ep: 1332, time 81.14433455467224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
goal_identified
=== ep: 1333, time 74.63097786903381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
goal_identified
=== ep: 1334, time 77.78860902786255, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
goal_identified
goal_identified
goal_identified
=== ep: 1335, time 80.26007032394409, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
goal_identified
goal_identified
goal_identified
=== ep: 1336, time 79.15885591506958, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1337, time 75.4210593700409, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1338, time 76.15989184379578, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
goal_identified
goal_identified
goal_identified
=== ep: 1339, time 85.9621856212616, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1340, time 79.35340309143066, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
goal_identified
goal_identified
=== ep: 1341, time 65.64435911178589, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
goal_identified
goal_identified
goal_identified
=== ep: 1342, time 83.87351369857788, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1343, time 84.39323329925537, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
goal_identified
goal_identified
=== ep: 1344, time 67.71409010887146, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1345, time 71.92395997047424, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
goal_identified
goal_identified
goal_identified
=== ep: 1346, time 79.1645131111145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1347, time 56.40760564804077, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1348, time 77.48556900024414, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1349, time 81.66617703437805, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1350, time 75.40707468986511, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1351, time 75.39539384841919, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1352, time 83.62548732757568, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1353, time 70.26563692092896, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
goal_identified
=== ep: 1354, time 72.4462559223175, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
goal_identified
goal_identified
=== ep: 1355, time 82.79932713508606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1356, time 81.12466955184937, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
goal_identified
goal_identified
=== ep: 1357, time 70.87415838241577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
goal_identified
goal_identified
=== ep: 1358, time 72.50467705726624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
goal_identified
=== ep: 1359, time 72.30216598510742, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1360, time 78.206218957901, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
goal_identified
goal_identified
=== ep: 1361, time 81.68123245239258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1362, time 78.68313241004944, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1363, time 71.38086557388306, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1364, time 74.11210346221924, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1365, time 76.9585325717926, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
goal_identified
goal_identified
=== ep: 1366, time 71.38423466682434, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
goal_identified
goal_identified
=== ep: 1367, time 78.84491920471191, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
goal_identified
goal_identified
=== ep: 1368, time 78.48394799232483, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
goal_identified
goal_identified
goal_identified
=== ep: 1369, time 78.24188995361328, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1370, time 66.85528326034546, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1371, time 81.25195050239563, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
goal_identified
goal_identified
goal_identified
=== ep: 1372, time 83.94816946983337, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1373, time 75.75648260116577, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1374, time 67.18462252616882, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
goal_identified
=== ep: 1375, time 80.57042098045349, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
goal_identified
goal_identified
=== ep: 1376, time 65.07244563102722, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1376
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1377, time 70.13310933113098, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1378, time 85.11866736412048, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
goal_identified
goal_identified
=== ep: 1379, time 80.92092061042786, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
goal_identified
goal_identified
=== ep: 1380, time 70.7672119140625, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1381, time 77.23391938209534, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
goal_identified
goal_identified
=== ep: 1382, time 79.27354550361633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1383, time 78.08095097541809, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
=== ep: 1384, time 78.32865381240845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
goal_identified
goal_identified
=== ep: 1385, time 77.56388354301453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
goal_identified
goal_identified
=== ep: 1386, time 66.54600858688354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1387, time 73.31291031837463, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
goal_identified
goal_identified
=== ep: 1388, time 66.05964422225952, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1389, time 61.345104694366455, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
goal_identified
goal_identified
=== ep: 1390, time 74.57311081886292, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
goal_identified
goal_identified
goal_identified
=== ep: 1391, time 57.50581336021423, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
goal_identified
goal_identified
goal_identified
=== ep: 1392, time 76.1405417919159, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
goal_identified
goal_identified
=== ep: 1393, time 80.89983057975769, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
goal_identified
goal_identified
goal_identified
=== ep: 1394, time 73.01248693466187, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
goal_identified
goal_identified
goal_identified
=== ep: 1395, time 73.13332891464233, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1396, time 80.93971800804138, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1397, time 72.38815712928772, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
goal_identified
goal_identified
goal_identified
=== ep: 1398, time 67.52243566513062, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1399, time 78.49061059951782, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
goal_identified
=== ep: 1400, time 78.87776231765747, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1401, time 74.98289966583252, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
goal_identified
goal_identified
goal_identified
=== ep: 1402, time 71.61917924880981, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
goal_identified
goal_identified
goal_identified
=== ep: 1403, time 73.12670612335205, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1404, time 72.71009254455566, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1405, time 76.40202188491821, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1406, time 79.12300753593445, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
goal_identified
goal_identified
goal_identified
=== ep: 1407, time 73.73936104774475, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1408, time 77.56574606895447, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1409, time 76.86329984664917, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
goal_identified
goal_identified
goal_identified
=== ep: 1410, time 54.803372383117676, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
goal_identified
goal_identified
=== ep: 1411, time 79.79627656936646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
goal_identified
goal_identified
goal_identified
=== ep: 1412, time 72.45058345794678, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1413, time 73.85161304473877, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1414, time 83.75372123718262, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
goal_identified
=== ep: 1415, time 76.48073887825012, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1416, time 63.09180402755737, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
goal_identified
goal_identified
goal_identified
=== ep: 1417, time 82.48987197875977, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
goal_identified
goal_identified
=== ep: 1418, time 73.54258847236633, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
goal_identified
goal_identified
goal_identified
=== ep: 1419, time 72.51192426681519, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
goal_identified
goal_identified
goal_identified
=== ep: 1420, time 81.34071516990662, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
goal_identified
goal_identified
=== ep: 1421, time 81.55459856987, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1422, time 67.45300912857056, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1423, time 70.529541015625, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 72.72577571868896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
goal_identified
goal_identified
goal_identified
=== ep: 1425, time 73.50575137138367, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1426, time 78.82821726799011, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
goal_identified
goal_identified
=== ep: 1427, time 77.33274555206299, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
goal_identified
goal_identified
goal_identified
=== ep: 1428, time 73.6293077468872, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
goal_identified
goal_identified
goal_identified
=== ep: 1429, time 79.24836754798889, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1430, time 71.97948837280273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1431, time 71.13288569450378, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1432, time 81.57275700569153, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
=== ep: 1433, time 78.20344877243042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
goal_identified
goal_identified
goal_identified
=== ep: 1434, time 59.382150173187256, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
goal_identified
=== ep: 1435, time 74.63365578651428, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1436, time 78.49822783470154, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1437, time 72.97530627250671, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1438, time 75.92534685134888, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1439, time 82.5045838356018, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
goal_identified
goal_identified
goal_identified
=== ep: 1440, time 72.27689051628113, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1441, time 65.65568161010742, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
goal_identified
goal_identified
goal_identified
=== ep: 1442, time 83.84777164459229, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
goal_identified
goal_identified
=== ep: 1443, time 75.85481476783752, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1444, time 66.81199479103088, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
goal_identified
=== ep: 1445, time 82.25562238693237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1446, time 74.39990472793579, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1447, time 69.68057155609131, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
goal_identified
goal_identified
=== ep: 1448, time 79.52004027366638, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1449, time 78.51865196228027, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1450, time 72.23682928085327, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1451, time 73.39267587661743, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
=== ep: 1452, time 79.63230156898499, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
goal_identified
=== ep: 1453, time 78.6899676322937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
goal_identified
goal_identified
=== ep: 1454, time 69.4856789112091, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1454
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1455, time 72.21507382392883, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
goal_identified
goal_identified
goal_identified
=== ep: 1456, time 71.64774489402771, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1457, time 76.03165054321289, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1458, time 77.43992900848389, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
goal_identified
goal_identified
=== ep: 1459, time 72.10834050178528, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
goal_identified
goal_identified
=== ep: 1460, time 71.88460040092468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1461, time 77.30393934249878, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
goal_identified
goal_identified
=== ep: 1462, time 70.67881941795349, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1463, time 74.23342847824097, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1464, time 77.86520171165466, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
goal_identified
=== ep: 1465, time 78.3441104888916, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1466, time 67.6285150051117, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1467, time 72.41097617149353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
goal_identified
goal_identified
=== ep: 1468, time 76.44599485397339, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1469, time 69.75750160217285, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
goal_identified
goal_identified
goal_identified
=== ep: 1470, time 77.51622915267944, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
goal_identified
goal_identified
goal_identified
=== ep: 1471, time 82.2303695678711, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
goal_identified
goal_identified
goal_identified
=== ep: 1472, time 72.5198175907135, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
goal_identified
goal_identified
=== ep: 1473, time 65.32273530960083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
goal_identified
=== ep: 1474, time 79.8153862953186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
goal_identified
goal_identified
=== ep: 1475, time 59.95717668533325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
goal_identified
goal_identified
goal_identified
=== ep: 1476, time 71.3126745223999, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
goal_identified
goal_identified
goal_identified
=== ep: 1477, time 79.51709699630737, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1478, time 76.44888925552368, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
goal_identified
goal_identified
=== ep: 1479, time 72.60929870605469, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
=== ep: 1480, time 75.58503222465515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1481, time 73.58770275115967, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
goal_identified
goal_identified
goal_identified
=== ep: 1482, time 70.43935990333557, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
goal_identified
goal_identified
=== ep: 1483, time 74.31366753578186, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1484, time 74.93074440956116, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
goal_identified
goal_identified
goal_identified
=== ep: 1485, time 77.22492551803589, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1486, time 75.1571273803711, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
goal_identified
goal_identified
goal_identified
=== ep: 1487, time 69.12768721580505, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
goal_identified
goal_identified
goal_identified
=== ep: 1488, time 68.15994024276733, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1489, time 65.07203769683838, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1490, time 61.96611928939819, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
goal_identified
goal_identified
=== ep: 1491, time 75.22808289527893, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1492, time 76.9518940448761, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
goal_identified
goal_identified
goal_identified
=== ep: 1493, time 72.42238759994507, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1494, time 77.38078761100769, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1495, time 82.93683695793152, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
goal_identified
goal_identified
goal_identified
=== ep: 1496, time 68.248530626297, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1497, time 67.87560963630676, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1498, time 81.72009682655334, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
goal_identified
goal_identified
goal_identified
=== ep: 1499, time 79.95737361907959, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
goal_identified
goal_identified
=== ep: 1500, time 70.26904153823853, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1501, time 78.68176603317261, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
goal_identified
goal_identified
=== ep: 1502, time 74.79126501083374, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 65.26047730445862, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
goal_identified
goal_identified
goal_identified
=== ep: 1504, time 80.91237044334412, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
goal_identified
=== ep: 1505, time 77.22718691825867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1506, time 73.71636414527893, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
goal_identified
goal_identified
goal_identified
=== ep: 1507, time 72.72292375564575, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1508, time 78.73329496383667, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
goal_identified
goal_identified
goal_identified
=== ep: 1509, time 56.843361139297485, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
goal_identified
goal_identified
goal_identified
=== ep: 1510, time 77.18771982192993, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1511, time 65.52556347846985, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
goal_identified
goal_identified
=== ep: 1512, time 64.40488052368164, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1513, time 74.01923394203186, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
goal_identified
goal_identified
goal_identified
=== ep: 1514, time 67.71246767044067, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1515, time 78.5937430858612, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1516, time 78.37350392341614, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
goal_identified
goal_identified
goal_identified
=== ep: 1517, time 71.62595438957214, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1518, time 67.0171868801117, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1519, time 73.17885708808899, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1520, time 76.77416920661926, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
goal_identified
goal_identified
=== ep: 1521, time 74.75546765327454, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
=== ep: 1522, time 74.15947365760803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
goal_identified
=== ep: 1523, time 73.70973086357117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
=== ep: 1524, time 77.82112789154053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
goal_identified
goal_identified
goal_identified
=== ep: 1525, time 79.21285724639893, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1526, time 75.71137309074402, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1527, time 72.12712287902832, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1528, time 80.32530570030212, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
goal_identified
=== ep: 1529, time 84.06093764305115, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
goal_identified
goal_identified
=== ep: 1530, time 72.63749694824219, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
goal_identified
=== ep: 1531, time 71.407794713974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1532, time 85.45575642585754, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1533, time 77.1663670539856, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
goal_identified
goal_identified
=== ep: 1534, time 66.07940101623535, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
goal_identified
goal_identified
goal_identified
=== ep: 1535, time 81.95669293403625, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
goal_identified
goal_identified
goal_identified
=== ep: 1536, time 70.21775770187378, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
goal_identified
goal_identified
goal_identified
=== ep: 1537, time 74.78791618347168, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1538, time 84.82446146011353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1539, time 78.47808003425598, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
goal_identified
goal_identified
goal_identified
=== ep: 1540, time 66.46755719184875, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1541, time 82.81990909576416, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1542, time 84.43001937866211, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
goal_identified
=== ep: 1543, time 74.34255361557007, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
goal_identified
goal_identified
=== ep: 1544, time 63.81867551803589, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1545, time 77.60538387298584, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
goal_identified
goal_identified
goal_identified
=== ep: 1546, time 74.79587697982788, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1547, time 76.330806016922, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1548, time 75.42373418807983, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
goal_identified
=== ep: 1549, time 71.1424777507782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1550, time 66.90824341773987, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1551, time 64.76030945777893, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
goal_identified
=== ep: 1552, time 62.962926149368286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1553, time 71.5328061580658, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1554, time 67.82214689254761, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
goal_identified
goal_identified
=== ep: 1555, time 68.8887836933136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1556, time 79.31708598136902, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1557, time 80.58873915672302, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1558, time 70.2804102897644, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
goal_identified
goal_identified
=== ep: 1559, time 75.76385617256165, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
goal_identified
goal_identified
=== ep: 1560, time 76.43593525886536, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
goal_identified
goal_identified
goal_identified
=== ep: 1561, time 70.41453266143799, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
goal_identified
goal_identified
goal_identified
=== ep: 1562, time 78.16642904281616, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
goal_identified
=== ep: 1563, time 81.96534967422485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
goal_identified
goal_identified
=== ep: 1564, time 73.70001792907715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
goal_identified
goal_identified
goal_identified
=== ep: 1565, time 66.09048795700073, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1566, time 83.63361167907715, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
goal_identified
=== ep: 1567, time 79.64999890327454, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
goal_identified
goal_identified
=== ep: 1568, time 64.37680387496948, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
goal_identified
goal_identified
goal_identified
=== ep: 1569, time 82.46889853477478, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
=== ep: 1570, time 75.43037128448486, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
goal_identified
goal_identified
=== ep: 1571, time 69.13435482978821, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
goal_identified
goal_identified
=== ep: 1572, time 83.15820074081421, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1573, time 77.62625169754028, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1574, time 64.7465136051178, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
goal_identified
goal_identified
=== ep: 1575, time 79.70792770385742, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1576, time 77.35479044914246, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1577, time 69.14901304244995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1578, time 77.28022861480713, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
goal_identified
goal_identified
goal_identified
=== ep: 1579, time 75.10682559013367, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
goal_identified
=== ep: 1580, time 70.60148096084595, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
goal_identified
goal_identified
goal_identified
=== ep: 1581, time 77.98305010795593, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1582, time 81.06900119781494, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
goal_identified
goal_identified
=== ep: 1583, time 69.54183530807495, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
goal_identified
goal_identified
goal_identified
=== ep: 1584, time 70.34539937973022, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1585, time 74.98983383178711, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1586, time 71.51552438735962, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
goal_identified
goal_identified
=== ep: 1587, time 75.969717502594, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
goal_identified
goal_identified
goal_identified
=== ep: 1588, time 76.70737624168396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1589, time 75.49373054504395, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
goal_identified
=== ep: 1590, time 71.49790954589844, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1591, time 72.09606838226318, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
goal_identified
goal_identified
goal_identified
=== ep: 1592, time 69.59611701965332, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1593, time 78.97510695457458, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1594, time 80.30082726478577, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1595, time 66.93641328811646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1596, time 67.79299139976501, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1597, time 73.34189629554749, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1598, time 60.13751149177551, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
goal_identified
goal_identified
=== ep: 1599, time 78.37628984451294, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
goal_identified
goal_identified
=== ep: 1600, time 79.44978785514832, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1600
goal_identified
=== ep: 1601, time 67.7670042514801, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1602, time 77.95314431190491, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
goal_identified
goal_identified
=== ep: 1603, time 79.41735744476318, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
goal_identified
goal_identified
goal_identified
=== ep: 1604, time 64.65660452842712, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
goal_identified
goal_identified
=== ep: 1605, time 79.33828353881836, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
goal_identified
goal_identified
goal_identified
=== ep: 1606, time 82.71234846115112, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
goal_identified
goal_identified
=== ep: 1607, time 66.16024494171143, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1608, time 76.09695792198181, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1609, time 78.84414958953857, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
goal_identified
goal_identified
goal_identified
=== ep: 1610, time 55.226956605911255, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1610
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1611, time 81.71462965011597, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1612, time 78.37107372283936, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1613, time 66.04334878921509, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1614, time 81.81851243972778, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1615, time 75.26686525344849, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
goal_identified
goal_identified
goal_identified
=== ep: 1616, time 67.31327795982361, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
goal_identified
goal_identified
goal_identified
=== ep: 1617, time 81.21423101425171, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
goal_identified
goal_identified
goal_identified
=== ep: 1618, time 79.46315932273865, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
goal_identified
goal_identified
=== ep: 1619, time 71.36069345474243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1620, time 74.09637904167175, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
goal_identified
goal_identified
=== ep: 1621, time 75.16227579116821, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
goal_identified
goal_identified
goal_identified
=== ep: 1622, time 75.68336462974548, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
goal_identified
goal_identified
=== ep: 1623, time 77.07536005973816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1624, time 74.0671796798706, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
goal_identified
goal_identified
=== ep: 1625, time 70.69461274147034, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
goal_identified
goal_identified
=== ep: 1626, time 77.67331790924072, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1627, time 75.11485290527344, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1627
goal_identified
goal_identified
goal_identified
=== ep: 1628, time 72.08708834648132, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1629, time 78.35996890068054, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
=== ep: 1630, time 80.07064604759216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
goal_identified
goal_identified
=== ep: 1631, time 68.25413107872009, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
goal_identified
goal_identified
=== ep: 1632, time 75.995108127594, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1633, time 83.55203628540039, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1516
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1634, time 73.1528491973877, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1634
goal_identified
goal_identified
goal_identified
=== ep: 1635, time 73.83438563346863, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1636, time 87.89061498641968, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1637, time 76.00597095489502, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1638, time 72.31192278862, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1639, time 84.41224455833435, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1639
goal_identified
goal_identified
=== ep: 1640, time 82.00046801567078, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1641, time 66.99986863136292, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1641
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1642, time 78.1997983455658, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1643, time 82.42027068138123, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
goal_identified
goal_identified
=== ep: 1644, time 75.93151187896729, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
goal_identified
goal_identified
goal_identified
=== ep: 1645, time 71.84867405891418, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
=== ep: 1646, time 83.38754487037659, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1647, time 76.39064621925354, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
=== ep: 1648, time 69.50160050392151, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1649, time 85.97007822990417, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
goal_identified
goal_identified
=== ep: 1650, time 80.17214012145996, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1651, time 67.34243297576904, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 76.03267621994019, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
goal_identified
goal_identified
goal_identified
=== ep: 1653, time 79.81405234336853, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1654, time 75.755380153656, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1655, time 76.63367533683777, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
goal_identified
goal_identified
=== ep: 1656, time 78.05763220787048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
goal_identified
=== ep: 1657, time 77.09258604049683, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
goal_identified
=== ep: 1658, time 74.67359566688538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1659, time 77.52431964874268, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1660, time 77.81301617622375, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1661, time 75.36602330207825, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
goal_identified
=== ep: 1662, time 76.20493030548096, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
goal_identified
=== ep: 1663, time 73.90978479385376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
goal_identified
goal_identified
=== ep: 1664, time 76.05969619750977, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
goal_identified
goal_identified
goal_identified
=== ep: 1665, time 81.40380954742432, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1666, time 75.81058931350708, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1667, time 70.63222050666809, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
goal_identified
goal_identified
=== ep: 1668, time 80.96293449401855, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
goal_identified
goal_identified
goal_identified
=== ep: 1669, time 72.97270560264587, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1670, time 70.6261637210846, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
goal_identified
goal_identified
=== ep: 1671, time 81.36380672454834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1672, time 78.47719836235046, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
goal_identified
goal_identified
goal_identified
=== ep: 1673, time 70.36972260475159, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
goal_identified
goal_identified
goal_identified
=== ep: 1674, time 77.91821360588074, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1675, time 82.01499676704407, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
goal_identified
=== ep: 1676, time 61.57235264778137, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
goal_identified
goal_identified
goal_identified
=== ep: 1677, time 82.39357352256775, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1678, time 82.92290163040161, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1679, time 63.076361656188965, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
goal_identified
goal_identified
goal_identified
=== ep: 1680, time 81.00633311271667, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
=== ep: 1681, time 80.40538835525513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
goal_identified
=== ep: 1682, time 66.72920083999634, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1683, time 80.76243472099304, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
goal_identified
goal_identified
=== ep: 1684, time 80.88146328926086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1685, time 66.69149446487427, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1686, time 74.71261548995972, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
goal_identified
goal_identified
goal_identified
=== ep: 1687, time 79.15587139129639, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1688, time 77.69879603385925, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
goal_identified
goal_identified
goal_identified
=== ep: 1689, time 76.42336201667786, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1690, time 76.91078782081604, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
goal_identified
goal_identified
goal_identified
=== ep: 1691, time 77.67844700813293, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1692, time 77.11578941345215, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
goal_identified
goal_identified
=== ep: 1693, time 75.7375009059906, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1694, time 73.27660751342773, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
goal_identified
goal_identified
goal_identified
=== ep: 1695, time 75.715176820755, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 80.75662326812744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
goal_identified
goal_identified
goal_identified
=== ep: 1697, time 75.1436607837677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
goal_identified
goal_identified
=== ep: 1698, time 69.31583023071289, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
goal_identified
goal_identified
goal_identified
=== ep: 1699, time 81.7761583328247, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
goal_identified
goal_identified
=== ep: 1700, time 70.41187763214111, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1701, time 76.5902271270752, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1702, time 82.89853119850159, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
goal_identified
goal_identified
goal_identified
=== ep: 1703, time 69.69917964935303, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
goal_identified
goal_identified
goal_identified
=== ep: 1704, time 69.62333726882935, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
goal_identified
goal_identified
=== ep: 1705, time 83.1017518043518, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
goal_identified
goal_identified
=== ep: 1706, time 78.77900218963623, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1707, time 68.2116072177887, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1708, time 79.42129015922546, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1709, time 78.79874753952026, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
goal_identified
goal_identified
goal_identified
=== ep: 1710, time 66.45486760139465, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
goal_identified
goal_identified
=== ep: 1711, time 82.42698740959167, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
goal_identified
goal_identified
goal_identified
=== ep: 1712, time 79.59266901016235, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
goal_identified
=== ep: 1713, time 68.36945629119873, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
goal_identified
goal_identified
=== ep: 1714, time 73.07849144935608, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
goal_identified
goal_identified
goal_identified
=== ep: 1715, time 75.66483235359192, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
goal_identified
goal_identified
goal_identified
=== ep: 1716, time 75.73950481414795, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1717, time 80.15631198883057, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
goal_identified
goal_identified
goal_identified
=== ep: 1718, time 77.17259788513184, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
goal_identified
goal_identified
goal_identified
=== ep: 1719, time 75.30276703834534, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1720, time 81.47860527038574, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1721, time 79.25920796394348, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
goal_identified
=== ep: 1722, time 74.31639742851257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
goal_identified
goal_identified
goal_identified
=== ep: 1723, time 75.03180813789368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1724, time 78.62012147903442, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
goal_identified
=== ep: 1725, time 54.36775302886963, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1726, time 75.07155632972717, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1727, time 49.228925943374634, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1728, time 75.95684504508972, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1729, time 62.81239676475525, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1730, time 68.345712184906, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
goal_identified
goal_identified
=== ep: 1731, time 78.16509628295898, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1732, time 69.2341103553772, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
goal_identified
goal_identified
goal_identified
=== ep: 1733, time 78.89134883880615, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
goal_identified
goal_identified
=== ep: 1734, time 77.71439266204834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1735, time 75.12037491798401, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
goal_identified
goal_identified
=== ep: 1736, time 74.04215335845947, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
goal_identified
=== ep: 1737, time 72.35556554794312, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
goal_identified
goal_identified
=== ep: 1738, time 76.77966523170471, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
goal_identified
goal_identified
=== ep: 1739, time 80.02978253364563, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
goal_identified
=== ep: 1740, time 77.71306991577148, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
goal_identified
goal_identified
goal_identified
=== ep: 1741, time 70.66250491142273, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1742, time 78.75877141952515, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
goal_identified
goal_identified
goal_identified
=== ep: 1743, time 80.81649661064148, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
goal_identified
goal_identified
=== ep: 1744, time 75.84437322616577, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
goal_identified
=== ep: 1745, time 68.27164959907532, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1746, time 78.82711172103882, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
goal_identified
goal_identified
goal_identified
=== ep: 1747, time 55.341105937957764, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
goal_identified
=== ep: 1748, time 82.79996180534363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
goal_identified
goal_identified
goal_identified
=== ep: 1749, time 73.74554085731506, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
goal_identified
goal_identified
goal_identified
=== ep: 1750, time 75.31921124458313, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1751, time 81.70507740974426, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
goal_identified
goal_identified
=== ep: 1752, time 79.28709959983826, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1753, time 68.5166757106781, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
=== ep: 1754, time 79.02134466171265, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
goal_identified
goal_identified
goal_identified
=== ep: 1755, time 81.97530937194824, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1756, time 77.72024273872375, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
goal_identified
goal_identified
=== ep: 1757, time 75.98622870445251, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1758, time 75.28908133506775, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1759, time 78.96207189559937, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
goal_identified
=== ep: 1760, time 80.55330109596252, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1761, time 78.23641848564148, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
goal_identified
goal_identified
=== ep: 1762, time 72.4799313545227, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1763, time 82.8461902141571, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
goal_identified
goal_identified
goal_identified
=== ep: 1764, time 83.01508522033691, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
goal_identified
goal_identified
=== ep: 1765, time 77.06291580200195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
goal_identified
goal_identified
=== ep: 1766, time 71.46910214424133, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1767, time 83.54744482040405, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
goal_identified
goal_identified
goal_identified
=== ep: 1768, time 68.75308012962341, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
goal_identified
goal_identified
=== ep: 1769, time 77.72757768630981, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 39/39)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1770, time 86.48122811317444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
goal_identified
goal_identified
=== ep: 1771, time 76.98475337028503, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1772, time 71.60988163948059, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1773, time 77.32470345497131, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1774, time 81.029123544693, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
goal_identified
goal_identified
goal_identified
=== ep: 1775, time 78.60758876800537, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
=== ep: 1776, time 76.94869804382324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
goal_identified
goal_identified
goal_identified
=== ep: 1777, time 75.36158990859985, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
goal_identified
=== ep: 1778, time 78.94501686096191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
=== ep: 1779, time 83.65962862968445, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1780, time 76.15713667869568, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1781, time 67.15948843955994, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
goal_identified
goal_identified
goal_identified
=== ep: 1782, time 83.9700174331665, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1783, time 64.78457403182983, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
goal_identified
goal_identified
=== ep: 1784, time 79.41685223579407, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
goal_identified
goal_identified
goal_identified
=== ep: 1785, time 86.26120448112488, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
goal_identified
goal_identified
goal_identified
=== ep: 1786, time 69.47434115409851, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
goal_identified
goal_identified
=== ep: 1787, time 70.33398270606995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
goal_identified
=== ep: 1788, time 85.60156154632568, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 74.53111171722412, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
goal_identified
goal_identified
goal_identified
=== ep: 1790, time 76.59503126144409, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1791, time 86.07737970352173, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1792, time 76.02293848991394, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1793, time 77.30805540084839, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
goal_identified
=== ep: 1794, time 79.266277551651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1795, time 79.69019603729248, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1796, time 75.88891196250916, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
goal_identified
goal_identified
goal_identified
=== ep: 1797, time 77.99879765510559, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
goal_identified
=== ep: 1798, time 77.32961320877075, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
goal_identified
goal_identified
goal_identified
=== ep: 1799, time 76.34613537788391, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
goal_identified
=== ep: 1800, time 78.11275124549866, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 60.183647871017456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
=== ep: 1802, time 70.68146991729736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1803, time 67.56016135215759, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1804, time 61.16334390640259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1805, time 79.84730005264282, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1806, time 72.73066639900208, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1807, time 79.38615369796753, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
goal_identified
goal_identified
goal_identified
=== ep: 1808, time 84.75703382492065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1809, time 82.35588383674622, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
goal_identified
goal_identified
goal_identified
=== ep: 1810, time 67.37228465080261, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
goal_identified
goal_identified
=== ep: 1811, time 78.35047960281372, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
goal_identified
goal_identified
=== ep: 1812, time 80.75034737586975, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
goal_identified
goal_identified
goal_identified
=== ep: 1813, time 77.96093559265137, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1814, time 72.85630393028259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 80.07770442962646, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1816, time 81.02463626861572, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
goal_identified
goal_identified
goal_identified
=== ep: 1817, time 76.06521821022034, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
goal_identified
goal_identified
=== ep: 1818, time 74.9670181274414, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1819, time 76.92268323898315, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
goal_identified
goal_identified
goal_identified
=== ep: 1820, time 79.49094009399414, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
goal_identified
goal_identified
goal_identified
=== ep: 1821, time 81.16149640083313, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
goal_identified
goal_identified
goal_identified
=== ep: 1822, time 72.79986596107483, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1823, time 78.36301565170288, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1824, time 83.7283787727356, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
goal_identified
goal_identified
=== ep: 1825, time 75.1376211643219, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1826, time 74.78664588928223, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1827, time 83.29920196533203, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
goal_identified
goal_identified
goal_identified
=== ep: 1828, time 66.8221664428711, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
goal_identified
goal_identified
=== ep: 1829, time 74.53308725357056, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
goal_identified
goal_identified
=== ep: 1830, time 85.20162630081177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
goal_identified
goal_identified
=== ep: 1831, time 76.73243737220764, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
goal_identified
goal_identified
=== ep: 1832, time 69.20140504837036, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
goal_identified
goal_identified
goal_identified
=== ep: 1833, time 83.49495577812195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1834, time 73.97707939147949, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1835, time 74.82679748535156, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
=== ep: 1836, time 81.88468408584595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1837, time 79.27427935600281, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1838, time 70.2671217918396, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1839, time 75.82796478271484, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1840, time 78.62858891487122, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
goal_identified
=== ep: 1841, time 75.20266032218933, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
goal_identified
=== ep: 1842, time 73.90353465080261, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1843, time 71.48255586624146, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
goal_identified
goal_identified
=== ep: 1844, time 77.25813245773315, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
goal_identified
goal_identified
=== ep: 1845, time 81.73450231552124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
goal_identified
goal_identified
=== ep: 1846, time 76.86200857162476, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
goal_identified
=== ep: 1847, time 68.70549416542053, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1848, time 82.18232893943787, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
goal_identified
goal_identified
=== ep: 1849, time 61.08462429046631, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1850, time 82.6166296005249, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
goal_identified
goal_identified
=== ep: 1851, time 85.78220176696777, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1852, time 71.86941313743591, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
=== ep: 1853, time 74.21274757385254, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1854, time 81.35158562660217, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
goal_identified
goal_identified
goal_identified
=== ep: 1855, time 71.86112451553345, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
goal_identified
goal_identified
=== ep: 1856, time 77.97202658653259, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1857, time 80.43827366828918, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1858, time 78.445072889328, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1859, time 77.38322257995605, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
goal_identified
=== ep: 1860, time 72.65990614891052, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
goal_identified
goal_identified
goal_identified
=== ep: 1861, time 78.3592963218689, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1861
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 79.59714150428772, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
goal_identified
goal_identified
goal_identified
=== ep: 1863, time 76.29444074630737, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
goal_identified
=== ep: 1864, time 72.52629709243774, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1865, time 79.80050563812256, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1866, time 84.24053692817688, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1867, time 70.90768909454346, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1868, time 70.66955018043518, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1869, time 78.53341126441956, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
goal_identified
goal_identified
goal_identified
=== ep: 1870, time 75.43098306655884, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
goal_identified
goal_identified
goal_identified
=== ep: 1871, time 80.28435492515564, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
goal_identified
goal_identified
goal_identified
=== ep: 1872, time 82.31009364128113, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1873, time 67.2422742843628, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
goal_identified
goal_identified
=== ep: 1874, time 78.27663493156433, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
goal_identified
goal_identified
goal_identified
=== ep: 1875, time 85.24659466743469, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
goal_identified
=== ep: 1876, time 78.63297486305237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
goal_identified
goal_identified
goal_identified
=== ep: 1877, time 74.5174310207367, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1878, time 72.56198024749756, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
goal_identified
goal_identified
goal_identified
=== ep: 1879, time 64.0972249507904, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1880, time 64.59190344810486, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
goal_identified
goal_identified
goal_identified
=== ep: 1881, time 73.91594672203064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
goal_identified
goal_identified
goal_identified
=== ep: 1882, time 73.08256196975708, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1883, time 68.18555116653442, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1884, time 66.37601685523987, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
goal_identified
goal_identified
=== ep: 1885, time 73.8533341884613, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1886, time 65.86646676063538, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1887, time 74.98603391647339, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1888, time 87.77307057380676, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
goal_identified
goal_identified
=== ep: 1889, time 82.85086035728455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 66.52088737487793, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
goal_identified
goal_identified
goal_identified
=== ep: 1891, time 73.94555521011353, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
goal_identified
goal_identified
=== ep: 1892, time 79.00998497009277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
goal_identified
goal_identified
goal_identified
=== ep: 1893, time 66.1611180305481, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
goal_identified
goal_identified
goal_identified
=== ep: 1894, time 84.09535789489746, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
goal_identified
goal_identified
=== ep: 1895, time 85.11846160888672, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
goal_identified
goal_identified
=== ep: 1896, time 74.88957405090332, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1897, time 76.14727449417114, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1898, time 87.18322277069092, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
goal_identified
goal_identified
=== ep: 1899, time 80.52031326293945, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1899
goal_identified
goal_identified
goal_identified
=== ep: 1900, time 76.22634196281433, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1900
goal_identified
goal_identified
=== ep: 1901, time 74.80245113372803, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1901
goal_identified
=== ep: 1902, time 75.31634449958801, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1902
goal_identified
goal_identified
goal_identified
=== ep: 1903, time 83.3523211479187, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1903
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1904, time 81.58003687858582, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1904
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1905, time 69.11489820480347, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1905
goal_identified
goal_identified
=== ep: 1906, time 81.0141396522522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1906
goal_identified
goal_identified
=== ep: 1907, time 83.73970794677734, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1907
goal_identified
goal_identified
goal_identified
=== ep: 1908, time 79.49195671081543, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1908
goal_identified
goal_identified
=== ep: 1909, time 72.29832077026367, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1909
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1910, time 82.95536732673645, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1910
goal_identified
goal_identified
goal_identified
=== ep: 1911, time 74.33597111701965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1911
goal_identified
=== ep: 1912, time 77.48501634597778, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1913, time 85.7117989063263, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1913
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1914, time 80.82024931907654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1914
goal_identified
goal_identified
=== ep: 1915, time 69.01556611061096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1915
goal_identified
goal_identified
goal_identified
=== ep: 1916, time 82.91206645965576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1916
goal_identified
goal_identified
goal_identified
=== ep: 1917, time 81.78619050979614, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1918, time 74.71683287620544, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1918
goal_identified
=== ep: 1919, time 74.02738308906555, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1919
goal_identified
goal_identified
goal_identified
=== ep: 1920, time 76.11388230323792, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1920
goal_identified
=== ep: 1921, time 82.2870683670044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1921
goal_identified
goal_identified
goal_identified
=== ep: 1922, time 81.73059439659119, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1922
goal_identified
=== ep: 1923, time 72.2690212726593, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1923
goal_identified
goal_identified
goal_identified
=== ep: 1924, time 77.66659545898438, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1924
goal_identified
goal_identified
=== ep: 1925, time 84.51328325271606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1925
goal_identified
goal_identified
=== ep: 1926, time 82.60674166679382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1926
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1927, time 74.53402280807495, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1927
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1928, time 83.89471793174744, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1928
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1929, time 85.47402238845825, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1929
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1930, time 67.13804125785828, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1930
goal_identified
goal_identified
goal_identified
=== ep: 1931, time 84.3736424446106, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1931
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1932, time 87.27618908882141, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1933, time 68.271892786026, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1933
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1934, time 81.25333571434021, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1934
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1935, time 85.49246764183044, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1935
goal_identified
=== ep: 1936, time 75.30646443367004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1936
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1937, time 79.33312582969666, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1937
goal_identified
goal_identified
goal_identified
=== ep: 1938, time 83.68128037452698, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1938
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1939, time 81.95453143119812, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
goal_identified
goal_identified
=== ep: 1940, time 78.94344782829285, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1940
goal_identified
goal_identified
=== ep: 1941, time 81.04773283004761, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1941
goal_identified
goal_identified
=== ep: 1942, time 73.70432019233704, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1942
goal_identified
goal_identified
goal_identified
=== ep: 1943, time 82.24629807472229, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1943
goal_identified
goal_identified
=== ep: 1944, time 85.8282618522644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1944
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1945, time 71.45619773864746, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1945
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1946, time 78.71449446678162, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1946
goal_identified
goal_identified
goal_identified
=== ep: 1947, time 86.38929891586304, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1948, time 74.02543687820435, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1948
goal_identified
=== ep: 1949, time 71.35470843315125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1949
goal_identified
goal_identified
=== ep: 1950, time 83.76992797851562, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1950
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1951, time 82.7218279838562, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1951
goal_identified
goal_identified
goal_identified
=== ep: 1952, time 74.97891116142273, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1952
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1953, time 76.32455730438232, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1953
goal_identified
goal_identified
goal_identified
=== ep: 1954, time 77.82614850997925, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1954
goal_identified
goal_identified
goal_identified
=== ep: 1955, time 81.75663924217224, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1955
goal_identified
goal_identified
=== ep: 1956, time 77.35924768447876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1956
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1957, time 76.52079772949219, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1957
goal_identified
goal_identified
goal_identified
=== ep: 1958, time 71.04439902305603, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1958
goal_identified
goal_identified
goal_identified
=== ep: 1959, time 84.25642490386963, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1959
goal_identified
goal_identified
goal_identified
=== ep: 1960, time 83.8299810886383, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1960
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1961, time 72.5720386505127, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1961
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1962, time 74.99491167068481, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1962
goal_identified
goal_identified
=== ep: 1963, time 81.97135019302368, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1963
=== ep: 1964, time 57.58015203475952, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1964
goal_identified
goal_identified
goal_identified
=== ep: 1965, time 75.36510062217712, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1965
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1966, time 73.31740617752075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1966
goal_identified
goal_identified
=== ep: 1967, time 74.5175895690918, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1967
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1968, time 85.80448126792908, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1969, time 74.76092267036438, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1969
goal_identified
goal_identified
goal_identified
=== ep: 1970, time 76.60375738143921, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1970
goal_identified
goal_identified
=== ep: 1971, time 84.53982734680176, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1971
goal_identified
goal_identified
goal_identified
=== ep: 1972, time 76.17960596084595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1972
goal_identified
goal_identified
goal_identified
=== ep: 1973, time 81.11969828605652, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1974, time 84.78297829627991, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1974
goal_identified
goal_identified
=== ep: 1975, time 79.69113397598267, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1976, time 67.32761311531067, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1976
goal_identified
=== ep: 1977, time 81.50757431983948, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1977
goal_identified
goal_identified
goal_identified
=== ep: 1978, time 81.39257287979126, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1978
goal_identified
goal_identified
=== ep: 1979, time 74.06338000297546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1979
goal_identified
=== ep: 1980, time 77.84031438827515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1980
goal_identified
goal_identified
=== ep: 1981, time 63.19921588897705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1981
goal_identified
goal_identified
goal_identified
=== ep: 1982, time 66.07805895805359, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1982
goal_identified
goal_identified
goal_identified
=== ep: 1983, time 78.54591417312622, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1983
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1984, time 67.61280012130737, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1984
goal_identified
goal_identified
goal_identified
=== ep: 1985, time 81.90208387374878, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1985
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1986, time 85.09419512748718, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1986
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1987, time 82.41213774681091, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1987
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1988, time 73.06249117851257, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1988
goal_identified
goal_identified
goal_identified
=== ep: 1989, time 80.92080426216125, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1989
goal_identified
goal_identified
=== ep: 1990, time 81.26082229614258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1990
goal_identified
goal_identified
goal_identified
=== ep: 1991, time 78.55479717254639, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1991
goal_identified
goal_identified
goal_identified
=== ep: 1992, time 79.37676882743835, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1992
goal_identified
goal_identified
=== ep: 1993, time 64.8649549484253, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1993
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1994, time 76.63850688934326, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1995, time 90.59848690032959, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1995
goal_identified
goal_identified
=== ep: 1996, time 83.45032572746277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1996
goal_identified
goal_identified
goal_identified
=== ep: 1997, time 65.5904450416565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1997
goal_identified
goal_identified
goal_identified
=== ep: 1998, time 85.45649766921997, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1999, time 78.02204012870789, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1999
goal_identified
goal_identified
goal_identified
=== ep: 2000, time 75.19853663444519, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2000
goal_identified
goal_identified
goal_identified
=== ep: 2001, time 84.33446621894836, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2001
goal_identified
goal_identified
goal_identified
=== ep: 2002, time 77.9447557926178, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2002
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2003, time 77.54915761947632, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2003
goal_identified
goal_identified
goal_identified
=== ep: 2004, time 83.50034284591675, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2004
goal_identified
=== ep: 2005, time 72.4928708076477, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2005
goal_identified
=== ep: 2006, time 79.27588748931885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2007, time 83.79615569114685, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2007
goal_identified
goal_identified
=== ep: 2008, time 76.9851336479187, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2008
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2009, time 58.693437814712524, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2009
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2010, time 77.31860399246216, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2010
goal_identified
goal_identified
goal_identified
=== ep: 2011, time 74.92605495452881, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2012, time 84.54477500915527, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
goal_identified
goal_identified
goal_identified
=== ep: 2013, time 83.63729166984558, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2013
goal_identified
=== ep: 2014, time 71.69507312774658, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2014
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2015, time 81.7445809841156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2015
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2016, time 88.02107620239258, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2016
goal_identified
=== ep: 2017, time 78.62952828407288, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2017
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2018, time 78.38875246047974, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2018
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2019, time 83.55688977241516, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2019
goal_identified
goal_identified
goal_identified
=== ep: 2020, time 83.91046786308289, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2020
goal_identified
goal_identified
goal_identified
=== ep: 2021, time 77.37738037109375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2021
goal_identified
=== ep: 2022, time 78.9193856716156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2022
goal_identified
goal_identified
goal_identified
=== ep: 2023, time 81.93739151954651, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2023
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2024, time 80.65336394309998, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2024
goal_identified
goal_identified
goal_identified
=== ep: 2025, time 84.33167171478271, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2025
goal_identified
goal_identified
goal_identified
=== ep: 2026, time 83.3759982585907, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2026
goal_identified
goal_identified
goal_identified
=== ep: 2027, time 74.16661143302917, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2027
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2028, time 74.40468311309814, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2028
goal_identified
goal_identified
goal_identified
=== ep: 2029, time 86.40451145172119, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2029
goal_identified
goal_identified
=== ep: 2030, time 78.1383056640625, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2030
goal_identified
goal_identified
=== ep: 2031, time 76.40145874023438, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2031
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2032, time 79.52044916152954, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2032
goal_identified
goal_identified
=== ep: 2033, time 73.95777297019958, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2033
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2034, time 84.44244194030762, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2035, time 83.31185698509216, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2035
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2036, time 75.22850370407104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2036
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2037, time 81.561527967453, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2037
goal_identified
goal_identified
=== ep: 2038, time 87.236004114151, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2038
goal_identified
goal_identified
=== ep: 2039, time 77.7703800201416, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2039
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2040, time 78.08155155181885, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2040
goal_identified
goal_identified
goal_identified
=== ep: 2041, time 82.86191153526306, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2041
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2042, time 85.26400017738342, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1939
goal_identified
goal_identified
goal_identified
=== ep: 2043, time 72.71770024299622, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2043
goal_identified
=== ep: 2044, time 76.79451990127563, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2044
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2045, time 88.18298983573914, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2045
goal_identified
goal_identified
=== ep: 2046, time 80.17386651039124, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2046
goal_identified
goal_identified
=== ep: 2047, time 64.83060550689697, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2047
goal_identified
goal_identified
goal_identified
=== ep: 2048, time 83.90391397476196, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2048
goal_identified
goal_identified
goal_identified
=== ep: 2049, time 82.37194752693176, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2049
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2050, time 82.43794965744019, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2050
goal_identified
=== ep: 2051, time 78.46381282806396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2051
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2052, time 71.73301291465759, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2052
goal_identified
goal_identified
goal_identified
=== ep: 2053, time 75.56250166893005, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2053
goal_identified
goal_identified
goal_identified
=== ep: 2054, time 86.46966242790222, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2054
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2055, time 85.423992395401, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2055
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2056, time 72.80085277557373, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2056
goal_identified
goal_identified
goal_identified
=== ep: 2057, time 79.60263776779175, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2057
goal_identified
goal_identified
goal_identified
=== ep: 2058, time 88.72744178771973, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2058
goal_identified
goal_identified
goal_identified
=== ep: 2059, time 77.70442080497742, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2059
goal_identified
goal_identified
=== ep: 2060, time 75.62815475463867, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2060
goal_identified
goal_identified
goal_identified
=== ep: 2061, time 84.22282648086548, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2061
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2062, time 84.09569048881531, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2062
goal_identified
goal_identified
goal_identified
=== ep: 2063, time 77.18147826194763, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2063
goal_identified
=== ep: 2064, time 81.56479406356812, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2064
=== ep: 2065, time 81.7519199848175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2065
goal_identified
goal_identified
goal_identified
=== ep: 2066, time 78.13305735588074, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2066
goal_identified
goal_identified
=== ep: 2067, time 81.342041015625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2067
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2068, time 78.93624138832092, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2068
=== ep: 2069, time 70.30039668083191, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2070, time 87.70456075668335, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2070
goal_identified
goal_identified
goal_identified
=== ep: 2071, time 88.128662109375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2071
goal_identified
goal_identified
goal_identified
=== ep: 2072, time 72.35137009620667, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2072
goal_identified
goal_identified
=== ep: 2073, time 73.56349229812622, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2073
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2074, time 84.8687858581543, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2074
goal_identified
goal_identified
=== ep: 2075, time 81.50798034667969, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2075
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2076, time 77.85292267799377, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2076
goal_identified
goal_identified
=== ep: 2077, time 75.05915951728821, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2077
goal_identified
goal_identified
goal_identified
=== ep: 2078, time 78.11593770980835, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2078
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2079, time 76.55990934371948, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2079
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2080, time 82.22859215736389, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2080
goal_identified
goal_identified
goal_identified
=== ep: 2081, time 78.40935754776001, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2082, time 77.42907094955444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2082
goal_identified
goal_identified
=== ep: 2083, time 85.82064962387085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2083
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2084, time 74.2528121471405, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2084
goal_identified
goal_identified
goal_identified
=== ep: 2085, time 77.83672261238098, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2085
goal_identified
goal_identified
=== ep: 2086, time 84.09440422058105, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2086
goal_identified
goal_identified
=== ep: 2087, time 80.91864371299744, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2087
goal_identified
goal_identified
=== ep: 2088, time 72.46667981147766, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2088
goal_identified
goal_identified
=== ep: 2089, time 79.80080223083496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2089
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2090, time 80.8139214515686, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2090
goal_identified
=== ep: 2091, time 79.90910148620605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2091
goal_identified
goal_identified
goal_identified
=== ep: 2092, time 80.35585045814514, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2092
goal_identified
goal_identified
goal_identified
=== ep: 2093, time 69.7724256515503, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2093
goal_identified
goal_identified
goal_identified
=== ep: 2094, time 80.87773156166077, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2094
goal_identified
goal_identified
=== ep: 2095, time 84.8150429725647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2095
goal_identified
goal_identified
=== ep: 2096, time 81.51252031326294, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2096
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2097, time 60.619455337524414, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2097
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2098, time 81.48272037506104, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2098
goal_identified
goal_identified
=== ep: 2099, time 77.2872703075409, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2099
goal_identified
goal_identified
goal_identified
=== ep: 2100, time 76.80060291290283, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2100
goal_identified
goal_identified
goal_identified
=== ep: 2101, time 83.4120101928711, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2101
goal_identified
goal_identified
goal_identified
=== ep: 2102, time 77.58632206916809, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2102
goal_identified
=== ep: 2103, time 76.56533670425415, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2103
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2104, time 84.6673355102539, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2105, time 76.78731894493103, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2105
goal_identified
goal_identified
goal_identified
=== ep: 2106, time 78.2389817237854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2106
goal_identified
=== ep: 2107, time 85.77546811103821, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2107
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2108, time 82.36927652359009, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2108
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2109, time 65.86142206192017, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1968
goal_identified
goal_identified
goal_identified
=== ep: 2110, time 87.30951166152954, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2110
goal_identified
goal_identified
goal_identified
=== ep: 2111, time 83.81046843528748, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2111
goal_identified
=== ep: 2112, time 71.11649537086487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2112
goal_identified
=== ep: 2113, time 77.69293713569641, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2113
goal_identified
goal_identified
goal_identified
=== ep: 2114, time 75.10055470466614, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2114
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2115, time 86.96391034126282, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2115
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2116, time 82.39150738716125, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2116
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2117, time 76.63668656349182, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2117
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2118, time 72.48896193504333, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2118
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2119, time 88.10884737968445, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2119
goal_identified
=== ep: 2120, time 80.8476333618164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2120
goal_identified
goal_identified
=== ep: 2121, time 77.13098549842834, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2121
goal_identified
goal_identified
=== ep: 2122, time 76.26772546768188, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2122
goal_identified
goal_identified
=== ep: 2123, time 78.7314121723175, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2123
goal_identified
goal_identified
=== ep: 2124, time 74.42062997817993, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2124
goal_identified
goal_identified
goal_identified
=== ep: 2125, time 79.32923436164856, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2126, time 82.14687371253967, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 74.69765758514404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2127
goal_identified
goal_identified
=== ep: 2128, time 85.40746879577637, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2128
goal_identified
goal_identified
=== ep: 2129, time 88.00428676605225, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2129
goal_identified
=== ep: 2130, time 73.94283485412598, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2130
goal_identified
goal_identified
goal_identified
=== ep: 2131, time 83.27127623558044, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2131
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2132, time 83.35269546508789, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2132
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2133, time 76.76008200645447, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2133
goal_identified
goal_identified
=== ep: 2134, time 80.14730763435364, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2134
goal_identified
goal_identified
=== ep: 2135, time 83.47223114967346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2135
goal_identified
goal_identified
=== ep: 2136, time 75.05857276916504, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2136
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2137, time 77.33838891983032, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2137
goal_identified
goal_identified
goal_identified
=== ep: 2138, time 81.72445726394653, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2139, time 70.09931373596191, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2139
goal_identified
goal_identified
=== ep: 2140, time 85.55761647224426, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2140
goal_identified
goal_identified
goal_identified
=== ep: 2141, time 83.57826781272888, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2141
goal_identified
=== ep: 2142, time 77.23567461967468, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2142
goal_identified
goal_identified
=== ep: 2143, time 71.8961329460144, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2143
goal_identified
goal_identified
=== ep: 2144, time 78.2843725681305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2144
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2145, time 82.59549498558044, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2145
goal_identified
goal_identified
goal_identified
=== ep: 2146, time 83.91591238975525, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2146
goal_identified
goal_identified
goal_identified
=== ep: 2147, time 84.22172927856445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2147
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2148, time 75.43846154212952, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2148
=== ep: 2149, time 76.87432217597961, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2149
goal_identified
goal_identified
=== ep: 2150, time 87.20637130737305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2150
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2151, time 85.06318044662476, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2151
goal_identified
=== ep: 2152, time 66.57615685462952, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2152
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2153, time 76.78207874298096, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2012
goal_identified
goal_identified
goal_identified
=== ep: 2154, time 77.81776094436646, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2154
goal_identified
goal_identified
=== ep: 2155, time 76.57942080497742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2155
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2156, time 82.281170129776, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2156
goal_identified
goal_identified
=== ep: 2157, time 79.01157283782959, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2157
goal_identified
=== ep: 2158, time 74.04982495307922, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2158
goal_identified
goal_identified
goal_identified
=== ep: 2159, time 79.88120412826538, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2159
goal_identified
=== ep: 2160, time 81.58511424064636, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2160
goal_identified
goal_identified
=== ep: 2161, time 72.41903257369995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2161
=== ep: 2162, time 82.79920291900635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2162
goal_identified
goal_identified
=== ep: 2163, time 85.97079038619995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2163
goal_identified
goal_identified
goal_identified
=== ep: 2164, time 75.6757173538208, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2164
goal_identified
goal_identified
=== ep: 2165, time 68.57223224639893, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2166, time 82.05526280403137, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2166
goal_identified
goal_identified
=== ep: 2167, time 76.23770809173584, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2167
goal_identified
=== ep: 2168, time 78.32217955589294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2168
goal_identified
=== ep: 2169, time 85.94418048858643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2169
goal_identified
goal_identified
=== ep: 2170, time 73.22672820091248, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2170
goal_identified
goal_identified
goal_identified
=== ep: 2171, time 77.22400784492493, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2171
goal_identified
goal_identified
=== ep: 2172, time 81.43199801445007, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2173, time 64.8054518699646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2173
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2174, time 84.6893138885498, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2174
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2175, time 87.94445705413818, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2034
=== ep: 2176, time 72.14299321174622, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2176
goal_identified
goal_identified
goal_identified
=== ep: 2177, time 74.34794783592224, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2177
goal_identified
goal_identified
=== ep: 2178, time 86.54359793663025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2178
goal_identified
goal_identified
goal_identified
=== ep: 2179, time 80.98366665840149, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2180, time 71.9053647518158, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2180
goal_identified
goal_identified
goal_identified
=== ep: 2181, time 80.69908475875854, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2181
goal_identified
goal_identified
goal_identified
=== ep: 2182, time 81.65252232551575, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2182
goal_identified
goal_identified
=== ep: 2183, time 80.67095804214478, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2183
goal_identified
goal_identified
=== ep: 2184, time 85.64487552642822, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2184
goal_identified
=== ep: 2185, time 75.58616614341736, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2185
=== ep: 2186, time 78.43852949142456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2186
goal_identified
goal_identified
goal_identified
=== ep: 2187, time 86.99362826347351, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2187
goal_identified
goal_identified
goal_identified
=== ep: 2188, time 81.63274908065796, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2188
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2189, time 69.0838623046875, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2190, time 84.06442141532898, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2190
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2191, time 76.14593458175659, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2042
goal_identified
goal_identified
=== ep: 2192, time 78.48420476913452, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2192
goal_identified
=== ep: 2193, time 78.58960509300232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2193
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2194, time 75.3507342338562, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2194
goal_identified
goal_identified
goal_identified
=== ep: 2195, time 79.38621401786804, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2195
goal_identified
goal_identified
goal_identified
=== ep: 2196, time 75.73255896568298, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2196
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2197, time 73.25561022758484, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2109
goal_identified
goal_identified
goal_identified
=== ep: 2198, time 80.08412075042725, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2198
goal_identified
goal_identified
=== ep: 2199, time 82.43542528152466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2199
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2200, time 69.26867127418518, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2201, time 77.58602046966553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2201
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2202, time 81.83008098602295, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2175
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2203, time 72.0598955154419, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2204, time 75.97052240371704, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2197
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2205, time 80.64065408706665, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2203
goal_identified
goal_identified
goal_identified
=== ep: 2206, time 76.56788277626038, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2206
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2207, time 77.91521906852722, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2204
goal_identified
goal_identified
goal_identified
=== ep: 2208, time 72.35644006729126, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2208
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2209, time 75.71474552154541, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2205
goal_identified
goal_identified
goal_identified
=== ep: 2210, time 83.87517213821411, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2210
goal_identified
=== ep: 2211, time 78.81701850891113, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2211
goal_identified
goal_identified
=== ep: 2212, time 66.6452043056488, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2212
goal_identified
=== ep: 2213, time 81.14669632911682, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2213
goal_identified
goal_identified
=== ep: 2214, time 76.49148082733154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2214
goal_identified
goal_identified
goal_identified
=== ep: 2215, time 73.37051010131836, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2215
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2216, time 80.52989888191223, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2216
goal_identified
goal_identified
goal_identified
=== ep: 2217, time 80.64745545387268, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2217
goal_identified
goal_identified
goal_identified
=== ep: 2218, time 75.72566747665405, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2218
goal_identified
goal_identified
=== ep: 2219, time 73.84275603294373, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2220, time 66.91371393203735, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2220
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2221, time 73.28562450408936, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2221
goal_identified
goal_identified
goal_identified
=== ep: 2222, time 69.96703171730042, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2222
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2223, time 55.646589040756226, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2223
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2224, time 71.3865020275116, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2224
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2225, time 46.62314987182617, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2226, time 75.21110272407532, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2226
goal_identified
goal_identified
goal_identified
=== ep: 2227, time 55.23912215232849, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2227
goal_identified
goal_identified
goal_identified
=== ep: 2228, time 74.89491486549377, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2228
goal_identified
goal_identified
=== ep: 2229, time 65.28569865226746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2230, time 65.91959547996521, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2230
goal_identified
goal_identified
=== ep: 2231, time 76.05464768409729, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2231
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2232, time 74.84181356430054, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2232
goal_identified
goal_identified
goal_identified
=== ep: 2233, time 77.92568564414978, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2233
goal_identified
=== ep: 2234, time 79.49438881874084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2234
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2235, time 74.74600028991699, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2235
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2236, time 79.60771489143372, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2236
goal_identified
goal_identified
goal_identified
=== ep: 2237, time 71.94486808776855, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2237
goal_identified
goal_identified
goal_identified
=== ep: 2238, time 66.88204622268677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2238
goal_identified
=== ep: 2239, time 80.36855745315552, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2239
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2240, time 74.32965660095215, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2240
goal_identified
=== ep: 2241, time 75.40638542175293, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2241
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2242, time 85.37175488471985, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2242
goal_identified
=== ep: 2243, time 74.0440764427185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2244, time 68.49217939376831, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 80.05675172805786, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2246, time 61.48899984359741, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2246
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2247, time 77.42925786972046, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2247
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2248, time 79.89310216903687, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2248
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2249, time 75.06081986427307, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2249
goal_identified
goal_identified
goal_identified
=== ep: 2250, time 75.30491280555725, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2250
goal_identified
goal_identified
goal_identified
=== ep: 2251, time 77.1403067111969, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2251
goal_identified
goal_identified
=== ep: 2252, time 76.19085597991943, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2252
goal_identified
goal_identified
goal_identified
=== ep: 2253, time 79.2437002658844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2253
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2254, time 75.05393767356873, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2255, time 80.16087031364441, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2255
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2256, time 79.4334454536438, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2209
goal_identified
goal_identified
=== ep: 2257, time 61.53271842002869, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2258, time 82.37040281295776, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2258
goal_identified
goal_identified
goal_identified
=== ep: 2259, time 78.59161710739136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2259
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2260, time 70.95330333709717, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2260
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2261, time 82.5023741722107, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2262, time 67.1306357383728, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2262
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2263, time 70.91902947425842, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2263
goal_identified
goal_identified
goal_identified
=== ep: 2264, time 71.57631421089172, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2264
goal_identified
goal_identified
=== ep: 2265, time 66.97701382637024, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2265
goal_identified
goal_identified
goal_identified
=== ep: 2266, time 75.71385765075684, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2267, time 78.68360233306885, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2267
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2268, time 81.93061923980713, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2268
goal_identified
goal_identified
goal_identified
=== ep: 2269, time 79.36139631271362, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2269
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2270, time 67.51267385482788, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2270
goal_identified
goal_identified
=== ep: 2271, time 75.69044995307922, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2271
goal_identified
goal_identified
goal_identified
=== ep: 2272, time 81.74951767921448, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2272
goal_identified
=== ep: 2273, time 66.02491593360901, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2274, time 83.61354160308838, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2274
goal_identified
goal_identified
goal_identified
=== ep: 2275, time 81.60378813743591, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2275
goal_identified
goal_identified
goal_identified
=== ep: 2276, time 65.6420030593872, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2276
goal_identified
=== ep: 2277, time 80.51493644714355, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2277
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2278, time 73.49520015716553, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2278
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2279, time 76.46008563041687, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2279
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2280, time 79.80065941810608, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2280
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2281, time 76.40327095985413, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2281
goal_identified
goal_identified
goal_identified
=== ep: 2282, time 73.84853410720825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2282
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2283, time 70.09218287467957, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2283
goal_identified
goal_identified
=== ep: 2284, time 71.58177161216736, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2284
goal_identified
goal_identified
goal_identified
=== ep: 2285, time 82.0862717628479, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2285
goal_identified
goal_identified
=== ep: 2286, time 79.10422205924988, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2286
goal_identified
goal_identified
goal_identified
=== ep: 2287, time 72.405766248703, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2287
goal_identified
goal_identified
=== ep: 2288, time 81.83980345726013, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2288
goal_identified
goal_identified
=== ep: 2289, time 77.35260462760925, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2289
goal_identified
goal_identified
=== ep: 2290, time 68.19708800315857, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2290
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2291, time 86.58019542694092, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2291
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2292, time 82.5076675415039, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2292
goal_identified
goal_identified
=== ep: 2293, time 69.07212448120117, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2293
goal_identified
goal_identified
goal_identified
=== ep: 2294, time 72.92100405693054, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2294
goal_identified
goal_identified
=== ep: 2295, time 75.74958038330078, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2295
goal_identified
goal_identified
=== ep: 2296, time 64.27424263954163, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2296
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2297, time 80.19058918952942, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2297
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2298, time 80.75427865982056, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2298
goal_identified
goal_identified
=== ep: 2299, time 77.63033723831177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 77.21471643447876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2300
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2301, time 72.21748304367065, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2301
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2302, time 70.43415856361389, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2303, time 69.23458409309387, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2303
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2304, time 63.29462671279907, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2305, time 74.06788039207458, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2305
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2306, time 61.95802092552185, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2306
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2307, time 73.0314450263977, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2307
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2308, time 76.57942271232605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2308
goal_identified
goal_identified
goal_identified
=== ep: 2309, time 71.04031610488892, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2309
goal_identified
goal_identified
=== ep: 2310, time 80.31489443778992, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2310
goal_identified
goal_identified
goal_identified
=== ep: 2311, time 88.42686057090759, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2311
goal_identified
goal_identified
=== ep: 2312, time 72.50928664207458, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2312
goal_identified
goal_identified
=== ep: 2313, time 66.82500004768372, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2313
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2314, time 82.6527955532074, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2314
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2315, time 70.92512512207031, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2315
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2316, time 76.88540840148926, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
goal_identified
goal_identified
goal_identified
=== ep: 2317, time 85.86812543869019, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2317
goal_identified
goal_identified
=== ep: 2318, time 80.63711094856262, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2318
goal_identified
goal_identified
=== ep: 2319, time 70.8444550037384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2319
goal_identified
goal_identified
=== ep: 2320, time 80.32926201820374, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2320
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2321, time 80.98543047904968, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2321
goal_identified
goal_identified
=== ep: 2322, time 77.33096241950989, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2322
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2323, time 78.04438781738281, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2323
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2324, time 75.26067328453064, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2324
goal_identified
=== ep: 2325, time 79.7671332359314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2325
goal_identified
goal_identified
goal_identified
=== ep: 2326, time 81.9403007030487, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2326
goal_identified
goal_identified
=== ep: 2327, time 76.53074288368225, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2327
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2328, time 76.63023948669434, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2328
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2329, time 83.13609218597412, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2329
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2330, time 81.73715949058533, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2330
goal_identified
=== ep: 2331, time 69.6686999797821, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2331
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2332, time 84.36159682273865, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2332
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2333, time 80.27247786521912, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2333
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2334, time 76.0853967666626, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2334
goal_identified
goal_identified
=== ep: 2335, time 76.25193428993225, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2336, time 87.62438869476318, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2336
goal_identified
goal_identified
=== ep: 2337, time 74.00081729888916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2338, time 75.64075207710266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2338
goal_identified
goal_identified
goal_identified
=== ep: 2339, time 85.08214497566223, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2339
goal_identified
goal_identified
=== ep: 2340, time 80.34856486320496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2340
goal_identified
goal_identified
goal_identified
=== ep: 2341, time 74.7109386920929, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2341
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2342, time 75.94274735450745, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2342
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2343, time 73.61139512062073, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2343
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2344, time 81.3699426651001, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2344
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2345, time 81.74180579185486, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2345
goal_identified
goal_identified
=== ep: 2346, time 75.0931715965271, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2346
goal_identified
goal_identified
goal_identified
=== ep: 2347, time 78.56029176712036, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2347
goal_identified
goal_identified
goal_identified
=== ep: 2348, time 83.59042310714722, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2348
goal_identified
goal_identified
goal_identified
=== ep: 2349, time 73.0800633430481, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2349
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2350, time 76.62715721130371, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2350
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2351, time 87.00918197631836, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2351
goal_identified
goal_identified
goal_identified
=== ep: 2352, time 78.08255815505981, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2352
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2353, time 69.20921325683594, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2353
goal_identified
goal_identified
=== ep: 2354, time 79.71492266654968, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2354
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2355, time 67.83609104156494, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2355
goal_identified
goal_identified
goal_identified
=== ep: 2356, time 75.78144121170044, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2356
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2357, time 81.14641427993774, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2357
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2358, time 84.14439034461975, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2358
goal_identified
goal_identified
=== ep: 2359, time 77.79353928565979, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2359
goal_identified
goal_identified
goal_identified
=== ep: 2360, time 77.809499502182, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2360
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2361, time 76.25648188591003, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2361
goal_identified
goal_identified
goal_identified
=== ep: 2362, time 79.80423903465271, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2362
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2363, time 77.32352709770203, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2363
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2364, time 70.62692880630493, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2365, time 83.737708568573, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2365
goal_identified
goal_identified
goal_identified
=== ep: 2366, time 85.79440426826477, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2366
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2367, time 68.86539697647095, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2367
goal_identified
goal_identified
goal_identified
=== ep: 2368, time 84.53958177566528, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2368
goal_identified
goal_identified
=== ep: 2369, time 84.82384824752808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2369
goal_identified
goal_identified
goal_identified
=== ep: 2370, time 73.40250706672668, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2370
goal_identified
goal_identified
=== ep: 2371, time 75.2380199432373, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2372, time 83.01191401481628, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2372
goal_identified
goal_identified
goal_identified
=== ep: 2373, time 76.91970181465149, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2373
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2374, time 77.80639600753784, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2374
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2375, time 76.49629998207092, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2375
goal_identified
goal_identified
=== ep: 2376, time 75.56060862541199, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2376
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2377, time 81.11766648292542, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2377
goal_identified
goal_identified
goal_identified
=== ep: 2378, time 81.85161447525024, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2378
goal_identified
goal_identified
=== ep: 2379, time 74.78774905204773, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2379
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2380, time 79.44940495491028, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2380
goal_identified
goal_identified
goal_identified
=== ep: 2381, time 82.59635353088379, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2381
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2382, time 69.14320015907288, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2382
goal_identified
goal_identified
goal_identified
=== ep: 2383, time 75.06178092956543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2383
goal_identified
goal_identified
goal_identified
=== ep: 2384, time 87.80626559257507, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2384
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2385, time 80.57373023033142, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2385
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2386, time 70.35193395614624, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2386
goal_identified
goal_identified
goal_identified
=== ep: 2387, time 85.2117965221405, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2387
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2388, time 77.37106966972351, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2388
goal_identified
goal_identified
goal_identified
=== ep: 2389, time 72.31502413749695, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2389
goal_identified
goal_identified
=== ep: 2390, time 80.52257084846497, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2390
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2391, time 80.67975330352783, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2391
goal_identified
goal_identified
goal_identified
=== ep: 2392, time 77.48770713806152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2392
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2393, time 78.578449010849, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2393
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2394, time 70.87687873840332, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2394
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2395, time 74.62523341178894, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2395
goal_identified
goal_identified
=== ep: 2396, time 81.3336570262909, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2396
goal_identified
goal_identified
goal_identified
=== ep: 2397, time 72.16330146789551, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2397
goal_identified
goal_identified
goal_identified
=== ep: 2398, time 80.19329357147217, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2398
goal_identified
goal_identified
goal_identified
=== ep: 2399, time 85.11278414726257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2399
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2400, time 79.42711162567139, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2400
goal_identified
goal_identified
=== ep: 2401, time 67.65457510948181, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2401
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2402, time 83.29826092720032, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2402
goal_identified
goal_identified
goal_identified
=== ep: 2403, time 79.50556206703186, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2403
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2404, time 79.91823625564575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2404
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2405, time 75.57211089134216, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2405
goal_identified
goal_identified
goal_identified
=== ep: 2406, time 83.00815415382385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2406
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2407, time 75.5798773765564, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 74.44809150695801, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2408
goal_identified
goal_identified
=== ep: 2409, time 75.11769366264343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2409
goal_identified
goal_identified
=== ep: 2410, time 79.49287915229797, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2410
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2411, time 84.71346163749695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 82.63499212265015, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2412
goal_identified
=== ep: 2413, time 70.5095534324646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2414, time 78.91028308868408, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2414
goal_identified
goal_identified
goal_identified
=== ep: 2415, time 79.66532230377197, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2415
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2416, time 60.284231424331665, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 83.30352354049683, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2417
goal_identified
=== ep: 2418, time 86.15062022209167, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2418
goal_identified
goal_identified
=== ep: 2419, time 69.86249780654907, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2419
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2420, time 72.11520338058472, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2420
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2421, time 77.12015128135681, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2421
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2422, time 56.976035833358765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2423, time 81.43392872810364, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2423
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2424, time 72.12880873680115, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2424
=== ep: 2425, time 73.98751544952393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2425
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2426, time 84.02126359939575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2426
goal_identified
goal_identified
=== ep: 2427, time 81.0036358833313, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2427
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2428, time 81.08121943473816, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2428
goal_identified
goal_identified
=== ep: 2429, time 78.62693905830383, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2429
goal_identified
goal_identified
=== ep: 2430, time 67.98497343063354, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2431, time 79.87004089355469, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2431
goal_identified
=== ep: 2432, time 82.00628900527954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2432
goal_identified
goal_identified
goal_identified
=== ep: 2433, time 77.64568161964417, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2434, time 75.83403515815735, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2434
goal_identified
goal_identified
=== ep: 2435, time 81.99727606773376, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2436, time 76.81731057167053, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2436
goal_identified
goal_identified
goal_identified
=== ep: 2437, time 65.87122416496277, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2437
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2438, time 86.91451001167297, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2438
goal_identified
goal_identified
goal_identified
=== ep: 2439, time 85.71775913238525, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2439
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2440, time 75.65431499481201, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2441, time 77.49541735649109, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
goal_identified
=== ep: 2442, time 79.86589407920837, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2442
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2443, time 71.48426842689514, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
goal_identified
=== ep: 2444, time 71.05137228965759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2444
goal_identified
goal_identified
=== ep: 2445, time 81.56977772712708, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2445
goal_identified
goal_identified
goal_identified
=== ep: 2446, time 81.04747343063354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2446
goal_identified
goal_identified
=== ep: 2447, time 79.34984135627747, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2447
goal_identified
=== ep: 2448, time 74.58929586410522, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2448
goal_identified
goal_identified
=== ep: 2449, time 68.66436386108398, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2449
goal_identified
goal_identified
=== ep: 2450, time 82.37142848968506, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2451, time 69.16860723495483, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2451
goal_identified
goal_identified
goal_identified
=== ep: 2452, time 77.77221345901489, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2453, time 84.98342156410217, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2453
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2454, time 81.16718053817749, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2454
goal_identified
goal_identified
goal_identified
=== ep: 2455, time 68.57696080207825, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2455
goal_identified
goal_identified
=== ep: 2456, time 76.86173939704895, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2456
goal_identified
goal_identified
goal_identified
=== ep: 2457, time 68.4390459060669, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2457
goal_identified
goal_identified
=== ep: 2458, time 70.40541076660156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2458
goal_identified
goal_identified
=== ep: 2459, time 76.48586654663086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2459
goal_identified
goal_identified
=== ep: 2460, time 79.66963505744934, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2460
goal_identified
goal_identified
goal_identified
=== ep: 2461, time 78.60325789451599, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2461
goal_identified
=== ep: 2462, time 77.30704069137573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2462
goal_identified
goal_identified
goal_identified
=== ep: 2463, time 75.98528385162354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2464, time 81.33593511581421, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2464
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2465, time 72.0094530582428, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2465
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2466, time 74.8101909160614, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2466
goal_identified
goal_identified
goal_identified
=== ep: 2467, time 82.84615802764893, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2467
goal_identified
goal_identified
=== ep: 2468, time 79.2347457408905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2468
goal_identified
goal_identified
=== ep: 2469, time 74.8574390411377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2469
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2470, time 81.3803060054779, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2470
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2471, time 77.45737862586975, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2471
goal_identified
goal_identified
=== ep: 2472, time 66.54268455505371, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2473, time 83.25003504753113, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2473
goal_identified
goal_identified
=== ep: 2474, time 79.01403760910034, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2474
goal_identified
goal_identified
goal_identified
=== ep: 2475, time 70.65525412559509, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2475
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2476, time 79.30603647232056, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2477, time 77.33394646644592, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2477
goal_identified
=== ep: 2478, time 74.07517170906067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2478
goal_identified
=== ep: 2479, time 78.58828520774841, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2479
goal_identified
goal_identified
goal_identified
=== ep: 2480, time 77.22599196434021, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2480
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2481, time 79.70432353019714, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2481
goal_identified
goal_identified
goal_identified
=== ep: 2482, time 80.69218015670776, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2482
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2483, time 75.26881432533264, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2483
goal_identified
goal_identified
goal_identified
=== ep: 2484, time 66.7203106880188, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2485, time 85.29017162322998, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2486, time 69.91888046264648, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2487, time 79.6229362487793, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2487
goal_identified
goal_identified
goal_identified
=== ep: 2488, time 79.61083579063416, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2488
goal_identified
goal_identified
goal_identified
=== ep: 2489, time 75.11460065841675, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2489
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2490, time 69.61157083511353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2491, time 72.95103049278259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2491
goal_identified
goal_identified
goal_identified
=== ep: 2492, time 71.8873770236969, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2492
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2493, time 78.77858018875122, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2493
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2494, time 68.89203524589539, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2494
goal_identified
goal_identified
goal_identified
=== ep: 2495, time 63.53120517730713, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2495
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2496, time 78.55499529838562, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2496
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2497, time 65.72742748260498, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2497
goal_identified
goal_identified
goal_identified
=== ep: 2498, time 75.18248534202576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 84.81975936889648, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2499
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2500, time 84.33624124526978, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2500
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2501, time 76.30779933929443, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2501
goal_identified
goal_identified
=== ep: 2502, time 74.54553580284119, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 70.01428294181824, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2503
goal_identified
=== ep: 2504, time 70.78887581825256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2504
goal_identified
goal_identified
=== ep: 2505, time 70.42177367210388, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2505
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2506, time 67.23529052734375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2506
goal_identified
goal_identified
=== ep: 2507, time 78.83937382698059, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2507
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2508, time 75.23103332519531, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2508
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2509, time 73.66587162017822, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2509
goal_identified
goal_identified
goal_identified
=== ep: 2510, time 84.4695234298706, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2510
goal_identified
goal_identified
=== ep: 2511, time 79.49933314323425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2512, time 75.09301447868347, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2512
goal_identified
goal_identified
=== ep: 2513, time 70.62500476837158, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2513
goal_identified
goal_identified
=== ep: 2514, time 72.8608226776123, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2514
goal_identified
goal_identified
goal_identified
=== ep: 2515, time 75.20768189430237, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2515
goal_identified
goal_identified
goal_identified
=== ep: 2516, time 81.34813141822815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2516
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2517, time 77.6130039691925, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2518, time 77.76406002044678, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2518
goal_identified
goal_identified
goal_identified
=== ep: 2519, time 78.92731261253357, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2519
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2520, time 80.63714265823364, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2520
goal_identified
goal_identified
goal_identified
=== ep: 2521, time 61.08759307861328, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2521
goal_identified
=== ep: 2522, time 72.59326386451721, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2523, time 77.36873078346252, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2523
goal_identified
=== ep: 2524, time 63.55778217315674, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2524
goal_identified
goal_identified
goal_identified
=== ep: 2525, time 72.23661375045776, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2525
goal_identified
goal_identified
goal_identified
=== ep: 2526, time 76.078120470047, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2526
goal_identified
goal_identified
goal_identified
=== ep: 2527, time 80.32703042030334, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2527
goal_identified
goal_identified
goal_identified
=== ep: 2528, time 80.4795434474945, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2528
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2529, time 77.37712621688843, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2529
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2530, time 76.22767758369446, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2530
goal_identified
goal_identified
=== ep: 2531, time 79.88026356697083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2531
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2532, time 67.117356300354, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2532
goal_identified
goal_identified
goal_identified
=== ep: 2533, time 68.67904448509216, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2533
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2534, time 74.75422310829163, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2534
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2535, time 61.63074994087219, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2535
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2536, time 76.176513671875, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2536
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2537, time 73.55804753303528, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2538, time 72.04276084899902, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2538
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2539, time 66.683265209198, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2539
goal_identified
goal_identified
goal_identified
=== ep: 2540, time 65.54965162277222, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2540
goal_identified
goal_identified
goal_identified
=== ep: 2541, time 71.98335695266724, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2541
goal_identified
goal_identified
=== ep: 2542, time 59.34082293510437, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2542
goal_identified
goal_identified
=== ep: 2543, time 77.47409129142761, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2543
goal_identified
goal_identified
goal_identified
=== ep: 2544, time 68.94725322723389, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2544
goal_identified
goal_identified
goal_identified
=== ep: 2545, time 62.90712070465088, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2545
goal_identified
goal_identified
goal_identified
=== ep: 2546, time 75.00404119491577, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2546
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2547, time 60.90377998352051, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
=== ep: 2548, time 66.57931637763977, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2548
goal_identified
goal_identified
=== ep: 2549, time 58.51417899131775, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2549
goal_identified
goal_identified
=== ep: 2550, time 65.01994347572327, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2550
goal_identified
goal_identified
=== ep: 2551, time 61.26512098312378, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2551
goal_identified
goal_identified
=== ep: 2552, time 73.46230840682983, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2552
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2553, time 69.67663478851318, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2553
goal_identified
goal_identified
goal_identified
=== ep: 2554, time 62.92186760902405, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2554
goal_identified
goal_identified
=== ep: 2555, time 78.5091462135315, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2556, time 60.186885356903076, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2556
goal_identified
goal_identified
goal_identified
=== ep: 2557, time 74.89748764038086, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2557
goal_identified
=== ep: 2558, time 79.74731779098511, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2558
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2559, time 77.01835799217224, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2559
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2560, time 74.83486557006836, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2560
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2561, time 76.4671835899353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2561
goal_identified
goal_identified
=== ep: 2562, time 72.81569528579712, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2562
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2563, time 81.56214737892151, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2563
goal_identified
goal_identified
=== ep: 2564, time 76.60934591293335, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2564
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2565, time 68.57921004295349, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2565
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2566, time 83.17398118972778, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2566
goal_identified
=== ep: 2567, time 80.61814260482788, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2567
goal_identified
goal_identified
goal_identified
=== ep: 2568, time 71.32858896255493, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2568
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2569, time 79.42556881904602, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 77.32525086402893, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2570
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2571, time 77.35155630111694, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2572, time 77.67322397232056, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2572
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2573, time 72.32079362869263, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2573
goal_identified
goal_identified
goal_identified
=== ep: 2574, time 72.64330053329468, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2574
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2575, time 82.89793682098389, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2575
goal_identified
goal_identified
goal_identified
=== ep: 2576, time 68.39582848548889, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2576
goal_identified
=== ep: 2577, time 72.30987024307251, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2577
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2578, time 82.46965503692627, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2578
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2579, time 80.53517627716064, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2579
goal_identified
goal_identified
goal_identified
=== ep: 2580, time 76.88631772994995, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2580
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2581, time 80.4907591342926, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2581
goal_identified
goal_identified
=== ep: 2582, time 72.63533639907837, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2582
goal_identified
goal_identified
=== ep: 2583, time 74.53970670700073, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2583
goal_identified
goal_identified
goal_identified
=== ep: 2584, time 73.26611685752869, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2584
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2585, time 68.58264231681824, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2585
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2586, time 78.35129427909851, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2587, time 74.74755311012268, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2587
goal_identified
goal_identified
=== ep: 2588, time 60.777302742004395, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2588
goal_identified
goal_identified
goal_identified
=== ep: 2589, time 78.15582203865051, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2589
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2590, time 62.227078914642334, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2591, time 74.16616106033325, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2591
goal_identified
goal_identified
=== ep: 2592, time 79.72280931472778, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2592
goal_identified
goal_identified
=== ep: 2593, time 80.52056050300598, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2593
goal_identified
goal_identified
goal_identified
=== ep: 2594, time 79.0197913646698, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2594
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2595, time 80.29987859725952, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2595
goal_identified
goal_identified
goal_identified
=== ep: 2596, time 68.58102297782898, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2596
goal_identified
goal_identified
=== ep: 2597, time 70.01964735984802, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2597
goal_identified
goal_identified
goal_identified
=== ep: 2598, time 79.2673065662384, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2599, time 62.40184783935547, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2599
goal_identified
goal_identified
=== ep: 2600, time 79.80461025238037, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2600
goal_identified
goal_identified
=== ep: 2601, time 79.04180073738098, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2601
goal_identified
goal_identified
goal_identified
=== ep: 2602, time 70.00774240493774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2602
goal_identified
goal_identified
goal_identified
=== ep: 2603, time 79.45105218887329, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2603
goal_identified
=== ep: 2604, time 77.02787232398987, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2604
goal_identified
goal_identified
=== ep: 2605, time 78.04469895362854, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2605
goal_identified
=== ep: 2606, time 80.59760904312134, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2606
goal_identified
goal_identified
goal_identified
=== ep: 2607, time 77.94235134124756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2607
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2608, time 63.91482925415039, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2609, time 80.10021352767944, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2609
goal_identified
goal_identified
=== ep: 2610, time 74.90843963623047, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2610
goal_identified
goal_identified
=== ep: 2611, time 71.84707760810852, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2611
goal_identified
goal_identified
goal_identified
=== ep: 2612, time 72.85046005249023, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2612
goal_identified
goal_identified
=== ep: 2613, time 73.65225100517273, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2613
goal_identified
goal_identified
=== ep: 2614, time 77.05352807044983, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2614
goal_identified
goal_identified
goal_identified
=== ep: 2615, time 87.3927538394928, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2615
goal_identified
goal_identified
=== ep: 2616, time 79.72282719612122, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2616
goal_identified
goal_identified
goal_identified
=== ep: 2617, time 66.73719501495361, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2617
goal_identified
goal_identified
=== ep: 2618, time 73.48540449142456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2618
goal_identified
=== ep: 2619, time 76.28881025314331, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2619
goal_identified
goal_identified
goal_identified
=== ep: 2620, time 72.25085353851318, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2620
goal_identified
goal_identified
goal_identified
=== ep: 2621, time 66.87706708908081, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2621
=== ep: 2622, time 62.18263101577759, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2622
goal_identified
goal_identified
goal_identified
=== ep: 2623, time 77.42356204986572, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2623
goal_identified
goal_identified
goal_identified
=== ep: 2624, time 59.312541484832764, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2624
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2625, time 78.92774724960327, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2626, time 84.26085424423218, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2626
goal_identified
goal_identified
goal_identified
=== ep: 2627, time 79.11584067344666, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2627
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2628, time 73.72639918327332, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2628
goal_identified
goal_identified
=== ep: 2629, time 73.65045070648193, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2629
goal_identified
goal_identified
=== ep: 2630, time 72.1618971824646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2630
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2631, time 79.47401857376099, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2631
goal_identified
goal_identified
=== ep: 2632, time 80.7841203212738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2632
goal_identified
goal_identified
goal_identified
=== ep: 2633, time 68.56175518035889, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2634, time 78.84902954101562, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2634
goal_identified
goal_identified
=== ep: 2635, time 84.78219437599182, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2635
goal_identified
=== ep: 2636, time 83.45026659965515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2636
=== ep: 2637, time 70.04031491279602, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2637
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2638, time 71.91558647155762, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2638
goal_identified
goal_identified
goal_identified
=== ep: 2639, time 72.35553932189941, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2639
goal_identified
=== ep: 2640, time 82.96517443656921, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2641, time 83.072678565979, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2641
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2642, time 67.87181353569031, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2642
goal_identified
goal_identified
=== ep: 2643, time 82.20646715164185, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2643
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2644, time 82.97131204605103, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2644
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2645, time 74.17912673950195, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2645
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2646, time 74.21048188209534, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2646
goal_identified
goal_identified
goal_identified
=== ep: 2647, time 78.82022309303284, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2647
goal_identified
goal_identified
goal_identified
=== ep: 2648, time 78.06373381614685, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2648
goal_identified
goal_identified
goal_identified
=== ep: 2649, time 82.65893530845642, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2649
goal_identified
goal_identified
=== ep: 2650, time 75.94259142875671, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2650
goal_identified
=== ep: 2651, time 70.85640740394592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2651
goal_identified
goal_identified
=== ep: 2652, time 81.17001485824585, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2652
goal_identified
goal_identified
=== ep: 2653, time 79.1897292137146, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2653
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2654, time 73.437908411026, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2654
goal_identified
=== ep: 2655, time 77.87010622024536, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2655
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2656, time 82.2656741142273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2656
goal_identified
goal_identified
goal_identified
=== ep: 2657, time 70.41007351875305, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2657
goal_identified
goal_identified
goal_identified
=== ep: 2658, time 76.44843673706055, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2658
goal_identified
=== ep: 2659, time 74.27684664726257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2660, time 78.57215762138367, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2660
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2661, time 87.5473964214325, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2661
goal_identified
goal_identified
=== ep: 2662, time 82.71057176589966, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2662
goal_identified
=== ep: 2663, time 67.76942443847656, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2664, time 81.29580426216125, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2664
goal_identified
goal_identified
goal_identified
=== ep: 2665, time 82.62555408477783, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2665
goal_identified
goal_identified
goal_identified
=== ep: 2666, time 69.95621728897095, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2666
goal_identified
=== ep: 2667, time 78.57599830627441, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2667
goal_identified
goal_identified
goal_identified
=== ep: 2668, time 79.38096833229065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2668
goal_identified
goal_identified
goal_identified
=== ep: 2669, time 76.8784191608429, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2669
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2670, time 75.25341868400574, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2670
goal_identified
goal_identified
=== ep: 2671, time 65.92301964759827, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2671
goal_identified
goal_identified
=== ep: 2672, time 71.64254665374756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2672
goal_identified
goal_identified
=== ep: 2673, time 77.36846160888672, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2673
goal_identified
=== ep: 2674, time 62.10893201828003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2674
goal_identified
goal_identified
goal_identified
=== ep: 2675, time 79.389808177948, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2675
goal_identified
goal_identified
=== ep: 2676, time 76.7206723690033, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2676
goal_identified
goal_identified
goal_identified
=== ep: 2677, time 66.11336636543274, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2678, time 75.46597456932068, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2678
goal_identified
=== ep: 2679, time 75.02554035186768, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2679
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2680, time 84.11428594589233, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2680
goal_identified
goal_identified
=== ep: 2681, time 85.09583187103271, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2681
goal_identified
goal_identified
=== ep: 2682, time 76.3217236995697, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2682
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2683, time 66.02147126197815, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2683
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2684, time 80.83853578567505, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2684
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2685, time 74.29891347885132, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2685
goal_identified
goal_identified
goal_identified
=== ep: 2686, time 70.24658060073853, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2686
goal_identified
goal_identified
goal_identified
=== ep: 2687, time 80.15979743003845, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2687
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2688, time 75.58458042144775, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2688
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2689, time 78.95138454437256, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2689
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2690, time 83.0543098449707, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2691, time 70.29629635810852, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2691
goal_identified
goal_identified
=== ep: 2692, time 71.89814519882202, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2692
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2693, time 86.69885444641113, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2693
goal_identified
goal_identified
=== ep: 2694, time 80.13062691688538, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2694
goal_identified
goal_identified
goal_identified
=== ep: 2695, time 76.66293263435364, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2695
goal_identified
goal_identified
goal_identified
=== ep: 2696, time 79.00818204879761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2696
goal_identified
=== ep: 2697, time 72.15618395805359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2697
goal_identified
goal_identified
=== ep: 2698, time 76.55730772018433, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2698
goal_identified
goal_identified
=== ep: 2699, time 78.39888668060303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2699
goal_identified
goal_identified
=== ep: 2700, time 59.95262098312378, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2701, time 81.26802229881287, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2701
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2702, time 84.6020622253418, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2702
goal_identified
goal_identified
=== ep: 2703, time 85.43009400367737, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2703
goal_identified
=== ep: 2704, time 75.43235993385315, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2704
goal_identified
goal_identified
=== ep: 2705, time 76.48784279823303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2706, time 67.59359574317932, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2706
goal_identified
goal_identified
=== ep: 2707, time 76.83303999900818, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2707
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2708, time 78.97047781944275, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2709, time 59.59653115272522, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2709
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2710, time 78.49168014526367, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2711, time 68.22725629806519, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2711
goal_identified
goal_identified
=== ep: 2712, time 63.61660718917847, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2713, time 67.05065155029297, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2713
goal_identified
=== ep: 2714, time 71.8495864868164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2714
goal_identified
goal_identified
goal_identified
=== ep: 2715, time 75.2684268951416, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2715
goal_identified
goal_identified
=== ep: 2716, time 65.0539710521698, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2716
goal_identified
goal_identified
goal_identified
=== ep: 2717, time 78.53583669662476, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2717
goal_identified
goal_identified
goal_identified
=== ep: 2718, time 82.9298198223114, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2718
goal_identified
goal_identified
goal_identified
=== ep: 2719, time 78.40448021888733, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2720, time 77.97565746307373, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2720
goal_identified
goal_identified
goal_identified
=== ep: 2721, time 75.44125556945801, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2721
goal_identified
goal_identified
=== ep: 2722, time 60.37684965133667, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2722
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2723, time 77.17429327964783, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2723
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2724, time 64.11594653129578, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2724
goal_identified
goal_identified
goal_identified
=== ep: 2725, time 74.18949508666992, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2725
goal_identified
goal_identified
goal_identified
=== ep: 2726, time 81.17534160614014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2726
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2727, time 81.99017786979675, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2727
goal_identified
=== ep: 2728, time 73.8484160900116, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2728
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2729, time 76.81074333190918, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2729
goal_identified
goal_identified
=== ep: 2730, time 69.70854067802429, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2730
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2731, time 81.14357328414917, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2732, time 80.00883769989014, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2732
goal_identified
goal_identified
=== ep: 2733, time 67.44319128990173, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2733
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2734, time 78.615549325943, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2734
goal_identified
=== ep: 2735, time 79.21706891059875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2735
goal_identified
goal_identified
goal_identified
=== ep: 2736, time 79.34822726249695, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2736
goal_identified
goal_identified
goal_identified
=== ep: 2737, time 77.00875878334045, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2737
goal_identified
goal_identified
=== ep: 2738, time 72.24577236175537, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2738
goal_identified
goal_identified
=== ep: 2739, time 65.08559346199036, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2739
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2740, time 80.28726124763489, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 65.36575627326965, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2741
goal_identified
goal_identified
goal_identified
=== ep: 2742, time 65.9166784286499, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2742
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2743, time 69.64939260482788, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2743
goal_identified
goal_identified
=== ep: 2744, time 59.84379720687866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2744
goal_identified
goal_identified
goal_identified
=== ep: 2745, time 59.10066246986389, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2745
goal_identified
goal_identified
goal_identified
=== ep: 2746, time 65.25475478172302, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2746
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2747, time 72.97471380233765, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2747
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2748, time 61.71221995353699, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2748
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2749, time 78.32984781265259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2749
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2750, time 83.11530947685242, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2750
goal_identified
goal_identified
=== ep: 2751, time 67.41235041618347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2751
goal_identified
goal_identified
goal_identified
=== ep: 2752, time 76.78523421287537, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2752
goal_identified
=== ep: 2753, time 79.98331046104431, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2753
goal_identified
goal_identified
goal_identified
=== ep: 2754, time 81.47974252700806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2754
goal_identified
goal_identified
goal_identified
=== ep: 2755, time 75.92291212081909, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2755
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2756, time 75.3072600364685, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2756
goal_identified
goal_identified
goal_identified
=== ep: 2757, time 65.48198223114014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2757
goal_identified
goal_identified
goal_identified
=== ep: 2758, time 84.10322570800781, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2758
goal_identified
=== ep: 2759, time 82.55327248573303, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2759
goal_identified
=== ep: 2760, time 71.16811537742615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2760
=== ep: 2761, time 81.38978576660156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2761
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2762, time 76.29382014274597, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2762
goal_identified
goal_identified
=== ep: 2763, time 77.16163611412048, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2763
goal_identified
goal_identified
goal_identified
=== ep: 2764, time 84.25013947486877, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2764
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2765, time 77.63066172599792, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2765
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2766, time 72.75295114517212, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
goal_identified
goal_identified
goal_identified
=== ep: 2767, time 81.58777499198914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2767
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2768, time 70.4963755607605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2769, time 74.03541731834412, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2769
goal_identified
goal_identified
goal_identified
=== ep: 2770, time 81.25613641738892, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2770
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2771, time 80.6583251953125, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2771
goal_identified
goal_identified
goal_identified
=== ep: 2772, time 81.06304478645325, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2772
=== ep: 2773, time 78.1325089931488, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2773
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2774, time 69.09878325462341, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2774
goal_identified
goal_identified
=== ep: 2775, time 76.74902057647705, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2775
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2776, time 84.10253429412842, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2776
goal_identified
goal_identified
goal_identified
=== ep: 2777, time 66.64922547340393, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2777
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2778, time 76.858469247818, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2778
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2779, time 83.28410768508911, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2779
goal_identified
goal_identified
=== ep: 2780, time 81.1536054611206, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2780
goal_identified
goal_identified
=== ep: 2781, time 80.09524440765381, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2781
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2782, time 75.38320326805115, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2782
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2783, time 63.71326732635498, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2783
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2784, time 78.05094122886658, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2784
goal_identified
goal_identified
=== ep: 2785, time 77.1527156829834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2785
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2786, time 66.24773406982422, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2786
goal_identified
goal_identified
goal_identified
=== ep: 2787, time 80.01634669303894, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2787
goal_identified
=== ep: 2788, time 78.53299164772034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2788
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2789, time 82.33487844467163, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2789
=== ep: 2790, time 81.40229845046997, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2791, time 76.92274117469788, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2791
goal_identified
goal_identified
=== ep: 2792, time 64.30214071273804, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2792
goal_identified
goal_identified
goal_identified
=== ep: 2793, time 80.73494124412537, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2793
goal_identified
goal_identified
goal_identified
=== ep: 2794, time 69.67328405380249, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2795, time 69.85248613357544, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
goal_identified
goal_identified
goal_identified
=== ep: 2796, time 76.99109244346619, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2796
goal_identified
goal_identified
=== ep: 2797, time 80.05955362319946, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2798, time 83.51529717445374, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2798
goal_identified
=== ep: 2799, time 76.83433294296265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2799
goal_identified
goal_identified
=== ep: 2800, time 64.36285352706909, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2800
goal_identified
goal_identified
goal_identified
=== ep: 2801, time 76.01745533943176, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2801
goal_identified
goal_identified
goal_identified
=== ep: 2802, time 79.34457445144653, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2802
goal_identified
goal_identified
goal_identified
=== ep: 2803, time 66.88870763778687, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2803
goal_identified
goal_identified
goal_identified
=== ep: 2804, time 74.26923871040344, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2804
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2805, time 72.8366379737854, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2805
goal_identified
goal_identified
goal_identified
=== ep: 2806, time 65.34528636932373, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2807, time 68.74146699905396, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2807
goal_identified
goal_identified
=== ep: 2808, time 57.502277851104736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2808
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2809, time 68.18850111961365, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2809
goal_identified
=== ep: 2810, time 57.37395882606506, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2810
goal_identified
goal_identified
goal_identified
=== ep: 2811, time 74.86594843864441, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2811
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2812, time 68.64088106155396, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2812
=== ep: 2813, time 61.57197833061218, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2813
goal_identified
goal_identified
goal_identified
=== ep: 2814, time 75.86227750778198, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2814
goal_identified
goal_identified
goal_identified
=== ep: 2815, time 68.92818808555603, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2815
goal_identified
goal_identified
goal_identified
=== ep: 2816, time 70.0341489315033, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2816
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2817, time 71.03555178642273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2817
=== ep: 2818, time 82.38705825805664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2818
goal_identified
=== ep: 2819, time 80.29726219177246, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2819
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2820, time 72.9565019607544, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2820
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2821, time 74.68327331542969, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2821
goal_identified
goal_identified
=== ep: 2822, time 81.70695328712463, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2823, time 80.38627338409424, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2824, time 76.45466089248657, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2824
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2825, time 73.77141261100769, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2825
goal_identified
=== ep: 2826, time 67.45875000953674, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2826
goal_identified
goal_identified
goal_identified
=== ep: 2827, time 74.81782793998718, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2827
goal_identified
goal_identified
=== ep: 2828, time 80.69082593917847, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2828
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2829, time 70.10106134414673, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2829
goal_identified
goal_identified
goal_identified
=== ep: 2830, time 78.83227872848511, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2830
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2831, time 82.89911031723022, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2831
goal_identified
=== ep: 2832, time 79.36185431480408, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2833, time 74.67952036857605, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2833
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2834, time 76.39143013954163, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2834
goal_identified
goal_identified
=== ep: 2835, time 66.59726047515869, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2836, time 78.46432423591614, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2836
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2837, time 78.92689037322998, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2838, time 71.65091395378113, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2838
goal_identified
goal_identified
goal_identified
=== ep: 2839, time 76.5669355392456, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2839
goal_identified
goal_identified
goal_identified
=== ep: 2840, time 73.53242540359497, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 72.38202214241028, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2841
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2842, time 72.17457890510559, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2842
goal_identified
goal_identified
goal_identified
=== ep: 2843, time 65.57981395721436, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 56.872517347335815, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2844
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2845, time 71.18758630752563, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 51.46273231506348, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2847, time 66.98668813705444, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2848, time 68.58605122566223, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2848
goal_identified
=== ep: 2849, time 67.04563093185425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2849
goal_identified
=== ep: 2850, time 63.52840280532837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2850
goal_identified
goal_identified
goal_identified
=== ep: 2851, time 60.276920318603516, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2851
goal_identified
goal_identified
goal_identified
=== ep: 2852, time 71.70723414421082, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2852
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2853, time 70.3171718120575, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2853
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2854, time 62.764394998550415, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2854
goal_identified
goal_identified
goal_identified
=== ep: 2855, time 73.66802263259888, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2855
goal_identified
=== ep: 2856, time 72.7135820388794, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2857, time 70.80996108055115, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
goal_identified
goal_identified
goal_identified
=== ep: 2858, time 71.9990963935852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2858
goal_identified
=== ep: 2859, time 66.76831007003784, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2859
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2860, time 62.96864104270935, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 76.8048734664917, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2862, time 72.75482439994812, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2862
goal_identified
=== ep: 2863, time 64.2233510017395, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2864, time 70.90079736709595, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2864
goal_identified
goal_identified
=== ep: 2865, time 69.16874861717224, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2865
goal_identified
goal_identified
goal_identified
=== ep: 2866, time 73.11742448806763, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2866
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2867, time 74.18089532852173, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2867
goal_identified
goal_identified
goal_identified
=== ep: 2868, time 60.16498136520386, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2869, time 70.15717720985413, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2869
goal_identified
goal_identified
=== ep: 2870, time 75.33832478523254, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2870
goal_identified
goal_identified
goal_identified
=== ep: 2871, time 66.10647797584534, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2871
goal_identified
goal_identified
=== ep: 2872, time 70.10858821868896, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2872
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2873, time 72.73740100860596, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2873
goal_identified
=== ep: 2874, time 66.68304300308228, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2874
goal_identified
goal_identified
=== ep: 2875, time 75.6909019947052, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2876, time 74.80309820175171, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2876
goal_identified
goal_identified
goal_identified
=== ep: 2877, time 58.166701555252075, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2877
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2878, time 73.13002300262451, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2878
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2879, time 72.73456740379333, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2879
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2880, time 67.29266452789307, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2880
goal_identified
goal_identified
goal_identified
=== ep: 2881, time 68.78789019584656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2881
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2882, time 64.18893098831177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2882
goal_identified
goal_identified
=== ep: 2883, time 66.04409861564636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2883
goal_identified
goal_identified
goal_identified
=== ep: 2884, time 75.05764245986938, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2884
goal_identified
goal_identified
=== ep: 2885, time 70.67885756492615, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2885
goal_identified
goal_identified
=== ep: 2886, time 69.88866925239563, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2886
goal_identified
goal_identified
goal_identified
=== ep: 2887, time 70.2815089225769, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2887
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2888, time 69.16088676452637, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2888
goal_identified
=== ep: 2889, time 68.70415878295898, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2889
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2890, time 71.6848373413086, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2890
goal_identified
=== ep: 2891, time 56.827773094177246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2891
goal_identified
goal_identified
goal_identified
=== ep: 2892, time 63.438190937042236, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2892
goal_identified
goal_identified
goal_identified
=== ep: 2893, time 69.72390794754028, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2893
goal_identified
goal_identified
goal_identified
=== ep: 2894, time 53.66888761520386, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2894
=== ep: 2895, time 69.14054918289185, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2895
goal_identified
goal_identified
=== ep: 2896, time 69.32071924209595, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2897, time 68.1582453250885, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2898, time 73.02442145347595, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2898
goal_identified
goal_identified
=== ep: 2899, time 70.92979383468628, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2899
goal_identified
=== ep: 2900, time 66.90221047401428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2900
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2901, time 73.93238186836243, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2901
goal_identified
=== ep: 2902, time 66.76316142082214, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2902
goal_identified
goal_identified
goal_identified
=== ep: 2903, time 48.95111036300659, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2904, time 67.07508969306946, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2904
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2905, time 62.225260496139526, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2905
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2906, time 56.35216569900513, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2906
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2907, time 58.10719180107117, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2907
goal_identified
goal_identified
goal_identified
=== ep: 2908, time 53.498879194259644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2908
goal_identified
=== ep: 2909, time 63.10215473175049, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2909
goal_identified
goal_identified
=== ep: 2910, time 50.063514709472656, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2910
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2911, time 62.43359017372131, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2911
goal_identified
goal_identified
=== ep: 2912, time 71.13014245033264, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2913, time 56.76611375808716, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2914, time 65.03315734863281, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2914
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2915, time 68.00728368759155, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2915
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2916, time 63.619059562683105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 67.21797323226929, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2917
goal_identified
goal_identified
goal_identified
=== ep: 2918, time 71.73982834815979, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2918
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2919, time 63.30712628364563, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2920, time 57.59517049789429, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2920
goal_identified
goal_identified
=== ep: 2921, time 70.99938178062439, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2921
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2922, time 71.83997941017151, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2922
goal_identified
=== ep: 2923, time 65.99188160896301, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2923
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2924, time 62.49143934249878, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2924
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2925, time 61.98602533340454, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2925
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2926, time 59.15779399871826, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2926
goal_identified
goal_identified
goal_identified
=== ep: 2927, time 65.1368019580841, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2927
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2928, time 48.671584129333496, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2928
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2929, time 65.15014410018921, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2929
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2930, time 74.41799831390381, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2930
goal_identified
goal_identified
=== ep: 2931, time 66.75926160812378, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2931
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2932, time 60.126349687576294, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
goal_identified
goal_identified
=== ep: 2933, time 69.11655879020691, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2933
goal_identified
=== ep: 2934, time 65.82227516174316, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2934
goal_identified
=== ep: 2935, time 66.61438536643982, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2935
goal_identified
goal_identified
goal_identified
=== ep: 2936, time 66.17259478569031, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2936
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2937, time 56.6497905254364, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2937
goal_identified
goal_identified
=== ep: 2938, time 56.552961111068726, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2938
goal_identified
goal_identified
goal_identified
=== ep: 2939, time 67.33616280555725, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2939
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2940, time 59.6648952960968, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2940
goal_identified
goal_identified
goal_identified
=== ep: 2941, time 60.70358157157898, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2941
goal_identified
goal_identified
goal_identified
=== ep: 2942, time 69.34114408493042, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2942
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2943, time 70.52455925941467, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2943
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2944, time 67.13253569602966, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2944
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2945, time 66.61843609809875, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2945
goal_identified
goal_identified
=== ep: 2946, time 65.17854428291321, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2946
goal_identified
goal_identified
goal_identified
=== ep: 2947, time 56.42170023918152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2948, time 67.50413179397583, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2948
goal_identified
goal_identified
=== ep: 2949, time 72.37332940101624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2949
goal_identified
goal_identified
goal_identified
=== ep: 2950, time 61.71053600311279, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2951, time 64.18268966674805, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2951
goal_identified
goal_identified
=== ep: 2952, time 68.50490641593933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2953, time 67.13450074195862, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2953
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2954, time 64.76023507118225, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2954
goal_identified
goal_identified
=== ep: 2955, time 67.1076672077179, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2956, time 63.05693435668945, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2956
goal_identified
=== ep: 2957, time 50.192978620529175, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2957
goal_identified
goal_identified
goal_identified
=== ep: 2958, time 69.13191485404968, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2958
goal_identified
goal_identified
goal_identified
=== ep: 2959, time 66.67087841033936, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2959
goal_identified
=== ep: 2960, time 58.168662786483765, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2960
goal_identified
goal_identified
=== ep: 2961, time 68.43462038040161, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2961
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2962, time 64.91237282752991, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2962
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2963, time 66.99289202690125, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2963
=== ep: 2964, time 72.83359742164612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2964
goal_identified
goal_identified
goal_identified
=== ep: 2965, time 65.80656266212463, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2965
goal_identified
goal_identified
goal_identified
=== ep: 2966, time 52.76450157165527, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2966
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2967, time 68.65514969825745, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2967
goal_identified
goal_identified
=== ep: 2968, time 68.17228198051453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2968
goal_identified
goal_identified
goal_identified
=== ep: 2969, time 65.12818384170532, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2969
goal_identified
goal_identified
=== ep: 2970, time 66.10770273208618, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2970
goal_identified
goal_identified
=== ep: 2971, time 59.57136917114258, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2971
goal_identified
goal_identified
goal_identified
=== ep: 2972, time 63.180888414382935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2972
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2973, time 68.21812987327576, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2973
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2974, time 66.70544910430908, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2974
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2975, time 51.74101281166077, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2975
goal_identified
goal_identified
goal_identified
=== ep: 2976, time 67.9192304611206, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2976
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2977, time 67.66766738891602, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2977
goal_identified
goal_identified
goal_identified
=== ep: 2978, time 55.86120009422302, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2978
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2979, time 61.94832730293274, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2979
goal_identified
goal_identified
=== ep: 2980, time 60.919575214385986, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2980
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2981, time 57.44485330581665, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2981
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2982, time 58.644944190979004, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2982
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2983, time 58.03745722770691, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2983
goal_identified
=== ep: 2984, time 52.903454303741455, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2984
goal_identified
goal_identified
goal_identified
=== ep: 2985, time 48.865556955337524, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2985
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2986, time 61.05395174026489, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2986
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2987, time 59.65703535079956, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2987
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2988, time 49.590113162994385, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2988
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2989, time 55.834020137786865, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2989
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2990, time 56.67792320251465, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2990
goal_identified
goal_identified
=== ep: 2991, time 53.504488468170166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2991
goal_identified
goal_identified
=== ep: 2992, time 53.55194163322449, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2992
goal_identified
goal_identified
goal_identified
=== ep: 2993, time 50.985889196395874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2993
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2994, time 48.56499695777893, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2994
goal_identified
=== ep: 2995, time 54.16186332702637, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2995
goal_identified
=== ep: 2996, time 61.24908900260925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2996
goal_identified
goal_identified
goal_identified
=== ep: 2997, time 57.380797147750854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2997
goal_identified
goal_identified
goal_identified
=== ep: 2998, time 49.638867139816284, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2998
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2999, time 56.81615686416626, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2999
goal_identified
goal_identified
goal_identified
=== ep: 3000, time 61.58657193183899, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3000
goal_identified
goal_identified
=== ep: 3001, time 45.9216570854187, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3001
goal_identified
goal_identified
=== ep: 3002, time 50.687299728393555, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3002
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3003, time 55.98346734046936, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3004, time 48.915483474731445, eps 0.001, sum reward: 9, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1932
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3005, time 51.14159178733826, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3005
=== ep: 3006, time 56.10944223403931, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3006
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3007, time 55.71402072906494, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2200
goal_identified
goal_identified
=== ep: 3008, time 55.85578179359436, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3008
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3009, time 57.309773683547974, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3009
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3010, time 55.70538091659546, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3010
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3011, time 49.368579387664795, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3012, time 54.45058226585388, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3012
goal_identified
goal_identified
goal_identified
=== ep: 3013, time 56.32594680786133, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3013
goal_identified
goal_identified
goal_identified
=== ep: 3014, time 52.84211850166321, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3014
goal_identified
goal_identified
goal_identified
=== ep: 3015, time 49.23829388618469, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3015
goal_identified
goal_identified
goal_identified
=== ep: 3016, time 61.02025008201599, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3016
goal_identified
goal_identified
goal_identified
=== ep: 3017, time 62.32605957984924, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3017
goal_identified
goal_identified
goal_identified
=== ep: 3018, time 47.59369969367981, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3018
goal_identified
goal_identified
=== ep: 3019, time 54.410441160202026, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3019
goal_identified
goal_identified
=== ep: 3020, time 60.86502122879028, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3020
goal_identified
goal_identified
goal_identified
=== ep: 3021, time 57.41847896575928, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3021
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3022, time 45.040749311447144, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3022
goal_identified
goal_identified
goal_identified
=== ep: 3023, time 54.570581912994385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3023
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3024, time 54.537739515304565, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3024
goal_identified
goal_identified
goal_identified
=== ep: 3025, time 51.78027105331421, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3025
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3026, time 51.26683282852173, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3026
goal_identified
goal_identified
=== ep: 3027, time 54.01008415222168, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3028, time 53.71191453933716, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3028
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3029, time 56.19919562339783, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3029
goal_identified
goal_identified
goal_identified
=== ep: 3030, time 56.23419213294983, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3030
goal_identified
=== ep: 3031, time 55.942893266677856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3031
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3032, time 47.82622146606445, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3032
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3033, time 54.229801654815674, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3033
goal_identified
goal_identified
goal_identified
=== ep: 3034, time 55.76305341720581, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3034
goal_identified
goal_identified
=== ep: 3035, time 48.92280673980713, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3035
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3036, time 49.357643127441406, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3036
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3037, time 60.43685030937195, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3038, time 54.68826365470886, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3038
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3039, time 50.16581344604492, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3039
goal_identified
goal_identified
=== ep: 3040, time 59.1011278629303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3040
goal_identified
=== ep: 3041, time 59.9089298248291, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3041
goal_identified
goal_identified
goal_identified
=== ep: 3042, time 45.876365184783936, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3042
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3043, time 58.93364930152893, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3043
goal_identified
=== ep: 3044, time 55.58029389381409, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3044
goal_identified
goal_identified
goal_identified
=== ep: 3045, time 45.72399401664734, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3045
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3046, time 55.501251459121704, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3046
goal_identified
goal_identified
=== ep: 3047, time 53.74069333076477, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3047
goal_identified
goal_identified
goal_identified
=== ep: 3048, time 53.59660077095032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3048
goal_identified
goal_identified
goal_identified
=== ep: 3049, time 54.691246509552, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3049
goal_identified
goal_identified
goal_identified
=== ep: 3050, time 54.580302476882935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3050
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3051, time 54.16105842590332, eps 0.001, sum reward: 6, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3051
goal_identified
goal_identified
goal_identified
=== ep: 3052, time 50.212435245513916, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3052
goal_identified
goal_identified
goal_identified
=== ep: 3053, time 60.525254249572754, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3053
goal_identified
=== ep: 3054, time 54.72647786140442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3054
goal_identified
goal_identified
goal_identified
=== ep: 3055, time 42.975924015045166, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3055
goal_identified
goal_identified
goal_identified
=== ep: 3056, time 57.325955390930176, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3056
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3057, time 55.62829637527466, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3058, time 41.81140398979187, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2202
goal_identified
goal_identified
=== ep: 3059, time 58.43744444847107, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3059
goal_identified
goal_identified
goal_identified
=== ep: 3060, time 57.77486515045166, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3060
goal_identified
goal_identified
goal_identified
=== ep: 3061, time 49.146541118621826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3061
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3062, time 55.11196208000183, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3062
goal_identified
goal_identified
goal_identified
=== ep: 3063, time 56.42425274848938, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3063
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3064, time 54.41555070877075, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3064
goal_identified
goal_identified
=== ep: 3065, time 55.18160629272461, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3065
goal_identified
=== ep: 3066, time 58.48703718185425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3066
goal_identified
goal_identified
=== ep: 3067, time 53.09553146362305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3067
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3068, time 44.70397710800171, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3068
goal_identified
=== ep: 3069, time 56.45605969429016, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3070, time 56.78218674659729, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3070
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3071, time 54.35157895088196, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3071
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3072, time 52.34112739562988, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3072
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3073, time 62.12353491783142, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3073
goal_identified
goal_identified
=== ep: 3074, time 59.656556844711304, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3074
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3075, time 47.05099081993103, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3075
goal_identified
goal_identified
goal_identified
=== ep: 3076, time 57.035460472106934, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3076
goal_identified
=== ep: 3077, time 58.31155037879944, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3077
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3078, time 49.90079855918884, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3078
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3079, time 50.31708836555481, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3079
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3080, time 53.811375856399536, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3080
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3081, time 51.91547679901123, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3082, time 55.719892501831055, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3082
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3083, time 57.39445471763611, eps 0.001, sum reward: 6, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3083
goal_identified
goal_identified
=== ep: 3084, time 57.02586889266968, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3084
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3085, time 51.484675884246826, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3085
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3086, time 52.80567908287048, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3086
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3087, time 59.32518768310547, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2207
goal_identified
goal_identified
goal_identified
=== ep: 3088, time 52.6063072681427, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3088
goal_identified
=== ep: 3089, time 44.986613750457764, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3089
goal_identified
=== ep: 3090, time 57.72198486328125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3090
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3091, time 53.36102294921875, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3091
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3092, time 48.64448595046997, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3092
goal_identified
goal_identified
=== ep: 3093, time 58.47475743293762, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3093
goal_identified
goal_identified
=== ep: 3094, time 59.32935094833374, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3094
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3095, time 56.705103635787964, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3095
goal_identified
goal_identified
=== ep: 3096, time 56.77719688415527, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3096
goal_identified
goal_identified
goal_identified
=== ep: 3097, time 51.96365022659302, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 53.20542812347412, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3098
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3099, time 52.42470598220825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3099
goal_identified
goal_identified
=== ep: 3100, time 54.588446617126465, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3100
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3101, time 56.160802125930786, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3101
goal_identified
goal_identified
goal_identified
=== ep: 3102, time 54.49599075317383, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3102
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3103, time 53.10209631919861, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3103
goal_identified
=== ep: 3104, time 55.92215180397034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3104
goal_identified
goal_identified
goal_identified
=== ep: 3105, time 59.74670219421387, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3105
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3106, time 53.65463137626648, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3106
goal_identified
goal_identified
=== ep: 3107, time 43.2929413318634, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3107
goal_identified
goal_identified
=== ep: 3108, time 55.18975210189819, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3108
goal_identified
goal_identified
goal_identified
=== ep: 3109, time 53.91917395591736, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3109
goal_identified
goal_identified
=== ep: 3110, time 44.99086356163025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3110
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3111, time 49.679768800735474, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3111
goal_identified
goal_identified
goal_identified
=== ep: 3112, time 55.05880403518677, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3112
goal_identified
goal_identified
goal_identified
=== ep: 3113, time 51.6827666759491, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3113
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3114, time 43.06299042701721, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3114
goal_identified
goal_identified
goal_identified
=== ep: 3115, time 55.65882229804993, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3115
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3116, time 55.522857666015625, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3116
goal_identified
=== ep: 3117, time 42.13603949546814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3117
goal_identified
goal_identified
goal_identified
=== ep: 3118, time 52.618536949157715, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3118
goal_identified
goal_identified
goal_identified
=== ep: 3119, time 54.51693797111511, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3119
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3120, time 52.009543657302856, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3120
goal_identified
goal_identified
goal_identified
=== ep: 3121, time 50.67865562438965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3121
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3122, time 51.568286418914795, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3122
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3123, time 54.09819269180298, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3123
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3124, time 54.4256911277771, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3124
goal_identified
goal_identified
=== ep: 3125, time 46.1947820186615, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3126, time 49.356919288635254, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3126
goal_identified
=== ep: 3127, time 51.433940410614014, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3127
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3128, time 51.10033917427063, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3128
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3129, time 50.52941370010376, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3129
goal_identified
goal_identified
=== ep: 3130, time 52.22696089744568, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3130
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3131, time 50.78933882713318, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3132, time 47.40697360038757, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3132
goal_identified
goal_identified
=== ep: 3133, time 49.14904761314392, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3133
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3134, time 51.611165046691895, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3134
goal_identified
goal_identified
=== ep: 3135, time 52.846811056137085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3135
goal_identified
goal_identified
goal_identified
=== ep: 3136, time 49.83909583091736, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3136
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3137, time 45.43704009056091, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3137
goal_identified
goal_identified
=== ep: 3138, time 52.139135122299194, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3138
goal_identified
=== ep: 3139, time 55.06085801124573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3139
goal_identified
goal_identified
=== ep: 3140, time 53.750096797943115, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3140
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3141, time 48.27568960189819, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3141
goal_identified
goal_identified
=== ep: 3142, time 46.20896077156067, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 55.20289731025696, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3144, time 54.08057522773743, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3144
goal_identified
goal_identified
=== ep: 3145, time 43.382818937301636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3145
goal_identified
goal_identified
=== ep: 3146, time 55.84482717514038, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3146
=== ep: 3147, time 54.88633489608765, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3147
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3148, time 51.48676776885986, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3148
goal_identified
goal_identified
=== ep: 3149, time 42.05551600456238, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3149
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3150, time 54.7314829826355, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3150
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3151, time 55.433695554733276, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3151
goal_identified
goal_identified
goal_identified
=== ep: 3152, time 41.67014932632446, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3152
goal_identified
goal_identified
=== ep: 3153, time 54.010416984558105, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3153
=== ep: 3154, time 56.1813805103302, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3154
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3155, time 48.65789556503296, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3156, time 46.66758489608765, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3156
goal_identified
=== ep: 3157, time 53.041390895843506, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3157
goal_identified
=== ep: 3158, time 52.58505964279175, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3158
goal_identified
=== ep: 3159, time 52.182350873947144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3159
goal_identified
goal_identified
goal_identified
=== ep: 3160, time 46.634220361709595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3160
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3161, time 51.618770599365234, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3161
=== ep: 3162, time 51.102885723114014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3162
goal_identified
goal_identified
=== ep: 3163, time 51.723717212677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3163
goal_identified
=== ep: 3164, time 52.242236375808716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3164
goal_identified
goal_identified
=== ep: 3165, time 50.0147545337677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3165
goal_identified
goal_identified
goal_identified
=== ep: 3166, time 49.43549871444702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 51.56091618537903, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3167
=== ep: 3168, time 49.73803353309631, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3168
goal_identified
=== ep: 3169, time 52.233856201171875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3169
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3170, time 51.23887276649475, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3170
goal_identified
goal_identified
goal_identified
=== ep: 3171, time 55.96721053123474, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3171
goal_identified
=== ep: 3172, time 50.083597898483276, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3173, time 47.628161668777466, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3173
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3174, time 53.12629580497742, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3174
goal_identified
goal_identified
goal_identified
=== ep: 3175, time 54.35350179672241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3175
goal_identified
goal_identified
goal_identified
=== ep: 3176, time 51.59828448295593, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3176
goal_identified
goal_identified
goal_identified
=== ep: 3177, time 43.47334671020508, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3177
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3178, time 51.7123908996582, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3178
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3179, time 53.67609906196594, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3179
goal_identified
=== ep: 3180, time 50.11878156661987, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3180
goal_identified
goal_identified
goal_identified
=== ep: 3181, time 44.217966079711914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3181
goal_identified
goal_identified
goal_identified
=== ep: 3182, time 54.652124643325806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3182
goal_identified
goal_identified
=== ep: 3183, time 54.696242332458496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3184, time 50.96126747131348, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3184
goal_identified
goal_identified
=== ep: 3185, time 43.369102478027344, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3185
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3186, time 54.76144099235535, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3186
goal_identified
=== ep: 3187, time 56.660595417022705, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3187
goal_identified
=== ep: 3188, time 46.652793169021606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3188
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3189, time 47.30347990989685, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3189
goal_identified
goal_identified
goal_identified
=== ep: 3190, time 55.638550996780396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3190
goal_identified
=== ep: 3191, time 50.05090689659119, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3191
goal_identified
=== ep: 3192, time 45.158910036087036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3192
goal_identified
goal_identified
=== ep: 3193, time 54.73050045967102, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3193
goal_identified
=== ep: 3194, time 54.51147770881653, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3194
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3195, time 42.33071684837341, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3195
goal_identified
goal_identified
=== ep: 3196, time 58.129865646362305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3196
goal_identified
goal_identified
=== ep: 3197, time 55.80383658409119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3197
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3198, time 45.83846688270569, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 49.0039746761322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3199
goal_identified
=== ep: 3200, time 57.65614032745361, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3200
goal_identified
=== ep: 3201, time 51.46709203720093, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3201
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3202, time 42.61588168144226, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3202
goal_identified
=== ep: 3203, time 55.82305121421814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3203
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3204, time 55.59000778198242, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3204
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3205, time 50.56981563568115, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3205
goal_identified
goal_identified
=== ep: 3206, time 43.6805534362793, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3206
goal_identified
goal_identified
goal_identified
=== ep: 3207, time 52.04100465774536, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3207
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3208, time 52.54956841468811, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3208
goal_identified
goal_identified
=== ep: 3209, time 52.60911154747009, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3209
=== ep: 3210, time 48.836801290512085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3210
goal_identified
goal_identified
goal_identified
=== ep: 3211, time 51.149863958358765, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3211
goal_identified
=== ep: 3212, time 50.85076594352722, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3212
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3213, time 51.812312841415405, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2256
goal_identified
goal_identified
goal_identified
=== ep: 3214, time 53.67641019821167, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3214
goal_identified
goal_identified
=== ep: 3215, time 50.16030669212341, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3215
goal_identified
goal_identified
goal_identified
=== ep: 3216, time 48.1601619720459, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3216
goal_identified
goal_identified
=== ep: 3217, time 51.28903388977051, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3217
goal_identified
=== ep: 3218, time 51.77663064002991, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3218
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3219, time 50.750362157821655, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3220, time 52.55652642250061, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3220
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3221, time 50.71634340286255, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3221
goal_identified
goal_identified
=== ep: 3222, time 57.20410513877869, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3222
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3223, time 47.8496470451355, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3223
goal_identified
goal_identified
goal_identified
=== ep: 3224, time 51.44600033760071, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3224
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3225, time 52.102250814437866, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3226, time 50.311829805374146, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3226
=== ep: 3227, time 51.08128643035889, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3227
goal_identified
goal_identified
goal_identified
=== ep: 3228, time 46.78289556503296, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3228
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3229, time 52.31794309616089, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3229
goal_identified
goal_identified
goal_identified
=== ep: 3230, time 54.78155827522278, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3230
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3231, time 52.202698707580566, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3231
goal_identified
goal_identified
goal_identified
=== ep: 3232, time 51.094178676605225, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3232
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3233, time 44.531880140304565, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3233
goal_identified
goal_identified
=== ep: 3234, time 54.18393111228943, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3235, time 55.16869044303894, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3236, time 52.303858518600464, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3236
goal_identified
=== ep: 3237, time 42.78863024711609, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3237
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3238, time 54.34513330459595, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3238
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3239, time 56.74033856391907, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3239
goal_identified
goal_identified
goal_identified
=== ep: 3240, time 54.33877515792847, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3240
goal_identified
=== ep: 3241, time 42.6661274433136, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3241
goal_identified
goal_identified
=== ep: 3242, time 54.355040073394775, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3242
goal_identified
goal_identified
=== ep: 3243, time 56.66805577278137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3244, time 52.86278295516968, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3244
=== ep: 3245, time 42.65680766105652, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3245
goal_identified
goal_identified
=== ep: 3246, time 52.352784872055054, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3246
=== ep: 3247, time 61.18031311035156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3247
goal_identified
=== ep: 3248, time 53.51709032058716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3248
goal_identified
goal_identified
goal_identified
=== ep: 3249, time 42.7741162776947, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3249
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3250, time 54.65210270881653, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3250
goal_identified
=== ep: 3251, time 56.027642250061035, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3251
=== ep: 3252, time 42.68838691711426, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3252
goal_identified
goal_identified
goal_identified
=== ep: 3253, time 51.618364334106445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3253
=== ep: 3254, time 55.93724584579468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3254
goal_identified
goal_identified
=== ep: 3255, time 52.7521071434021, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3255
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3256, time 42.615604877471924, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3256
=== ep: 3257, time 55.7849383354187, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3257
goal_identified
=== ep: 3258, time 53.42591857910156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3258
goal_identified
goal_identified
=== ep: 3259, time 53.98270082473755, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3259
goal_identified
goal_identified
=== ep: 3260, time 48.43445014953613, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3260
goal_identified
goal_identified
=== ep: 3261, time 49.66015291213989, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3261
goal_identified
goal_identified
=== ep: 3262, time 52.38988447189331, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3262
goal_identified
goal_identified
goal_identified
=== ep: 3263, time 54.22988796234131, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3263
goal_identified
=== ep: 3264, time 54.627832889556885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3264
goal_identified
=== ep: 3265, time 51.406243085861206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3265
goal_identified
goal_identified
goal_identified
=== ep: 3266, time 47.218661308288574, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3266
=== ep: 3267, time 57.95828938484192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3267
=== ep: 3268, time 55.224830865859985, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3268
goal_identified
=== ep: 3269, time 54.96493887901306, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3269
=== ep: 3270, time 49.98700189590454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3270
=== ep: 3271, time 58.01007652282715, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3271
=== ep: 3272, time 54.47750234603882, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3272
goal_identified
=== ep: 3273, time 52.161487340927124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3273
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3274, time 49.132402181625366, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3274
=== ep: 3275, time 55.081257820129395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3275
goal_identified
goal_identified
goal_identified
=== ep: 3276, time 52.637309551239014, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3276
goal_identified
goal_identified
=== ep: 3277, time 52.686386585235596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3277
goal_identified
goal_identified
goal_identified
=== ep: 3278, time 49.19706630706787, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3278
=== ep: 3279, time 47.619208574295044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3279
goal_identified
goal_identified
=== ep: 3280, time 53.85717749595642, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3280
goal_identified
=== ep: 3281, time 52.338897943496704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3281
goal_identified
goal_identified
goal_identified
=== ep: 3282, time 50.92253255844116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3282
goal_identified
goal_identified
goal_identified
=== ep: 3283, time 47.55648398399353, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3283
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3284, time 50.584721088409424, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3284
goal_identified
=== ep: 3285, time 53.04303240776062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3285
=== ep: 3286, time 52.391412019729614, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3286
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3287, time 50.022820711135864, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3287
=== ep: 3288, time 50.81798267364502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3288
goal_identified
=== ep: 3289, time 54.30214285850525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3289
goal_identified
=== ep: 3290, time 52.505417346954346, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3290
goal_identified
=== ep: 3291, time 52.28840112686157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3291
goal_identified
goal_identified
=== ep: 3292, time 51.4321768283844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3292
goal_identified
goal_identified
=== ep: 3293, time 51.46180558204651, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3293
goal_identified
=== ep: 3294, time 51.57564663887024, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3294
goal_identified
goal_identified
=== ep: 3295, time 52.12683582305908, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3295
goal_identified
=== ep: 3296, time 48.19334411621094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 21/21)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3296
goal_identified
goal_identified
=== ep: 3297, time 54.09193682670593, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3298, time 47.3840708732605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3298
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3299, time 52.77532148361206, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3299
goal_identified
goal_identified
=== ep: 3300, time 54.328588008880615, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3300
goal_identified
goal_identified
goal_identified
=== ep: 3301, time 52.419713497161865, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3301
goal_identified
goal_identified
=== ep: 3302, time 45.87793517112732, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3302
goal_identified
goal_identified
=== ep: 3303, time 52.703779220581055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3303
goal_identified
=== ep: 3304, time 56.75629234313965, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3304
goal_identified
goal_identified
=== ep: 3305, time 53.669254779815674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3306, time 44.41839909553528, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3306
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3307, time 51.24993133544922, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3307
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3308, time 55.88663125038147, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3308
goal_identified
goal_identified
=== ep: 3309, time 49.83629846572876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3309
goal_identified
goal_identified
goal_identified
=== ep: 3310, time 44.22883176803589, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3310
goal_identified
=== ep: 3311, time 58.36335253715515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3311
goal_identified
goal_identified
=== ep: 3312, time 55.29211091995239, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3312
goal_identified
=== ep: 3313, time 42.93820571899414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3313
goal_identified
goal_identified
=== ep: 3314, time 53.38635182380676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3314
goal_identified
goal_identified
goal_identified
=== ep: 3315, time 55.94816970825195, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3315
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3316, time 47.00960445404053, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3316
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3317, time 47.028650522232056, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 54.38885974884033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3318
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3319, time 53.577773094177246, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3319
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3320, time 45.67724275588989, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3320
goal_identified
=== ep: 3321, time 51.272032260894775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3321
goal_identified
=== ep: 3322, time 54.91788125038147, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3322
=== ep: 3323, time 58.12416434288025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3323
=== ep: 3324, time 53.155192852020264, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3324
goal_identified
goal_identified
=== ep: 3325, time 51.711604833602905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3325
=== ep: 3326, time 50.457401514053345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3326
goal_identified
=== ep: 3327, time 51.84108924865723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3327
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3328, time 51.6834282875061, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3328
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3329, time 52.46286129951477, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3329
goal_identified
goal_identified
goal_identified
=== ep: 3330, time 52.0499906539917, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3330
goal_identified
goal_identified
=== ep: 3331, time 53.38432240486145, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3331
goal_identified
goal_identified
=== ep: 3332, time 51.024431467056274, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3332
goal_identified
goal_identified
=== ep: 3333, time 50.32673931121826, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3333
goal_identified
goal_identified
goal_identified
=== ep: 3334, time 50.244343996047974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3335, time 48.921422481536865, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3335
goal_identified
goal_identified
goal_identified
=== ep: 3336, time 51.24730134010315, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3336
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3337, time 51.22352862358093, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3337
goal_identified
goal_identified
=== ep: 3338, time 48.35752987861633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3338
goal_identified
goal_identified
=== ep: 3339, time 47.41513156890869, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3339
goal_identified
=== ep: 3340, time 54.820561170578, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3340
=== ep: 3341, time 53.62173843383789, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3341
=== ep: 3342, time 55.59628677368164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3342
goal_identified
goal_identified
=== ep: 3343, time 51.003650188446045, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3343
goal_identified
=== ep: 3344, time 46.98073720932007, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3344
goal_identified
goal_identified
goal_identified
=== ep: 3345, time 52.44096755981445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3345
goal_identified
=== ep: 3346, time 56.20503568649292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3346
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3347, time 53.51156449317932, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3348, time 51.23884057998657, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3349, time 45.04863715171814, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3349
goal_identified
=== ep: 3350, time 57.010968923568726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3350
goal_identified
goal_identified
=== ep: 3351, time 54.605149269104004, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3351
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3352, time 53.32866907119751, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3352
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3353, time 51.61695742607117, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2441
goal_identified
goal_identified
=== ep: 3354, time 45.784634828567505, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3354
goal_identified
goal_identified
=== ep: 3355, time 53.44198155403137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3355
goal_identified
goal_identified
goal_identified
=== ep: 3356, time 54.41736555099487, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3356
goal_identified
goal_identified
goal_identified
=== ep: 3357, time 54.40251040458679, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3357
goal_identified
goal_identified
goal_identified
=== ep: 3358, time 46.954185962677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3358
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3359, time 49.12139892578125, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3359
goal_identified
=== ep: 3360, time 57.3759024143219, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3360
goal_identified
goal_identified
=== ep: 3361, time 54.962878465652466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3361
goal_identified
=== ep: 3362, time 42.22314405441284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3362
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3363, time 53.10242033004761, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3363
goal_identified
goal_identified
=== ep: 3364, time 56.95039701461792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3364
=== ep: 3365, time 45.8187689781189, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3365
=== ep: 3366, time 47.87594509124756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3366
goal_identified
goal_identified
=== ep: 3367, time 55.657979249954224, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3367
goal_identified
goal_identified
=== ep: 3368, time 54.306933641433716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3368
goal_identified
goal_identified
goal_identified
=== ep: 3369, time 43.01826786994934, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3369
goal_identified
=== ep: 3370, time 53.750840187072754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3370
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3371, time 57.9630024433136, eps 0.001, sum reward: 4, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3371
goal_identified
goal_identified
=== ep: 3372, time 51.55078172683716, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3373, time 43.125855922698975, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3373
goal_identified
=== ep: 3374, time 54.18315672874451, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3374
goal_identified
goal_identified
=== ep: 3375, time 54.71160101890564, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3375
goal_identified
goal_identified
=== ep: 3376, time 58.81180191040039, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3376
goal_identified
goal_identified
=== ep: 3377, time 49.91277503967285, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3377
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3378, time 45.47844910621643, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3379, time 52.0363667011261, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3379
goal_identified
goal_identified
=== ep: 3380, time 53.56787037849426, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3380
goal_identified
goal_identified
=== ep: 3381, time 52.79739570617676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3381
goal_identified
goal_identified
=== ep: 3382, time 45.76354622840881, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3382
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3383, time 50.11639189720154, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3383
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3384, time 53.939547538757324, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3384
goal_identified
goal_identified
=== ep: 3385, time 54.95845103263855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3385
goal_identified
goal_identified
=== ep: 3386, time 52.56216478347778, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3386
goal_identified
=== ep: 3387, time 48.11422038078308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3387
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3388, time 50.583298206329346, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3388
goal_identified
goal_identified
goal_identified
=== ep: 3389, time 49.652052879333496, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3389
goal_identified
goal_identified
=== ep: 3390, time 50.82472372055054, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3390
goal_identified
goal_identified
=== ep: 3391, time 45.89681100845337, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3391
goal_identified
goal_identified
=== ep: 3392, time 50.8095006942749, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3392
goal_identified
goal_identified
goal_identified
=== ep: 3393, time 50.42048788070679, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3393
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3394, time 49.45291876792908, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3394
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3395, time 50.95517873764038, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3395
goal_identified
=== ep: 3396, time 51.24244952201843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3397, time 50.16261529922485, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2443
goal_identified
goal_identified
goal_identified
=== ep: 3398, time 48.92288303375244, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3398
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3399, time 50.45912957191467, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3399
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3400, time 51.219096422195435, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3400
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3401, time 51.28989219665527, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3402, time 49.71580791473389, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3402
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3403, time 45.458091259002686, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3403
goal_identified
=== ep: 3404, time 54.41068172454834, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3404
goal_identified
goal_identified
goal_identified
=== ep: 3405, time 51.44339084625244, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3405
goal_identified
goal_identified
goal_identified
=== ep: 3406, time 52.247209787368774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3406
goal_identified
goal_identified
goal_identified
=== ep: 3407, time 49.36520051956177, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3408, time 45.09006071090698, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3408
goal_identified
goal_identified
goal_identified
=== ep: 3409, time 51.123364210128784, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3409
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3410, time 52.78029489517212, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3411, time 50.02425265312195, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3412, time 45.94140386581421, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3412
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3413, time 51.64419102668762, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3414, time 54.38065528869629, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3414
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3415, time 52.080925941467285, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3415
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3416, time 44.13049292564392, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2608
goal_identified
goal_identified
goal_identified
=== ep: 3417, time 51.041364669799805, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3417
goal_identified
goal_identified
=== ep: 3418, time 53.49276280403137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3418
goal_identified
goal_identified
goal_identified
=== ep: 3419, time 51.62152910232544, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3419
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3420, time 45.04087162017822, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3420
goal_identified
goal_identified
goal_identified
=== ep: 3421, time 48.7927360534668, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3421
goal_identified
goal_identified
goal_identified
=== ep: 3422, time 53.41938805580139, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3423, time 51.49266862869263, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3423
goal_identified
goal_identified
goal_identified
=== ep: 3424, time 43.926533937454224, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3424
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3425, time 50.24753022193909, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3425
goal_identified
goal_identified
=== ep: 3426, time 53.49820923805237, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3426
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3427, time 50.86856937408447, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3427
goal_identified
goal_identified
goal_identified
=== ep: 3428, time 41.00466585159302, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3428
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3429, time 54.44405746459961, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3430, time 54.50993013381958, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3431, time 41.33960723876953, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3432, time 53.4457426071167, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3432
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3433, time 59.209577322006226, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3433
goal_identified
=== ep: 3434, time 44.14166712760925, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3434
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3435, time 47.92263746261597, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3436, time 53.77075529098511, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3436
goal_identified
goal_identified
=== ep: 3437, time 44.262961626052856, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3437
goal_identified
goal_identified
goal_identified
=== ep: 3438, time 47.467530250549316, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3438
goal_identified
goal_identified
=== ep: 3439, time 53.8854718208313, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3439
goal_identified
goal_identified
goal_identified
=== ep: 3440, time 50.28039479255676, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3440
goal_identified
goal_identified
goal_identified
=== ep: 3441, time 42.426016092300415, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3441
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3442, time 53.560949087142944, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3442
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3443, time 54.71639680862427, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3443
goal_identified
goal_identified
=== ep: 3444, time 46.033549785614014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3444
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3445, time 47.99761605262756, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3446, time 53.9808931350708, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3446
goal_identified
goal_identified
goal_identified
=== ep: 3447, time 55.00648379325867, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 53.512059450149536, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3448
goal_identified
goal_identified
=== ep: 3449, time 44.37091588973999, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3449
goal_identified
goal_identified
=== ep: 3450, time 52.68270492553711, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3450
goal_identified
goal_identified
=== ep: 3451, time 55.51394295692444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3451
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3452, time 53.31383275985718, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2795
goal_identified
goal_identified
goal_identified
=== ep: 3453, time 43.69139814376831, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3453
goal_identified
goal_identified
goal_identified
=== ep: 3454, time 52.11155724525452, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3454
goal_identified
goal_identified
goal_identified
=== ep: 3455, time 54.016610860824585, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3455
goal_identified
goal_identified
goal_identified
=== ep: 3456, time 52.39055347442627, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3457, time 51.10391354560852, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2837
goal_identified
goal_identified
goal_identified
=== ep: 3458, time 48.26108121871948, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3458
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3459, time 50.966721296310425, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3459
goal_identified
goal_identified
=== ep: 3460, time 52.21836876869202, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3460
goal_identified
goal_identified
goal_identified
=== ep: 3461, time 55.72599482536316, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3461
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3462, time 51.18813943862915, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3463, time 51.492217779159546, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3463
goal_identified
=== ep: 3464, time 51.542139530181885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3464
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3465, time 51.248544454574585, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3465
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3466, time 52.43423104286194, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3466
goal_identified
goal_identified
goal_identified
=== ep: 3467, time 51.46655225753784, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3467
goal_identified
=== ep: 3468, time 51.83638644218445, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3468
goal_identified
=== ep: 3469, time 51.70340132713318, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3469
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3470, time 51.029189348220825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3470
goal_identified
=== ep: 3471, time 45.25741648674011, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3471
goal_identified
=== ep: 3472, time 53.58686375617981, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3472
goal_identified
goal_identified
=== ep: 3473, time 53.82474493980408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3473
goal_identified
goal_identified
goal_identified
=== ep: 3474, time 53.79371786117554, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3474
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3475, time 46.21349263191223, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3475
goal_identified
goal_identified
goal_identified
=== ep: 3476, time 49.79622173309326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3476
goal_identified
goal_identified
=== ep: 3477, time 54.51703476905823, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 53.68121266365051, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3478
goal_identified
goal_identified
=== ep: 3479, time 45.77873754501343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3479
goal_identified
goal_identified
=== ep: 3480, time 50.15090799331665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3480
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3481, time 55.54721236228943, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3481
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3482, time 52.74271273612976, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3482
goal_identified
goal_identified
goal_identified
=== ep: 3483, time 42.76347827911377, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3483
goal_identified
goal_identified
=== ep: 3484, time 54.172492027282715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3484
goal_identified
goal_identified
goal_identified
=== ep: 3485, time 56.00006318092346, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3485
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3486, time 50.24638032913208, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3486
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3487, time 44.31042957305908, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3488, time 55.46703505516052, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3489, time 60.78781867027283, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3489
goal_identified
goal_identified
=== ep: 3490, time 46.55503869056702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3491, time 47.93838810920715, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3491
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3492, time 56.11755394935608, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3492
goal_identified
=== ep: 3493, time 49.94300723075867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3493
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3494, time 43.45200252532959, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3494
goal_identified
goal_identified
=== ep: 3495, time 55.42785573005676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3495
goal_identified
=== ep: 3496, time 54.8763165473938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3496
goal_identified
goal_identified
=== ep: 3497, time 44.94826149940491, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3497
goal_identified
goal_identified
goal_identified
=== ep: 3498, time 49.96266841888428, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3498
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3499, time 54.52033019065857, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3499
goal_identified
goal_identified
=== ep: 3500, time 53.89922642707825, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3500
goal_identified
goal_identified
=== ep: 3501, time 47.205960273742676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3502, time 49.343059062957764, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3502
goal_identified
goal_identified
goal_identified
=== ep: 3503, time 52.87544012069702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3503
goal_identified
goal_identified
=== ep: 3504, time 52.62330627441406, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3504
goal_identified
goal_identified
=== ep: 3505, time 51.73761558532715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3505
goal_identified
goal_identified
goal_identified
=== ep: 3506, time 49.22184944152832, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3506
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3507, time 48.751967430114746, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3507
goal_identified
=== ep: 3508, time 51.59500479698181, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3508
goal_identified
goal_identified
=== ep: 3509, time 50.51770281791687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3509
goal_identified
goal_identified
=== ep: 3510, time 51.97073411941528, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3510
goal_identified
goal_identified
goal_identified
=== ep: 3511, time 51.66653275489807, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3512, time 52.222206592559814, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3512
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3513, time 51.514511585235596, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3514, time 52.291664361953735, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3514
goal_identified
goal_identified
goal_identified
=== ep: 3515, time 51.870213985443115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3515
goal_identified
goal_identified
goal_identified
=== ep: 3516, time 52.93497896194458, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3516
goal_identified
goal_identified
=== ep: 3517, time 59.419440507888794, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3517
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3518, time 55.258792877197266, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3518
goal_identified
goal_identified
=== ep: 3519, time 53.765141010284424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3520, time 46.05307626724243, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3520
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3521, time 54.46039867401123, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3521
goal_identified
goal_identified
=== ep: 3522, time 57.84207272529602, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3523, time 55.0419716835022, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3523
goal_identified
goal_identified
goal_identified
=== ep: 3524, time 45.873677253723145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3524
goal_identified
goal_identified
goal_identified
=== ep: 3525, time 52.75103807449341, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3525
goal_identified
goal_identified
goal_identified
=== ep: 3526, time 54.9773633480072, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3526
goal_identified
goal_identified
goal_identified
=== ep: 3527, time 53.04727792739868, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3527
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3528, time 51.83772826194763, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3528
goal_identified
goal_identified
goal_identified
=== ep: 3529, time 53.04216241836548, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3529
goal_identified
=== ep: 3530, time 52.08677339553833, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3530
goal_identified
goal_identified
=== ep: 3531, time 54.203045129776, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3531
goal_identified
goal_identified
goal_identified
=== ep: 3532, time 52.680255651474, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3532
goal_identified
goal_identified
goal_identified
=== ep: 3533, time 50.47067999839783, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3533
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3534, time 47.92932724952698, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3534
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3535, time 55.544705867767334, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3535
goal_identified
goal_identified
goal_identified
=== ep: 3536, time 57.754067182540894, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3536
goal_identified
goal_identified
=== ep: 3537, time 53.003188610076904, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3538, time 46.407625675201416, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3538
goal_identified
=== ep: 3539, time 54.41030478477478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3539
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3540, time 56.393165826797485, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3540
goal_identified
goal_identified
goal_identified
=== ep: 3541, time 54.955071210861206, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3542, time 50.62559223175049, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3542
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3543, time 49.91209816932678, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3543
goal_identified
goal_identified
goal_identified
=== ep: 3544, time 52.602352142333984, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3544
goal_identified
goal_identified
goal_identified
=== ep: 3545, time 58.44099020957947, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3545
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3546, time 47.22426509857178, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3546
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3547, time 53.25749158859253, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3547
goal_identified
goal_identified
goal_identified
=== ep: 3548, time 56.93143939971924, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3548
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3549, time 55.86854290962219, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3549
goal_identified
goal_identified
goal_identified
=== ep: 3550, time 50.46378993988037, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3550
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3551, time 47.94660139083862, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3551
goal_identified
goal_identified
goal_identified
=== ep: 3552, time 55.859835624694824, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3552
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3553, time 57.13378977775574, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3554, time 53.9995858669281, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3554
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3555, time 47.463590145111084, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3555
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3556, time 53.06581974029541, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3556
goal_identified
goal_identified
=== ep: 3557, time 53.26164221763611, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3557
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3558, time 52.085429191589355, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3558
=== ep: 3559, time 51.31794214248657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3559
goal_identified
goal_identified
=== ep: 3560, time 54.39972949028015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3560
goal_identified
goal_identified
=== ep: 3561, time 56.41764211654663, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3561
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3562, time 56.06480860710144, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3562
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3563, time 55.80272364616394, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3563
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3564, time 48.925766706466675, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3564
goal_identified
=== ep: 3565, time 50.43407106399536, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3565
goal_identified
=== ep: 3566, time 57.75720453262329, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3566
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3567, time 56.94398832321167, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3567
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3568, time 49.91620635986328, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3568
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3569, time 48.46199035644531, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3569
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3570, time 55.46077513694763, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2913
goal_identified
goal_identified
=== ep: 3571, time 53.481353759765625, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3572, time 51.05274224281311, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3572
goal_identified
goal_identified
=== ep: 3573, time 57.050965309143066, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3574, time 53.88701105117798, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3574
goal_identified
goal_identified
goal_identified
=== ep: 3575, time 53.9819860458374, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3575
goal_identified
goal_identified
goal_identified
=== ep: 3576, time 53.777754068374634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3576
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3577, time 54.16056561470032, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3577
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3578, time 49.309075593948364, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3578
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3579, time 49.75159430503845, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3579
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3580, time 56.25169062614441, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3580
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3581, time 57.06407833099365, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3582, time 53.69657754898071, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3582
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3583, time 45.488611698150635, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3583
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3584, time 54.04891490936279, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3584
goal_identified
goal_identified
=== ep: 3585, time 57.387840032577515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3586, time 55.52263259887695, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3587, time 49.17963647842407, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 50.16480374336243, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3588
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3589, time 53.40297293663025, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3589
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3590, time 51.80223083496094, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3591, time 49.86160731315613, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 54.53043031692505, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3592
goal_identified
goal_identified
goal_identified
=== ep: 3593, time 54.47095036506653, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3593
goal_identified
goal_identified
=== ep: 3594, time 53.4336256980896, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3594
goal_identified
goal_identified
goal_identified
=== ep: 3595, time 49.03070569038391, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3595
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3596, time 50.09373879432678, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3596
goal_identified
=== ep: 3597, time 56.16527962684631, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3597
goal_identified
goal_identified
goal_identified
=== ep: 3598, time 57.13204789161682, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3598
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3599, time 52.644758224487305, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3600, time 45.39256238937378, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3600
goal_identified
goal_identified
goal_identified
=== ep: 3601, time 56.10421705245972, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3601
goal_identified
=== ep: 3602, time 61.10635280609131, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3602
goal_identified
goal_identified
goal_identified
=== ep: 3603, time 55.806450605392456, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3603
goal_identified
goal_identified
=== ep: 3604, time 52.84166955947876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3604
goal_identified
goal_identified
=== ep: 3605, time 49.17662692070007, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3605
goal_identified
goal_identified
goal_identified
=== ep: 3606, time 53.509480476379395, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3606
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3607, time 53.43282389640808, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3607
goal_identified
goal_identified
goal_identified
=== ep: 3608, time 47.444114685058594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3608
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3609, time 52.98956775665283, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3609
goal_identified
goal_identified
goal_identified
=== ep: 3610, time 56.290592193603516, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3611, time 55.35102462768555, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3611
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3612, time 48.728755712509155, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3612
goal_identified
goal_identified
=== ep: 3613, time 48.94053792953491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3614, time 56.39803457260132, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3614
goal_identified
goal_identified
goal_identified
=== ep: 3615, time 56.52767467498779, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3615
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3616, time 48.817415952682495, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3616
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3617, time 48.80238485336304, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3617
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3618, time 53.81471920013428, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3618
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3619, time 52.395323753356934, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3619
goal_identified
goal_identified
goal_identified
=== ep: 3620, time 50.76535153388977, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3620
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3621, time 54.68106722831726, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3622, time 55.8850998878479, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3622
goal_identified
goal_identified
goal_identified
=== ep: 3623, time 54.2764937877655, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3624, time 49.134246587753296, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3624
goal_identified
goal_identified
goal_identified
=== ep: 3625, time 48.6352698802948, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3626, time 56.731590032577515, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3626
goal_identified
goal_identified
goal_identified
=== ep: 3627, time 56.935882806777954, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3627
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3628, time 49.61985802650452, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3628
goal_identified
goal_identified
=== ep: 3629, time 48.72242617607117, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3629
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3630, time 55.274914264678955, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2932
goal_identified
goal_identified
goal_identified
=== ep: 3631, time 60.206204414367676, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3631
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3632, time 55.629382848739624, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3632
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3633, time 53.17334604263306, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3633
goal_identified
goal_identified
=== ep: 3634, time 51.38110685348511, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3634
goal_identified
goal_identified
=== ep: 3635, time 53.561997413635254, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3636, time 53.62918305397034, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3637, time 47.978174448013306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3637
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3638, time 50.67244815826416, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3638
=== ep: 3639, time 55.378352880477905, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3639
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3640, time 56.113587856292725, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3640
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3641, time 51.31442594528198, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3004
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3642, time 46.56328892707825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3643, time 56.25298523902893, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3643
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3644, time 57.17169690132141, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3645, time 52.3524694442749, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3645
goal_identified
goal_identified
=== ep: 3646, time 45.94186878204346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3646
goal_identified
goal_identified
goal_identified
=== ep: 3647, time 56.07763981819153, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3647
goal_identified
goal_identified
goal_identified
=== ep: 3648, time 56.917805433273315, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3648
goal_identified
goal_identified
goal_identified
=== ep: 3649, time 50.08513069152832, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3649
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3650, time 49.25988793373108, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3650
goal_identified
goal_identified
goal_identified
=== ep: 3651, time 54.50122833251953, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 55.141605854034424, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3652
goal_identified
goal_identified
goal_identified
=== ep: 3653, time 54.47459387779236, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3653
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3654, time 52.60843300819397, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3654
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3655, time 50.7587947845459, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3655
goal_identified
goal_identified
=== ep: 3656, time 52.69738054275513, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3656
goal_identified
goal_identified
=== ep: 3657, time 51.82440233230591, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3657
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3658, time 51.869300365448, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3658
goal_identified
=== ep: 3659, time 53.277854681015015, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3659
goal_identified
goal_identified
goal_identified
=== ep: 3660, time 59.551929235458374, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3660
goal_identified
=== ep: 3661, time 53.966203689575195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3661
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3662, time 55.11133122444153, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3662
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3663, time 54.269853591918945, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3663
goal_identified
goal_identified
=== ep: 3664, time 46.00021815299988, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3664
goal_identified
goal_identified
goal_identified
=== ep: 3665, time 52.73403072357178, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3665
goal_identified
goal_identified
=== ep: 3666, time 57.195563554763794, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3666
goal_identified
goal_identified
=== ep: 3667, time 55.6595823764801, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3667
goal_identified
=== ep: 3668, time 45.307817697525024, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3668
goal_identified
=== ep: 3669, time 54.56883978843689, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3669
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3670, time 58.94446039199829, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3670
goal_identified
goal_identified
=== ep: 3671, time 54.71295118331909, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3672, time 48.39108061790466, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3672
goal_identified
goal_identified
=== ep: 3673, time 50.63302302360535, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3673
goal_identified
goal_identified
goal_identified
=== ep: 3674, time 56.521235704422, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3674
goal_identified
goal_identified
goal_identified
=== ep: 3675, time 54.6768856048584, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3675
goal_identified
goal_identified
goal_identified
=== ep: 3676, time 49.087878465652466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3676
goal_identified
goal_identified
=== ep: 3677, time 51.27414011955261, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3678, time 55.69861149787903, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3678
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3679, time 54.500338315963745, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3679
goal_identified
goal_identified
=== ep: 3680, time 53.59677767753601, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3680
goal_identified
goal_identified
goal_identified
=== ep: 3681, time 51.315712451934814, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3681
goal_identified
goal_identified
=== ep: 3682, time 49.97414302825928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3682
goal_identified
goal_identified
goal_identified
=== ep: 3683, time 53.52786660194397, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3683
goal_identified
goal_identified
goal_identified
=== ep: 3684, time 52.73583364486694, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3684
goal_identified
goal_identified
=== ep: 3685, time 53.29894208908081, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3685
goal_identified
goal_identified
=== ep: 3686, time 53.834739446640015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3686
goal_identified
goal_identified
=== ep: 3687, time 53.698368310928345, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3687
goal_identified
goal_identified
goal_identified
=== ep: 3688, time 54.352640867233276, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3688
goal_identified
=== ep: 3689, time 59.605971813201904, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3689
=== ep: 3690, time 51.66854739189148, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3690
goal_identified
goal_identified
=== ep: 3691, time 49.70524978637695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3691
=== ep: 3692, time 54.81793761253357, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3692
goal_identified
=== ep: 3693, time 56.1543185710907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3694, time 56.0538969039917, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3694
goal_identified
=== ep: 3695, time 57.14340400695801, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3695
goal_identified
goal_identified
=== ep: 3696, time 51.095036029815674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3696
goal_identified
goal_identified
=== ep: 3697, time 51.099401235580444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3697
=== ep: 3698, time 56.26044249534607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3698
goal_identified
=== ep: 3699, time 57.100316524505615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3699
goal_identified
goal_identified
=== ep: 3700, time 55.64735674858093, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3700
goal_identified
goal_identified
goal_identified
=== ep: 3701, time 47.55889964103699, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3701
goal_identified
goal_identified
goal_identified
=== ep: 3702, time 53.32470631599426, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3702
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3703, time 57.251270055770874, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3703
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3704, time 55.2945613861084, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3704
goal_identified
goal_identified
goal_identified
=== ep: 3705, time 45.698588848114014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3706, time 56.49151945114136, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3706
goal_identified
goal_identified
=== ep: 3707, time 60.059858560562134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3707
goal_identified
goal_identified
=== ep: 3708, time 54.59951090812683, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3708
goal_identified
goal_identified
=== ep: 3709, time 47.13776350021362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3709
goal_identified
goal_identified
goal_identified
=== ep: 3710, time 54.45835089683533, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3710
goal_identified
=== ep: 3711, time 56.88825082778931, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3711
goal_identified
=== ep: 3712, time 56.86317253112793, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3712
goal_identified
goal_identified
=== ep: 3713, time 54.46743202209473, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3713
goal_identified
goal_identified
goal_identified
=== ep: 3714, time 49.59626269340515, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3714
goal_identified
=== ep: 3715, time 54.25107383728027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3715
goal_identified
goal_identified
goal_identified
=== ep: 3716, time 54.38738489151001, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3716
goal_identified
=== ep: 3717, time 57.285948514938354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3717
goal_identified
goal_identified
=== ep: 3718, time 52.78147578239441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3718
goal_identified
goal_identified
=== ep: 3719, time 56.38818621635437, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3719
goal_identified
=== ep: 3720, time 55.364243507385254, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3720
goal_identified
goal_identified
goal_identified
=== ep: 3721, time 54.6996705532074, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3721
goal_identified
=== ep: 3722, time 54.6886260509491, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3722
goal_identified
=== ep: 3723, time 51.65009570121765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3723
goal_identified
=== ep: 3724, time 51.12806248664856, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3724
=== ep: 3725, time 56.2803168296814, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3725
goal_identified
goal_identified
=== ep: 3726, time 57.63157343864441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3726
=== ep: 3727, time 57.0307183265686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3727
goal_identified
goal_identified
goal_identified
=== ep: 3728, time 49.81476664543152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3728
goal_identified
goal_identified
=== ep: 3729, time 50.72330975532532, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3729
goal_identified
goal_identified
goal_identified
=== ep: 3730, time 58.51126718521118, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3730
goal_identified
goal_identified
=== ep: 3731, time 58.47147989273071, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3731
goal_identified
goal_identified
goal_identified
=== ep: 3732, time 51.02580237388611, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3732
goal_identified
=== ep: 3733, time 51.4195032119751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3733
=== ep: 3734, time 57.562368869781494, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3734
goal_identified
=== ep: 3735, time 57.50247597694397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3735
goal_identified
goal_identified
=== ep: 3736, time 55.70961904525757, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3736
goal_identified
=== ep: 3737, time 49.86242151260376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3737
goal_identified
goal_identified
=== ep: 3738, time 51.984251499176025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3738
goal_identified
=== ep: 3739, time 55.06529974937439, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3739
goal_identified
goal_identified
=== ep: 3740, time 52.300294399261475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3740
=== ep: 3741, time 53.11290740966797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3741
=== ep: 3742, time 56.33088302612305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3742
=== ep: 3743, time 56.38176918029785, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3743
goal_identified
goal_identified
=== ep: 3744, time 56.416237354278564, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3744
goal_identified
goal_identified
=== ep: 3745, time 59.04074215888977, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3745
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3746, time 51.5866813659668, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3746
goal_identified
=== ep: 3747, time 49.98115658760071, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 58.64929819107056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3748
=== ep: 3749, time 58.68307137489319, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3749
goal_identified
=== ep: 3750, time 52.77618217468262, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 48.15238666534424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3751
=== ep: 3752, time 58.8914475440979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3752
goal_identified
goal_identified
goal_identified
=== ep: 3753, time 59.42828345298767, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3753
goal_identified
=== ep: 3754, time 54.65926790237427, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3754
=== ep: 3755, time 48.03259587287903, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3755
=== ep: 3756, time 56.10485553741455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 33/33)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3756
goal_identified
=== ep: 3757, time 57.75802540779114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3757
goal_identified
=== ep: 3758, time 57.57782793045044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3758
=== ep: 3759, time 51.48572063446045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3759
goal_identified
=== ep: 3760, time 54.027958393096924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3760
goal_identified
goal_identified
=== ep: 3761, time 55.79183912277222, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3761
goal_identified
=== ep: 3762, time 54.07376766204834, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3762
goal_identified
goal_identified
=== ep: 3763, time 50.21936869621277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3763
goal_identified
=== ep: 3764, time 54.95838260650635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3764
goal_identified
=== ep: 3765, time 57.98681950569153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3765
goal_identified
=== ep: 3766, time 58.2635064125061, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3766
goal_identified
goal_identified
=== ep: 3767, time 55.51066780090332, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3767
goal_identified
goal_identified
=== ep: 3768, time 46.9503710269928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3768
goal_identified
goal_identified
goal_identified
=== ep: 3769, time 55.47889256477356, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3769
goal_identified
goal_identified
=== ep: 3770, time 59.63438868522644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3770
=== ep: 3771, time 56.53034234046936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3771
goal_identified
=== ep: 3772, time 56.543123722076416, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3772
goal_identified
=== ep: 3773, time 49.87974166870117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3773
goal_identified
=== ep: 3774, time 57.49820685386658, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3774
=== ep: 3775, time 57.15091061592102, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3775
goal_identified
=== ep: 3776, time 54.166913986206055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3776
=== ep: 3777, time 50.16003370285034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3777
=== ep: 3778, time 54.77448081970215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3778
goal_identified
goal_identified
=== ep: 3779, time 55.23538017272949, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 52.266124963760376, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3780
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3781, time 52.26702427864075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3781
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3782, time 52.96052598953247, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3782
goal_identified
goal_identified
=== ep: 3783, time 51.90316200256348, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3783
goal_identified
goal_identified
=== ep: 3784, time 52.11705470085144, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3784
goal_identified
goal_identified
goal_identified
=== ep: 3785, time 49.92102313041687, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3785
goal_identified
goal_identified
goal_identified
=== ep: 3786, time 50.55587720870972, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3786
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3787, time 54.38018608093262, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3787
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 55.41348099708557, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3789, time 51.08961868286133, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3007
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3790, time 47.23050379753113, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3790
goal_identified
goal_identified
=== ep: 3791, time 56.041709184646606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3792, time 56.76715111732483, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3058
=== ep: 3793, time 50.48594617843628, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3793
goal_identified
goal_identified
goal_identified
=== ep: 3794, time 47.195069789886475, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3794
goal_identified
goal_identified
goal_identified
=== ep: 3795, time 55.83912110328674, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3795
goal_identified
goal_identified
goal_identified
=== ep: 3796, time 56.75766062736511, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3797, time 45.63895511627197, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3798, time 52.94695258140564, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3798
goal_identified
goal_identified
goal_identified
=== ep: 3799, time 56.062432050704956, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3799
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3800, time 53.58451175689697, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3087
goal_identified
goal_identified
=== ep: 3801, time 45.50233006477356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3801
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3802, time 54.528666496276855, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3802
goal_identified
goal_identified
=== ep: 3803, time 60.4973566532135, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3803
goal_identified
=== ep: 3804, time 53.7918803691864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3804
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3805, time 47.13211178779602, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3805
goal_identified
goal_identified
goal_identified
=== ep: 3806, time 51.70808792114258, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3806
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3807, time 52.380518674850464, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3807
goal_identified
goal_identified
=== ep: 3808, time 52.84854578971863, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3808
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3809, time 52.54254388809204, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3810, time 52.82336735725403, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3810
goal_identified
goal_identified
goal_identified
=== ep: 3811, time 53.065786600112915, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3811
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3812, time 53.355504274368286, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3812
goal_identified
goal_identified
goal_identified
=== ep: 3813, time 49.9296178817749, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3813
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3814, time 49.52606821060181, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3235
goal_identified
goal_identified
goal_identified
=== ep: 3815, time 54.53197121620178, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3815
goal_identified
=== ep: 3816, time 55.27262353897095, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3817, time 48.69371509552002, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3817
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3818, time 48.4460871219635, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3818
goal_identified
goal_identified
goal_identified
=== ep: 3819, time 55.70689582824707, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3820, time 54.53260803222656, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3397
goal_identified
=== ep: 3821, time 44.66798257827759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3821
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3822, time 53.23084831237793, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3822
goal_identified
goal_identified
goal_identified
=== ep: 3823, time 54.85426902770996, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3823
goal_identified
goal_identified
goal_identified
=== ep: 3824, time 52.928475856781006, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3824
goal_identified
goal_identified
goal_identified
=== ep: 3825, time 49.272392988204956, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3826, time 49.68695545196533, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3826
goal_identified
goal_identified
goal_identified
=== ep: 3827, time 51.37846350669861, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3827
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3828, time 51.38389229774475, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3828
goal_identified
goal_identified
goal_identified
=== ep: 3829, time 51.040627241134644, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3829
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3830, time 52.51357436180115, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3830
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3831, time 53.75180220603943, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3416
goal_identified
goal_identified
goal_identified
=== ep: 3832, time 52.288453102111816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3832
goal_identified
goal_identified
goal_identified
=== ep: 3833, time 46.27517127990723, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3833
goal_identified
goal_identified
=== ep: 3834, time 52.05189323425293, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3834
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3835, time 55.934481620788574, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3836, time 60.017839670181274, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3836
goal_identified
goal_identified
goal_identified
=== ep: 3837, time 47.51751923561096, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3838, time 48.597522497177124, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3838
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3839, time 55.08090853691101, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3839
goal_identified
goal_identified
=== ep: 3840, time 54.23813486099243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3840
goal_identified
goal_identified
goal_identified
=== ep: 3841, time 45.78290057182312, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 51.56487035751343, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3842
goal_identified
=== ep: 3843, time 53.087698459625244, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3843
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3844, time 53.29392862319946, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3844
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3845, time 50.81745982170105, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3845
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3846, time 51.53139686584473, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3847, time 51.74322509765625, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3847
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3848, time 48.158193826675415, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3445
goal_identified
=== ep: 3849, time 48.854422092437744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3850, time 53.231947898864746, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3850
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3851, time 52.250182151794434, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3851
goal_identified
goal_identified
goal_identified
=== ep: 3852, time 46.192569732666016, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3852
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3853, time 49.549766302108765, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3854, time 53.680737018585205, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3641
goal_identified
=== ep: 3855, time 51.46578073501587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3855
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3856, time 43.51290488243103, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3857, time 54.16843819618225, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3857
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3858, time 53.48854303359985, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3858
goal_identified
=== ep: 3859, time 44.99370193481445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3859
goal_identified
goal_identified
goal_identified
=== ep: 3860, time 49.76426458358765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3860
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3861, time 53.1876585483551, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3861
goal_identified
goal_identified
goal_identified
=== ep: 3862, time 50.60043907165527, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3862
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3863, time 47.31628465652466, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3863
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3864, time 49.86216473579407, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3865, time 49.319628953933716, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3865
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3866, time 47.02805829048157, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3866
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3867, time 50.71704435348511, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3868, time 52.444632053375244, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3868
goal_identified
goal_identified
=== ep: 3869, time 50.48693037033081, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3869
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3870, time 44.53652596473694, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3870
goal_identified
=== ep: 3871, time 51.46163272857666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3871
goal_identified
goal_identified
goal_identified
=== ep: 3872, time 53.56540298461914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3872
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3873, time 56.31817936897278, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3873
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3874, time 43.06531763076782, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3874
goal_identified
goal_identified
=== ep: 3875, time 51.43984031677246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3875
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3876, time 52.2527813911438, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3876
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3877, time 45.88982343673706, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3878, time 48.50797200202942, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3878
goal_identified
goal_identified
goal_identified
=== ep: 3879, time 50.65654706954956, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3879
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3880, time 51.881418228149414, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3789
goal_identified
goal_identified
=== ep: 3881, time 49.51186680793762, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3881
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3882, time 48.264042139053345, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3792
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3883, time 50.07136583328247, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3883
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3884, time 47.865400314331055, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3884
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3885, time 48.58817195892334, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3800
goal_identified
goal_identified
=== ep: 3886, time 51.47111463546753, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3886
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3887, time 51.39418292045593, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3888, time 47.818294048309326, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3888
goal_identified
goal_identified
=== ep: 3889, time 46.13526654243469, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3889
goal_identified
goal_identified
goal_identified
=== ep: 3890, time 53.34891724586487, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3890
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3891, time 52.7676682472229, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3891
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3892, time 43.63482117652893, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3809
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3893, time 52.0102059841156, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3893
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3894, time 52.899046897888184, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3894
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3895, time 47.476542234420776, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3820
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3896, time 46.77755904197693, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3854
goal_identified
goal_identified
=== ep: 3897, time 51.72919988632202, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3897
goal_identified
goal_identified
=== ep: 3898, time 51.30858612060547, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3899, time 49.646711587905884, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3899
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3900, time 47.23367929458618, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3900
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3901, time 49.2019784450531, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3901
goal_identified
goal_identified
=== ep: 3902, time 47.760868310928345, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3902
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3903, time 49.251160860061646, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3904, time 51.79459619522095, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3904
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3905, time 50.86549711227417, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3905
goal_identified
goal_identified
goal_identified
=== ep: 3906, time 46.684422969818115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3906
goal_identified
goal_identified
=== ep: 3907, time 49.376659870147705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3907
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3908, time 53.14439129829407, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3908
goal_identified
=== ep: 3909, time 51.63344764709473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3909
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3910, time 43.3601291179657, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3911, time 51.493335008621216, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3911
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3912, time 52.929725646972656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3912
goal_identified
goal_identified
goal_identified
=== ep: 3913, time 53.71330976486206, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3913
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3914, time 45.50397801399231, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3914
goal_identified
goal_identified
goal_identified
=== ep: 3915, time 52.78761601448059, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3915
goal_identified
=== ep: 3916, time 51.231781005859375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3916
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3917, time 45.90478277206421, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3918, time 48.36122417449951, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3880
goal_identified
goal_identified
=== ep: 3919, time 49.261911153793335, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3919
goal_identified
goal_identified
goal_identified
=== ep: 3920, time 47.06924557685852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3920
goal_identified
goal_identified
goal_identified
=== ep: 3921, time 49.72258186340332, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3921
goal_identified
goal_identified
goal_identified
=== ep: 3922, time 50.65088105201721, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3922
goal_identified
goal_identified
=== ep: 3923, time 50.291868686676025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3923
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3924, time 45.34801125526428, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3924
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3925, time 51.253116607666016, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3925
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3926, time 51.62070822715759, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3926
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3927, time 45.71391558647156, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3928, time 47.467238664627075, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3929, time 52.636842250823975, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3929
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3930, time 45.435715436935425, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3885
goal_identified
goal_identified
goal_identified
=== ep: 3931, time 48.028002977371216, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3931
goal_identified
goal_identified
=== ep: 3932, time 52.69874548912048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3932
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3933, time 47.015931129455566, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3933
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3934, time 45.7392578125, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3934
goal_identified
goal_identified
=== ep: 3935, time 51.2147901058197, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 50.9232451915741, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3936
goal_identified
goal_identified
=== ep: 3937, time 46.28089189529419, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3937
goal_identified
goal_identified
goal_identified
=== ep: 3938, time 48.471439361572266, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3938
goal_identified
goal_identified
goal_identified
=== ep: 3939, time 47.29457902908325, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3939
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3940, time 48.60908007621765, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3940
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3941, time 49.00286340713501, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3941
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3942, time 49.597352504730225, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3942
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3943, time 49.19231081008911, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3892
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3944, time 46.20623731613159, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3944
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3945, time 50.54633665084839, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3945
goal_identified
goal_identified
=== ep: 3946, time 52.31560158729553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3946
goal_identified
goal_identified
goal_identified
=== ep: 3947, time 50.89851474761963, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3948, time 43.023810625076294, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3948
goal_identified
=== ep: 3949, time 50.03547430038452, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3949
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3950, time 53.17004895210266, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3951, time 45.41579461097717, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3951
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3952, time 53.26417565345764, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3953, time 52.51888298988342, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3953
goal_identified
goal_identified
goal_identified
=== ep: 3954, time 51.300397872924805, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3954
goal_identified
goal_identified
goal_identified
=== ep: 3955, time 45.79157733917236, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3956, time 49.36990714073181, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3956
goal_identified
goal_identified
=== ep: 3957, time 48.56706619262695, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3958, time 46.58184456825256, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3958
goal_identified
goal_identified
=== ep: 3959, time 48.57107925415039, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3959
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3960, time 51.71914553642273, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3961, time 49.03196978569031, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3961
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3962, time 45.01046109199524, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3896
goal_identified
goal_identified
goal_identified
=== ep: 3963, time 52.14529800415039, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3963
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3964, time 53.00323963165283, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3964
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3965, time 45.07989501953125, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3965
goal_identified
goal_identified
goal_identified
=== ep: 3966, time 46.9654016494751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3966
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3967, time 53.05158853530884, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3967
goal_identified
goal_identified
goal_identified
=== ep: 3968, time 48.189741134643555, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3968
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3969, time 44.697097301483154, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3969
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3970, time 51.58029341697693, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3970
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3971, time 51.4322464466095, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3971
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3972, time 48.63616681098938, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3972
goal_identified
goal_identified
goal_identified
=== ep: 3973, time 49.35662126541138, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3974, time 48.469273805618286, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3974
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3975, time 45.08944606781006, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3975
goal_identified
goal_identified
goal_identified
=== ep: 3976, time 49.779831886291504, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3976
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3977, time 52.580729246139526, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3918
goal_identified
goal_identified
=== ep: 3978, time 50.729551553726196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3978
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3979, time 43.34361457824707, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3979
goal_identified
goal_identified
=== ep: 3980, time 51.74023723602295, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3980
goal_identified
goal_identified
goal_identified
=== ep: 3981, time 53.26475143432617, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3981
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3982, time 43.772122383117676, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3982
goal_identified
goal_identified
=== ep: 3983, time 49.72855567932129, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3983
goal_identified
=== ep: 3984, time 53.66691017150879, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3984
goal_identified
goal_identified
goal_identified
=== ep: 3985, time 48.6670298576355, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3985
goal_identified
goal_identified
goal_identified
=== ep: 3986, time 45.486876487731934, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3987, time 51.23861312866211, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3987
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3988, time 50.057055950164795, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3930
goal_identified
goal_identified
goal_identified
=== ep: 3989, time 49.582491874694824, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3989
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3990, time 50.11548042297363, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3990
goal_identified
goal_identified
goal_identified
=== ep: 3991, time 51.204599380493164, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3992, time 55.64597773551941, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 44.845069885253906, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3993
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3994, time 50.05644917488098, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3994
goal_identified
goal_identified
goal_identified
=== ep: 3995, time 54.18028020858765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3995
goal_identified
goal_identified
=== ep: 3996, time 51.1321816444397, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3996
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3997, time 43.17398142814636, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3997
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3998, time 53.40119743347168, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3999, time 53.14264798164368, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
