==> Playing in 11_vs_11_hard_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 7 layers and 500 hidden units.
goal_identified
=== ep: 0, time 27.18040132522583, eps 0.9, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
goal_identified
=== ep: 1, time 26.769808530807495, eps 0.8561552526261419, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
=== ep: 2, time 26.876259088516235, eps 0.8144488388143276, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
goal_identified
=== ep: 3, time 27.332618713378906, eps 0.774776470806127, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 4, time 27.311840295791626, eps 0.7370389470171057, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
=== ep: 5, time 27.642032384872437, eps 0.701141903981193, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
=== ep: 6, time 27.586809635162354, eps 0.6669955803928644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
goal_identified
goal_identified
goal_identified
=== ep: 7, time 27.469663858413696, eps 0.6345145926571234, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
=== ep: 8, time 27.66087818145752, eps 0.6036177213860398, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
goal_identified
goal_identified
=== ep: 9, time 31.613781929016113, eps 0.5742277083079742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
=== ep: 10, time 27.722486972808838, eps 0.5462710630816575, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 11, time 27.59746479988098, eps 0.5196778795320575, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
goal_identified
=== ep: 12, time 27.633589029312134, eps 0.49438166084852986, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
goal_identified
goal_identified
=== ep: 13, time 27.14084219932556, eps 0.47031915330815344, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
=== ep: 14, time 27.64685368537903, eps 0.4474301881084772, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 15, time 27.757874488830566, eps 0.42565753091417224, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
=== ep: 16, time 28.074200868606567, eps 0.4049467387413822, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
=== ep: 17, time 28.12053656578064, eps 0.3852460238219053, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
goal_identified
=== ep: 18, time 27.750373601913452, eps 0.3665061241067986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
=== ep: 19, time 30.592201471328735, eps 0.3486801800855966, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
goal_identified
=== ep: 20, time 27.940350770950317, eps 0.3317236176131267, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
=== ep: 21, time 28.22142791748047, eps 0.31559403645092865, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
goal_identified
goal_identified
=== ep: 22, time 27.80345344543457, eps 0.3002511042445735, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
=== ep: 23, time 28.291724920272827, eps 0.2856564556717689, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
goal_identified
=== ep: 24, time 27.59998345375061, eps 0.27177359650906974, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 25, time 27.724943161010742, eps 0.2585678123773109, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 26, time 28.23877739906311, eps 0.24600608193757734, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
=== ep: 27, time 28.011282444000244, eps 0.23405699432065646, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
goal_identified
=== ep: 28, time 28.025167226791382, eps 0.22269067058350425, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
goal_identified
=== ep: 29, time 31.075786352157593, eps 0.2118786889963241, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
=== ep: 30, time 27.626754999160767, eps 0.2015940139734384, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
goal_identified
=== ep: 31, time 28.158280849456787, eps 0.191810928470242, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
=== ep: 32, time 27.880006551742554, eps 0.1825049696771952, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
=== ep: 33, time 27.626311779022217, eps 0.17365286785005798, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
=== ep: 34, time 27.502467393875122, eps 0.16523248812340846, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
=== ep: 35, time 28.08573079109192, eps 0.15722277516195018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
=== ep: 36, time 28.37899684906006, eps 0.1496037005112063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 37, time 27.949113368988037, eps 0.14235621251595124, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
=== ep: 38, time 27.59111189842224, eps 0.13546218868114893, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 39, time 31.90438413619995, eps 0.1289043903562757, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
=== ep: 40, time 27.843272924423218, eps 0.12266641962971482, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
=== ep: 41, time 27.82234025001526, eps 0.116732678325436, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
goal_identified
=== ep: 42, time 27.968071937561035, eps 0.11108832899943073, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
=== ep: 43, time 28.09488296508789, eps 0.10571925783837377, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
=== ep: 44, time 28.167365074157715, eps 0.10061203936773815, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 45, time 28.639322757720947, eps 0.09575390288111604, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
goal_identified
goal_identified
=== ep: 46, time 28.14772343635559, eps 0.09113270050680057, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
=== ep: 47, time 28.054280042648315, eps 0.08673687683177911, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
goal_identified
=== ep: 48, time 28.391940593719482, eps 0.08255544000718185, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
goal_identified
=== ep: 49, time 32.668421268463135, eps 0.07857793426293408, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 50, time 28.440671682357788, eps 0.07479441376288502, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
=== ep: 51, time 27.906230449676514, eps 0.0711954177350367, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
=== ep: 52, time 28.14011788368225, eps 0.06777194681468615, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
=== ep: 53, time 27.76586675643921, eps 0.06451544054132621, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 54, time 27.957353353500366, eps 0.06141775595303503, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
=== ep: 55, time 27.88012957572937, eps 0.05847114722483011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 28.564667463302612, eps 0.05566824630007096, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 28.045215606689453, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 27.52915668487549, eps 0.050465874830710106, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
=== ep: 59, time 30.906545162200928, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
=== ep: 60, time 25.090192317962646, eps 0.045758574462709686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 24.56264567375183, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
goal_identified
=== ep: 62, time 23.424141883850098, eps 0.04149923295180846, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
=== ep: 63, time 23.605093955993652, eps 0.03952406205346913, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
goal_identified
=== ep: 64, time 23.446361303329468, eps 0.03764522137655123, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
=== ep: 65, time 23.44592595100403, eps 0.03585801284071809, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 66, time 23.621943950653076, eps 0.034157967493714775, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 67, time 23.63513731956482, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
=== ep: 68, time 23.369569063186646, eps 0.031002569694333147, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
=== ep: 69, time 28.897186040878296, eps 0.02953932710388308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
goal_identified
=== ep: 70, time 23.598665475845337, eps 0.028147447696664333, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
=== ep: 71, time 23.461261987686157, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 23.756733179092407, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 73, time 23.535759687423706, eps 0.02436602477210106, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
=== ep: 74, time 23.298873901367188, eps 0.02322645029683511, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 75, time 23.44484829902649, eps 0.02214245352455219, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 23.893564701080322, eps 0.02111132389869288, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 23.46705985069275, eps 0.020130483058101077, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 23.475363969802856, eps 0.019197478389778148, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
goal_identified
goal_identified
=== ep: 79, time 29.076335668563843, eps 0.018309976896072843, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 80, time 23.393258571624756, eps 0.017465759360972027, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
goal_identified
=== ep: 81, time 23.26764488220215, eps 0.01666271480090467, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 82, time 23.17840337753296, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
=== ep: 83, time 23.828173398971558, eps 0.015172210419884185, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
=== ep: 84, time 23.538621425628662, eps 0.014481023561609456, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 85, time 23.487841844558716, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
=== ep: 86, time 23.628854990005493, eps 0.013198134551968641, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 87, time 23.460830688476562, eps 0.012603224509851407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
=== ep: 88, time 23.605971097946167, eps 0.012037328572858524, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
=== ep: 89, time 30.0759916305542, eps 0.011499031706385502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
=== ep: 90, time 23.723952531814575, eps 0.010986987887879832, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
goal_identified
=== ep: 91, time 23.588787078857422, eps 0.010499916741083536, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
=== ep: 92, time 23.869396209716797, eps 0.010036600334425595, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 23.70160984992981, eps 0.00959588013555861, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
=== ep: 94, time 23.636346340179443, eps 0.009176654114424539, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
=== ep: 95, time 23.918587684631348, eps 0.00877787398760545, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 23.76786708831787, eps 0.008398542597069007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
=== ep: 97, time 23.421392917633057, eps 0.008037711416753971, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
=== ep: 98, time 23.99436092376709, eps 0.00769447818076098, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
=== ep: 99, time 30.422532558441162, eps 0.007367984627217855, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 100, time 23.94132161140442, eps 0.007057414352177835, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 101, time 23.534591913223267, eps 0.006761990768184489, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
=== ep: 102, time 23.781935214996338, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
=== ep: 103, time 23.87240481376648, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
goal_identified
=== ep: 104, time 23.63573169708252, eps 0.005959391414263934, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 105, time 23.73926067352295, eps 0.005717519040864065, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
goal_identified
=== ep: 106, time 24.007831811904907, eps 0.005487442922312285, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
goal_identified
goal_identified
goal_identified
=== ep: 107, time 23.900413513183594, eps 0.005268587748470919, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
=== ep: 108, time 23.601917266845703, eps 0.005060406267408787, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
=== ep: 109, time 28.114742279052734, eps 0.004862377916986354, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
goal_identified
=== ep: 110, time 24.25912380218506, eps 0.004674007523179196, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
goal_identified
=== ep: 111, time 23.95914340019226, eps 0.004494824061885041, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 23.56996774673462, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 23.70542812347412, eps 0.0041622475806460035, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 23.67266607284546, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
goal_identified
=== ep: 115, time 23.826632022857666, eps 0.0038613199360621906, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
goal_identified
=== ep: 116, time 23.856008768081665, eps 0.003721771716092858, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 23.919247150421143, eps 0.0035890293431213305, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
goal_identified
=== ep: 118, time 23.865177631378174, eps 0.0034627608920727634, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
goal_identified
=== ep: 119, time 28.703956127166748, eps 0.00334265062604924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
=== ep: 120, time 23.86124348640442, eps 0.0032283982068230565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 23.93715500831604, eps 0.0031197179438347193, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 23.702768564224243, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
=== ep: 123, time 23.939550161361694, eps 0.0029180001112638996, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 23.632105112075806, eps 0.002824458142029865, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
=== ep: 125, time 23.781609296798706, eps 0.0027354782684687108, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
=== ep: 126, time 23.916905164718628, eps 0.0026508379945489875, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
=== ep: 127, time 23.720460653305054, eps 0.0025703256754987464, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
goal_identified
=== ep: 128, time 23.77957057952881, eps 0.0024937399885833667, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 129, time 29.85523796081543, eps 0.0024208894296938593, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
=== ep: 130, time 24.064745664596558, eps 0.0023515918344868374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
goal_identified
=== ep: 131, time 24.354811191558838, eps 0.002285673922878779, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
=== ep: 132, time 23.982919216156006, eps 0.0022229708657555565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 23.821473598480225, eps 0.0021633258728137976, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
goal_identified
=== ep: 134, time 24.126202821731567, eps 0.0021065898005034594, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 24.307093858718872, eps 0.002052620779091266, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 24.110450506210327, eps 0.0020012838579124784, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
=== ep: 137, time 24.01063370704651, eps 0.0019524506679239415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
=== ep: 138, time 24.217571020126343, eps 0.001905999100714611, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 30.75460433959961, eps 0.001861813003170924, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 23.719196319580078, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
goal_identified
=== ep: 141, time 24.258445739746094, eps 0.0017798006526189953, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
goal_identified
=== ep: 142, time 24.47839379310608, eps 0.0017417693260160481, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 24.27806258201599, eps 0.0017055928090985275, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 23.900551557540894, eps 0.0016711806417306348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 24.022061347961426, eps 0.0016384467755694515, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 24.03284502029419, eps 0.0016073093588992661, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
goal_identified
=== ep: 147, time 24.240148782730103, eps 0.0015776905319596466, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
goal_identified
=== ep: 148, time 24.141119480133057, eps 0.0015495162322554856, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
goal_identified
=== ep: 149, time 30.944491863250732, eps 0.0015227160093621863, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
=== ep: 150, time 24.145283699035645, eps 0.0014972228487629025, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 24.447216272354126, eps 0.0014729730042773413, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 24.17659068107605, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 24.354063034057617, eps 0.00142796367199102, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 154, time 24.37404727935791, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
=== ep: 155, time 24.194222927093506, eps 0.001387237543977543, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 24.498405933380127, eps 0.0013683517461028282, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 24.157204627990723, eps 0.0013503870194592265, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
=== ep: 158, time 24.034550189971924, eps 0.0013332984428727204, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
=== ep: 159, time 30.15898370742798, eps 0.001317043286000802, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 160, time 24.118087768554688, eps 0.0013015809024843582, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 23.843955516815186, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
goal_identified
=== ep: 162, time 24.50770139694214, eps 0.0012728816851329014, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 163, time 24.068572759628296, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
=== ep: 164, time 24.05477237701416, eps 0.001246913559404956, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
=== ep: 165, time 24.217259645462036, eps 0.0012348714430141991, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
goal_identified
=== ep: 166, time 24.041619300842285, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
=== ep: 167, time 24.145960569381714, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
=== ep: 168, time 24.51173734664917, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 28.918429851531982, eps 0.0011922964734155277, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
=== ep: 170, time 23.98700475692749, eps 0.001182918063740569, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
goal_identified
=== ep: 171, time 23.966999292373657, eps 0.0011739970445027263, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 172, time 23.827982902526855, eps 0.0011655111085071537, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
=== ep: 173, time 23.928894996643066, eps 0.001157439036493735, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
=== ep: 174, time 23.907331228256226, eps 0.0011497606440778825, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 175, time 24.0515398979187, eps 0.0011424567312790603, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
=== ep: 176, time 24.366655111312866, eps 0.0011355090345108335, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 24.1542227268219, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 178, time 24.427897691726685, eps 0.0011226136449073282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
=== ep: 179, time 30.75928568840027, eps 0.001116633706881133, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
=== ep: 180, time 24.202275276184082, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 23.95868754386902, eps 0.001105534542190287, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
goal_identified
goal_identified
=== ep: 182, time 24.799498796463013, eps 0.0011003875618326132, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 183, time 24.41702151298523, eps 0.0010954916026690664, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
=== ep: 184, time 24.194448471069336, eps 0.001090834422251547, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 185, time 24.39725112915039, eps 0.0010864043752031938, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
goal_identified
=== ep: 186, time 24.383020162582397, eps 0.0010821903840988777, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 187, time 24.285488605499268, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
=== ep: 188, time 24.486774921417236, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 30.006431579589844, eps 0.0010707419191793434, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 190, time 24.267978191375732, eps 0.0010672917950690429, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
=== ep: 191, time 24.15950584411621, eps 0.0010640099354971456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
=== ep: 192, time 23.977736234664917, eps 0.0010608881341052777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 24.345256328582764, eps 0.0010579185847638855, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
=== ep: 194, time 23.98815679550171, eps 0.0010550938620528466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
goal_identified
=== ep: 195, time 24.445194244384766, eps 0.001052406902694051, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 24.229300022125244, eps 0.001049850987889527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 24.236918210983276, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 24.397689819335938, eps 0.0010451070391685015, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
=== ep: 199, time 29.953017473220825, eps 0.001042907142909185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 24.25608777999878, eps 0.001040814536856474, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 24.49481987953186, eps 0.0010388239884052469, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
goal_identified
goal_identified
=== ep: 202, time 24.331359386444092, eps 0.0010369305201475454, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 203, time 24.2565655708313, eps 0.0010351293974264616, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
=== ep: 204, time 24.312965631484985, eps 0.00103341611649703, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
=== ep: 205, time 24.26829195022583, eps 0.0010317863932645186, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 24.170923471450806, eps 0.0010302361525719613, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 24.09218168258667, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 24.422696828842163, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
=== ep: 209, time 30.979146480560303, eps 0.0010260244976950921, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
goal_identified
=== ep: 210, time 24.17140793800354, eps 0.0010247552679654227, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
=== ep: 211, time 24.306443452835083, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 212, time 24.781723260879517, eps 0.0010223994927486214, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
goal_identified
=== ep: 213, time 24.574064016342163, eps 0.001021307056596379, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
goal_identified
=== ep: 214, time 24.17719316482544, eps 0.0010202678991839778, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
=== ep: 215, time 24.28140950202942, eps 0.0010192794220766138, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
=== ep: 216, time 24.389581203460693, eps 0.0010183391535666436, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
=== ep: 217, time 24.2439968585968, eps 0.0010174447424930286, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 218, time 24.187093019485474, eps 0.0010165939523622068, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
goal_identified
goal_identified
=== ep: 219, time 31.47036385536194, eps 0.0010157846557556941, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
=== ep: 220, time 24.244738340377808, eps 0.001015014829010431, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 24.52936887741089, eps 0.0010142825471585687, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
=== ep: 222, time 24.578266382217407, eps 0.0010135859791140496, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
goal_identified
goal_identified
=== ep: 223, time 24.237367630004883, eps 0.0010129233830939361, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 224, time 24.15925693511963, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
goal_identified
=== ep: 225, time 24.28935718536377, eps 0.001011693560591007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 24.615044116973877, eps 0.0010111232589113477, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
=== ep: 227, time 24.490384340286255, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
=== ep: 228, time 24.603101015090942, eps 0.0010100647408734893, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 30.512359142303467, eps 0.001009573877668838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 24.30205464363098, eps 0.001009106954145169, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 24.112919092178345, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 24.658530712127686, eps 0.0010082403128748867, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 24.243365049362183, eps 0.0010078384280736842, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 234, time 24.63311457633972, eps 0.001007456143425521, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
=== ep: 235, time 24.232697248458862, eps 0.001007092503019653, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
=== ep: 236, time 24.213369607925415, eps 0.001006746597565654, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
goal_identified
=== ep: 237, time 24.237592935562134, eps 0.001006417562119715, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
=== ep: 238, time 24.54567837715149, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 30.532161712646484, eps 0.0010058068503384884, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
goal_identified
=== ep: 240, time 24.50280785560608, eps 0.001005523646905642, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
goal_identified
goal_identified
=== ep: 241, time 24.966431140899658, eps 0.001005254255467199, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 24.44215965270996, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 24.486006021499634, eps 0.0010047542469506416, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
=== ep: 244, time 24.177837133407593, eps 0.0010045223795907931, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
=== ep: 245, time 26.26213836669922, eps 0.001004301820535524, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 24.69012761116028, eps 0.0010040920182723119, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
goal_identified
goal_identified
=== ep: 247, time 24.12843918800354, eps 0.0010038924481862177, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
=== ep: 248, time 24.4843966960907, eps 0.0010037026112480747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
goal_identified
=== ep: 249, time 32.18494129180908, eps 0.0010035220327666559, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
=== ep: 250, time 24.236139059066772, eps 0.0010033502612016988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
=== ep: 251, time 24.734951496124268, eps 0.001003186867034819, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
=== ep: 252, time 24.35654067993164, eps 0.001003031441695491, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
=== ep: 253, time 24.621294498443604, eps 0.0010028835965394094, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
goal_identified
=== ep: 254, time 24.614033222198486, eps 0.0010027429618766747, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 255, time 24.130903005599976, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 24.235914707183838, eps 0.0010024819345422614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
=== ep: 257, time 24.417343616485596, eps 0.0010023608891662839, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 24.363101959228516, eps 0.001002245747242954, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 33.69978380203247, eps 0.0010021362208574892, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 24.450278520584106, eps 0.001002032036136876, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
=== ep: 261, time 24.501232147216797, eps 0.0010019329325650452, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
=== ep: 262, time 24.4550199508667, eps 0.0010018386623314465, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
goal_identified
=== ep: 263, time 24.431792974472046, eps 0.0010017489897113931, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 264, time 24.483291387557983, eps 0.0010016636904766263, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
goal_identified
=== ep: 265, time 24.475591897964478, eps 0.0010015825513346283, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 24.54179072380066, eps 0.0010015053693952815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 24.463049173355103, eps 0.0010014319516635345, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
goal_identified
=== ep: 268, time 24.516908645629883, eps 0.0010013621145568167, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 34.08087134361267, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 24.72424817085266, eps 0.0010012324922186594, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 24.081899166107178, eps 0.001001172382863857, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 24.400443077087402, eps 0.0010011152050768812, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 25.043707370758057, eps 0.0010010608158834819, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 24.783926725387573, eps 0.0010010090792823456, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
goal_identified
=== ep: 275, time 24.01337218284607, eps 0.0010009598659050213, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 276, time 24.471556186676025, eps 0.0010009130526924313, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 24.200998306274414, eps 0.0010008685225871602, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
=== ep: 278, time 24.972146034240723, eps 0.0010008261642407504, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 35.49717164039612, eps 0.001000785871735272, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
=== ep: 280, time 24.42009425163269, eps 0.0010007475443184742, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
goal_identified
goal_identified
=== ep: 281, time 24.535063982009888, eps 0.001000711086151851, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 24.753971099853516, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 24.557838201522827, eps 0.001000643417357642, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
goal_identified
goal_identified
goal_identified
=== ep: 284, time 24.334417819976807, eps 0.0010006120375228235, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
goal_identified
=== ep: 285, time 24.653523445129395, eps 0.0010005821881006083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 24.75902795791626, eps 0.0010005537944518927, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
=== ep: 287, time 24.855201959609985, eps 0.0010005267855777657, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
=== ep: 288, time 24.464790105819702, eps 0.0010005010939419733, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
goal_identified
=== ep: 289, time 34.955129623413086, eps 0.001000476655302044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
=== ep: 290, time 24.373114585876465, eps 0.0010004534085486486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 24.60591745376587, eps 0.0010004312955527947, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
goal_identified
=== ep: 292, time 24.660091400146484, eps 0.0010004102610204745, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
goal_identified
=== ep: 293, time 24.25179934501648, eps 0.0010003902523544011, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
=== ep: 294, time 25.022310495376587, eps 0.0010003712195224871, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
=== ep: 295, time 24.24738574028015, eps 0.0010003531149327387, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
goal_identified
goal_identified
=== ep: 296, time 24.741663455963135, eps 0.0010003358933142518, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
=== ep: 297, time 24.76823401451111, eps 0.0010003195116040093, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
goal_identified
=== ep: 298, time 24.90025043487549, eps 0.0010003039288392032, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
goal_identified
=== ep: 299, time 34.718358278274536, eps 0.0010002891060548044, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
=== ep: 300, time 24.567062377929688, eps 0.0010002750061861312, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
=== ep: 301, time 24.492623329162598, eps 0.0010002615939761676, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
=== ep: 302, time 24.839529275894165, eps 0.001000248835887403, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 24.57176446914673, eps 0.0010002367000179694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
=== ep: 304, time 24.411455392837524, eps 0.0010002251560218723, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
goal_identified
=== ep: 305, time 24.53036117553711, eps 0.0010002141750331084, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
=== ep: 306, time 25.010849237442017, eps 0.0010002037295934862, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
=== ep: 307, time 24.963889598846436, eps 0.0010001937935839656, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
goal_identified
=== ep: 308, time 24.46689462661743, eps 0.0010001843421593476, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 34.17624831199646, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
goal_identified
=== ep: 310, time 24.928279638290405, eps 0.0010001667996834991, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 24.18776535987854, eps 0.001000158664766942, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 312, time 24.647716999053955, eps 0.0010001509265949466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
=== ep: 313, time 25.084407329559326, eps 0.001000143565818053, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
=== ep: 314, time 24.87193512916565, eps 0.0010001365640304844, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 24.75870108604431, eps 0.0010001299037241253, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
=== ep: 316, time 24.768064498901367, eps 0.0010001235682447402, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
=== ep: 317, time 24.897192239761353, eps 0.0010001175417503308, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 25.106181144714355, eps 0.0010001118091715218, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 34.16525602340698, eps 0.0010001063561738807, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
=== ep: 320, time 24.44913339614868, eps 0.0010001011691220727, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 321, time 24.54537034034729, eps 0.0010000962350457665, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
goal_identified
goal_identified
goal_identified
=== ep: 322, time 24.47895646095276, eps 0.0010000915416072012, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 323, time 24.87008309364319, eps 0.0010000870770703358, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
goal_identified
=== ep: 324, time 24.671610116958618, eps 0.0010000828302715028, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 24.865406274795532, eps 0.0010000787905914928, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
goal_identified
=== ep: 326, time 24.765100955963135, eps 0.0010000749479290019, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
=== ep: 327, time 24.720927715301514, eps 0.001000071292675372, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 24.847557544708252, eps 0.001000067815690565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
goal_identified
=== ep: 329, time 34.88904619216919, eps 0.0010000645082803084, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
goal_identified
=== ep: 330, time 24.72659921646118, eps 0.0010000613621743532, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 31.311105251312256, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
goal_identified
=== ep: 332, time 24.57237458229065, eps 0.0010000555227914069, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 24.471226692199707, eps 0.0010000528149129166, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
=== ep: 334, time 24.820059776306152, eps 0.0010000502390992187, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
goal_identified
=== ep: 335, time 24.859376430511475, eps 0.0010000477889094373, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
=== ep: 336, time 24.973660469055176, eps 0.0010000454582168217, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 24.918551206588745, eps 0.001000043241193426, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 25.3814754486084, eps 0.0010000411322955373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 339, time 35.22355651855469, eps 0.0010000391262498123, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 24.935065746307373, eps 0.001000037218040092, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 24.943022966384888, eps 0.0010000354028948577, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 24.987370014190674, eps 0.0010000336762753012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 25.13366389274597, eps 0.001000032033863974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
goal_identified
=== ep: 344, time 24.72414207458496, eps 0.0010000304715539925, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
=== ep: 345, time 24.79290533065796, eps 0.001000028985438768, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 24.93938636779785, eps 0.001000027571802238, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 24.772173643112183, eps 0.0010000262271095755, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
=== ep: 348, time 24.939560651779175, eps 0.0010000249479983478, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
=== ep: 349, time 35.193365812301636, eps 0.0010000237312701107, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 25.128674745559692, eps 0.00100002257388241, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 24.754744052886963, eps 0.0010000214729411737, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
goal_identified
=== ep: 352, time 24.306633472442627, eps 0.0010000204256934752, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
=== ep: 353, time 24.63036608695984, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
goal_identified
=== ep: 354, time 24.63791060447693, eps 0.0010000184819317455, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
=== ep: 355, time 24.51422095298767, eps 0.001000017580557298, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
goal_identified
goal_identified
=== ep: 356, time 24.99547553062439, eps 0.001000016723143401, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
goal_identified
=== ep: 357, time 24.584826946258545, eps 0.0010000159075460732, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 24.560014963150024, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 35.25168061256409, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 24.770081281661987, eps 0.0010000136917517905, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
=== ep: 361, time 24.934990406036377, eps 0.001000013023997176, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 362, time 24.64435601234436, eps 0.0010000123888093385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 24.59247374534607, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 24.52658247947693, eps 0.0010000112098582543, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 24.967886447906494, eps 0.001000010663147016, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 25.202840089797974, eps 0.0010000101430991996, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
goal_identified
=== ep: 367, time 24.77292013168335, eps 0.0010000096484144142, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 25.58685851097107, eps 0.0010000091778556905, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 34.84818720817566, eps 0.0010000087302463867, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
=== ep: 370, time 25.13401436805725, eps 0.001000008304467246, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 24.7024142742157, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 24.912659168243408, eps 0.0010000075141927012, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
=== ep: 373, time 25.2127845287323, eps 0.0010000071477211988, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
goal_identified
goal_identified
=== ep: 374, time 24.662357330322266, eps 0.0010000067991227223, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
=== ep: 375, time 25.019686222076416, eps 0.0010000064675255943, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
goal_identified
=== ep: 376, time 24.97233009338379, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
goal_identified
=== ep: 377, time 24.767520666122437, eps 0.0010000058520591598, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 24.396652460098267, eps 0.0010000055666508666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
goal_identified
=== ep: 379, time 34.8624107837677, eps 0.0010000052951621003, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 25.021517515182495, eps 0.0010000050369139975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
=== ep: 381, time 24.973586082458496, eps 0.001000004791260803, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
=== ep: 382, time 24.708863258361816, eps 0.0010000045575882562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 24.91879177093506, eps 0.001000004335312054, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
=== ep: 384, time 25.31200909614563, eps 0.0010000041238763903, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
=== ep: 385, time 24.95248293876648, eps 0.0010000039227525655, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 24.989644527435303, eps 0.0010000037314376652, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 24.911380290985107, eps 0.001000003549453303, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
=== ep: 388, time 24.953858375549316, eps 0.0010000033763444226, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 389, time 35.29757308959961, eps 0.001000003211678162, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 25.248659372329712, eps 0.0010000030550427698, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
=== ep: 391, time 25.026976823806763, eps 0.0010000029060465757, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 24.64364743232727, eps 0.0010000027643170119, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 393, time 24.595723867416382, eps 0.0010000026294996803, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
=== ep: 394, time 24.994450092315674, eps 0.0010000025012574677, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
=== ep: 395, time 25.28756093978882, eps 0.0010000023792697014, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
goal_identified
=== ep: 396, time 24.74143385887146, eps 0.0010000022632313489, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 24.902122259140015, eps 0.0010000021528522535, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
=== ep: 398, time 24.93232536315918, eps 0.00100000204785641, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 35.201119899749756, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 400, time 24.730267763137817, eps 0.0010000018529771066, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
=== ep: 401, time 25.190261840820312, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
goal_identified
=== ep: 402, time 24.796885013580322, eps 0.0010000016766430208, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
goal_identified
=== ep: 403, time 25.326945781707764, eps 0.0010000015948721758, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 24.944555044174194, eps 0.001000001517089342, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
=== ep: 405, time 25.0618314743042, eps 0.0010000014431000217, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
goal_identified
goal_identified
=== ep: 406, time 24.66878843307495, eps 0.001000001372719203, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
goal_identified
=== ep: 407, time 24.818023920059204, eps 0.0010000013057708975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 24.838470697402954, eps 0.0010000012420876994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
=== ep: 409, time 35.48045063018799, eps 0.0010000011815103674, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 410, time 24.833174228668213, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
goal_identified
=== ep: 411, time 24.846850633621216, eps 0.0010000010690747903, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
goal_identified
=== ep: 412, time 24.958773612976074, eps 0.0010000010169353975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 24.743942499160767, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
=== ep: 414, time 24.940141439437866, eps 0.0010000009201611994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
goal_identified
goal_identified
=== ep: 415, time 25.335720539093018, eps 0.0010000008752844081, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 25.306809186935425, eps 0.0010000008325962838, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
=== ep: 417, time 24.797024965286255, eps 0.001000000791990084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
=== ep: 418, time 25.35323166847229, eps 0.0010000007533642718, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
=== ep: 419, time 34.95460391044617, eps 0.0010000007166222626, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
goal_identified
goal_identified
=== ep: 420, time 24.837116718292236, eps 0.0010000006816721825, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
goal_identified
=== ep: 421, time 24.764324426651, eps 0.001000000648426638, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 25.0379536151886, eps 0.0010000006168024976, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 24.895244359970093, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
goal_identified
goal_identified
=== ep: 424, time 24.780197620391846, eps 0.0010000005581059794, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
=== ep: 425, time 25.2027325630188, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
goal_identified
goal_identified
=== ep: 426, time 24.614381551742554, eps 0.0010000005049951733, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
=== ep: 427, time 24.874568462371826, eps 0.001000000480366268, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 24.70625138282776, eps 0.0010000004569385287, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 33.68087148666382, eps 0.0010000004346533736, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
=== ep: 430, time 24.60179042816162, eps 0.0010000004134550786, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 26.067930698394775, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
=== ep: 432, time 25.013594388961792, eps 0.0010000003741096257, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 24.97590708732605, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 24.860909461975098, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
=== ep: 435, time 25.23526644706726, eps 0.001000000321999139, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 25.09019947052002, eps 0.0010000003062950555, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 24.9451744556427, eps 0.0010000002913568694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 24.88616418838501, eps 0.0010000002771472273, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
=== ep: 439, time 33.55092406272888, eps 0.0010000002636305976, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
goal_identified
=== ep: 440, time 24.94062614440918, eps 0.0010000002507731815, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 441, time 25.174211740493774, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
=== ep: 442, time 25.505491018295288, eps 0.0010000002269089582, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
goal_identified
=== ep: 443, time 25.077085494995117, eps 0.0010000002158424776, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
=== ep: 444, time 24.92533826828003, eps 0.0010000002053157158, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 25.117721557617188, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
=== ep: 446, time 24.73742413520813, eps 0.001000000185777342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 447, time 25.102215051651, eps 0.0010000001767168742, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
goal_identified
=== ep: 448, time 24.99140763282776, eps 0.0010000001680982905, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
=== ep: 449, time 32.42970371246338, eps 0.0010000001599000403, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
goal_identified
=== ep: 450, time 24.883835077285767, eps 0.0010000001521016232, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
=== ep: 451, time 25.06257152557373, eps 0.0010000001446835395, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 24.938406467437744, eps 0.0010000001376272401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
goal_identified
=== ep: 453, time 24.92242193222046, eps 0.0010000001309150804, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
goal_identified
=== ep: 454, time 25.029916763305664, eps 0.0010000001245302765, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
=== ep: 455, time 24.79422378540039, eps 0.0010000001184568633, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
goal_identified
=== ep: 456, time 24.7621431350708, eps 0.0010000001126796538, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
goal_identified
=== ep: 457, time 24.9425311088562, eps 0.0010000001071842023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
goal_identified
=== ep: 458, time 24.71530246734619, eps 0.001000000101956767, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 31.761990547180176, eps 0.001000000096984277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
=== ep: 460, time 24.87645387649536, eps 0.001000000092254298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
goal_identified
=== ep: 461, time 24.72397232055664, eps 0.0010000000877550027, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 24.611434936523438, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
goal_identified
=== ep: 463, time 24.835479021072388, eps 0.00100000007940401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 24.980574369430542, eps 0.0010000000755314307, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
=== ep: 465, time 25.54640293121338, eps 0.0010000000718477194, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 24.84942388534546, eps 0.0010000000683436647, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 25.771852254867554, eps 0.001000000065010505, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
=== ep: 468, time 25.051246166229248, eps 0.0010000000618399052, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
goal_identified
goal_identified
=== ep: 469, time 31.747037649154663, eps 0.0010000000588239375, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
=== ep: 470, time 25.49504566192627, eps 0.0010000000559550603, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
=== ep: 471, time 25.190003633499146, eps 0.0010000000532260998, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
=== ep: 472, time 25.250351428985596, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 473, time 25.181767463684082, eps 0.0010000000481609666, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
goal_identified
=== ep: 474, time 25.024344444274902, eps 0.0010000000458121286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 24.745407104492188, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 476, time 25.029217004776, eps 0.001000000041452528, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
goal_identified
=== ep: 477, time 24.59876847267151, eps 0.0010000000394308644, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 25.115312576293945, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
=== ep: 479, time 32.54568862915039, eps 0.0010000000356785216, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 25.058722734451294, eps 0.0010000000339384595, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
=== ep: 481, time 25.120426416397095, eps 0.0010000000322832614, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 24.812718629837036, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
=== ep: 483, time 24.941234588623047, eps 0.001000000029211103, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 24.686272859573364, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
goal_identified
=== ep: 485, time 24.8423969745636, eps 0.0010000000264312988, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
=== ep: 486, time 24.944565057754517, eps 0.0010000000251422292, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 25.49840211868286, eps 0.0010000000239160282, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
=== ep: 488, time 25.057260990142822, eps 0.00100000002274963, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
=== ep: 489, time 32.900779247283936, eps 0.0010000000216401172, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
goal_identified
=== ep: 490, time 25.16383647918701, eps 0.0010000000205847162, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 25.112961053848267, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 24.85679054260254, eps 0.0010000000186258216, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
goal_identified
=== ep: 493, time 25.04137420654297, eps 0.0010000000177174295, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 25.141062021255493, eps 0.0010000000168533404, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
goal_identified
goal_identified
goal_identified
=== ep: 495, time 24.59646964073181, eps 0.0010000000160313932, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
goal_identified
=== ep: 496, time 25.09106159210205, eps 0.001000000015249533, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 25.414792776107788, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 25.260348796844482, eps 0.001000000013798348, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
goal_identified
=== ep: 499, time 32.17471790313721, eps 0.0010000000131253947, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 500, time 25.093307733535767, eps 0.0010000000124852615, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 25.25317096710205, eps 0.0010000000118763482, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 24.84353017807007, eps 0.0010000000112971319, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 25.10702657699585, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 24.87817692756653, eps 0.0010000000102220676, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 25.25995111465454, eps 0.0010000000097235315, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
=== ep: 506, time 24.969877004623413, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 25.358593225479126, eps 0.0010000000087982152, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 24.62655520439148, eps 0.0010000000083691212, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 31.544926643371582, eps 0.0010000000079609542, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
goal_identified
goal_identified
=== ep: 510, time 25.406654357910156, eps 0.001000000007572694, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
goal_identified
=== ep: 511, time 26.72240161895752, eps 0.0010000000072033692, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 24.919860363006592, eps 0.001000000006852057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
=== ep: 513, time 24.840636253356934, eps 0.001000000006517878, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
goal_identified
=== ep: 514, time 25.01333975791931, eps 0.0010000000061999974, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 25.386489868164062, eps 0.0010000000058976199, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
=== ep: 516, time 24.957011699676514, eps 0.0010000000056099897, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
goal_identified
=== ep: 517, time 25.00578474998474, eps 0.0010000000053363872, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
goal_identified
=== ep: 518, time 24.950774908065796, eps 0.0010000000050761286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
=== ep: 519, time 31.456759691238403, eps 0.001000000004828563, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
=== ep: 520, time 25.242284297943115, eps 0.001000000004593071, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 24.970908880233765, eps 0.0010000000043690644, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
goal_identified
=== ep: 522, time 25.029216766357422, eps 0.0010000000041559827, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
goal_identified
goal_identified
=== ep: 523, time 25.377152681350708, eps 0.0010000000039532928, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
goal_identified
=== ep: 524, time 25.1038658618927, eps 0.0010000000037604885, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
=== ep: 525, time 25.254210710525513, eps 0.0010000000035770874, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
goal_identified
=== ep: 526, time 25.441003799438477, eps 0.0010000000034026306, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 25.00907611846924, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
=== ep: 528, time 25.51890468597412, eps 0.0010000000030788276, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
goal_identified
goal_identified
=== ep: 529, time 32.346426486968994, eps 0.0010000000029286714, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 530, time 25.131561756134033, eps 0.0010000000027858384, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 25.320932626724243, eps 0.0010000000026499714, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 532, time 25.54571533203125, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
goal_identified
=== ep: 533, time 24.80461311340332, eps 0.0010000000023977934, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
=== ep: 534, time 25.181305170059204, eps 0.0010000000022808515, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
goal_identified
=== ep: 535, time 25.119285821914673, eps 0.0010000000021696133, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
=== ep: 536, time 25.701204299926758, eps 0.0010000000020637999, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
goal_identified
=== ep: 537, time 25.401001691818237, eps 0.0010000000019631471, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
=== ep: 538, time 25.330819845199585, eps 0.0010000000018674034, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
goal_identified
=== ep: 539, time 33.07256484031677, eps 0.001000000001776329, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
goal_identified
=== ep: 540, time 25.081730365753174, eps 0.0010000000016896964, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
=== ep: 541, time 24.985037326812744, eps 0.001000000001607289, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
goal_identified
=== ep: 542, time 25.10902714729309, eps 0.0010000000015289005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
goal_identified
=== ep: 543, time 25.22135305404663, eps 0.0010000000014543352, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
goal_identified
=== ep: 544, time 25.64208149909973, eps 0.0010000000013834064, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
goal_identified
=== ep: 545, time 25.61431360244751, eps 0.001000000001315937, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 25.12705159187317, eps 0.0010000000012517578, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
goal_identified
=== ep: 547, time 25.044622659683228, eps 0.001000000001190709, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
goal_identified
=== ep: 548, time 24.962805032730103, eps 0.0010000000011326374, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 32.07484197616577, eps 0.001000000001077398, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 25.12274694442749, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 25.4200119972229, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
=== ep: 552, time 25.514894485473633, eps 0.001000000000927325, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
goal_identified
=== ep: 553, time 25.328834295272827, eps 0.0010000000008820989, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
=== ep: 554, time 24.99332070350647, eps 0.0010000000008390784, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 24.97634196281433, eps 0.001000000000798156, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
=== ep: 556, time 25.56061601638794, eps 0.0010000000007592295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 25.49965000152588, eps 0.0010000000007222014, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
goal_identified
goal_identified
=== ep: 558, time 25.211049556732178, eps 0.0010000000006869794, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
goal_identified
=== ep: 559, time 33.32907962799072, eps 0.001000000000653475, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 25.09523892402649, eps 0.0010000000006216046, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
goal_identified
goal_identified
=== ep: 561, time 25.18366551399231, eps 0.0010000000005912885, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
=== ep: 562, time 25.363460063934326, eps 0.0010000000005624511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
=== ep: 563, time 25.388213634490967, eps 0.00100000000053502, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
goal_identified
=== ep: 564, time 25.289612770080566, eps 0.001000000000508927, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
=== ep: 565, time 25.33092474937439, eps 0.001000000000484106, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
=== ep: 566, time 25.333731651306152, eps 0.001000000000460496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
goal_identified
=== ep: 567, time 25.312041997909546, eps 0.0010000000004380374, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 25.13179850578308, eps 0.001000000000416674, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 32.887938022613525, eps 0.0010000000003963527, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
goal_identified
=== ep: 570, time 25.1160831451416, eps 0.0010000000003770222, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
=== ep: 571, time 25.05381417274475, eps 0.0010000000003586346, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
goal_identified
=== ep: 572, time 25.164267778396606, eps 0.0010000000003411438, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
=== ep: 573, time 25.19602632522583, eps 0.001000000000324506, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
goal_identified
=== ep: 574, time 25.10265350341797, eps 0.0010000000003086798, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 25.19786834716797, eps 0.0010000000002936252, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
goal_identified
=== ep: 576, time 25.338180541992188, eps 0.001000000000279305, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
goal_identified
=== ep: 577, time 25.01533603668213, eps 0.0010000000002656831, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 25.524738788604736, eps 0.0010000000002527256, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
goal_identified
=== ep: 579, time 32.20747709274292, eps 0.0010000000002404, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
goal_identified
=== ep: 580, time 24.93809938430786, eps 0.0010000000002286756, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
=== ep: 581, time 24.94494915008545, eps 0.0010000000002175229, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
=== ep: 582, time 25.338338136672974, eps 0.0010000000002069142, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
goal_identified
=== ep: 583, time 25.026055097579956, eps 0.0010000000001968228, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
goal_identified
goal_identified
=== ep: 584, time 24.943876266479492, eps 0.0010000000001872237, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
=== ep: 585, time 24.818238973617554, eps 0.0010000000001780928, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 586, time 25.188175678253174, eps 0.001000000000169407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 25.488654375076294, eps 0.001000000000161145, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
=== ep: 588, time 25.42629861831665, eps 0.0010000000001532858, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
=== ep: 589, time 32.455920696258545, eps 0.00100000000014581, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
goal_identified
=== ep: 590, time 24.99618172645569, eps 0.0010000000001386988, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 25.498359203338623, eps 0.0010000000001319344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
=== ep: 592, time 25.159194231033325, eps 0.0010000000001255, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 25.146340370178223, eps 0.0010000000001193791, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 24.693126678466797, eps 0.001000000000113557, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
goal_identified
=== ep: 595, time 25.33099389076233, eps 0.0010000000001080186, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
goal_identified
=== ep: 596, time 24.93883204460144, eps 0.0010000000001027505, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
=== ep: 597, time 24.848005056381226, eps 0.0010000000000977393, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
goal_identified
=== ep: 598, time 25.42192244529724, eps 0.0010000000000929725, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
goal_identified
=== ep: 599, time 32.42508912086487, eps 0.0010000000000884382, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
goal_identified
=== ep: 600, time 25.533785581588745, eps 0.001000000000084125, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 601, time 25.028943061828613, eps 0.0010000000000800222, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 24.953741312026978, eps 0.0010000000000761195, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 603, time 25.584169387817383, eps 0.0010000000000724072, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
goal_identified
=== ep: 604, time 24.929824829101562, eps 0.0010000000000688757, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
goal_identified
=== ep: 605, time 25.228867292404175, eps 0.0010000000000655166, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
goal_identified
=== ep: 606, time 25.289644718170166, eps 0.0010000000000623215, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 24.984445571899414, eps 0.001000000000059282, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
goal_identified
=== ep: 608, time 25.321742296218872, eps 0.0010000000000563907, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
=== ep: 609, time 34.180959939956665, eps 0.0010000000000536405, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 25.154606103897095, eps 0.0010000000000510245, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 25.144407749176025, eps 0.0010000000000485358, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
goal_identified
goal_identified
=== ep: 612, time 25.48108744621277, eps 0.0010000000000461688, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
goal_identified
=== ep: 613, time 25.00747847557068, eps 0.0010000000000439171, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
=== ep: 614, time 25.373141765594482, eps 0.0010000000000417752, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 615, time 25.252935647964478, eps 0.0010000000000397378, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
=== ep: 616, time 25.396173238754272, eps 0.0010000000000377999, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
=== ep: 617, time 25.21786856651306, eps 0.0010000000000359563, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
=== ep: 618, time 25.129176139831543, eps 0.0010000000000342027, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 32.91311979293823, eps 0.0010000000000325345, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
=== ep: 620, time 25.41310477256775, eps 0.001000000000030948, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
=== ep: 621, time 25.152010202407837, eps 0.0010000000000294385, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
=== ep: 622, time 25.564192056655884, eps 0.0010000000000280028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 25.235283374786377, eps 0.0010000000000266371, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
=== ep: 624, time 25.210695028305054, eps 0.001000000000025338, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 25.455188512802124, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
goal_identified
=== ep: 626, time 25.13227605819702, eps 0.0010000000000229268, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
goal_identified
=== ep: 627, time 25.059197664260864, eps 0.0010000000000218085, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
goal_identified
=== ep: 628, time 25.105852127075195, eps 0.001000000000020745, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
goal_identified
goal_identified
=== ep: 629, time 33.9628791809082, eps 0.0010000000000197332, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
=== ep: 630, time 25.367907524108887, eps 0.0010000000000187708, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
goal_identified
=== ep: 631, time 25.121599197387695, eps 0.0010000000000178553, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 632, time 25.160118579864502, eps 0.0010000000000169845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
goal_identified
=== ep: 633, time 25.75657033920288, eps 0.0010000000000161562, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
goal_identified
=== ep: 634, time 25.274129629135132, eps 0.0010000000000153684, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 25.806798934936523, eps 0.0010000000000146188, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
=== ep: 636, time 25.211685180664062, eps 0.0010000000000139058, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
goal_identified
goal_identified
=== ep: 637, time 25.020497798919678, eps 0.0010000000000132275, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 638, time 25.083597421646118, eps 0.0010000000000125824, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 34.77033734321594, eps 0.0010000000000119687, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 25.198819637298584, eps 0.001000000000011385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
=== ep: 641, time 25.215712785720825, eps 0.00100000000001083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
=== ep: 642, time 25.407716512680054, eps 0.0010000000000103017, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
goal_identified
=== ep: 643, time 25.165363788604736, eps 0.0010000000000097993, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
goal_identified
goal_identified
=== ep: 644, time 25.285332202911377, eps 0.0010000000000093213, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
=== ep: 645, time 25.368857622146606, eps 0.0010000000000088666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
=== ep: 646, time 24.823812007904053, eps 0.0010000000000084342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 25.04971170425415, eps 0.001000000000008023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
=== ep: 648, time 24.985685348510742, eps 0.0010000000000076317, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
goal_identified
=== ep: 649, time 35.07063817977905, eps 0.0010000000000072594, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 25.19664168357849, eps 0.0010000000000069055, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 25.343382358551025, eps 0.0010000000000065686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 25.28184723854065, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 25.11379075050354, eps 0.0010000000000059436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
=== ep: 654, time 24.934473037719727, eps 0.0010000000000056537, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
=== ep: 655, time 24.963683366775513, eps 0.0010000000000053779, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 25.514729499816895, eps 0.0010000000000051157, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
=== ep: 657, time 25.453389406204224, eps 0.0010000000000048661, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
=== ep: 658, time 25.428674459457397, eps 0.001000000000004629, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
goal_identified
=== ep: 659, time 36.322346448898315, eps 0.0010000000000044032, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
=== ep: 660, time 25.42242741584778, eps 0.0010000000000041883, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 25.15184187889099, eps 0.001000000000003984, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
=== ep: 662, time 25.44315791130066, eps 0.0010000000000037897, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 24.594698667526245, eps 0.001000000000003605, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 25.030460596084595, eps 0.0010000000000034291, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
=== ep: 665, time 25.195340156555176, eps 0.001000000000003262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
=== ep: 666, time 25.036961555480957, eps 0.0010000000000031028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
=== ep: 667, time 25.430058240890503, eps 0.0010000000000029514, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
=== ep: 668, time 25.659783124923706, eps 0.0010000000000028075, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
goal_identified
=== ep: 669, time 35.218849182128906, eps 0.0010000000000026706, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 25.12729573249817, eps 0.0010000000000025403, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 671, time 25.58695912361145, eps 0.0010000000000024165, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
goal_identified
=== ep: 672, time 24.91657066345215, eps 0.0010000000000022985, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
goal_identified
=== ep: 673, time 24.78472876548767, eps 0.0010000000000021864, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 25.43847918510437, eps 0.00100000000000208, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 25.265836715698242, eps 0.0010000000000019785, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
=== ep: 676, time 25.1594660282135, eps 0.001000000000001882, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
=== ep: 677, time 25.55854344367981, eps 0.0010000000000017903, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
=== ep: 678, time 25.120261430740356, eps 0.0010000000000017029, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
=== ep: 679, time 36.775670528411865, eps 0.0010000000000016198, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 25.185787200927734, eps 0.0010000000000015409, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
goal_identified
=== ep: 681, time 25.594263792037964, eps 0.0010000000000014656, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 25.156541109085083, eps 0.0010000000000013943, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
goal_identified
=== ep: 683, time 25.641779899597168, eps 0.0010000000000013262, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
=== ep: 684, time 25.062914848327637, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
=== ep: 685, time 25.615507125854492, eps 0.0010000000000012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
goal_identified
=== ep: 686, time 25.50645685195923, eps 0.0010000000000011415, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
=== ep: 687, time 25.157986164093018, eps 0.0010000000000010857, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
=== ep: 688, time 25.29777455329895, eps 0.0010000000000010328, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
goal_identified
=== ep: 689, time 35.442312479019165, eps 0.0010000000000009825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
goal_identified
goal_identified
=== ep: 690, time 25.048884868621826, eps 0.0010000000000009346, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 25.555680513381958, eps 0.001000000000000889, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 25.196210145950317, eps 0.0010000000000008457, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 24.9363751411438, eps 0.0010000000000008045, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
goal_identified
goal_identified
=== ep: 694, time 25.35701012611389, eps 0.0010000000000007653, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
goal_identified
=== ep: 695, time 25.234405517578125, eps 0.0010000000000007277, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
goal_identified
=== ep: 696, time 25.217302083969116, eps 0.0010000000000006924, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
goal_identified
=== ep: 697, time 25.36817717552185, eps 0.0010000000000006586, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
goal_identified
=== ep: 698, time 25.12470555305481, eps 0.0010000000000006265, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
goal_identified
goal_identified
=== ep: 699, time 34.96082258224487, eps 0.001000000000000596, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
goal_identified
=== ep: 700, time 25.305975198745728, eps 0.0010000000000005668, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
=== ep: 701, time 24.98322629928589, eps 0.0010000000000005393, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
goal_identified
=== ep: 702, time 25.933578491210938, eps 0.0010000000000005128, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 25.33771300315857, eps 0.001000000000000488, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
goal_identified
=== ep: 704, time 25.672502040863037, eps 0.001000000000000464, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
goal_identified
=== ep: 705, time 25.393168449401855, eps 0.0010000000000004415, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
=== ep: 706, time 25.168206930160522, eps 0.00100000000000042, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
=== ep: 707, time 25.309558391571045, eps 0.0010000000000003994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
=== ep: 708, time 25.512039184570312, eps 0.00100000000000038, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
=== ep: 709, time 35.349790811538696, eps 0.0010000000000003615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
=== ep: 710, time 25.37607192993164, eps 0.0010000000000003437, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
=== ep: 711, time 25.320300817489624, eps 0.001000000000000327, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
goal_identified
=== ep: 712, time 26.006068229675293, eps 0.0010000000000003112, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 25.417165994644165, eps 0.001000000000000296, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
=== ep: 714, time 30.70574188232422, eps 0.0010000000000002815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
goal_identified
goal_identified
=== ep: 715, time 24.84695792198181, eps 0.0010000000000002678, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 716, time 25.445197820663452, eps 0.0010000000000002548, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
goal_identified
=== ep: 717, time 25.622715711593628, eps 0.0010000000000002422, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
goal_identified
=== ep: 718, time 25.145737171173096, eps 0.0010000000000002305, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
goal_identified
=== ep: 719, time 35.00314807891846, eps 0.0010000000000002192, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
=== ep: 720, time 25.446544885635376, eps 0.0010000000000002086, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
goal_identified
=== ep: 721, time 24.768235683441162, eps 0.0010000000000001984, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
=== ep: 722, time 25.430216550827026, eps 0.0010000000000001887, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
=== ep: 723, time 25.12698721885681, eps 0.0010000000000001796, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
=== ep: 724, time 25.251919507980347, eps 0.0010000000000001707, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 25.41460943222046, eps 0.0010000000000001624, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
goal_identified
=== ep: 726, time 25.32075572013855, eps 0.0010000000000001544, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
goal_identified
=== ep: 727, time 25.33803701400757, eps 0.001000000000000147, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 25.373948335647583, eps 0.0010000000000001399, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
goal_identified
goal_identified
=== ep: 729, time 34.86339044570923, eps 0.001000000000000133, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 730, time 25.34220004081726, eps 0.0010000000000001264, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
=== ep: 731, time 25.043991088867188, eps 0.0010000000000001204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
goal_identified
=== ep: 732, time 25.183021545410156, eps 0.0010000000000001145, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
=== ep: 733, time 25.642436504364014, eps 0.0010000000000001089, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
=== ep: 734, time 25.32938051223755, eps 0.0010000000000001037, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
=== ep: 735, time 25.542530298233032, eps 0.0010000000000000985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
goal_identified
=== ep: 736, time 25.315661191940308, eps 0.0010000000000000937, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 737, time 25.164883613586426, eps 0.0010000000000000891, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
=== ep: 738, time 25.29388666152954, eps 0.0010000000000000848, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
=== ep: 739, time 34.57699179649353, eps 0.0010000000000000807, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
=== ep: 740, time 25.225054502487183, eps 0.0010000000000000768, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 25.809727668762207, eps 0.001000000000000073, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 25.283661603927612, eps 0.0010000000000000694, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
=== ep: 743, time 25.01058864593506, eps 0.001000000000000066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
goal_identified
=== ep: 744, time 25.372122049331665, eps 0.001000000000000063, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
goal_identified
=== ep: 745, time 25.409485340118408, eps 0.0010000000000000599, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
goal_identified
goal_identified
=== ep: 746, time 25.198182821273804, eps 0.0010000000000000568, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
=== ep: 747, time 25.52199411392212, eps 0.001000000000000054, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 25.253240823745728, eps 0.0010000000000000514, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
=== ep: 749, time 34.35496115684509, eps 0.001000000000000049, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
=== ep: 750, time 25.36376428604126, eps 0.0010000000000000466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 25.12063694000244, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 25.627063989639282, eps 0.001000000000000042, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
goal_identified
=== ep: 753, time 25.581430912017822, eps 0.0010000000000000401, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
=== ep: 754, time 25.62391686439514, eps 0.0010000000000000382, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
=== ep: 755, time 25.283315420150757, eps 0.0010000000000000362, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 756, time 25.191786527633667, eps 0.0010000000000000345, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
goal_identified
=== ep: 757, time 25.93290090560913, eps 0.0010000000000000328, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
=== ep: 758, time 26.135032653808594, eps 0.0010000000000000312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 34.67677569389343, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
=== ep: 760, time 25.485924005508423, eps 0.0010000000000000282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
=== ep: 761, time 25.342669010162354, eps 0.001000000000000027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
=== ep: 762, time 25.390036821365356, eps 0.0010000000000000256, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
=== ep: 763, time 25.51767921447754, eps 0.0010000000000000243, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 25.15861964225769, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
=== ep: 765, time 25.62297511100769, eps 0.001000000000000022, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
=== ep: 766, time 25.562844276428223, eps 0.0010000000000000208, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
goal_identified
goal_identified
=== ep: 767, time 25.73590612411499, eps 0.00100000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
=== ep: 768, time 25.927162408828735, eps 0.0010000000000000189, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
=== ep: 769, time 35.15065574645996, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 770, time 25.437840938568115, eps 0.0010000000000000172, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
goal_identified
=== ep: 771, time 25.53152847290039, eps 0.0010000000000000163, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
goal_identified
=== ep: 772, time 25.26196575164795, eps 0.0010000000000000154, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 25.332406044006348, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
goal_identified
=== ep: 774, time 25.368449926376343, eps 0.0010000000000000141, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
=== ep: 775, time 25.18954062461853, eps 0.0010000000000000132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
goal_identified
=== ep: 776, time 25.26663374900818, eps 0.0010000000000000126, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
=== ep: 777, time 25.27575707435608, eps 0.0010000000000000122, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
=== ep: 778, time 25.115556001663208, eps 0.0010000000000000115, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 34.732094526290894, eps 0.0010000000000000109, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
=== ep: 780, time 25.412851810455322, eps 0.0010000000000000104, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
=== ep: 781, time 25.39605474472046, eps 0.00100000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
=== ep: 782, time 25.324077129364014, eps 0.0010000000000000093, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 25.67999267578125, eps 0.001000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
=== ep: 784, time 25.693438053131104, eps 0.0010000000000000085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 25.3268404006958, eps 0.001000000000000008, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
goal_identified
=== ep: 786, time 25.654346704483032, eps 0.0010000000000000076, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 25.30330491065979, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
goal_identified
=== ep: 788, time 25.42953848838806, eps 0.001000000000000007, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
=== ep: 789, time 34.531105518341064, eps 0.0010000000000000067, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
=== ep: 790, time 25.11781358718872, eps 0.0010000000000000063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
=== ep: 791, time 25.20005774497986, eps 0.001000000000000006, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
=== ep: 792, time 33.64219689369202, eps 0.0010000000000000057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
goal_identified
=== ep: 793, time 25.86325240135193, eps 0.0010000000000000054, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
goal_identified
=== ep: 794, time 25.26226496696472, eps 0.0010000000000000052, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 25.956606149673462, eps 0.001000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
=== ep: 796, time 25.421894073486328, eps 0.0010000000000000048, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
goal_identified
goal_identified
=== ep: 797, time 25.04946255683899, eps 0.0010000000000000044, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
goal_identified
=== ep: 798, time 25.67623805999756, eps 0.0010000000000000041, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
=== ep: 799, time 32.67895197868347, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
=== ep: 800, time 25.039300680160522, eps 0.001000000000000004, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 25.291632652282715, eps 0.0010000000000000037, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
=== ep: 802, time 25.601497411727905, eps 0.0010000000000000035, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
goal_identified
=== ep: 803, time 25.336185216903687, eps 0.0010000000000000033, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
goal_identified
goal_identified
=== ep: 804, time 25.303972721099854, eps 0.001000000000000003, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
goal_identified
=== ep: 805, time 25.546141624450684, eps 0.001000000000000003, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
=== ep: 806, time 25.433207035064697, eps 0.0010000000000000028, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
=== ep: 807, time 25.570525407791138, eps 0.0010000000000000026, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
goal_identified
goal_identified
=== ep: 808, time 25.502252340316772, eps 0.0010000000000000026, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
goal_identified
=== ep: 809, time 33.010650873184204, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
goal_identified
goal_identified
goal_identified
=== ep: 810, time 25.510032176971436, eps 0.0010000000000000024, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
goal_identified
=== ep: 811, time 25.606337308883667, eps 0.0010000000000000022, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
=== ep: 812, time 25.507431745529175, eps 0.0010000000000000022, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
goal_identified
=== ep: 813, time 25.302794218063354, eps 0.001000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
goal_identified
=== ep: 814, time 25.575044870376587, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
goal_identified
=== ep: 815, time 25.197261571884155, eps 0.0010000000000000018, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
goal_identified
goal_identified
=== ep: 816, time 25.401405096054077, eps 0.0010000000000000018, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
=== ep: 817, time 25.633815050125122, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
goal_identified
goal_identified
goal_identified
=== ep: 818, time 25.20579481124878, eps 0.0010000000000000015, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
goal_identified
=== ep: 819, time 34.21553707122803, eps 0.0010000000000000015, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
goal_identified
=== ep: 820, time 25.757460594177246, eps 0.0010000000000000013, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
goal_identified
goal_identified
=== ep: 821, time 25.602385759353638, eps 0.0010000000000000013, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
=== ep: 822, time 25.313047170639038, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 25.390803337097168, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
goal_identified
=== ep: 824, time 25.430286407470703, eps 0.001000000000000001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
goal_identified
=== ep: 825, time 25.35627293586731, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 826, time 25.670924186706543, eps 0.001000000000000001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
goal_identified
goal_identified
=== ep: 827, time 25.497634172439575, eps 0.001000000000000001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
goal_identified
goal_identified
=== ep: 828, time 25.644836902618408, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
=== ep: 829, time 34.5603883266449, eps 0.0010000000000000009, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
=== ep: 830, time 25.392651081085205, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
=== ep: 831, time 25.58948588371277, eps 0.0010000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
goal_identified
=== ep: 832, time 25.590524196624756, eps 0.0010000000000000009, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
goal_identified
=== ep: 833, time 25.381743669509888, eps 0.0010000000000000007, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
=== ep: 834, time 25.49479031562805, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
goal_identified
=== ep: 835, time 25.62116503715515, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
goal_identified
=== ep: 836, time 25.744658946990967, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
=== ep: 837, time 26.201247692108154, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
=== ep: 838, time 26.759285926818848, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
=== ep: 839, time 35.96824407577515, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
goal_identified
=== ep: 840, time 26.547561645507812, eps 0.0010000000000000005, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 841, time 26.321341514587402, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
=== ep: 842, time 26.080020427703857, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
=== ep: 843, time 26.583577871322632, eps 0.0010000000000000005, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
goal_identified
=== ep: 844, time 26.44722294807434, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
goal_identified
=== ep: 845, time 26.50017547607422, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
=== ep: 846, time 26.52741503715515, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
goal_identified
goal_identified
=== ep: 847, time 26.710483074188232, eps 0.0010000000000000005, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
goal_identified
=== ep: 848, time 25.956103563308716, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
goal_identified
=== ep: 849, time 36.04103398323059, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
=== ep: 850, time 26.785967588424683, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 26.29885768890381, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
goal_identified
goal_identified
=== ep: 852, time 26.36028480529785, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
goal_identified
=== ep: 853, time 26.19644522666931, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
goal_identified
goal_identified
=== ep: 854, time 26.277939558029175, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
=== ep: 855, time 26.594223737716675, eps 0.0010000000000000002, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
=== ep: 856, time 26.951438188552856, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
=== ep: 857, time 26.38511562347412, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
goal_identified
=== ep: 858, time 26.543562173843384, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
=== ep: 859, time 34.68916296958923, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
goal_identified
=== ep: 860, time 26.483363151550293, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
=== ep: 861, time 26.192566633224487, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
=== ep: 862, time 26.433955430984497, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
=== ep: 863, time 26.377573490142822, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
=== ep: 864, time 26.78456997871399, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
goal_identified
goal_identified
=== ep: 865, time 26.403399229049683, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
goal_identified
=== ep: 866, time 26.365200519561768, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
goal_identified
=== ep: 867, time 25.9974684715271, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
goal_identified
goal_identified
=== ep: 868, time 26.087318897247314, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
=== ep: 869, time 34.42512059211731, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
=== ep: 870, time 26.435370683670044, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 26.808889865875244, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
=== ep: 872, time 26.52223491668701, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
=== ep: 873, time 26.359328508377075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
=== ep: 874, time 26.386121034622192, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
goal_identified
=== ep: 875, time 26.036340951919556, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
=== ep: 876, time 26.523101568222046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 26.29430627822876, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
=== ep: 878, time 26.439982175827026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
goal_identified
=== ep: 879, time 34.32874417304993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
=== ep: 880, time 26.260314226150513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
goal_identified
=== ep: 881, time 26.22802448272705, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
goal_identified
goal_identified
=== ep: 882, time 26.18877339363098, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
=== ep: 883, time 26.462899923324585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
goal_identified
=== ep: 884, time 26.37981605529785, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
=== ep: 885, time 26.232205152511597, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
=== ep: 886, time 26.154601335525513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
goal_identified
goal_identified
=== ep: 887, time 26.18862771987915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 888, time 26.37098002433777, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
goal_identified
=== ep: 889, time 33.91453433036804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 26.668105125427246, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
=== ep: 891, time 26.77161455154419, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
=== ep: 892, time 26.41018557548523, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
=== ep: 893, time 26.442999601364136, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
=== ep: 894, time 26.744023323059082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
=== ep: 895, time 26.618450164794922, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
=== ep: 896, time 26.75360679626465, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
=== ep: 897, time 26.512639045715332, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
goal_identified
=== ep: 898, time 26.448795318603516, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
=== ep: 899, time 33.82200288772583, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
goal_identified
=== ep: 900, time 26.762877941131592, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
goal_identified
goal_identified
=== ep: 901, time 26.81572198867798, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
=== ep: 902, time 26.35594081878662, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
goal_identified
=== ep: 903, time 26.266791105270386, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
=== ep: 904, time 26.895057916641235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
goal_identified
goal_identified
goal_identified
=== ep: 905, time 26.49482226371765, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
=== ep: 906, time 26.6977641582489, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
=== ep: 907, time 26.29970932006836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
=== ep: 908, time 26.616211891174316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
=== ep: 909, time 34.68825960159302, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
=== ep: 910, time 26.36719560623169, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
goal_identified
goal_identified
=== ep: 911, time 26.30945348739624, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
goal_identified
=== ep: 912, time 26.706870555877686, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
=== ep: 913, time 26.725906133651733, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 26.05616593360901, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
=== ep: 915, time 26.81005096435547, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 915
=== ep: 916, time 26.463684797286987, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 26.27304482460022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
=== ep: 918, time 26.53236174583435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
=== ep: 919, time 34.25420546531677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
=== ep: 920, time 26.53282356262207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
=== ep: 921, time 26.009063243865967, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
goal_identified
=== ep: 922, time 26.16974401473999, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 25.67981266975403, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
goal_identified
=== ep: 924, time 25.720702171325684, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
=== ep: 925, time 25.511276721954346, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
goal_identified
=== ep: 926, time 25.60839056968689, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
goal_identified
=== ep: 927, time 25.596274375915527, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
=== ep: 928, time 25.602946996688843, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
=== ep: 929, time 32.61787438392639, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
=== ep: 930, time 25.619543313980103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
=== ep: 931, time 25.55166006088257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
goal_identified
=== ep: 932, time 25.462526559829712, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
=== ep: 933, time 25.64841318130493, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 934, time 26.076964139938354, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
goal_identified
=== ep: 935, time 25.50488018989563, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
goal_identified
=== ep: 936, time 25.383080005645752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
=== ep: 937, time 25.569714069366455, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
=== ep: 938, time 25.45487666130066, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
=== ep: 939, time 32.928014278411865, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
=== ep: 940, time 25.77731680870056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
=== ep: 941, time 26.559056282043457, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
=== ep: 942, time 26.671427965164185, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
goal_identified
=== ep: 943, time 26.45211887359619, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
=== ep: 944, time 26.55827236175537, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
=== ep: 945, time 26.439926147460938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
=== ep: 946, time 26.046223878860474, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
=== ep: 947, time 26.67413902282715, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
goal_identified
=== ep: 948, time 26.41390585899353, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
=== ep: 949, time 34.07564401626587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
=== ep: 950, time 26.352051973342896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
goal_identified
=== ep: 951, time 26.607455253601074, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
=== ep: 952, time 26.63939070701599, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
=== ep: 953, time 26.309626817703247, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
=== ep: 954, time 26.313738346099854, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
goal_identified
goal_identified
=== ep: 955, time 26.39102554321289, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
goal_identified
=== ep: 956, time 26.28082776069641, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
=== ep: 957, time 26.625806093215942, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 26.64847755432129, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
goal_identified
=== ep: 959, time 34.617733001708984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
=== ep: 960, time 26.3061785697937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
goal_identified
=== ep: 961, time 26.279152631759644, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
goal_identified
=== ep: 962, time 26.749196529388428, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
goal_identified
=== ep: 963, time 26.66480803489685, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
goal_identified
=== ep: 964, time 26.46760869026184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
=== ep: 965, time 26.24084734916687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
=== ep: 966, time 26.383214473724365, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 26.633942127227783, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 26.2721266746521, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
goal_identified
goal_identified
=== ep: 969, time 36.04867959022522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
=== ep: 970, time 26.327267169952393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
goal_identified
=== ep: 971, time 27.33091115951538, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
=== ep: 972, time 27.775482177734375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
goal_identified
=== ep: 973, time 27.893871307373047, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
goal_identified
=== ep: 974, time 27.55763578414917, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
goal_identified
=== ep: 975, time 27.439889907836914, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
=== ep: 976, time 27.592080116271973, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
goal_identified
=== ep: 977, time 27.759209871292114, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
=== ep: 978, time 27.571360111236572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
goal_identified
=== ep: 979, time 36.34693360328674, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
=== ep: 980, time 27.802232027053833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
=== ep: 981, time 27.60038661956787, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
=== ep: 982, time 27.295289278030396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 27.449986696243286, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
=== ep: 984, time 27.17275857925415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
=== ep: 985, time 27.28810977935791, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
=== ep: 986, time 28.164631128311157, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
=== ep: 987, time 27.319623947143555, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
goal_identified
=== ep: 988, time 27.211089849472046, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
goal_identified
=== ep: 989, time 36.797494411468506, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
=== ep: 990, time 28.852884769439697, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
=== ep: 991, time 28.405370712280273, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
goal_identified
=== ep: 992, time 28.498932361602783, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
=== ep: 993, time 29.065372228622437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
=== ep: 994, time 27.99976873397827, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
=== ep: 995, time 26.37862801551819, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
=== ep: 996, time 25.566076278686523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
=== ep: 997, time 25.626510858535767, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
=== ep: 998, time 25.635944843292236, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
=== ep: 999, time 33.24306893348694, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
goal_identified
goal_identified
goal_identified
=== ep: 1000, time 25.659619569778442, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
=== ep: 1001, time 25.781347513198853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
goal_identified
goal_identified
=== ep: 1002, time 26.85436248779297, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
goal_identified
goal_identified
=== ep: 1003, time 26.17446756362915, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
goal_identified
=== ep: 1004, time 25.62251591682434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
goal_identified
goal_identified
=== ep: 1005, time 25.59703803062439, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
goal_identified
=== ep: 1006, time 25.133947134017944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
goal_identified
=== ep: 1007, time 28.3140549659729, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
goal_identified
goal_identified
=== ep: 1008, time 28.22551465034485, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
goal_identified
=== ep: 1009, time 36.62830710411072, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
=== ep: 1010, time 28.11982250213623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
=== ep: 1011, time 28.46126103401184, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
=== ep: 1012, time 28.32854914665222, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
=== ep: 1013, time 28.400927782058716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
=== ep: 1014, time 26.28945755958557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 17/17)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
=== ep: 1015, time 28.369123697280884, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
goal_identified
=== ep: 1016, time 28.356346368789673, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
=== ep: 1017, time 28.45468759536743, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
goal_identified
=== ep: 1018, time 28.58338165283203, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
goal_identified
=== ep: 1019, time 38.20690131187439, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
goal_identified
goal_identified
=== ep: 1020, time 28.386560916900635, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
=== ep: 1021, time 28.340449333190918, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
goal_identified
goal_identified
=== ep: 1022, time 28.44135880470276, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
=== ep: 1023, time 28.005417585372925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
=== ep: 1024, time 28.148377656936646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
goal_identified
=== ep: 1025, time 30.566450595855713, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
=== ep: 1026, time 28.53213667869568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
goal_identified
goal_identified
=== ep: 1027, time 27.968334913253784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
=== ep: 1028, time 28.383838653564453, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
=== ep: 1029, time 38.65185785293579, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
=== ep: 1030, time 28.906091690063477, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
=== ep: 1031, time 28.945059299468994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
=== ep: 1032, time 36.432565689086914, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
=== ep: 1033, time 29.08645987510681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
=== ep: 1034, time 29.123998880386353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
=== ep: 1035, time 28.903213262557983, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
=== ep: 1036, time 28.596702098846436, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
=== ep: 1037, time 29.253033876419067, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
goal_identified
goal_identified
=== ep: 1038, time 28.50206446647644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
=== ep: 1039, time 39.11687135696411, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
=== ep: 1040, time 29.00360083580017, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
=== ep: 1041, time 28.484771251678467, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1041
=== ep: 1042, time 28.784475803375244, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
=== ep: 1043, time 29.088462352752686, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
=== ep: 1044, time 29.676496267318726, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
=== ep: 1045, time 29.18703866004944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
=== ep: 1046, time 29.793096780776978, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
=== ep: 1047, time 28.742159843444824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
goal_identified
=== ep: 1048, time 29.317657232284546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
=== ep: 1049, time 39.150761127471924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
goal_identified
=== ep: 1050, time 29.35227394104004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
=== ep: 1051, time 28.780601739883423, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
goal_identified
=== ep: 1052, time 29.126147270202637, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
goal_identified
goal_identified
=== ep: 1053, time 29.500160455703735, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
goal_identified
=== ep: 1054, time 29.350652933120728, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
=== ep: 1055, time 29.277268409729004, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
goal_identified
=== ep: 1056, time 29.52754282951355, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
goal_identified
=== ep: 1057, time 29.803224086761475, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
goal_identified
goal_identified
=== ep: 1058, time 29.36177682876587, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
goal_identified
goal_identified
=== ep: 1059, time 39.18607306480408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
goal_identified
=== ep: 1060, time 29.80977702140808, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
goal_identified
goal_identified
goal_identified
=== ep: 1061, time 29.152377605438232, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
=== ep: 1062, time 29.448638916015625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
=== ep: 1063, time 29.247690439224243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
=== ep: 1064, time 29.487762451171875, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
=== ep: 1065, time 29.235902309417725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
goal_identified
=== ep: 1066, time 29.61483144760132, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
goal_identified
=== ep: 1067, time 29.643824815750122, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
goal_identified
=== ep: 1068, time 29.20087432861328, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
goal_identified
goal_identified
=== ep: 1069, time 39.100679874420166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
=== ep: 1070, time 29.384535312652588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
=== ep: 1071, time 29.2521493434906, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
=== ep: 1072, time 29.271395206451416, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
goal_identified
=== ep: 1073, time 29.450096368789673, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
=== ep: 1074, time 29.60610556602478, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
=== ep: 1075, time 29.796191930770874, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
goal_identified
=== ep: 1076, time 29.22429871559143, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
=== ep: 1077, time 29.77280592918396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
goal_identified
=== ep: 1078, time 29.45284914970398, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
=== ep: 1079, time 39.26538681983948, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
=== ep: 1080, time 29.45171308517456, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
=== ep: 1081, time 29.640958547592163, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
=== ep: 1082, time 29.181209802627563, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
=== ep: 1083, time 29.096829652786255, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
goal_identified
=== ep: 1084, time 29.662773609161377, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
goal_identified
=== ep: 1085, time 29.157548189163208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
=== ep: 1086, time 29.36598515510559, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
goal_identified
=== ep: 1087, time 29.14732551574707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
goal_identified
=== ep: 1088, time 29.3269784450531, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
=== ep: 1089, time 39.03209066390991, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
=== ep: 1090, time 29.040662050247192, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
=== ep: 1091, time 28.346360445022583, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
=== ep: 1092, time 27.9635591506958, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
=== ep: 1093, time 28.78795576095581, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
goal_identified
=== ep: 1094, time 28.546239137649536, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
=== ep: 1095, time 28.047858238220215, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
goal_identified
=== ep: 1096, time 28.131500482559204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
=== ep: 1097, time 27.864037036895752, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
=== ep: 1098, time 28.486845016479492, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
=== ep: 1099, time 37.66259741783142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
=== ep: 1100, time 28.860923290252686, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
=== ep: 1101, time 27.992347240447998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
goal_identified
=== ep: 1102, time 28.47107768058777, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
=== ep: 1103, time 28.09060049057007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
=== ep: 1104, time 28.685023307800293, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
=== ep: 1105, time 28.37190818786621, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
=== ep: 1106, time 28.56317710876465, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
goal_identified
=== ep: 1107, time 28.303986072540283, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
goal_identified
=== ep: 1108, time 28.67540740966797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
=== ep: 1109, time 37.500730991363525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
=== ep: 1110, time 28.340853452682495, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
goal_identified
=== ep: 1111, time 28.80893850326538, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
=== ep: 1112, time 28.435472011566162, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
goal_identified
=== ep: 1113, time 28.288257122039795, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
goal_identified
goal_identified
=== ep: 1114, time 28.325393199920654, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
=== ep: 1115, time 28.21410608291626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
=== ep: 1116, time 28.867866277694702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
=== ep: 1117, time 28.414838314056396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
=== ep: 1118, time 28.716851472854614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
=== ep: 1119, time 37.62551188468933, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
goal_identified
goal_identified
=== ep: 1120, time 28.208549976348877, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
goal_identified
=== ep: 1121, time 28.418726921081543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
=== ep: 1122, time 28.418022632598877, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 28.4509060382843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
goal_identified
=== ep: 1124, time 28.158637285232544, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
goal_identified
=== ep: 1125, time 28.97785258293152, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
goal_identified
=== ep: 1126, time 28.357458114624023, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
=== ep: 1127, time 28.700634717941284, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
=== ep: 1128, time 28.051373958587646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
=== ep: 1129, time 37.553237438201904, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
=== ep: 1130, time 27.76715350151062, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
=== ep: 1131, time 27.64648962020874, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
=== ep: 1132, time 25.82087802886963, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
=== ep: 1133, time 26.02444100379944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
=== ep: 1134, time 25.863975763320923, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
=== ep: 1135, time 25.50481915473938, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
goal_identified
=== ep: 1136, time 25.206579208374023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
=== ep: 1137, time 25.57260012626648, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
=== ep: 1138, time 25.59198498725891, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
=== ep: 1139, time 36.73695111274719, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
goal_identified
=== ep: 1140, time 26.87550687789917, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
goal_identified
goal_identified
=== ep: 1141, time 27.00950002670288, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
=== ep: 1142, time 26.98919439315796, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
=== ep: 1143, time 26.838644981384277, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
=== ep: 1144, time 26.873502492904663, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
goal_identified
=== ep: 1145, time 26.607304334640503, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
goal_identified
=== ep: 1146, time 26.87787413597107, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
=== ep: 1147, time 26.59240198135376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
goal_identified
=== ep: 1148, time 26.820655822753906, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
goal_identified
=== ep: 1149, time 36.15196752548218, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
=== ep: 1150, time 26.990655660629272, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
=== ep: 1151, time 26.792303323745728, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
=== ep: 1152, time 26.964940547943115, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
goal_identified
=== ep: 1153, time 26.889414310455322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
=== ep: 1154, time 26.872390508651733, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
=== ep: 1155, time 26.39638376235962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
=== ep: 1156, time 27.09896945953369, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
=== ep: 1157, time 26.94979214668274, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
=== ep: 1158, time 26.748160123825073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
=== ep: 1159, time 36.95219850540161, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
=== ep: 1160, time 27.12514042854309, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
=== ep: 1161, time 27.275699377059937, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
=== ep: 1162, time 27.291649103164673, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
=== ep: 1163, time 27.416772842407227, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
=== ep: 1164, time 26.87817668914795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
=== ep: 1165, time 27.32321071624756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
=== ep: 1166, time 26.859581470489502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
goal_identified
=== ep: 1167, time 26.703185081481934, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
=== ep: 1168, time 26.85709309577942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
=== ep: 1169, time 36.87643027305603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
goal_identified
=== ep: 1170, time 26.95604181289673, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
=== ep: 1171, time 26.417141437530518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
=== ep: 1172, time 27.002012729644775, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
=== ep: 1173, time 26.667177200317383, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 26.949989557266235, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
=== ep: 1175, time 26.622419834136963, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
=== ep: 1176, time 27.007339000701904, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
=== ep: 1177, time 25.924818992614746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
goal_identified
=== ep: 1178, time 25.110846519470215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
=== ep: 1179, time 35.66430163383484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
=== ep: 1180, time 26.75104069709778, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
=== ep: 1181, time 26.893324851989746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
=== ep: 1182, time 26.42786407470703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
=== ep: 1183, time 26.639764547348022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
goal_identified
=== ep: 1184, time 26.877124547958374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
goal_identified
=== ep: 1185, time 26.78525161743164, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
goal_identified
=== ep: 1186, time 26.271410703659058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
=== ep: 1187, time 26.704819917678833, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
=== ep: 1188, time 26.301268100738525, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
=== ep: 1189, time 35.96012735366821, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
goal_identified
goal_identified
=== ep: 1190, time 26.30959701538086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
goal_identified
=== ep: 1191, time 26.616692066192627, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
=== ep: 1192, time 26.80522847175598, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
=== ep: 1193, time 26.893125772476196, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
goal_identified
=== ep: 1194, time 26.941237449645996, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
=== ep: 1195, time 26.851367235183716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
goal_identified
=== ep: 1196, time 26.386889934539795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
=== ep: 1197, time 26.788734674453735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
goal_identified
=== ep: 1198, time 26.5859956741333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
=== ep: 1199, time 37.17582869529724, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
goal_identified
=== ep: 1200, time 26.709118366241455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
=== ep: 1201, time 26.637934923171997, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
=== ep: 1202, time 26.7294442653656, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
goal_identified
=== ep: 1203, time 26.39651656150818, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
=== ep: 1204, time 26.83141255378723, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
=== ep: 1205, time 26.722349882125854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
=== ep: 1206, time 26.45229172706604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
goal_identified
goal_identified
=== ep: 1207, time 26.86002802848816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
=== ep: 1208, time 26.617458820343018, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
=== ep: 1209, time 36.95780920982361, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
=== ep: 1210, time 26.876031637191772, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
=== ep: 1211, time 26.69094729423523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
goal_identified
=== ep: 1212, time 26.448870182037354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
goal_identified
=== ep: 1213, time 26.483728647232056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
goal_identified
=== ep: 1214, time 26.81963872909546, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
=== ep: 1215, time 26.33087658882141, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
=== ep: 1216, time 26.57970881462097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
=== ep: 1217, time 26.480573892593384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
=== ep: 1218, time 26.14348006248474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
=== ep: 1219, time 36.42270040512085, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
goal_identified
=== ep: 1220, time 26.499890565872192, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
=== ep: 1221, time 26.599288940429688, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
goal_identified
=== ep: 1222, time 26.400453329086304, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
goal_identified
=== ep: 1223, time 26.52911615371704, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 26.18458843231201, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
=== ep: 1225, time 26.739898443222046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
goal_identified
=== ep: 1226, time 26.647937774658203, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
=== ep: 1227, time 26.998819828033447, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
=== ep: 1228, time 26.38647985458374, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
=== ep: 1229, time 38.16091465950012, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
=== ep: 1230, time 27.240634441375732, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
goal_identified
=== ep: 1231, time 26.828933000564575, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 26.78444766998291, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
goal_identified
=== ep: 1233, time 26.67947268486023, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
=== ep: 1234, time 26.79177212715149, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
=== ep: 1235, time 26.79375386238098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1235
=== ep: 1236, time 26.181346893310547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
goal_identified
=== ep: 1237, time 26.27281379699707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
goal_identified
=== ep: 1238, time 26.99494743347168, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
=== ep: 1239, time 37.164539098739624, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
goal_identified
=== ep: 1240, time 26.576252937316895, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
goal_identified
=== ep: 1241, time 26.77622652053833, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
=== ep: 1242, time 26.4377658367157, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
=== ep: 1243, time 26.78170108795166, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
=== ep: 1244, time 26.524614810943604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
goal_identified
=== ep: 1245, time 26.564090728759766, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
=== ep: 1246, time 26.4891414642334, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
goal_identified
=== ep: 1247, time 26.58431601524353, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
=== ep: 1248, time 26.43681263923645, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
=== ep: 1249, time 36.99775505065918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
=== ep: 1250, time 26.26482319831848, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
goal_identified
goal_identified
=== ep: 1251, time 26.505467414855957, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
goal_identified
goal_identified
=== ep: 1252, time 26.74116826057434, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
goal_identified
=== ep: 1253, time 26.83889889717102, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
=== ep: 1254, time 26.669437408447266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
=== ep: 1255, time 26.615138053894043, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
=== ep: 1256, time 27.33959436416626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
goal_identified
=== ep: 1257, time 26.342469453811646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
=== ep: 1258, time 26.539307594299316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
=== ep: 1259, time 35.838592290878296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
=== ep: 1260, time 26.78325867652893, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
=== ep: 1261, time 26.597912311553955, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
=== ep: 1262, time 26.866989374160767, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
=== ep: 1263, time 26.951985359191895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 27.009459733963013, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
goal_identified
=== ep: 1265, time 26.346965789794922, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
=== ep: 1266, time 26.661406755447388, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
=== ep: 1267, time 26.829177856445312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
=== ep: 1268, time 26.863810062408447, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
=== ep: 1269, time 35.03379774093628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
=== ep: 1270, time 26.59650683403015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
=== ep: 1271, time 26.552478075027466, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
goal_identified
=== ep: 1272, time 26.80725121498108, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
=== ep: 1273, time 26.642822742462158, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
goal_identified
=== ep: 1274, time 26.641472101211548, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
goal_identified
=== ep: 1275, time 26.601689100265503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
goal_identified
=== ep: 1276, time 26.698681831359863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
goal_identified
=== ep: 1277, time 26.627943992614746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
=== ep: 1278, time 26.54803729057312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
=== ep: 1279, time 35.74244809150696, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
goal_identified
=== ep: 1280, time 26.54156756401062, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
=== ep: 1281, time 27.106212615966797, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
=== ep: 1282, time 26.583815574645996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
=== ep: 1283, time 26.580995082855225, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
goal_identified
goal_identified
=== ep: 1284, time 26.417174577713013, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
goal_identified
=== ep: 1285, time 27.03497338294983, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
goal_identified
=== ep: 1286, time 26.83529233932495, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
=== ep: 1287, time 26.684468269348145, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
=== ep: 1288, time 26.79691505432129, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
=== ep: 1289, time 35.60498261451721, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
=== ep: 1290, time 26.898645877838135, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
goal_identified
=== ep: 1291, time 26.572947025299072, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
=== ep: 1292, time 26.429455995559692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
=== ep: 1293, time 26.64552092552185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
=== ep: 1294, time 26.62058162689209, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
goal_identified
=== ep: 1295, time 26.970739364624023, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
=== ep: 1296, time 26.813350439071655, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
goal_identified
=== ep: 1297, time 26.616292476654053, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
=== ep: 1298, time 26.77222228050232, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
=== ep: 1299, time 35.39401841163635, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
goal_identified
=== ep: 1300, time 26.495054483413696, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
goal_identified
goal_identified
=== ep: 1301, time 26.87788701057434, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
=== ep: 1302, time 27.255309581756592, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
=== ep: 1303, time 27.513885736465454, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
=== ep: 1304, time 27.174827337265015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
=== ep: 1305, time 27.353372812271118, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
goal_identified
=== ep: 1306, time 27.182353258132935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
=== ep: 1307, time 27.194765329360962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
=== ep: 1308, time 27.57984495162964, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
=== ep: 1309, time 36.680867433547974, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
goal_identified
=== ep: 1310, time 27.14790964126587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
goal_identified
=== ep: 1311, time 27.460026264190674, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
goal_identified
=== ep: 1312, time 27.440890073776245, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
=== ep: 1313, time 27.04475426673889, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
=== ep: 1314, time 27.348105430603027, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
=== ep: 1315, time 27.560792207717896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
goal_identified
=== ep: 1316, time 27.284881830215454, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
=== ep: 1317, time 27.57268714904785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
goal_identified
=== ep: 1318, time 27.76762294769287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
=== ep: 1319, time 36.01739239692688, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
goal_identified
=== ep: 1320, time 27.769832849502563, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
goal_identified
=== ep: 1321, time 27.025017499923706, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
goal_identified
=== ep: 1322, time 27.095439434051514, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
=== ep: 1323, time 26.701526403427124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
goal_identified
goal_identified
=== ep: 1324, time 26.837542057037354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
=== ep: 1325, time 27.267175436019897, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
goal_identified
goal_identified
=== ep: 1326, time 26.939399480819702, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
goal_identified
=== ep: 1327, time 26.957879543304443, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
goal_identified
=== ep: 1328, time 26.975810527801514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
=== ep: 1329, time 36.99862623214722, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
=== ep: 1330, time 26.91819667816162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
=== ep: 1331, time 28.056618452072144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
=== ep: 1332, time 27.01887345314026, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
=== ep: 1333, time 27.131646871566772, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
=== ep: 1334, time 27.12787127494812, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
goal_identified
=== ep: 1335, time 26.43618154525757, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
goal_identified
goal_identified
=== ep: 1336, time 27.2288920879364, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
goal_identified
goal_identified
=== ep: 1337, time 27.059231996536255, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
goal_identified
=== ep: 1338, time 27.021681308746338, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
=== ep: 1339, time 36.65378546714783, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
goal_identified
goal_identified
=== ep: 1340, time 26.852195978164673, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
goal_identified
=== ep: 1341, time 27.21420431137085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
goal_identified
=== ep: 1342, time 27.075628995895386, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
goal_identified
goal_identified
=== ep: 1343, time 27.02132821083069, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
=== ep: 1344, time 27.39073133468628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
=== ep: 1345, time 27.195476531982422, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
goal_identified
goal_identified
=== ep: 1346, time 26.870160818099976, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
=== ep: 1347, time 27.141353607177734, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
goal_identified
=== ep: 1348, time 27.30852961540222, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
goal_identified
goal_identified
=== ep: 1349, time 36.784916162490845, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
goal_identified
=== ep: 1350, time 27.18968439102173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
=== ep: 1351, time 27.344555377960205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
=== ep: 1352, time 27.142133951187134, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
goal_identified
=== ep: 1353, time 27.060020208358765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
=== ep: 1354, time 27.41041922569275, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
goal_identified
=== ep: 1355, time 27.270297288894653, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
=== ep: 1356, time 26.939049243927002, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
=== ep: 1357, time 26.88334059715271, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
=== ep: 1358, time 26.96580481529236, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
goal_identified
goal_identified
=== ep: 1359, time 36.40467357635498, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
goal_identified
=== ep: 1360, time 27.071748971939087, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
goal_identified
=== ep: 1361, time 26.835756063461304, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
=== ep: 1362, time 26.86818838119507, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
=== ep: 1363, time 27.10880160331726, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
=== ep: 1364, time 27.274850130081177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
=== ep: 1365, time 27.477157592773438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
=== ep: 1366, time 27.03043293952942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
=== ep: 1367, time 27.082006692886353, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
=== ep: 1368, time 26.99164342880249, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
=== ep: 1369, time 40.57679057121277, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
goal_identified
=== ep: 1370, time 27.220423221588135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
goal_identified
=== ep: 1371, time 27.02975583076477, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
=== ep: 1372, time 27.293632745742798, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
=== ep: 1373, time 26.754295587539673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
goal_identified
=== ep: 1374, time 27.36033010482788, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
goal_identified
=== ep: 1375, time 26.832719326019287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
=== ep: 1376, time 27.022486925125122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
goal_identified
goal_identified
=== ep: 1377, time 27.010910749435425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
=== ep: 1378, time 27.111730813980103, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
=== ep: 1379, time 36.73031687736511, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
=== ep: 1380, time 27.29216957092285, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
=== ep: 1381, time 27.441911458969116, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
=== ep: 1382, time 26.78615403175354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
=== ep: 1383, time 27.34265971183777, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
=== ep: 1384, time 27.414782524108887, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
=== ep: 1385, time 27.292895555496216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
=== ep: 1386, time 27.299183130264282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
=== ep: 1387, time 27.173433780670166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
goal_identified
=== ep: 1388, time 27.910076379776, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
goal_identified
=== ep: 1389, time 37.26386046409607, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
=== ep: 1390, time 27.424116849899292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
goal_identified
goal_identified
=== ep: 1391, time 27.320642709732056, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
goal_identified
=== ep: 1392, time 27.55266499519348, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
=== ep: 1393, time 26.84980583190918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
=== ep: 1394, time 27.417590856552124, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
goal_identified
=== ep: 1395, time 27.07810664176941, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
=== ep: 1396, time 27.083150148391724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
=== ep: 1397, time 27.401373624801636, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
=== ep: 1398, time 27.07599377632141, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
=== ep: 1399, time 38.32404589653015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
=== ep: 1400, time 27.29240894317627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
=== ep: 1401, time 27.04517698287964, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
goal_identified
=== ep: 1402, time 27.009705305099487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
=== ep: 1403, time 27.096874475479126, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
=== ep: 1404, time 27.016406774520874, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
goal_identified
=== ep: 1405, time 27.116450309753418, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
=== ep: 1406, time 27.393802165985107, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
=== ep: 1407, time 27.42219877243042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
goal_identified
=== ep: 1408, time 27.301798105239868, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
=== ep: 1409, time 37.61749219894409, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
goal_identified
=== ep: 1410, time 27.290671586990356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
=== ep: 1411, time 27.40073537826538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
=== ep: 1412, time 27.313374996185303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
=== ep: 1413, time 27.32113528251648, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
=== ep: 1414, time 27.104790449142456, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
=== ep: 1415, time 27.359091997146606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1415
=== ep: 1416, time 26.995060682296753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
=== ep: 1417, time 27.349259853363037, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
goal_identified
goal_identified
=== ep: 1418, time 27.43500781059265, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
goal_identified
=== ep: 1419, time 37.02923345565796, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
goal_identified
=== ep: 1420, time 26.771308422088623, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 26.906655311584473, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
goal_identified
=== ep: 1422, time 27.269299507141113, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
=== ep: 1423, time 27.396244049072266, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 27.15099835395813, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
goal_identified
=== ep: 1425, time 27.219278812408447, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
=== ep: 1426, time 27.08599543571472, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
goal_identified
=== ep: 1427, time 26.78491973876953, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
goal_identified
=== ep: 1428, time 26.961533069610596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
goal_identified
=== ep: 1429, time 36.96387767791748, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
=== ep: 1430, time 27.18263006210327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
=== ep: 1431, time 27.059288501739502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
=== ep: 1432, time 27.26356339454651, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
=== ep: 1433, time 27.253754377365112, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
=== ep: 1434, time 27.00870370864868, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
=== ep: 1435, time 27.50893521308899, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
=== ep: 1436, time 27.150655269622803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
=== ep: 1437, time 26.743361711502075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
=== ep: 1438, time 27.24128246307373, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
=== ep: 1439, time 37.308576583862305, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
=== ep: 1440, time 27.54875421524048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
=== ep: 1441, time 26.8484103679657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
=== ep: 1442, time 27.359336376190186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
goal_identified
goal_identified
=== ep: 1443, time 26.95971155166626, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
goal_identified
goal_identified
=== ep: 1444, time 27.206154108047485, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
=== ep: 1445, time 26.89061665534973, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
goal_identified
goal_identified
goal_identified
=== ep: 1446, time 27.32684350013733, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
=== ep: 1447, time 27.400317668914795, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
=== ep: 1448, time 27.233131408691406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
goal_identified
=== ep: 1449, time 36.649388551712036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
goal_identified
=== ep: 1450, time 27.10185146331787, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
=== ep: 1451, time 27.19442391395569, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
goal_identified
=== ep: 1452, time 27.354278087615967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
goal_identified
=== ep: 1453, time 27.266054391860962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
=== ep: 1454, time 27.991371154785156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
goal_identified
goal_identified
=== ep: 1455, time 26.81606698036194, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
goal_identified
goal_identified
=== ep: 1456, time 27.60833764076233, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
goal_identified
=== ep: 1457, time 27.253065586090088, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
goal_identified
goal_identified
=== ep: 1458, time 27.06696605682373, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
=== ep: 1459, time 37.15860319137573, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
=== ep: 1460, time 27.46532154083252, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
goal_identified
=== ep: 1461, time 27.344155311584473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
=== ep: 1462, time 27.048997402191162, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
goal_identified
=== ep: 1463, time 27.17736554145813, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
=== ep: 1464, time 27.021204233169556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
=== ep: 1465, time 27.56384015083313, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
=== ep: 1466, time 27.302220106124878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
goal_identified
=== ep: 1467, time 27.029627084732056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
goal_identified
goal_identified
=== ep: 1468, time 27.461653470993042, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
=== ep: 1469, time 36.736653566360474, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
goal_identified
=== ep: 1470, time 27.109821796417236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
=== ep: 1471, time 27.60372567176819, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
=== ep: 1472, time 27.179133415222168, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
=== ep: 1473, time 27.166354179382324, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
=== ep: 1474, time 27.05981135368347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1475, time 27.136975049972534, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
goal_identified
=== ep: 1476, time 27.0695161819458, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
=== ep: 1477, time 27.47553300857544, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
=== ep: 1478, time 27.456790447235107, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
=== ep: 1479, time 36.594473361968994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
goal_identified
goal_identified
goal_identified
=== ep: 1480, time 27.148192882537842, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
=== ep: 1481, time 27.828410863876343, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
goal_identified
=== ep: 1482, time 27.426884174346924, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1482
=== ep: 1483, time 27.51804280281067, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
=== ep: 1484, time 27.50856900215149, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
=== ep: 1485, time 27.001556158065796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
=== ep: 1486, time 27.18102264404297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
goal_identified
goal_identified
=== ep: 1487, time 27.325058698654175, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
=== ep: 1488, time 27.36567234992981, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
=== ep: 1489, time 47.6605863571167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
=== ep: 1490, time 27.247446060180664, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
=== ep: 1491, time 27.333326816558838, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
goal_identified
=== ep: 1492, time 27.703826904296875, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
goal_identified
=== ep: 1493, time 27.143691539764404, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 27.38734459877014, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
goal_identified
goal_identified
=== ep: 1495, time 27.116859436035156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
goal_identified
=== ep: 1496, time 27.351958751678467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1496
goal_identified
=== ep: 1497, time 27.280762672424316, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
goal_identified
=== ep: 1498, time 27.355271100997925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
=== ep: 1499, time 36.68524503707886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
=== ep: 1500, time 27.393731594085693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
goal_identified
=== ep: 1501, time 27.20479416847229, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
=== ep: 1502, time 27.324422359466553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
goal_identified
goal_identified
=== ep: 1503, time 27.053276538848877, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
goal_identified
=== ep: 1504, time 27.187265396118164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
goal_identified
=== ep: 1505, time 27.370004653930664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
goal_identified
=== ep: 1506, time 27.4212589263916, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
=== ep: 1507, time 27.208728790283203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1507
=== ep: 1508, time 27.039947748184204, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
=== ep: 1509, time 36.860677003860474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
=== ep: 1510, time 27.49168610572815, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
=== ep: 1511, time 27.64073634147644, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
=== ep: 1512, time 27.81415581703186, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
goal_identified
goal_identified
=== ep: 1513, time 27.563634157180786, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
goal_identified
=== ep: 1514, time 27.565996170043945, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
=== ep: 1515, time 27.200552701950073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
=== ep: 1516, time 27.124247550964355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
=== ep: 1517, time 27.278263807296753, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
=== ep: 1518, time 27.670454740524292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
=== ep: 1519, time 36.64337158203125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
=== ep: 1520, time 27.18495512008667, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
=== ep: 1521, time 27.75035786628723, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
=== ep: 1522, time 27.489324808120728, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
=== ep: 1523, time 27.138494968414307, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
goal_identified
=== ep: 1524, time 27.495930194854736, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
goal_identified
=== ep: 1525, time 27.23338007926941, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
goal_identified
=== ep: 1526, time 27.363126277923584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
=== ep: 1527, time 27.231292724609375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1527
=== ep: 1528, time 27.172738552093506, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
goal_identified
=== ep: 1529, time 37.210184812545776, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
goal_identified
=== ep: 1530, time 27.63443088531494, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
=== ep: 1531, time 27.662105083465576, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
=== ep: 1532, time 27.677719354629517, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
goal_identified
=== ep: 1533, time 27.81318187713623, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
goal_identified
=== ep: 1534, time 27.50075602531433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
goal_identified
=== ep: 1535, time 27.519802808761597, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
=== ep: 1536, time 27.72358226776123, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
goal_identified
=== ep: 1537, time 27.314568042755127, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
=== ep: 1538, time 27.680758476257324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
=== ep: 1539, time 37.304171323776245, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
=== ep: 1540, time 27.742380142211914, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
goal_identified
goal_identified
=== ep: 1541, time 27.224181413650513, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
=== ep: 1542, time 27.541929006576538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
goal_identified
goal_identified
=== ep: 1543, time 27.410109758377075, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
=== ep: 1544, time 27.373759508132935, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 27.305395364761353, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
goal_identified
=== ep: 1546, time 27.460253477096558, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
=== ep: 1547, time 27.845548152923584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1547
goal_identified
goal_identified
=== ep: 1548, time 27.193899154663086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
=== ep: 1549, time 37.72321367263794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
=== ep: 1550, time 27.296732187271118, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
goal_identified
=== ep: 1551, time 28.359589338302612, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
=== ep: 1552, time 27.306196451187134, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
goal_identified
goal_identified
=== ep: 1553, time 27.490906476974487, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
=== ep: 1554, time 27.48508310317993, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
goal_identified
=== ep: 1555, time 27.36897850036621, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
goal_identified
=== ep: 1556, time 27.87188410758972, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
goal_identified
=== ep: 1557, time 27.86810541152954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
=== ep: 1558, time 27.350261926651, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
goal_identified
=== ep: 1559, time 36.87867331504822, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
goal_identified
=== ep: 1560, time 27.318393230438232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
=== ep: 1561, time 27.85418152809143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
=== ep: 1562, time 27.651028633117676, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
=== ep: 1563, time 27.289616107940674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
=== ep: 1564, time 27.708876132965088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
=== ep: 1565, time 27.558574438095093, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
=== ep: 1566, time 27.80215072631836, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
goal_identified
goal_identified
=== ep: 1567, time 27.451805591583252, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
goal_identified
=== ep: 1568, time 27.29036259651184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
=== ep: 1569, time 37.75820016860962, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
goal_identified
=== ep: 1570, time 27.97064709663391, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
goal_identified
=== ep: 1571, time 27.336538553237915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
goal_identified
=== ep: 1572, time 27.295319318771362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
=== ep: 1573, time 27.638792753219604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
goal_identified
goal_identified
=== ep: 1574, time 27.240697383880615, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
goal_identified
=== ep: 1575, time 27.61962080001831, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
=== ep: 1576, time 26.875831604003906, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
goal_identified
goal_identified
=== ep: 1577, time 27.773597240447998, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
=== ep: 1578, time 27.720606327056885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
=== ep: 1579, time 36.88285756111145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1579
goal_identified
=== ep: 1580, time 27.249276399612427, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
=== ep: 1581, time 27.457664728164673, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
goal_identified
goal_identified
=== ep: 1582, time 27.542012453079224, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
goal_identified
goal_identified
=== ep: 1583, time 27.712575674057007, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 27.321924686431885, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1584
=== ep: 1585, time 27.30305504798889, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
=== ep: 1586, time 27.50034523010254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
=== ep: 1587, time 27.561245441436768, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
=== ep: 1588, time 27.506468057632446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
=== ep: 1589, time 36.571277379989624, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
=== ep: 1590, time 27.5028293132782, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
=== ep: 1591, time 27.388715028762817, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
=== ep: 1592, time 27.615837335586548, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
=== ep: 1593, time 26.712292671203613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
=== ep: 1594, time 27.70977783203125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
=== ep: 1595, time 28.181222915649414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
=== ep: 1596, time 27.69247341156006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
goal_identified
=== ep: 1597, time 26.61072874069214, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
=== ep: 1598, time 27.462684869766235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
=== ep: 1599, time 37.08752083778381, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
goal_identified
=== ep: 1600, time 27.894461154937744, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1600
=== ep: 1601, time 27.67752766609192, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
=== ep: 1602, time 28.351135969161987, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
=== ep: 1603, time 27.850040197372437, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
=== ep: 1604, time 27.626715660095215, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
=== ep: 1605, time 27.089484930038452, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
goal_identified
=== ep: 1606, time 27.4125394821167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
goal_identified
=== ep: 1607, time 27.287999868392944, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
=== ep: 1608, time 27.610997438430786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
goal_identified
goal_identified
=== ep: 1609, time 37.54608416557312, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
=== ep: 1610, time 27.233452320098877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
=== ep: 1611, time 27.94568634033203, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
=== ep: 1612, time 27.922176599502563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
goal_identified
goal_identified
=== ep: 1613, time 27.343805074691772, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
goal_identified
=== ep: 1614, time 26.981449842453003, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
=== ep: 1615, time 27.857572555541992, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
goal_identified
=== ep: 1616, time 27.6246919631958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
=== ep: 1617, time 27.428948879241943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
goal_identified
=== ep: 1618, time 27.273752689361572, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
goal_identified
=== ep: 1619, time 36.70337533950806, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
goal_identified
=== ep: 1620, time 27.381852626800537, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
goal_identified
=== ep: 1621, time 27.398847103118896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
=== ep: 1622, time 27.46915602684021, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
=== ep: 1623, time 27.389872312545776, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
goal_identified
=== ep: 1624, time 27.72874665260315, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
=== ep: 1625, time 27.628080129623413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
goal_identified
goal_identified
=== ep: 1626, time 27.614713191986084, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
goal_identified
goal_identified
=== ep: 1627, time 27.24480438232422, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
=== ep: 1628, time 28.04914689064026, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
goal_identified
=== ep: 1629, time 36.752949953079224, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
=== ep: 1630, time 27.456522703170776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
=== ep: 1631, time 27.14432120323181, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
goal_identified
=== ep: 1632, time 27.679077625274658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
goal_identified
=== ep: 1633, time 27.603283643722534, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
=== ep: 1634, time 27.490328550338745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
=== ep: 1635, time 27.688666343688965, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 27.523077487945557, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
=== ep: 1637, time 27.26262068748474, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
=== ep: 1638, time 27.562403917312622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
goal_identified
=== ep: 1639, time 36.83662939071655, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
=== ep: 1640, time 27.36692976951599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
goal_identified
=== ep: 1641, time 27.494417190551758, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
=== ep: 1642, time 27.337733268737793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
=== ep: 1643, time 28.006468772888184, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
=== ep: 1644, time 27.39877963066101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
=== ep: 1645, time 27.352753400802612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
=== ep: 1646, time 27.804844617843628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
=== ep: 1647, time 27.696850776672363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
=== ep: 1648, time 27.270230531692505, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 37.170520305633545, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
goal_identified
=== ep: 1650, time 27.5702006816864, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
goal_identified
=== ep: 1651, time 27.435081958770752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
=== ep: 1652, time 27.24755597114563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
goal_identified
=== ep: 1653, time 27.667317867279053, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
goal_identified
=== ep: 1654, time 27.516765832901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
=== ep: 1655, time 27.408647298812866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
=== ep: 1656, time 27.213655710220337, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
goal_identified
=== ep: 1657, time 27.052212238311768, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
=== ep: 1658, time 27.871457815170288, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
goal_identified
=== ep: 1659, time 37.03395414352417, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
goal_identified
=== ep: 1660, time 27.399096250534058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
goal_identified
=== ep: 1661, time 27.752476930618286, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
goal_identified
goal_identified
=== ep: 1662, time 27.659220695495605, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
=== ep: 1663, time 27.45585036277771, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
=== ep: 1664, time 27.944885969161987, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
goal_identified
=== ep: 1665, time 26.940821886062622, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
goal_identified
=== ep: 1666, time 27.91768503189087, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
=== ep: 1667, time 28.40714168548584, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
goal_identified
=== ep: 1668, time 27.49775266647339, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
goal_identified
=== ep: 1669, time 36.84543466567993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
=== ep: 1670, time 27.727758646011353, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
goal_identified
=== ep: 1671, time 27.98259949684143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
goal_identified
=== ep: 1672, time 27.586809635162354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
goal_identified
=== ep: 1673, time 27.660008668899536, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
goal_identified
=== ep: 1674, time 27.672674894332886, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 27.493793964385986, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
=== ep: 1676, time 27.373958826065063, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1676
goal_identified
=== ep: 1677, time 27.50613045692444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
goal_identified
=== ep: 1678, time 27.18025779724121, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
=== ep: 1679, time 37.05076718330383, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
=== ep: 1680, time 28.135541200637817, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
goal_identified
=== ep: 1681, time 27.46631908416748, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
goal_identified
=== ep: 1682, time 26.97183322906494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
goal_identified
goal_identified
=== ep: 1683, time 27.258220672607422, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
=== ep: 1684, time 27.623707056045532, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
goal_identified
=== ep: 1685, time 27.5408833026886, eps 0.001, sum reward: 1, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
=== ep: 1686, time 27.280879259109497, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
=== ep: 1687, time 27.34684109687805, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
goal_identified
goal_identified
=== ep: 1688, time 27.82122802734375, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
=== ep: 1689, time 37.09404897689819, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
goal_identified
goal_identified
=== ep: 1690, time 27.484896183013916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
=== ep: 1691, time 27.210564613342285, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
goal_identified
goal_identified
=== ep: 1692, time 27.480093002319336, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
goal_identified
goal_identified
=== ep: 1693, time 27.559621572494507, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
goal_identified
=== ep: 1694, time 27.664794445037842, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
=== ep: 1695, time 27.24942135810852, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 27.444772958755493, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
=== ep: 1697, time 27.746540546417236, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
=== ep: 1698, time 27.407953023910522, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
goal_identified
=== ep: 1699, time 39.297812938690186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 27.398953676223755, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
=== ep: 1701, time 28.109596729278564, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
goal_identified
=== ep: 1702, time 27.39611530303955, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1702
goal_identified
=== ep: 1703, time 27.472160816192627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1703
=== ep: 1704, time 27.32070231437683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1704
goal_identified
=== ep: 1705, time 27.405376434326172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1705
goal_identified
goal_identified
goal_identified
=== ep: 1706, time 27.179102659225464, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
goal_identified
=== ep: 1707, time 28.09690260887146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1707
goal_identified
goal_identified
=== ep: 1708, time 27.37089705467224, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1708
=== ep: 1709, time 36.941492557525635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1709
goal_identified
=== ep: 1710, time 27.708102703094482, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1710
=== ep: 1711, time 27.904247760772705, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1711
goal_identified
=== ep: 1712, time 26.8032808303833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1712
goal_identified
=== ep: 1713, time 27.680412530899048, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1713
=== ep: 1714, time 27.676427841186523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1714
=== ep: 1715, time 27.408034086227417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1715
=== ep: 1716, time 27.630676984786987, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1716
=== ep: 1717, time 27.654637575149536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1717
goal_identified
=== ep: 1718, time 27.697253704071045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1718
goal_identified
=== ep: 1719, time 37.257829427719116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1719
goal_identified
=== ep: 1720, time 27.19509792327881, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1720
goal_identified
=== ep: 1721, time 27.38285207748413, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1721
=== ep: 1722, time 28.184186697006226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1722
goal_identified
goal_identified
=== ep: 1723, time 27.606904983520508, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1723
=== ep: 1724, time 27.74917459487915, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1724
=== ep: 1725, time 27.067877531051636, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1725
=== ep: 1726, time 27.913265466690063, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1726
goal_identified
=== ep: 1727, time 27.274574518203735, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1727
goal_identified
=== ep: 1728, time 27.620208024978638, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1728
=== ep: 1729, time 38.29121422767639, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1729
=== ep: 1730, time 27.94567370414734, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1730
=== ep: 1731, time 27.74222421646118, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1731
=== ep: 1732, time 27.11806344985962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1732
goal_identified
=== ep: 1733, time 27.597397565841675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1733
=== ep: 1734, time 27.706979513168335, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1734
=== ep: 1735, time 27.251728773117065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1735
goal_identified
=== ep: 1736, time 27.615267992019653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1736
=== ep: 1737, time 27.666149139404297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1737
=== ep: 1738, time 27.147361278533936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1738
=== ep: 1739, time 37.53608965873718, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1739
goal_identified
goal_identified
=== ep: 1740, time 27.98004913330078, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1740
=== ep: 1741, time 27.88691234588623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1741
=== ep: 1742, time 27.304492235183716, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1742
=== ep: 1743, time 27.871934175491333, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1743
=== ep: 1744, time 27.985655784606934, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 139/139)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1744
=== ep: 1745, time 27.57394576072693, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1745
=== ep: 1746, time 27.240875244140625, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1746
goal_identified
=== ep: 1747, time 27.346476793289185, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1747
=== ep: 1748, time 27.457301378250122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1748
=== ep: 1749, time 36.44554662704468, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1749
=== ep: 1750, time 27.706305027008057, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1750
=== ep: 1751, time 27.08374524116516, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1751
=== ep: 1752, time 27.447645902633667, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1752
goal_identified
=== ep: 1753, time 27.51812243461609, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1753
goal_identified
goal_identified
=== ep: 1754, time 27.457618951797485, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1754
=== ep: 1755, time 27.20440649986267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1755
goal_identified
goal_identified
=== ep: 1756, time 27.556659936904907, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1756
goal_identified
=== ep: 1757, time 27.205586910247803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1757
=== ep: 1758, time 27.85193967819214, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1758
goal_identified
=== ep: 1759, time 37.10353922843933, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1759
=== ep: 1760, time 27.296756505966187, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1760
=== ep: 1761, time 27.246942043304443, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1761
=== ep: 1762, time 28.052433013916016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1762
goal_identified
=== ep: 1763, time 27.404550790786743, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1763
=== ep: 1764, time 27.292118310928345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1764
=== ep: 1765, time 27.423349857330322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1765
=== ep: 1766, time 27.96674418449402, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1766
goal_identified
=== ep: 1767, time 27.542484760284424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1767
=== ep: 1768, time 27.894582509994507, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1768
=== ep: 1769, time 36.590330839157104, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1769
=== ep: 1770, time 27.937328815460205, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1770
=== ep: 1771, time 27.71655011177063, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1771
=== ep: 1772, time 27.44551706314087, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1772
goal_identified
=== ep: 1773, time 26.98005247116089, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1773
=== ep: 1774, time 27.365907907485962, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1774
=== ep: 1775, time 27.60399580001831, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1775
goal_identified
=== ep: 1776, time 27.991098165512085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1776
goal_identified
=== ep: 1777, time 27.56095266342163, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1777
=== ep: 1778, time 27.516490936279297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1778
goal_identified
=== ep: 1779, time 37.036540508270264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1779
=== ep: 1780, time 27.67140531539917, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1780
=== ep: 1781, time 27.55228066444397, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1781
=== ep: 1782, time 27.15322518348694, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1782
goal_identified
=== ep: 1783, time 27.332152366638184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1783
=== ep: 1784, time 27.525349378585815, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1784
=== ep: 1785, time 27.748246669769287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1785
=== ep: 1786, time 27.460292100906372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1786
=== ep: 1787, time 27.157651901245117, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1787
=== ep: 1788, time 27.626557111740112, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1788
goal_identified
=== ep: 1789, time 36.77870154380798, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1789
goal_identified
goal_identified
=== ep: 1790, time 27.490013122558594, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1790
=== ep: 1791, time 27.624911785125732, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1791
goal_identified
=== ep: 1792, time 28.156540155410767, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1792
goal_identified
=== ep: 1793, time 27.840678930282593, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1793
=== ep: 1794, time 27.61190629005432, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1794
=== ep: 1795, time 27.026007890701294, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1795
goal_identified
goal_identified
=== ep: 1796, time 27.02217721939087, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1796
goal_identified
=== ep: 1797, time 27.419525861740112, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1797
goal_identified
=== ep: 1798, time 27.69337797164917, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1798
=== ep: 1799, time 36.822348833084106, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1799
=== ep: 1800, time 27.147383451461792, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 27.43644070625305, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 27.216310739517212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1802
=== ep: 1803, time 28.910901069641113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1803
=== ep: 1804, time 27.823771476745605, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1804
=== ep: 1805, time 27.343003511428833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1805
goal_identified
=== ep: 1806, time 27.205503940582275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1806
goal_identified
=== ep: 1807, time 27.57647204399109, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1807
goal_identified
=== ep: 1808, time 27.84249520301819, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 37.026155948638916, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1809
goal_identified
goal_identified
=== ep: 1810, time 27.27476930618286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1810
goal_identified
goal_identified
=== ep: 1811, time 27.429647207260132, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1811
=== ep: 1812, time 27.696226119995117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1812
=== ep: 1813, time 26.908302545547485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1813
=== ep: 1814, time 27.804080486297607, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 27.1773943901062, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1815
goal_identified
=== ep: 1816, time 27.48016667366028, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1816
goal_identified
=== ep: 1817, time 27.52682852745056, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1817
=== ep: 1818, time 27.511173963546753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1818
=== ep: 1819, time 37.25862216949463, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1819
goal_identified
=== ep: 1820, time 27.560243606567383, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1820
=== ep: 1821, time 27.431564569473267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1821
=== ep: 1822, time 26.99446153640747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1822
=== ep: 1823, time 27.40158772468567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1823
=== ep: 1824, time 27.769704818725586, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1824
=== ep: 1825, time 27.36838936805725, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1825
=== ep: 1826, time 27.090553998947144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1826
goal_identified
=== ep: 1827, time 27.2057523727417, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1827
=== ep: 1828, time 27.40457034111023, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1828
goal_identified
=== ep: 1829, time 37.04632759094238, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1829
=== ep: 1830, time 27.274833917617798, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1830
=== ep: 1831, time 27.601908922195435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1831
goal_identified
goal_identified
=== ep: 1832, time 27.117015600204468, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1832
=== ep: 1833, time 27.611432790756226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1833
=== ep: 1834, time 27.402560710906982, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1834
goal_identified
=== ep: 1835, time 27.67826533317566, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1835
=== ep: 1836, time 26.822128772735596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1836
=== ep: 1837, time 27.279863357543945, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1837
=== ep: 1838, time 27.553352117538452, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1838
goal_identified
=== ep: 1839, time 36.488707065582275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1839
=== ep: 1840, time 26.88017511367798, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1840
=== ep: 1841, time 27.46366000175476, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1841
=== ep: 1842, time 27.242034196853638, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1842
goal_identified
=== ep: 1843, time 27.25153875350952, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1843
goal_identified
=== ep: 1844, time 27.43791675567627, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1844
=== ep: 1845, time 27.096024990081787, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1845
=== ep: 1846, time 27.22510552406311, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1846
=== ep: 1847, time 27.18478560447693, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1847
=== ep: 1848, time 27.466896533966064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1848
goal_identified
=== ep: 1849, time 36.25912642478943, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1849
goal_identified
=== ep: 1850, time 27.00458335876465, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1850
=== ep: 1851, time 27.322887420654297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1851
=== ep: 1852, time 26.847365856170654, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1852
goal_identified
=== ep: 1853, time 27.084670543670654, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1853
=== ep: 1854, time 27.39299201965332, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1854
goal_identified
=== ep: 1855, time 26.894803047180176, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1855
=== ep: 1856, time 26.989318132400513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1856
=== ep: 1857, time 27.205369234085083, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1857
=== ep: 1858, time 26.788746118545532, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1858
goal_identified
goal_identified
=== ep: 1859, time 35.52901220321655, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1859
goal_identified
=== ep: 1860, time 26.56108021736145, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1860
=== ep: 1861, time 26.235691785812378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1861
=== ep: 1862, time 27.103020668029785, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1862
goal_identified
=== ep: 1863, time 26.439016103744507, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1863
=== ep: 1864, time 26.15226912498474, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1864
goal_identified
=== ep: 1865, time 26.461944818496704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1865
=== ep: 1866, time 26.920546293258667, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1866
=== ep: 1867, time 26.321794509887695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1867
goal_identified
=== ep: 1868, time 25.908820867538452, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1868
goal_identified
=== ep: 1869, time 36.26538133621216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1869
=== ep: 1870, time 26.130722761154175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1870
=== ep: 1871, time 26.1396906375885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1871
goal_identified
goal_identified
=== ep: 1872, time 26.23244595527649, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1872
goal_identified
=== ep: 1873, time 25.300627946853638, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1873
goal_identified
=== ep: 1874, time 25.906759023666382, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1874
goal_identified
=== ep: 1875, time 26.368072271347046, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1875
=== ep: 1876, time 26.102927207946777, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1876
=== ep: 1877, time 26.18985867500305, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1877
=== ep: 1878, time 25.67189884185791, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1878
=== ep: 1879, time 36.121811628341675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1879
=== ep: 1880, time 26.080607175827026, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1880
goal_identified
=== ep: 1881, time 25.954851388931274, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1881
goal_identified
=== ep: 1882, time 25.974645376205444, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1882
=== ep: 1883, time 25.86715030670166, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1883
=== ep: 1884, time 25.93190836906433, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1884
goal_identified
=== ep: 1885, time 26.050464868545532, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1885
=== ep: 1886, time 26.06736421585083, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1886
=== ep: 1887, time 25.86317467689514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1887
goal_identified
=== ep: 1888, time 25.931098222732544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1888
=== ep: 1889, time 34.72300386428833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1889
=== ep: 1890, time 26.29301643371582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1890
=== ep: 1891, time 26.193384170532227, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1891
=== ep: 1892, time 25.85396671295166, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1892
=== ep: 1893, time 25.84518051147461, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1893
=== ep: 1894, time 25.996193885803223, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1894
=== ep: 1895, time 26.16477060317993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1895
goal_identified
=== ep: 1896, time 25.930302143096924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1896
goal_identified
=== ep: 1897, time 25.908661603927612, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1897
=== ep: 1898, time 26.609360218048096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1898
=== ep: 1899, time 35.482168674468994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1899
goal_identified
goal_identified
=== ep: 1900, time 26.118038177490234, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1900
=== ep: 1901, time 25.942808866500854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1901
=== ep: 1902, time 25.932612419128418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1902
goal_identified
=== ep: 1903, time 26.27290654182434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1903
goal_identified
=== ep: 1904, time 26.172489881515503, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1904
=== ep: 1905, time 26.554205417633057, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1905
=== ep: 1906, time 25.720431327819824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1906
goal_identified
=== ep: 1907, time 25.648184299468994, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1907
=== ep: 1908, time 25.962181568145752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1908
=== ep: 1909, time 35.759674310684204, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1909
=== ep: 1910, time 26.185924768447876, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1910
=== ep: 1911, time 25.852996110916138, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1911
=== ep: 1912, time 26.136510610580444, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1912
goal_identified
=== ep: 1913, time 26.570675373077393, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1913
=== ep: 1914, time 26.105803728103638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1914
goal_identified
=== ep: 1915, time 25.83838725090027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1915
goal_identified
=== ep: 1916, time 25.66995906829834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1916
=== ep: 1917, time 26.563859701156616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1917
goal_identified
=== ep: 1918, time 26.38157892227173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1918
=== ep: 1919, time 35.26693415641785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1919
=== ep: 1920, time 25.861279487609863, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1920
=== ep: 1921, time 25.910799980163574, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1921
=== ep: 1922, time 25.964205741882324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1922
=== ep: 1923, time 26.212300539016724, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1923
goal_identified
=== ep: 1924, time 26.191215753555298, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1924
=== ep: 1925, time 26.073649168014526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1925
=== ep: 1926, time 25.99327516555786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1926
=== ep: 1927, time 26.50227427482605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1927
=== ep: 1928, time 26.524739265441895, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1928
=== ep: 1929, time 35.44864225387573, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1929
=== ep: 1930, time 25.92916440963745, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1930
=== ep: 1931, time 26.158456563949585, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1931
=== ep: 1932, time 26.017515420913696, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1932
=== ep: 1933, time 26.754894018173218, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1933
goal_identified
goal_identified
=== ep: 1934, time 25.926005363464355, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1934
=== ep: 1935, time 26.093202352523804, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1935
=== ep: 1936, time 26.28632879257202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1936
=== ep: 1937, time 26.23596215248108, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1937
=== ep: 1938, time 25.925830841064453, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1938
goal_identified
=== ep: 1939, time 35.36383843421936, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1939
goal_identified
=== ep: 1940, time 26.245056867599487, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1940
=== ep: 1941, time 26.291633367538452, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1941
=== ep: 1942, time 26.050188541412354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1942
goal_identified
=== ep: 1943, time 26.25473427772522, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1943
=== ep: 1944, time 26.087129592895508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1944
=== ep: 1945, time 25.932615995407104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1945
=== ep: 1946, time 26.775095224380493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1946
=== ep: 1947, time 26.76866102218628, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1947
goal_identified
=== ep: 1948, time 26.17827796936035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1948
=== ep: 1949, time 35.23211598396301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1949
=== ep: 1950, time 26.156630516052246, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1950
goal_identified
=== ep: 1951, time 26.674501657485962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1951
=== ep: 1952, time 26.248889923095703, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1952
=== ep: 1953, time 26.95942997932434, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1953
=== ep: 1954, time 26.0146267414093, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1954
goal_identified
=== ep: 1955, time 26.66054129600525, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1955
=== ep: 1956, time 26.526378870010376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1956
goal_identified
=== ep: 1957, time 26.512553930282593, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1957
goal_identified
=== ep: 1958, time 25.80396819114685, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1958
goal_identified
=== ep: 1959, time 35.200674295425415, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1959
=== ep: 1960, time 26.80533790588379, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1960
=== ep: 1961, time 26.268043756484985, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1961
=== ep: 1962, time 26.588061094284058, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1962
=== ep: 1963, time 26.73267960548401, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1963
goal_identified
=== ep: 1964, time 26.407684564590454, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1964
=== ep: 1965, time 26.682273149490356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1965
=== ep: 1966, time 26.39093518257141, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1966
goal_identified
=== ep: 1967, time 26.417858123779297, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1967
=== ep: 1968, time 26.21855115890503, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1968
=== ep: 1969, time 35.46239233016968, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1969
=== ep: 1970, time 26.645194053649902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1970
=== ep: 1971, time 26.521413564682007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1971
=== ep: 1972, time 26.800920009613037, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1972
=== ep: 1973, time 26.470757961273193, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1973
=== ep: 1974, time 26.616737127304077, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1974
goal_identified
=== ep: 1975, time 26.44354557991028, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1975
=== ep: 1976, time 26.706128120422363, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1976
goal_identified
=== ep: 1977, time 26.106481790542603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1977
=== ep: 1978, time 25.86730146408081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1978
goal_identified
=== ep: 1979, time 37.12416934967041, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1979
=== ep: 1980, time 26.265366554260254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1980
goal_identified
goal_identified
=== ep: 1981, time 26.25445318222046, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1981
=== ep: 1982, time 26.56040072441101, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1982
goal_identified
=== ep: 1983, time 26.255847454071045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1983
=== ep: 1984, time 26.293962001800537, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1984
=== ep: 1985, time 26.748531341552734, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1985
=== ep: 1986, time 26.429961442947388, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1986
goal_identified
=== ep: 1987, time 26.551252603530884, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1987
=== ep: 1988, time 26.808992624282837, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1988
=== ep: 1989, time 37.37271571159363, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1989
goal_identified
=== ep: 1990, time 26.41089963912964, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1990
=== ep: 1991, time 26.133516550064087, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1991
goal_identified
=== ep: 1992, time 25.943114519119263, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1992
goal_identified
goal_identified
=== ep: 1993, time 26.413065671920776, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1993
goal_identified
goal_identified
=== ep: 1994, time 26.369004726409912, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1994
goal_identified
goal_identified
=== ep: 1995, time 26.06472945213318, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1995
goal_identified
goal_identified
=== ep: 1996, time 26.406392097473145, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1996
goal_identified
=== ep: 1997, time 26.277105569839478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1997
goal_identified
=== ep: 1998, time 26.592348337173462, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1998
=== ep: 1999, time 36.570913553237915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1999
=== ep: 2000, time 26.366119146347046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2000
goal_identified
=== ep: 2001, time 26.41017484664917, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2001
goal_identified
=== ep: 2002, time 26.525049448013306, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2002
=== ep: 2003, time 26.724900722503662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2003
=== ep: 2004, time 26.978967428207397, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2004
goal_identified
=== ep: 2005, time 26.149798154830933, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2005
=== ep: 2006, time 26.419416666030884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2006
goal_identified
=== ep: 2007, time 26.64539384841919, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2007
goal_identified
=== ep: 2008, time 26.40422296524048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2008
=== ep: 2009, time 35.34275150299072, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2009
=== ep: 2010, time 26.647555589675903, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2010
=== ep: 2011, time 26.152775287628174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2011
=== ep: 2012, time 27.21642303466797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2012
=== ep: 2013, time 26.482208728790283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2013
=== ep: 2014, time 26.94492220878601, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2014
=== ep: 2015, time 26.451345682144165, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2015
=== ep: 2016, time 26.580313205718994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2016
=== ep: 2017, time 26.819411754608154, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2017
goal_identified
goal_identified
=== ep: 2018, time 26.712320566177368, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2018
=== ep: 2019, time 36.22355937957764, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2019
goal_identified
=== ep: 2020, time 25.940264463424683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2020
goal_identified
goal_identified
=== ep: 2021, time 26.311575889587402, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2021
=== ep: 2022, time 26.58634638786316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2022
goal_identified
=== ep: 2023, time 26.610863208770752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2023
goal_identified
=== ep: 2024, time 26.330406665802002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2024
=== ep: 2025, time 26.692131519317627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2025
goal_identified
=== ep: 2026, time 26.912113189697266, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2026
goal_identified
=== ep: 2027, time 26.69541072845459, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2027
=== ep: 2028, time 27.09059739112854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2028
=== ep: 2029, time 35.02478241920471, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2029
goal_identified
=== ep: 2030, time 26.886902570724487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2030
goal_identified
=== ep: 2031, time 26.747467279434204, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2031
goal_identified
=== ep: 2032, time 26.44512915611267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2032
goal_identified
=== ep: 2033, time 26.42179560661316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2033
=== ep: 2034, time 26.15115237236023, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2034
=== ep: 2035, time 26.747878074645996, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2035
=== ep: 2036, time 26.920851230621338, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2036
=== ep: 2037, time 26.469871997833252, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2037
goal_identified
goal_identified
=== ep: 2038, time 26.44259786605835, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2038
=== ep: 2039, time 35.69790530204773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2039
=== ep: 2040, time 26.650432109832764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2040
=== ep: 2041, time 26.719719409942627, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2041
goal_identified
goal_identified
=== ep: 2042, time 26.49901270866394, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2042
goal_identified
=== ep: 2043, time 26.50205111503601, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2043
=== ep: 2044, time 26.364503860473633, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2044
goal_identified
=== ep: 2045, time 26.715580701828003, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2045
=== ep: 2046, time 26.771225452423096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2046
=== ep: 2047, time 26.423746347427368, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2047
=== ep: 2048, time 26.78368902206421, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2048
=== ep: 2049, time 36.234055519104004, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2049
=== ep: 2050, time 27.19779109954834, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2050
goal_identified
goal_identified
=== ep: 2051, time 26.341187953948975, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2051
goal_identified
=== ep: 2052, time 26.662741899490356, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2052
goal_identified
=== ep: 2053, time 27.03435516357422, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2053
goal_identified
=== ep: 2054, time 27.07500720024109, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2054
=== ep: 2055, time 26.946749687194824, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2055
=== ep: 2056, time 26.457271814346313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2056
=== ep: 2057, time 26.680333614349365, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2057
=== ep: 2058, time 27.602797031402588, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2058
=== ep: 2059, time 35.77537441253662, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2059
=== ep: 2060, time 26.7024347782135, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2060
=== ep: 2061, time 26.153557777404785, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2061
goal_identified
=== ep: 2062, time 26.49952721595764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2062
goal_identified
=== ep: 2063, time 26.322222232818604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2063
goal_identified
=== ep: 2064, time 26.74330711364746, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2064
=== ep: 2065, time 26.72995638847351, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2065
=== ep: 2066, time 26.418524742126465, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2066
=== ep: 2067, time 26.97185707092285, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2067
=== ep: 2068, time 26.691041231155396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2068
=== ep: 2069, time 36.358149766922, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2069
goal_identified
=== ep: 2070, time 26.477125644683838, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2070
goal_identified
goal_identified
=== ep: 2071, time 26.33356738090515, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2071
=== ep: 2072, time 26.792531728744507, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2072
=== ep: 2073, time 26.65634560585022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2073
goal_identified
=== ep: 2074, time 27.045236825942993, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2074
=== ep: 2075, time 26.726465463638306, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2075
goal_identified
=== ep: 2076, time 26.988950490951538, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2076
=== ep: 2077, time 26.87513017654419, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2077
goal_identified
goal_identified
=== ep: 2078, time 26.80961537361145, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2078
goal_identified
=== ep: 2079, time 35.547935009002686, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2079
=== ep: 2080, time 26.38040566444397, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2080
=== ep: 2081, time 27.192768573760986, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2081
goal_identified
goal_identified
goal_identified
=== ep: 2082, time 26.527955055236816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
=== ep: 2083, time 26.84040117263794, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2083
=== ep: 2084, time 26.12131118774414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2084
=== ep: 2085, time 26.61568570137024, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2085
=== ep: 2086, time 26.9271023273468, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2086
=== ep: 2087, time 26.74234890937805, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2087
=== ep: 2088, time 26.814984798431396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2088
goal_identified
goal_identified
=== ep: 2089, time 34.28312039375305, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2089
=== ep: 2090, time 26.525041103363037, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2090
=== ep: 2091, time 26.57028079032898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2091
=== ep: 2092, time 26.54257869720459, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2092
goal_identified
=== ep: 2093, time 26.715683698654175, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2093
goal_identified
goal_identified
=== ep: 2094, time 26.045175313949585, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2094
=== ep: 2095, time 26.352524757385254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2095
goal_identified
=== ep: 2096, time 26.56196093559265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2096
goal_identified
=== ep: 2097, time 26.395431756973267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2097
goal_identified
=== ep: 2098, time 26.49954390525818, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2098
goal_identified
=== ep: 2099, time 34.686673641204834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2099
goal_identified
=== ep: 2100, time 26.289941549301147, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2100
=== ep: 2101, time 27.023834466934204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2101
goal_identified
=== ep: 2102, time 26.714255809783936, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2102
goal_identified
=== ep: 2103, time 26.422466039657593, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2103
=== ep: 2104, time 26.729830265045166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2104
=== ep: 2105, time 27.12238121032715, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2105
=== ep: 2106, time 26.804240226745605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2106
=== ep: 2107, time 26.862545251846313, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2107
goal_identified
goal_identified
=== ep: 2108, time 27.082512140274048, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2108
=== ep: 2109, time 35.45785903930664, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2109
=== ep: 2110, time 26.942250967025757, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2110
=== ep: 2111, time 26.788954257965088, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2111
=== ep: 2112, time 26.308902978897095, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2112
=== ep: 2113, time 27.0566885471344, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2113
=== ep: 2114, time 26.886291980743408, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2114
goal_identified
=== ep: 2115, time 27.698325395584106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2115
goal_identified
=== ep: 2116, time 27.302011251449585, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2116
=== ep: 2117, time 26.777669429779053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2117
goal_identified
goal_identified
=== ep: 2118, time 26.423458099365234, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2118
goal_identified
=== ep: 2119, time 36.67543911933899, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2119
goal_identified
=== ep: 2120, time 26.903641939163208, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2120
=== ep: 2121, time 26.40695834159851, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2121
goal_identified
goal_identified
=== ep: 2122, time 26.479803562164307, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2122
goal_identified
goal_identified
=== ep: 2123, time 27.051112413406372, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2123
goal_identified
=== ep: 2124, time 26.769588708877563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2124
goal_identified
=== ep: 2125, time 26.805745363235474, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2125
=== ep: 2126, time 26.440945148468018, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2126
goal_identified
goal_identified
goal_identified
=== ep: 2127, time 26.516175746917725, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
goal_identified
=== ep: 2128, time 27.266424894332886, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2128
goal_identified
goal_identified
=== ep: 2129, time 35.14607620239258, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2129
goal_identified
=== ep: 2130, time 26.631566762924194, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2130
goal_identified
=== ep: 2131, time 26.135932445526123, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2131
=== ep: 2132, time 26.686417818069458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2132
=== ep: 2133, time 26.74092745780945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2133
goal_identified
goal_identified
=== ep: 2134, time 26.743226766586304, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2134
goal_identified
=== ep: 2135, time 26.350364208221436, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2135
=== ep: 2136, time 26.258302450180054, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2136
goal_identified
=== ep: 2137, time 27.266449213027954, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2137
goal_identified
goal_identified
=== ep: 2138, time 27.099773406982422, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2138
goal_identified
=== ep: 2139, time 38.47568702697754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2139
goal_identified
=== ep: 2140, time 26.849401712417603, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2140
=== ep: 2141, time 26.68716549873352, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2141
=== ep: 2142, time 27.111164569854736, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2142
=== ep: 2143, time 26.94227409362793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2143
=== ep: 2144, time 26.878315210342407, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2144
=== ep: 2145, time 26.421780824661255, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2145
goal_identified
=== ep: 2146, time 26.812365531921387, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2146
=== ep: 2147, time 27.3937509059906, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2147
goal_identified
=== ep: 2148, time 26.881911516189575, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2148
=== ep: 2149, time 35.07224917411804, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2149
goal_identified
=== ep: 2150, time 26.887251377105713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2150
goal_identified
=== ep: 2151, time 26.63943839073181, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2151
=== ep: 2152, time 27.221039533615112, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2152
goal_identified
goal_identified
=== ep: 2153, time 26.97998571395874, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2153
goal_identified
=== ep: 2154, time 26.686516523361206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2154
goal_identified
=== ep: 2155, time 27.065104246139526, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2155
goal_identified
=== ep: 2156, time 27.28590703010559, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2156
=== ep: 2157, time 27.113794565200806, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2157
=== ep: 2158, time 26.680965662002563, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2158
=== ep: 2159, time 34.925954818725586, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2159
goal_identified
=== ep: 2160, time 26.72361993789673, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2160
goal_identified
=== ep: 2161, time 26.676239490509033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2161
=== ep: 2162, time 27.373128414154053, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2162
goal_identified
=== ep: 2163, time 26.444931268692017, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2163
=== ep: 2164, time 26.813542366027832, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2164
=== ep: 2165, time 26.696133375167847, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2165
=== ep: 2166, time 26.755004405975342, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2166
=== ep: 2167, time 27.061211824417114, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2167
goal_identified
=== ep: 2168, time 26.523961544036865, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2168
goal_identified
goal_identified
=== ep: 2169, time 34.94909119606018, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2169
goal_identified
=== ep: 2170, time 27.029070138931274, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2170
=== ep: 2171, time 26.845422744750977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2171
goal_identified
=== ep: 2172, time 26.495830059051514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2172
goal_identified
=== ep: 2173, time 27.101491689682007, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2173
goal_identified
=== ep: 2174, time 26.468490600585938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2174
goal_identified
=== ep: 2175, time 26.68270516395569, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2175
goal_identified
=== ep: 2176, time 27.065154552459717, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2176
=== ep: 2177, time 26.71046280860901, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2177
goal_identified
=== ep: 2178, time 27.005363941192627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2178
=== ep: 2179, time 35.6090350151062, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2179
=== ep: 2180, time 26.710314750671387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2180
=== ep: 2181, time 27.098409175872803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2181
=== ep: 2182, time 26.589537620544434, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2182
goal_identified
=== ep: 2183, time 27.128969430923462, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2183
=== ep: 2184, time 27.34048366546631, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2184
goal_identified
=== ep: 2185, time 26.844736576080322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2185
goal_identified
=== ep: 2186, time 26.75485587120056, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2186
goal_identified
=== ep: 2187, time 26.66153645515442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2187
goal_identified
=== ep: 2188, time 26.76947593688965, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2188
=== ep: 2189, time 35.441545486450195, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2189
=== ep: 2190, time 26.5446138381958, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2190
goal_identified
goal_identified
=== ep: 2191, time 26.720759391784668, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2191
=== ep: 2192, time 26.798298358917236, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2192
=== ep: 2193, time 27.053508758544922, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2193
=== ep: 2194, time 26.998315572738647, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2194
goal_identified
=== ep: 2195, time 25.425880670547485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2195
goal_identified
=== ep: 2196, time 26.71149182319641, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2196
=== ep: 2197, time 26.894456148147583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2197
=== ep: 2198, time 27.00054097175598, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2198
=== ep: 2199, time 35.24502754211426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2199
goal_identified
=== ep: 2200, time 27.521873235702515, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2200
goal_identified
goal_identified
=== ep: 2201, time 26.91563892364502, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2201
goal_identified
goal_identified
=== ep: 2202, time 27.33939790725708, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2202
=== ep: 2203, time 27.013381004333496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2203
goal_identified
=== ep: 2204, time 27.055665493011475, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2204
goal_identified
goal_identified
=== ep: 2205, time 26.75653839111328, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2205
=== ep: 2206, time 27.101953268051147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2206
=== ep: 2207, time 27.102924585342407, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2207
goal_identified
=== ep: 2208, time 27.037208080291748, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2208
goal_identified
=== ep: 2209, time 34.99724507331848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2209
goal_identified
=== ep: 2210, time 26.466047286987305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2210
goal_identified
=== ep: 2211, time 27.96743869781494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2211
goal_identified
=== ep: 2212, time 27.365455865859985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2212
=== ep: 2213, time 26.90053939819336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2213
=== ep: 2214, time 26.869312286376953, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2214
=== ep: 2215, time 27.10226273536682, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2215
=== ep: 2216, time 27.11063551902771, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2216
goal_identified
=== ep: 2217, time 27.205126762390137, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2217
goal_identified
=== ep: 2218, time 27.311524152755737, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2218
goal_identified
=== ep: 2219, time 35.3396782875061, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2219
=== ep: 2220, time 27.22421884536743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2220
goal_identified
=== ep: 2221, time 27.886833429336548, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2221
=== ep: 2222, time 27.217812299728394, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2222
=== ep: 2223, time 27.249572038650513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2223
goal_identified
=== ep: 2224, time 26.6413254737854, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2224
=== ep: 2225, time 27.62261962890625, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2225
=== ep: 2226, time 27.345792293548584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2226
goal_identified
goal_identified
=== ep: 2227, time 27.69584369659424, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2227
=== ep: 2228, time 27.11198854446411, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2228
goal_identified
=== ep: 2229, time 35.41733646392822, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2229
=== ep: 2230, time 27.130187273025513, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2230
goal_identified
=== ep: 2231, time 26.70839786529541, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2231
=== ep: 2232, time 27.66471290588379, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2232
goal_identified
=== ep: 2233, time 26.461353302001953, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2233
=== ep: 2234, time 27.298935890197754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2234
=== ep: 2235, time 27.236761808395386, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2235
=== ep: 2236, time 27.16521668434143, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2236
=== ep: 2237, time 26.720555782318115, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2237
=== ep: 2238, time 26.689290285110474, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2238
=== ep: 2239, time 35.07799696922302, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2239
goal_identified
=== ep: 2240, time 27.272517204284668, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2240
=== ep: 2241, time 26.76818084716797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2241
=== ep: 2242, time 26.785900115966797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2242
=== ep: 2243, time 27.04164743423462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2243
goal_identified
=== ep: 2244, time 27.061816215515137, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2244
=== ep: 2245, time 27.314321994781494, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2245
=== ep: 2246, time 27.479026317596436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2246
=== ep: 2247, time 26.962514638900757, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2247
=== ep: 2248, time 27.25647282600403, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2248
=== ep: 2249, time 35.033140659332275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2249
=== ep: 2250, time 27.391143321990967, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2250
goal_identified
goal_identified
=== ep: 2251, time 26.926910638809204, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2251
=== ep: 2252, time 26.852829694747925, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2252
goal_identified
goal_identified
=== ep: 2253, time 27.341733932495117, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2253
goal_identified
=== ep: 2254, time 27.047523736953735, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2254
=== ep: 2255, time 27.124818801879883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2255
=== ep: 2256, time 26.945605278015137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2256
=== ep: 2257, time 26.79548692703247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2257
goal_identified
=== ep: 2258, time 26.734898567199707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2258
=== ep: 2259, time 35.559322357177734, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2259
goal_identified
=== ep: 2260, time 26.935893535614014, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2260
goal_identified
goal_identified
=== ep: 2261, time 27.006078243255615, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2261
=== ep: 2262, time 27.197412729263306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2262
goal_identified
=== ep: 2263, time 27.06136202812195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2263
=== ep: 2264, time 27.432804346084595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2264
goal_identified
=== ep: 2265, time 26.80907654762268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2265
=== ep: 2266, time 27.328298807144165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2266
=== ep: 2267, time 27.10451889038086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2267
goal_identified
=== ep: 2268, time 26.927154064178467, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2268
=== ep: 2269, time 36.315898180007935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2269
goal_identified
=== ep: 2270, time 27.011194229125977, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2270
goal_identified
=== ep: 2271, time 26.96943426132202, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2271
goal_identified
=== ep: 2272, time 27.39034414291382, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2272
=== ep: 2273, time 27.396647691726685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2273
=== ep: 2274, time 27.000873565673828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2274
=== ep: 2275, time 27.56726336479187, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2275
=== ep: 2276, time 27.25878119468689, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2276
=== ep: 2277, time 27.395095586776733, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2277
=== ep: 2278, time 27.09364652633667, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2278
=== ep: 2279, time 35.417845726013184, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2279
goal_identified
=== ep: 2280, time 27.187960147857666, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2280
goal_identified
goal_identified
=== ep: 2281, time 26.908201456069946, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2281
goal_identified
=== ep: 2282, time 27.49004316329956, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2282
=== ep: 2283, time 27.317864894866943, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2283
=== ep: 2284, time 26.99998426437378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2284
=== ep: 2285, time 27.40168595314026, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2285
goal_identified
goal_identified
=== ep: 2286, time 27.13264298439026, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2286
goal_identified
=== ep: 2287, time 26.951401710510254, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2287
goal_identified
=== ep: 2288, time 27.243191719055176, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2288
=== ep: 2289, time 34.60252380371094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2289
goal_identified
goal_identified
=== ep: 2290, time 27.307071685791016, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2290
=== ep: 2291, time 27.609466791152954, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2291
goal_identified
=== ep: 2292, time 27.09813380241394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2292
=== ep: 2293, time 26.93378520011902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2293
goal_identified
=== ep: 2294, time 27.261418104171753, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2294
goal_identified
goal_identified
=== ep: 2295, time 27.15836453437805, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2295
goal_identified
=== ep: 2296, time 27.52420473098755, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2296
=== ep: 2297, time 27.18005657196045, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2297
=== ep: 2298, time 26.83340358734131, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2298
goal_identified
=== ep: 2299, time 35.23003268241882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2299
goal_identified
=== ep: 2300, time 27.427150011062622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2300
goal_identified
=== ep: 2301, time 27.52077293395996, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2301
=== ep: 2302, time 26.936347723007202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2302
goal_identified
goal_identified
=== ep: 2303, time 27.204561471939087, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2303
=== ep: 2304, time 27.10740327835083, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2304
=== ep: 2305, time 27.27467703819275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2305
goal_identified
=== ep: 2306, time 27.081174612045288, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2306
goal_identified
=== ep: 2307, time 27.352213382720947, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2307
goal_identified
=== ep: 2308, time 27.499438285827637, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2308
=== ep: 2309, time 37.038533210754395, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2309
goal_identified
=== ep: 2310, time 27.56336212158203, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2310
goal_identified
goal_identified
=== ep: 2311, time 26.956248998641968, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2311
=== ep: 2312, time 27.39942979812622, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2312
goal_identified
=== ep: 2313, time 27.040910959243774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2313
goal_identified
=== ep: 2314, time 27.174514532089233, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2314
goal_identified
goal_identified
=== ep: 2315, time 27.341928720474243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2315
goal_identified
=== ep: 2316, time 26.95343828201294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2316
goal_identified
=== ep: 2317, time 26.80048680305481, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2317
goal_identified
=== ep: 2318, time 27.19966220855713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2318
=== ep: 2319, time 36.07745122909546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2319
goal_identified
=== ep: 2320, time 27.293461561203003, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2320
goal_identified
=== ep: 2321, time 26.731914281845093, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2321
goal_identified
goal_identified
=== ep: 2322, time 27.063114404678345, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2322
goal_identified
=== ep: 2323, time 27.708908557891846, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2323
goal_identified
=== ep: 2324, time 27.45958638191223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2324
=== ep: 2325, time 27.237789154052734, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2325
=== ep: 2326, time 26.81799292564392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2326
goal_identified
=== ep: 2327, time 27.42762589454651, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2327
goal_identified
=== ep: 2328, time 27.51935052871704, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2328
goal_identified
goal_identified
=== ep: 2329, time 36.83830451965332, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2329
=== ep: 2330, time 26.65858006477356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2330
goal_identified
=== ep: 2331, time 27.59401297569275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2331
=== ep: 2332, time 27.3117995262146, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2332
goal_identified
goal_identified
=== ep: 2333, time 26.873366832733154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2333
=== ep: 2334, time 27.534015655517578, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2334
goal_identified
=== ep: 2335, time 26.776333570480347, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2335
=== ep: 2336, time 27.52038812637329, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2336
goal_identified
goal_identified
=== ep: 2337, time 27.175750970840454, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2337
goal_identified
goal_identified
=== ep: 2338, time 26.98748755455017, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2338
goal_identified
goal_identified
=== ep: 2339, time 37.27470636367798, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2339
goal_identified
=== ep: 2340, time 27.234643697738647, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2340
goal_identified
=== ep: 2341, time 27.15435481071472, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2341
goal_identified
=== ep: 2342, time 27.392533540725708, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2342
=== ep: 2343, time 27.129487991333008, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2343
goal_identified
=== ep: 2344, time 27.5410795211792, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2344
goal_identified
=== ep: 2345, time 26.904587984085083, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2345
=== ep: 2346, time 27.108121871948242, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2346
goal_identified
=== ep: 2347, time 27.127522706985474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2347
goal_identified
goal_identified
=== ep: 2348, time 27.520181894302368, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2348
goal_identified
=== ep: 2349, time 36.416897773742676, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2349
=== ep: 2350, time 27.304936408996582, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2350
goal_identified
=== ep: 2351, time 27.11733913421631, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2351
goal_identified
=== ep: 2352, time 27.345135927200317, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2352
=== ep: 2353, time 27.22715973854065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2353
=== ep: 2354, time 26.578269243240356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2354
=== ep: 2355, time 27.144196033477783, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2355
=== ep: 2356, time 27.53901720046997, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2356
goal_identified
goal_identified
=== ep: 2357, time 27.314438343048096, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2357
goal_identified
goal_identified
=== ep: 2358, time 27.392791271209717, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2358
=== ep: 2359, time 35.57604670524597, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2359
goal_identified
goal_identified
=== ep: 2360, time 26.999762535095215, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2360
goal_identified
=== ep: 2361, time 27.290049076080322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2361
goal_identified
=== ep: 2362, time 27.48912763595581, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2362
goal_identified
=== ep: 2363, time 26.952367305755615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2363
goal_identified
=== ep: 2364, time 26.76058268547058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2364
=== ep: 2365, time 27.359857082366943, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2365
goal_identified
=== ep: 2366, time 27.190549850463867, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2366
=== ep: 2367, time 27.825111150741577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2367
goal_identified
=== ep: 2368, time 27.092158794403076, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
goal_identified
== current size of memory is eps 11 > 10.0 and we are deleting ep 2368
=== ep: 2369, time 34.7454731464386, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2369
=== ep: 2370, time 27.452144384384155, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2370
=== ep: 2371, time 27.446649074554443, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2371
goal_identified
=== ep: 2372, time 27.845775842666626, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2372
=== ep: 2373, time 27.186751127243042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2373
goal_identified
=== ep: 2374, time 27.238723278045654, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2374
goal_identified
goal_identified
=== ep: 2375, time 27.584067583084106, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2375
goal_identified
goal_identified
=== ep: 2376, time 27.543129205703735, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2376
goal_identified
=== ep: 2377, time 27.246423721313477, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2377
goal_identified
=== ep: 2378, time 27.2118239402771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2378
goal_identified
=== ep: 2379, time 35.31800389289856, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2379
=== ep: 2380, time 27.749879121780396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2380
=== ep: 2381, time 27.310279607772827, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2381
goal_identified
=== ep: 2382, time 27.212482213974, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2382
=== ep: 2383, time 27.566753387451172, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2383
goal_identified
=== ep: 2384, time 27.62546420097351, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2384
goal_identified
=== ep: 2385, time 27.312798738479614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2385
goal_identified
=== ep: 2386, time 27.498573541641235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2386
=== ep: 2387, time 27.521551609039307, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2387
=== ep: 2388, time 27.78531789779663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2388
=== ep: 2389, time 35.647599935531616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2389
=== ep: 2390, time 28.07179069519043, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2390
=== ep: 2391, time 27.88021206855774, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2391
=== ep: 2392, time 27.379148960113525, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2392
goal_identified
=== ep: 2393, time 27.39443302154541, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2393
goal_identified
goal_identified
=== ep: 2394, time 27.527958154678345, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2394
goal_identified
=== ep: 2395, time 27.522509574890137, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2395
goal_identified
=== ep: 2396, time 27.803311824798584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2396
=== ep: 2397, time 27.617404460906982, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2397
goal_identified
=== ep: 2398, time 27.317917346954346, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2398
=== ep: 2399, time 35.55955195426941, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2399
=== ep: 2400, time 26.97985863685608, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2400
goal_identified
=== ep: 2401, time 27.324740409851074, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2401
=== ep: 2402, time 27.05193257331848, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2402
goal_identified
goal_identified
=== ep: 2403, time 27.159627676010132, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2403
goal_identified
goal_identified
=== ep: 2404, time 27.508689403533936, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2404
=== ep: 2405, time 27.521053791046143, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2405
=== ep: 2406, time 27.201582431793213, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2406
goal_identified
goal_identified
=== ep: 2407, time 27.682143449783325, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2407
goal_identified
=== ep: 2408, time 27.557558059692383, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2408
goal_identified
=== ep: 2409, time 35.26187753677368, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2409
=== ep: 2410, time 27.171314001083374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2410
=== ep: 2411, time 26.95032787322998, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2411
=== ep: 2412, time 27.246601343154907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2412
=== ep: 2413, time 27.61337900161743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2413
=== ep: 2414, time 27.757153034210205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2414
=== ep: 2415, time 27.655498504638672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2415
=== ep: 2416, time 27.46582794189453, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2416
=== ep: 2417, time 27.68787908554077, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2417
=== ep: 2418, time 27.41744589805603, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2418
=== ep: 2419, time 35.72389483451843, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2419
goal_identified
=== ep: 2420, time 27.331186532974243, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2420
goal_identified
=== ep: 2421, time 27.437777757644653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2421
=== ep: 2422, time 27.68294334411621, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2422
=== ep: 2423, time 27.325637102127075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2423
goal_identified
=== ep: 2424, time 27.495988845825195, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2424
=== ep: 2425, time 27.02856135368347, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2425
goal_identified
goal_identified
=== ep: 2426, time 28.023333311080933, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2426
goal_identified
goal_identified
=== ep: 2427, time 28.004799842834473, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2427
goal_identified
=== ep: 2428, time 27.659493684768677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2428
=== ep: 2429, time 35.45368576049805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2429
=== ep: 2430, time 26.77277636528015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2430
=== ep: 2431, time 27.256788730621338, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2431
=== ep: 2432, time 27.99616050720215, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2432
=== ep: 2433, time 27.427610874176025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2433
=== ep: 2434, time 27.498782873153687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2434
=== ep: 2435, time 27.232972145080566, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2435
goal_identified
=== ep: 2436, time 27.477372646331787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2436
goal_identified
=== ep: 2437, time 27.60054326057434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2437
=== ep: 2438, time 27.295469522476196, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2438
goal_identified
=== ep: 2439, time 40.059797286987305, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2439
goal_identified
=== ep: 2440, time 27.639703512191772, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2440
goal_identified
goal_identified
=== ep: 2441, time 27.416365385055542, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2441
goal_identified
goal_identified
goal_identified
=== ep: 2442, time 27.186339378356934, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2442
goal_identified
=== ep: 2443, time 27.52028465270996, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2443
=== ep: 2444, time 27.54710626602173, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2444
goal_identified
goal_identified
=== ep: 2445, time 27.584503173828125, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2445
=== ep: 2446, time 27.532611846923828, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2446
goal_identified
=== ep: 2447, time 27.66553544998169, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2447
goal_identified
=== ep: 2448, time 27.683846950531006, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2448
goal_identified
=== ep: 2449, time 34.88744807243347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2449
=== ep: 2450, time 27.39647150039673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2450
goal_identified
=== ep: 2451, time 27.626127243041992, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2451
=== ep: 2452, time 27.514209508895874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2452
=== ep: 2453, time 27.10870337486267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2453
=== ep: 2454, time 27.710074186325073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2454
=== ep: 2455, time 27.52589702606201, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2455
=== ep: 2456, time 27.54415202140808, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2456
goal_identified
=== ep: 2457, time 27.137607097625732, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2457
=== ep: 2458, time 27.065733194351196, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2458
=== ep: 2459, time 36.645068407058716, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2459
=== ep: 2460, time 27.419423580169678, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2460
=== ep: 2461, time 27.686333894729614, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2461
=== ep: 2462, time 27.59002923965454, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2462
=== ep: 2463, time 27.41601276397705, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2463
goal_identified
=== ep: 2464, time 27.80899739265442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2464
=== ep: 2465, time 27.63225793838501, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2465
=== ep: 2466, time 27.75719904899597, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2466
goal_identified
=== ep: 2467, time 27.342808485031128, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2467
=== ep: 2468, time 28.071328163146973, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2468
=== ep: 2469, time 35.56646251678467, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2469
=== ep: 2470, time 27.416754245758057, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2470
=== ep: 2471, time 26.993417263031006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2471
goal_identified
goal_identified
=== ep: 2472, time 27.58018469810486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2472
=== ep: 2473, time 27.737329483032227, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2473
goal_identified
goal_identified
=== ep: 2474, time 27.990821599960327, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2474
=== ep: 2475, time 27.274792909622192, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2475
=== ep: 2476, time 27.508081436157227, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2476
goal_identified
=== ep: 2477, time 27.59612226486206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2477
goal_identified
goal_identified
=== ep: 2478, time 27.50418734550476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2478
goal_identified
goal_identified
=== ep: 2479, time 36.23019027709961, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2479
=== ep: 2480, time 28.025907516479492, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2480
=== ep: 2481, time 26.90801191329956, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2481
=== ep: 2482, time 28.069092750549316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2482
goal_identified
=== ep: 2483, time 27.786388158798218, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2483
=== ep: 2484, time 27.5496666431427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2484
=== ep: 2485, time 27.230042695999146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2485
goal_identified
=== ep: 2486, time 27.370343685150146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2486
goal_identified
goal_identified
=== ep: 2487, time 27.66546654701233, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2487
goal_identified
goal_identified
=== ep: 2488, time 27.697656631469727, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2488
=== ep: 2489, time 35.383373737335205, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2489
goal_identified
=== ep: 2490, time 27.24519658088684, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2490
goal_identified
=== ep: 2491, time 27.3812518119812, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2491
goal_identified
goal_identified
goal_identified
=== ep: 2492, time 27.698411226272583, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2492
=== ep: 2493, time 27.60627579689026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2493
=== ep: 2494, time 27.841914653778076, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2494
goal_identified
=== ep: 2495, time 27.464930057525635, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2495
=== ep: 2496, time 27.593148946762085, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2496
=== ep: 2497, time 27.543466329574585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2497
goal_identified
=== ep: 2498, time 27.363422393798828, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2498
=== ep: 2499, time 35.291584968566895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2499
=== ep: 2500, time 27.2485294342041, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2500
goal_identified
=== ep: 2501, time 27.203924894332886, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2501
goal_identified
=== ep: 2502, time 27.500789642333984, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 27.282601356506348, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2503
goal_identified
=== ep: 2504, time 27.325060606002808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2504
=== ep: 2505, time 27.231998682022095, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2505
goal_identified
=== ep: 2506, time 27.076717138290405, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2506
goal_identified
goal_identified
goal_identified
=== ep: 2507, time 27.629781246185303, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2507
goal_identified
=== ep: 2508, time 27.77536630630493, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2508
goal_identified
goal_identified
=== ep: 2509, time 36.716426372528076, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2509
=== ep: 2510, time 27.115893125534058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2510
goal_identified
=== ep: 2511, time 27.465736389160156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2511
goal_identified
=== ep: 2512, time 28.297282695770264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2512
=== ep: 2513, time 27.3234224319458, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2513
goal_identified
=== ep: 2514, time 27.567262649536133, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2514
goal_identified
goal_identified
goal_identified
=== ep: 2515, time 26.81675410270691, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2515
goal_identified
goal_identified
=== ep: 2516, time 27.773436069488525, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2516
goal_identified
=== ep: 2517, time 27.418338298797607, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2517
=== ep: 2518, time 27.270751953125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2518
goal_identified
goal_identified
=== ep: 2519, time 35.40757203102112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2519
goal_identified
goal_identified
=== ep: 2520, time 26.991158962249756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2520
goal_identified
=== ep: 2521, time 27.495471477508545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2521
goal_identified
=== ep: 2522, time 27.22443985939026, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2522
goal_identified
=== ep: 2523, time 27.54595184326172, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2523
=== ep: 2524, time 27.07459044456482, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2524
=== ep: 2525, time 27.3660569190979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2525
=== ep: 2526, time 27.216840267181396, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2526
=== ep: 2527, time 27.66771411895752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2527
goal_identified
goal_identified
=== ep: 2528, time 27.592857837677002, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2528
=== ep: 2529, time 35.25847792625427, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2529
goal_identified
=== ep: 2530, time 27.366660118103027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2530
goal_identified
=== ep: 2531, time 27.436527490615845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2531
=== ep: 2532, time 27.97771143913269, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2532
goal_identified
=== ep: 2533, time 28.019097805023193, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2533
goal_identified
=== ep: 2534, time 27.256439447402954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2534
goal_identified
=== ep: 2535, time 27.603930950164795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2535
=== ep: 2536, time 28.088939428329468, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2536
=== ep: 2537, time 27.57016396522522, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2537
=== ep: 2538, time 26.896817684173584, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2538
=== ep: 2539, time 35.274620056152344, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2539
=== ep: 2540, time 27.464911460876465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2540
goal_identified
=== ep: 2541, time 27.388470888137817, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2541
=== ep: 2542, time 27.150724411010742, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2542
goal_identified
goal_identified
=== ep: 2543, time 27.243048906326294, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2543
=== ep: 2544, time 27.211281538009644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2544
goal_identified
=== ep: 2545, time 27.32678484916687, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2545
goal_identified
=== ep: 2546, time 27.727266550064087, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2546
goal_identified
=== ep: 2547, time 27.07990336418152, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2547
=== ep: 2548, time 27.391875982284546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2548
=== ep: 2549, time 35.87210297584534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2549
=== ep: 2550, time 28.04001235961914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2550
=== ep: 2551, time 27.851011514663696, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2551
=== ep: 2552, time 27.079907655715942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2552
goal_identified
=== ep: 2553, time 28.07351040840149, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2553
=== ep: 2554, time 27.52695107460022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2554
=== ep: 2555, time 27.85060477256775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2555
goal_identified
goal_identified
=== ep: 2556, time 27.460745811462402, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2556
goal_identified
=== ep: 2557, time 26.991747617721558, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2557
goal_identified
goal_identified
=== ep: 2558, time 27.543208837509155, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2558
goal_identified
=== ep: 2559, time 36.10614323616028, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2559
goal_identified
=== ep: 2560, time 27.532593727111816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2560
goal_identified
=== ep: 2561, time 27.338678121566772, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2561
goal_identified
=== ep: 2562, time 27.53886079788208, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2562
=== ep: 2563, time 27.824560165405273, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2563
=== ep: 2564, time 27.666211128234863, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2564
goal_identified
=== ep: 2565, time 27.395996570587158, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2565
goal_identified
=== ep: 2566, time 27.246493101119995, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2566
goal_identified
=== ep: 2567, time 27.370190382003784, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2567
=== ep: 2568, time 27.51232385635376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2568
=== ep: 2569, time 35.42364287376404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2569
=== ep: 2570, time 27.633378744125366, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2570
goal_identified
=== ep: 2571, time 26.756829500198364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2571
goal_identified
=== ep: 2572, time 27.402385473251343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2572
=== ep: 2573, time 27.87600088119507, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2573
=== ep: 2574, time 27.42836332321167, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2574
=== ep: 2575, time 27.436142683029175, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2575
=== ep: 2576, time 27.371440649032593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2576
=== ep: 2577, time 27.381800889968872, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2577
goal_identified
=== ep: 2578, time 27.622251749038696, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2578
=== ep: 2579, time 35.46395444869995, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2579
=== ep: 2580, time 27.851473093032837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2580
=== ep: 2581, time 27.065165281295776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2581
=== ep: 2582, time 27.813493013381958, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2582
goal_identified
=== ep: 2583, time 27.466249704360962, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2583
goal_identified
=== ep: 2584, time 27.37171697616577, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2584
goal_identified
=== ep: 2585, time 27.162466526031494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2585
goal_identified
=== ep: 2586, time 27.240090131759644, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2586
goal_identified
goal_identified
=== ep: 2587, time 27.595945358276367, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2587
goal_identified
=== ep: 2588, time 27.94342565536499, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2588
goal_identified
=== ep: 2589, time 35.25140690803528, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2589
goal_identified
=== ep: 2590, time 27.233818531036377, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2590
=== ep: 2591, time 27.038079023361206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2591
=== ep: 2592, time 27.87021541595459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2592
=== ep: 2593, time 27.59217119216919, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2593
=== ep: 2594, time 27.8619122505188, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2594
goal_identified
=== ep: 2595, time 26.8893039226532, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2595
goal_identified
=== ep: 2596, time 27.26620864868164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2596
goal_identified
=== ep: 2597, time 27.357108116149902, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2597
goal_identified
=== ep: 2598, time 27.509833574295044, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2598
goal_identified
goal_identified
=== ep: 2599, time 36.03688192367554, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2599
goal_identified
=== ep: 2600, time 27.269567728042603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2600
=== ep: 2601, time 27.72296690940857, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2601
=== ep: 2602, time 27.565013647079468, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2602
goal_identified
=== ep: 2603, time 27.508803606033325, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2603
goal_identified
=== ep: 2604, time 27.420570373535156, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2604
goal_identified
=== ep: 2605, time 27.36848545074463, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2605
goal_identified
=== ep: 2606, time 27.214698314666748, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2606
=== ep: 2607, time 27.579054594039917, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2607
goal_identified
goal_identified
=== ep: 2608, time 27.51210594177246, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2608
goal_identified
=== ep: 2609, time 35.88597750663757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2609
=== ep: 2610, time 27.754117965698242, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2610
=== ep: 2611, time 27.324315071105957, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2611
goal_identified
goal_identified
=== ep: 2612, time 27.577452182769775, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2612
goal_identified
=== ep: 2613, time 27.78117060661316, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2613
=== ep: 2614, time 27.286381006240845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2614
=== ep: 2615, time 27.70335292816162, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2615
goal_identified
goal_identified
=== ep: 2616, time 27.12168312072754, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2616
=== ep: 2617, time 27.9045889377594, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2617
goal_identified
goal_identified
=== ep: 2618, time 27.44072413444519, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2618
=== ep: 2619, time 35.68114757537842, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2619
goal_identified
=== ep: 2620, time 27.365237712860107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2620
=== ep: 2621, time 27.485055685043335, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2621
goal_identified
goal_identified
goal_identified
=== ep: 2622, time 27.540475845336914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
=== ep: 2623, time 27.566563367843628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2623
=== ep: 2624, time 27.07698941230774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2624
=== ep: 2625, time 27.772545099258423, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2625
goal_identified
=== ep: 2626, time 27.6967031955719, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2626
=== ep: 2627, time 27.23040270805359, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2627
goal_identified
=== ep: 2628, time 27.4358811378479, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2628
goal_identified
=== ep: 2629, time 36.51191329956055, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2629
=== ep: 2630, time 27.33146905899048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2630
=== ep: 2631, time 27.372102975845337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2631
goal_identified
=== ep: 2632, time 27.382286310195923, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2632
goal_identified
=== ep: 2633, time 26.724801778793335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2633
goal_identified
=== ep: 2634, time 27.410804510116577, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2634
=== ep: 2635, time 27.545933723449707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2635
goal_identified
=== ep: 2636, time 27.531258583068848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2636
goal_identified
=== ep: 2637, time 27.637045860290527, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2637
=== ep: 2638, time 27.03507685661316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2638
=== ep: 2639, time 35.836060762405396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2639
goal_identified
=== ep: 2640, time 27.495551347732544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2640
=== ep: 2641, time 27.833734035491943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2641
=== ep: 2642, time 27.481101751327515, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2642
goal_identified
=== ep: 2643, time 27.223663568496704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2643
goal_identified
=== ep: 2644, time 28.341515064239502, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2644
goal_identified
goal_identified
=== ep: 2645, time 27.693829774856567, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2645
goal_identified
=== ep: 2646, time 27.49677348136902, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2646
goal_identified
goal_identified
goal_identified
=== ep: 2647, time 26.796895503997803, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
goal_identified
=== ep: 2648, time 27.66875958442688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2648
goal_identified
=== ep: 2649, time 36.13916635513306, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2649
goal_identified
=== ep: 2650, time 27.274035453796387, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2650
goal_identified
goal_identified
=== ep: 2651, time 27.56914496421814, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2651
goal_identified
goal_identified
=== ep: 2652, time 27.242987155914307, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2652
goal_identified
goal_identified
=== ep: 2653, time 27.81662607192993, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2653
goal_identified
=== ep: 2654, time 27.429627895355225, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2654
goal_identified
=== ep: 2655, time 27.700819730758667, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2655
goal_identified
=== ep: 2656, time 27.545944929122925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2656
goal_identified
=== ep: 2657, time 27.147810220718384, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2657
goal_identified
=== ep: 2658, time 27.75303077697754, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2658
goal_identified
=== ep: 2659, time 36.659024715423584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2659
=== ep: 2660, time 27.69583487510681, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2660
=== ep: 2661, time 26.933497428894043, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2661
goal_identified
=== ep: 2662, time 27.243139266967773, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2662
=== ep: 2663, time 27.58687949180603, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2663
=== ep: 2664, time 27.710121393203735, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2664
=== ep: 2665, time 27.548381567001343, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2665
goal_identified
=== ep: 2666, time 27.967018604278564, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2666
=== ep: 2667, time 26.99115753173828, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2667
goal_identified
goal_identified
goal_identified
=== ep: 2668, time 27.482197523117065, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2668
=== ep: 2669, time 36.34393310546875, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2669
=== ep: 2670, time 27.598164796829224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2670
=== ep: 2671, time 27.414881229400635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2671
=== ep: 2672, time 27.19323492050171, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2672
=== ep: 2673, time 27.826167345046997, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2673
=== ep: 2674, time 27.62072443962097, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2674
goal_identified
=== ep: 2675, time 27.596303939819336, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2675
goal_identified
=== ep: 2676, time 29.92540740966797, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2676
goal_identified
goal_identified
=== ep: 2677, time 27.172817945480347, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2677
goal_identified
=== ep: 2678, time 27.78913974761963, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2678
=== ep: 2679, time 36.347206115722656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2679
goal_identified
=== ep: 2680, time 27.436817169189453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2680
goal_identified
=== ep: 2681, time 27.507440090179443, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2681
=== ep: 2682, time 28.018552780151367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2682
=== ep: 2683, time 28.02837085723877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2683
=== ep: 2684, time 27.783594369888306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2684
goal_identified
goal_identified
=== ep: 2685, time 27.388071537017822, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2685
goal_identified
goal_identified
=== ep: 2686, time 27.34444546699524, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2686
=== ep: 2687, time 27.331072568893433, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2687
goal_identified
=== ep: 2688, time 27.30943751335144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2688
=== ep: 2689, time 36.48757362365723, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2689
=== ep: 2690, time 27.125287532806396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2690
=== ep: 2691, time 27.222841024398804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2691
=== ep: 2692, time 27.561033248901367, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2692
=== ep: 2693, time 27.748526096343994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2693
=== ep: 2694, time 27.778621435165405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2694
goal_identified
=== ep: 2695, time 27.405526638031006, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2695
=== ep: 2696, time 27.535927057266235, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2696
goal_identified
=== ep: 2697, time 27.5502347946167, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2697
goal_identified
goal_identified
=== ep: 2698, time 27.51444125175476, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2698
=== ep: 2699, time 36.171539545059204, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2699
goal_identified
=== ep: 2700, time 27.899070501327515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2700
=== ep: 2701, time 27.05060601234436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2701
=== ep: 2702, time 27.844712018966675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2702
goal_identified
=== ep: 2703, time 27.712400913238525, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2703
goal_identified
goal_identified
=== ep: 2704, time 27.309300184249878, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2704
goal_identified
goal_identified
=== ep: 2705, time 27.505579710006714, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2705
=== ep: 2706, time 27.192729473114014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2706
=== ep: 2707, time 27.702025651931763, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2707
=== ep: 2708, time 27.50590229034424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2708
=== ep: 2709, time 36.754615783691406, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2709
goal_identified
=== ep: 2710, time 27.01259469985962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2710
=== ep: 2711, time 27.748989582061768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2711
=== ep: 2712, time 27.841055154800415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2712
=== ep: 2713, time 27.8959698677063, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2713
goal_identified
=== ep: 2714, time 27.74809432029724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2714
=== ep: 2715, time 27.30991506576538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2715
goal_identified
=== ep: 2716, time 27.461535930633545, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2716
=== ep: 2717, time 27.52426242828369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2717
=== ep: 2718, time 27.854583740234375, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2718
goal_identified
=== ep: 2719, time 37.27219223976135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2719
=== ep: 2720, time 27.745209455490112, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2720
=== ep: 2721, time 27.84479069709778, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2721
=== ep: 2722, time 28.086363315582275, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2722
=== ep: 2723, time 27.86877179145813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2723
goal_identified
=== ep: 2724, time 27.362151384353638, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2724
goal_identified
goal_identified
=== ep: 2725, time 27.728360652923584, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2725
goal_identified
=== ep: 2726, time 27.737361431121826, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2726
goal_identified
=== ep: 2727, time 27.441394090652466, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2727
goal_identified
=== ep: 2728, time 27.269097328186035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2728
=== ep: 2729, time 36.137346267700195, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2729
goal_identified
=== ep: 2730, time 27.253198623657227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2730
goal_identified
=== ep: 2731, time 27.450323820114136, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2731
goal_identified
=== ep: 2732, time 27.417185306549072, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2732
=== ep: 2733, time 27.65615749359131, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2733
=== ep: 2734, time 27.48438262939453, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2734
=== ep: 2735, time 27.504230499267578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2735
goal_identified
goal_identified
=== ep: 2736, time 27.819669008255005, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2736
=== ep: 2737, time 28.148869276046753, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2737
goal_identified
=== ep: 2738, time 27.8919620513916, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2738
goal_identified
goal_identified
=== ep: 2739, time 37.084062814712524, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2739
=== ep: 2740, time 27.44152855873108, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 27.60446858406067, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2741
goal_identified
goal_identified
=== ep: 2742, time 27.49347186088562, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 27.230706214904785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2743
=== ep: 2744, time 27.459171772003174, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2744
goal_identified
=== ep: 2745, time 27.670042514801025, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2745
goal_identified
=== ep: 2746, time 27.465917110443115, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2746
goal_identified
=== ep: 2747, time 27.376342296600342, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2747
=== ep: 2748, time 27.338987350463867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2748
=== ep: 2749, time 35.39467406272888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2749
=== ep: 2750, time 27.418535709381104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 27.91407608985901, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2751
=== ep: 2752, time 27.318784952163696, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2752
=== ep: 2753, time 26.90498924255371, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2753
goal_identified
=== ep: 2754, time 27.190430164337158, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2754
=== ep: 2755, time 27.66801905632019, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2755
=== ep: 2756, time 27.28164577484131, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2756
=== ep: 2757, time 27.046842575073242, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2757
goal_identified
=== ep: 2758, time 27.409900903701782, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2758
=== ep: 2759, time 36.370699644088745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2759
=== ep: 2760, time 27.680183172225952, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2760
goal_identified
=== ep: 2761, time 27.43959927558899, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2761
=== ep: 2762, time 27.622209310531616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2762
goal_identified
goal_identified
=== ep: 2763, time 27.146379470825195, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2763
=== ep: 2764, time 27.608824253082275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2764
goal_identified
goal_identified
=== ep: 2765, time 27.363313913345337, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2765
goal_identified
=== ep: 2766, time 27.454245805740356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2766
=== ep: 2767, time 27.282946586608887, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2767
goal_identified
=== ep: 2768, time 27.303935766220093, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2768
=== ep: 2769, time 36.65422010421753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2769
=== ep: 2770, time 28.164237022399902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2770
=== ep: 2771, time 27.899786472320557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2771
=== ep: 2772, time 27.181715488433838, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2772
=== ep: 2773, time 27.492380142211914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2773
=== ep: 2774, time 27.668489694595337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2774
goal_identified
=== ep: 2775, time 27.500313997268677, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2775
goal_identified
=== ep: 2776, time 27.49409294128418, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2776
goal_identified
=== ep: 2777, time 27.30579376220703, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2777
goal_identified
=== ep: 2778, time 27.401978015899658, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2778
=== ep: 2779, time 37.133755922317505, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2779
goal_identified
=== ep: 2780, time 28.166164875030518, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2780
=== ep: 2781, time 27.40553307533264, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2781
=== ep: 2782, time 27.44206142425537, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2782
goal_identified
=== ep: 2783, time 27.401979207992554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2783
=== ep: 2784, time 27.390445470809937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2784
goal_identified
=== ep: 2785, time 27.639008045196533, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2785
=== ep: 2786, time 27.051618576049805, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2786
=== ep: 2787, time 27.692371368408203, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2787
=== ep: 2788, time 25.4795138835907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 27/27)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2788
=== ep: 2789, time 36.778260469436646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2789
=== ep: 2790, time 27.329818964004517, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2790
goal_identified
goal_identified
=== ep: 2791, time 26.840821504592896, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2791
goal_identified
goal_identified
=== ep: 2792, time 27.369402408599854, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2792
=== ep: 2793, time 27.527169942855835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2793
=== ep: 2794, time 27.47204566001892, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2794
=== ep: 2795, time 27.706214904785156, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2795
goal_identified
=== ep: 2796, time 27.35235047340393, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2796
goal_identified
=== ep: 2797, time 27.40265917778015, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2797
=== ep: 2798, time 27.127913236618042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2798
=== ep: 2799, time 36.836132526397705, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2799
=== ep: 2800, time 27.255845069885254, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2800
=== ep: 2801, time 27.459101676940918, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2801
=== ep: 2802, time 27.73928737640381, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2802
goal_identified
=== ep: 2803, time 27.695990800857544, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2803
=== ep: 2804, time 27.511096954345703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2804
=== ep: 2805, time 27.301712036132812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2805
goal_identified
goal_identified
=== ep: 2806, time 27.478349685668945, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2806
goal_identified
=== ep: 2807, time 27.596740007400513, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2807
goal_identified
=== ep: 2808, time 27.80749273300171, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2808
=== ep: 2809, time 37.209352254867554, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2809
goal_identified
=== ep: 2810, time 27.719637632369995, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2810
=== ep: 2811, time 27.639411687850952, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2811
=== ep: 2812, time 27.94676375389099, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2812
goal_identified
=== ep: 2813, time 27.624831676483154, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2813
goal_identified
=== ep: 2814, time 28.079079389572144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2814
goal_identified
=== ep: 2815, time 27.22279191017151, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2815
goal_identified
=== ep: 2816, time 27.35469388961792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2816
=== ep: 2817, time 27.600319385528564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2817
=== ep: 2818, time 28.075663089752197, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2818
goal_identified
=== ep: 2819, time 36.52840995788574, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2819
=== ep: 2820, time 27.167452335357666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2820
goal_identified
=== ep: 2821, time 27.59752321243286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2821
goal_identified
=== ep: 2822, time 27.371545791625977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2822
=== ep: 2823, time 27.63561177253723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2823
goal_identified
goal_identified
=== ep: 2824, time 27.10552668571472, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2824
goal_identified
=== ep: 2825, time 27.299712896347046, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2825
goal_identified
=== ep: 2826, time 27.735942125320435, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2826
=== ep: 2827, time 27.367104291915894, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2827
goal_identified
=== ep: 2828, time 27.4518404006958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2828
=== ep: 2829, time 36.30708146095276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2829
goal_identified
=== ep: 2830, time 27.540212631225586, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2830
=== ep: 2831, time 27.451502799987793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2831
=== ep: 2832, time 27.58336853981018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2832
=== ep: 2833, time 27.645792484283447, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2833
goal_identified
goal_identified
=== ep: 2834, time 27.355544090270996, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2834
=== ep: 2835, time 27.814670085906982, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2835
goal_identified
=== ep: 2836, time 27.599450826644897, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2836
=== ep: 2837, time 27.685500383377075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2837
=== ep: 2838, time 27.646644592285156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2838
=== ep: 2839, time 35.70651650428772, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2839
goal_identified
=== ep: 2840, time 26.89907169342041, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 27.396501302719116, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2841
=== ep: 2842, time 27.372210264205933, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2842
goal_identified
=== ep: 2843, time 27.08119511604309, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2843
goal_identified
=== ep: 2844, time 26.765247106552124, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2844
goal_identified
=== ep: 2845, time 26.992327451705933, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2845
goal_identified
=== ep: 2846, time 27.420472383499146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2846
goal_identified
=== ep: 2847, time 27.387081384658813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2847
goal_identified
=== ep: 2848, time 27.261227130889893, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2848
=== ep: 2849, time 36.74472451210022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2849
=== ep: 2850, time 27.116214752197266, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2850
goal_identified
=== ep: 2851, time 27.321314811706543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2851
=== ep: 2852, time 27.074942111968994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2852
goal_identified
=== ep: 2853, time 27.538054704666138, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2853
=== ep: 2854, time 26.992302179336548, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2854
=== ep: 2855, time 27.36906123161316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2855
=== ep: 2856, time 27.413172960281372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2856
=== ep: 2857, time 27.52647376060486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2857
=== ep: 2858, time 27.432396411895752, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2858
=== ep: 2859, time 37.63356947898865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2859
=== ep: 2860, time 31.751259326934814, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2860
=== ep: 2861, time 27.373122453689575, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2861
goal_identified
=== ep: 2862, time 27.31583309173584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2862
goal_identified
=== ep: 2863, time 27.26962685585022, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2863
=== ep: 2864, time 26.847450256347656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2864
goal_identified
=== ep: 2865, time 27.506478786468506, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2865
=== ep: 2866, time 27.425607204437256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2866
=== ep: 2867, time 27.368371963500977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2867
=== ep: 2868, time 27.103113174438477, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2868
goal_identified
=== ep: 2869, time 36.41268444061279, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2869
=== ep: 2870, time 27.17774724960327, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2870
=== ep: 2871, time 27.23254656791687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2871
=== ep: 2872, time 27.409795999526978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2872
=== ep: 2873, time 26.95842146873474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2873
=== ep: 2874, time 27.519546270370483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2874
goal_identified
=== ep: 2875, time 27.38955307006836, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2875
goal_identified
=== ep: 2876, time 27.43205213546753, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2876
=== ep: 2877, time 27.178853511810303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2877
=== ep: 2878, time 27.149383306503296, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2878
=== ep: 2879, time 33.55346989631653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 41/41)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2879
=== ep: 2880, time 27.63380265235901, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2880
=== ep: 2881, time 27.194805145263672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2881
=== ep: 2882, time 27.107898950576782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2882
goal_identified
=== ep: 2883, time 26.868884086608887, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2883
=== ep: 2884, time 26.941349506378174, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2884
=== ep: 2885, time 27.58300542831421, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2885
=== ep: 2886, time 27.364798307418823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2886
goal_identified
goal_identified
=== ep: 2887, time 27.564428567886353, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2887
=== ep: 2888, time 26.98506760597229, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2888
=== ep: 2889, time 36.670979738235474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2889
=== ep: 2890, time 27.61095690727234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2890
goal_identified
goal_identified
=== ep: 2891, time 27.448199033737183, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2891
=== ep: 2892, time 27.61654019355774, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2892
goal_identified
=== ep: 2893, time 27.11010241508484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2893
=== ep: 2894, time 27.165703535079956, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2894
=== ep: 2895, time 27.215132236480713, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2895
=== ep: 2896, time 27.411467790603638, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2896
=== ep: 2897, time 30.26499581336975, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2897
=== ep: 2898, time 26.919775009155273, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2898
=== ep: 2899, time 36.948110580444336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2899
goal_identified
=== ep: 2900, time 27.34171462059021, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2900
=== ep: 2901, time 27.371281385421753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2901
=== ep: 2902, time 27.169357538223267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2902
=== ep: 2903, time 26.526047468185425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2903
goal_identified
goal_identified
=== ep: 2904, time 27.514771461486816, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2904
=== ep: 2905, time 27.12466526031494, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2905
=== ep: 2906, time 27.530459880828857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2906
=== ep: 2907, time 27.72717261314392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2907
=== ep: 2908, time 27.22452998161316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2908
=== ep: 2909, time 37.19558787345886, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2909
=== ep: 2910, time 27.482009649276733, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2910
goal_identified
=== ep: 2911, time 27.470104455947876, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2911
goal_identified
=== ep: 2912, time 27.429001331329346, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2912
=== ep: 2913, time 26.705196380615234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2913
=== ep: 2914, time 27.66137385368347, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2914
=== ep: 2915, time 27.527791023254395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2915
=== ep: 2916, time 27.114948511123657, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2916
=== ep: 2917, time 27.802542209625244, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2917
=== ep: 2918, time 27.192668199539185, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2918
=== ep: 2919, time 35.90276384353638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2919
goal_identified
=== ep: 2920, time 27.4692702293396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2920
goal_identified
=== ep: 2921, time 27.875437021255493, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2921
=== ep: 2922, time 27.41422462463379, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2922
goal_identified
=== ep: 2923, time 26.81639051437378, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2923
=== ep: 2924, time 27.71962261199951, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2924
goal_identified
=== ep: 2925, time 27.464179277420044, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2925
=== ep: 2926, time 27.356931686401367, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2926
goal_identified
=== ep: 2927, time 27.33694362640381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2927
=== ep: 2928, time 27.547940969467163, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 36.43004035949707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2929
goal_identified
=== ep: 2930, time 27.51719307899475, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2930
=== ep: 2931, time 28.075371742248535, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2931
goal_identified
=== ep: 2932, time 27.361360788345337, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2932
=== ep: 2933, time 26.75275731086731, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2933
goal_identified
goal_identified
=== ep: 2934, time 27.144304513931274, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2934
=== ep: 2935, time 27.47237491607666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2935
=== ep: 2936, time 27.350475311279297, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2936
goal_identified
=== ep: 2937, time 27.18605399131775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2937
=== ep: 2938, time 26.93771529197693, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2938
=== ep: 2939, time 35.936097145080566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2939
goal_identified
=== ep: 2940, time 27.80148696899414, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2940
=== ep: 2941, time 27.430444478988647, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2941
goal_identified
=== ep: 2942, time 27.45145845413208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2942
=== ep: 2943, time 27.3310763835907, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2943
goal_identified
=== ep: 2944, time 27.446550369262695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2944
=== ep: 2945, time 27.29437232017517, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2945
goal_identified
goal_identified
=== ep: 2946, time 27.23949384689331, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2946
goal_identified
=== ep: 2947, time 27.28604006767273, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2947
goal_identified
=== ep: 2948, time 26.971755981445312, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2948
goal_identified
=== ep: 2949, time 36.544822692871094, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2949
=== ep: 2950, time 27.550578355789185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2950
=== ep: 2951, time 27.427181720733643, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2951
goal_identified
=== ep: 2952, time 27.2571759223938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2952
=== ep: 2953, time 27.414385080337524, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2953
=== ep: 2954, time 27.504299640655518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2954
=== ep: 2955, time 27.88172698020935, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2955
=== ep: 2956, time 27.452520847320557, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2956
=== ep: 2957, time 27.69054913520813, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2957
goal_identified
=== ep: 2958, time 27.18673825263977, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2958
=== ep: 2959, time 36.260380268096924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2959
=== ep: 2960, time 27.804556131362915, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2960
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2961, time 27.643442630767822, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
goal_identified
=== ep: 2962, time 27.29737091064453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2962
goal_identified
=== ep: 2963, time 27.02253293991089, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2963
goal_identified
=== ep: 2964, time 27.49341917037964, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2964
goal_identified
=== ep: 2965, time 27.30836582183838, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2965
=== ep: 2966, time 27.625030755996704, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2966
=== ep: 2967, time 27.468048095703125, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2967
=== ep: 2968, time 27.342764377593994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2968
goal_identified
=== ep: 2969, time 36.57600259780884, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2969
=== ep: 2970, time 27.463282585144043, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2970
goal_identified
=== ep: 2971, time 26.93017339706421, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2971
=== ep: 2972, time 26.69789409637451, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2972
goal_identified
goal_identified
=== ep: 2973, time 27.21287512779236, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2973
goal_identified
=== ep: 2974, time 27.410173654556274, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2974
goal_identified
=== ep: 2975, time 27.228551149368286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2975
=== ep: 2976, time 27.299713134765625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2976
=== ep: 2977, time 27.805617094039917, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2977
goal_identified
=== ep: 2978, time 27.7518892288208, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2978
goal_identified
=== ep: 2979, time 37.387439012527466, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2979
=== ep: 2980, time 27.47103977203369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2980
=== ep: 2981, time 27.56150460243225, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2981
=== ep: 2982, time 26.9198215007782, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2982
goal_identified
=== ep: 2983, time 27.858370304107666, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2983
=== ep: 2984, time 27.53492307662964, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2984
goal_identified
=== ep: 2985, time 27.133214473724365, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2985
goal_identified
=== ep: 2986, time 27.73723554611206, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2986
goal_identified
=== ep: 2987, time 27.179787635803223, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2987
goal_identified
=== ep: 2988, time 26.97959589958191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2988
=== ep: 2989, time 36.49441599845886, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2989
=== ep: 2990, time 27.184667348861694, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2990
=== ep: 2991, time 27.317208290100098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2991
=== ep: 2992, time 27.424322843551636, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2992
=== ep: 2993, time 27.137808799743652, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2993
=== ep: 2994, time 27.407156944274902, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2994
goal_identified
=== ep: 2995, time 27.38116145133972, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2995
goal_identified
=== ep: 2996, time 27.544566869735718, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2996
=== ep: 2997, time 27.26681423187256, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2997
goal_identified
goal_identified
=== ep: 2998, time 26.930806636810303, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2998
goal_identified
=== ep: 2999, time 37.21210861206055, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2999
goal_identified
goal_identified
=== ep: 3000, time 27.40023946762085, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3000
=== ep: 3001, time 27.79349374771118, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3001
goal_identified
=== ep: 3002, time 27.053981065750122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3002
goal_identified
=== ep: 3003, time 27.695533275604248, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3003
=== ep: 3004, time 27.289797067642212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3004
goal_identified
=== ep: 3005, time 27.199823141098022, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3005
=== ep: 3006, time 28.021414518356323, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3006
goal_identified
=== ep: 3007, time 26.702409744262695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3007
=== ep: 3008, time 27.33745813369751, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3008
=== ep: 3009, time 37.66583585739136, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3009
goal_identified
=== ep: 3010, time 27.81899094581604, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3010
goal_identified
=== ep: 3011, time 27.210213899612427, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3011
goal_identified
=== ep: 3012, time 27.08884859085083, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3012
goal_identified
=== ep: 3013, time 27.098971366882324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3013
=== ep: 3014, time 27.6325466632843, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3014
goal_identified
goal_identified
=== ep: 3015, time 27.64869523048401, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3015
goal_identified
=== ep: 3016, time 27.29147434234619, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3016
goal_identified
=== ep: 3017, time 26.84700322151184, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3017
=== ep: 3018, time 27.114264726638794, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3018
=== ep: 3019, time 37.652695417404175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3019
goal_identified
goal_identified
=== ep: 3020, time 27.513525009155273, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3020
goal_identified
=== ep: 3021, time 26.913833618164062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3021
=== ep: 3022, time 27.7173011302948, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3022
=== ep: 3023, time 27.34231686592102, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3023
=== ep: 3024, time 27.19326663017273, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3024
=== ep: 3025, time 27.632044076919556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3025
goal_identified
=== ep: 3026, time 27.488568544387817, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3026
goal_identified
goal_identified
=== ep: 3027, time 27.011333465576172, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3027
goal_identified
=== ep: 3028, time 27.37322163581848, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3028
=== ep: 3029, time 36.12893056869507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3029
=== ep: 3030, time 27.566166400909424, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3030
=== ep: 3031, time 27.1430504322052, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3031
=== ep: 3032, time 27.310309171676636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3032
goal_identified
=== ep: 3033, time 27.502172231674194, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3033
=== ep: 3034, time 27.179153203964233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3034
=== ep: 3035, time 27.424147367477417, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3035
goal_identified
goal_identified
=== ep: 3036, time 26.7551589012146, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3036
goal_identified
goal_identified
=== ep: 3037, time 27.183306455612183, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3037
=== ep: 3038, time 27.800685167312622, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3038
=== ep: 3039, time 36.09616732597351, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3039
=== ep: 3040, time 27.183591604232788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 26.994056940078735, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3041
goal_identified
=== ep: 3042, time 27.88504719734192, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3042
goal_identified
=== ep: 3043, time 27.407880544662476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3043
=== ep: 3044, time 27.16446590423584, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3044
=== ep: 3045, time 26.397254467010498, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3045
goal_identified
=== ep: 3046, time 26.280475616455078, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3046
=== ep: 3047, time 25.664547204971313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3047
=== ep: 3048, time 25.384730100631714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3048
goal_identified
goal_identified
=== ep: 3049, time 34.980427980422974, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3049
=== ep: 3050, time 25.75420594215393, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3050
=== ep: 3051, time 25.56360697746277, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3051
=== ep: 3052, time 25.618243932724, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3052
goal_identified
=== ep: 3053, time 25.454189777374268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3053
=== ep: 3054, time 25.705955743789673, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3054
goal_identified
=== ep: 3055, time 25.658782720565796, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3055
goal_identified
=== ep: 3056, time 26.114711046218872, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3056
=== ep: 3057, time 25.577375411987305, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3057
goal_identified
=== ep: 3058, time 25.494187593460083, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3058
goal_identified
=== ep: 3059, time 34.29417324066162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3059
goal_identified
=== ep: 3060, time 25.701979875564575, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3060
=== ep: 3061, time 25.82513451576233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3061
=== ep: 3062, time 26.02431321144104, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3062
=== ep: 3063, time 26.171833992004395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3063
=== ep: 3064, time 26.825040340423584, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3064
goal_identified
=== ep: 3065, time 25.93458914756775, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3065
=== ep: 3066, time 26.055503845214844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3066
=== ep: 3067, time 26.538760662078857, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3067
goal_identified
=== ep: 3068, time 27.138965845108032, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3068
goal_identified
=== ep: 3069, time 36.942622423172, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3069
goal_identified
=== ep: 3070, time 26.883126258850098, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3070
goal_identified
goal_identified
=== ep: 3071, time 27.294705629348755, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3071
=== ep: 3072, time 27.720556497573853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3072
goal_identified
goal_identified
=== ep: 3073, time 26.913023948669434, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3073
=== ep: 3074, time 27.00545883178711, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3074
goal_identified
goal_identified
=== ep: 3075, time 27.54833960533142, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3075
goal_identified
goal_identified
=== ep: 3076, time 27.47232151031494, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3076
goal_identified
goal_identified
=== ep: 3077, time 26.987303972244263, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3077
goal_identified
=== ep: 3078, time 27.322962045669556, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3078
goal_identified
=== ep: 3079, time 36.56031918525696, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3079
=== ep: 3080, time 27.377007246017456, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3080
goal_identified
=== ep: 3081, time 27.19020438194275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3081
=== ep: 3082, time 27.173810482025146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3082
=== ep: 3083, time 27.431294679641724, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3083
=== ep: 3084, time 27.211846828460693, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3084
goal_identified
=== ep: 3085, time 27.619616508483887, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3085
goal_identified
=== ep: 3086, time 27.358771562576294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3086
goal_identified
=== ep: 3087, time 26.78597617149353, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3087
goal_identified
goal_identified
goal_identified
=== ep: 3088, time 27.48468017578125, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3088
=== ep: 3089, time 36.43739104270935, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3089
goal_identified
=== ep: 3090, time 27.482267379760742, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3090
=== ep: 3091, time 27.08196496963501, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3091
goal_identified
=== ep: 3092, time 26.940290927886963, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3092
goal_identified
=== ep: 3093, time 27.025580883026123, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3093
goal_identified
=== ep: 3094, time 27.348601579666138, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3094
goal_identified
=== ep: 3095, time 27.085811138153076, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3095
goal_identified
=== ep: 3096, time 27.153136491775513, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3096
=== ep: 3097, time 27.31295609474182, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 27.336438179016113, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3098
goal_identified
=== ep: 3099, time 36.33911681175232, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3099
=== ep: 3100, time 26.971020460128784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3100
=== ep: 3101, time 27.390817165374756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3101
goal_identified
goal_identified
=== ep: 3102, time 27.461623907089233, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3102
goal_identified
goal_identified
goal_identified
=== ep: 3103, time 26.9890456199646, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
goal_identified
goal_identified
goal_identified
=== ep: 3104, time 26.916484355926514, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3104
=== ep: 3105, time 27.043057203292847, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3105
goal_identified
=== ep: 3106, time 26.935227394104004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3106
=== ep: 3107, time 27.027841567993164, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3107
=== ep: 3108, time 27.387784719467163, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3108
goal_identified
=== ep: 3109, time 36.18504190444946, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3109
goal_identified
=== ep: 3110, time 26.898118495941162, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3110
goal_identified
=== ep: 3111, time 27.276888370513916, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3111
=== ep: 3112, time 26.85271978378296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3112
goal_identified
=== ep: 3113, time 27.021234035491943, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3113
=== ep: 3114, time 27.264408826828003, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3114
goal_identified
=== ep: 3115, time 27.421773433685303, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3115
=== ep: 3116, time 27.378771543502808, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3116
=== ep: 3117, time 26.999638080596924, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3117
=== ep: 3118, time 27.04559326171875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3118
goal_identified
=== ep: 3119, time 36.48493146896362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3119
=== ep: 3120, time 27.783535957336426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3120
goal_identified
=== ep: 3121, time 27.48966145515442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3121
goal_identified
=== ep: 3122, time 26.694931268692017, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3122
=== ep: 3123, time 27.00942850112915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3123
goal_identified
=== ep: 3124, time 27.20789647102356, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3124
goal_identified
=== ep: 3125, time 27.21886968612671, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3125
=== ep: 3126, time 27.45562243461609, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3126
=== ep: 3127, time 27.02950358390808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3127
goal_identified
=== ep: 3128, time 27.11986517906189, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3128
=== ep: 3129, time 35.97874093055725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3129
goal_identified
goal_identified
=== ep: 3130, time 27.565390825271606, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3130
goal_identified
=== ep: 3131, time 27.60700035095215, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3131
=== ep: 3132, time 27.288994312286377, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3132
=== ep: 3133, time 27.01757049560547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3133
goal_identified
=== ep: 3134, time 26.651434421539307, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3134
=== ep: 3135, time 27.119536876678467, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3135
goal_identified
=== ep: 3136, time 27.055253744125366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3136
=== ep: 3137, time 27.134498596191406, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3137
=== ep: 3138, time 26.95501685142517, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3138
goal_identified
=== ep: 3139, time 36.182549476623535, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3139
=== ep: 3140, time 27.06566548347473, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3140
goal_identified
=== ep: 3141, time 27.24369192123413, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3141
=== ep: 3142, time 27.0890793800354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 26.615126848220825, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3143
=== ep: 3144, time 27.186171531677246, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3144
=== ep: 3145, time 27.366333723068237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3145
=== ep: 3146, time 27.35770535469055, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3146
=== ep: 3147, time 26.876633644104004, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3147
=== ep: 3148, time 27.826478719711304, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3148
goal_identified
goal_identified
=== ep: 3149, time 36.23328876495361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3149
=== ep: 3150, time 27.52357244491577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3150
=== ep: 3151, time 27.14881992340088, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3151
=== ep: 3152, time 27.07106876373291, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3152
goal_identified
goal_identified
=== ep: 3153, time 27.204389572143555, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3153
=== ep: 3154, time 27.37445640563965, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3154
goal_identified
=== ep: 3155, time 27.130748748779297, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3155
goal_identified
goal_identified
=== ep: 3156, time 27.482651710510254, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3156
goal_identified
=== ep: 3157, time 27.028478384017944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3157
=== ep: 3158, time 26.694141149520874, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3158
goal_identified
goal_identified
goal_identified
=== ep: 3159, time 36.49641156196594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
=== ep: 3160, time 26.910593271255493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3160
goal_identified
=== ep: 3161, time 27.34743881225586, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3161
=== ep: 3162, time 27.239818811416626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3162
=== ep: 3163, time 27.373125553131104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3163
goal_identified
=== ep: 3164, time 26.887655019760132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3164
goal_identified
goal_identified
=== ep: 3165, time 27.350379467010498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3165
goal_identified
goal_identified
=== ep: 3166, time 26.947157382965088, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3166
=== ep: 3167, time 27.417819499969482, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3167
=== ep: 3168, time 27.46048927307129, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3168
=== ep: 3169, time 35.63153028488159, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3169
goal_identified
=== ep: 3170, time 26.9603111743927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3170
goal_identified
=== ep: 3171, time 26.923492908477783, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3171
goal_identified
=== ep: 3172, time 27.268368005752563, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3172
=== ep: 3173, time 27.090586185455322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3173
goal_identified
goal_identified
=== ep: 3174, time 27.28411865234375, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3174
=== ep: 3175, time 27.27591562271118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3175
goal_identified
=== ep: 3176, time 26.62190055847168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3176
goal_identified
goal_identified
=== ep: 3177, time 27.35334014892578, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3177
goal_identified
=== ep: 3178, time 27.444059133529663, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3178
=== ep: 3179, time 34.92267560958862, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3179
=== ep: 3180, time 27.58594536781311, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3180
=== ep: 3181, time 27.36676836013794, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3181
=== ep: 3182, time 27.463214874267578, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3182
goal_identified
=== ep: 3183, time 27.069023847579956, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3183
=== ep: 3184, time 26.754698038101196, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3184
goal_identified
=== ep: 3185, time 27.52769660949707, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3185
goal_identified
=== ep: 3186, time 27.473172664642334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3186
goal_identified
goal_identified
=== ep: 3187, time 27.14037775993347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3187
=== ep: 3188, time 27.10295033454895, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3188
goal_identified
=== ep: 3189, time 36.13055896759033, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3189
=== ep: 3190, time 27.296217918395996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3190
goal_identified
goal_identified
=== ep: 3191, time 26.92804527282715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3191
goal_identified
=== ep: 3192, time 27.317232131958008, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3192
=== ep: 3193, time 27.10104274749756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3193
=== ep: 3194, time 27.16137933731079, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3194
=== ep: 3195, time 26.87306571006775, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3195
=== ep: 3196, time 27.05996561050415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3196
=== ep: 3197, time 27.323477268218994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3197
=== ep: 3198, time 27.27746272087097, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 35.47311758995056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3199
=== ep: 3200, time 27.22920298576355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3200
goal_identified
=== ep: 3201, time 26.88481569290161, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3201
=== ep: 3202, time 27.033398628234863, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3202
goal_identified
=== ep: 3203, time 27.042644500732422, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3203
=== ep: 3204, time 27.056337118148804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3204
=== ep: 3205, time 27.019967079162598, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3205
goal_identified
=== ep: 3206, time 27.088301181793213, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3206
=== ep: 3207, time 27.356705904006958, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3207
goal_identified
=== ep: 3208, time 27.059328317642212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3208
=== ep: 3209, time 36.44959473609924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3209
=== ep: 3210, time 27.213935375213623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3210
=== ep: 3211, time 27.399858951568604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3211
goal_identified
=== ep: 3212, time 27.15722942352295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3212
goal_identified
=== ep: 3213, time 26.916407108306885, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3213
goal_identified
=== ep: 3214, time 27.11643075942993, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3214
goal_identified
goal_identified
=== ep: 3215, time 27.41833996772766, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3215
=== ep: 3216, time 27.365906715393066, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3216
goal_identified
=== ep: 3217, time 27.510026454925537, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3217
=== ep: 3218, time 27.676387310028076, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3218
goal_identified
=== ep: 3219, time 35.94176721572876, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3219
goal_identified
=== ep: 3220, time 27.364283323287964, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3220
=== ep: 3221, time 26.974997758865356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3221
=== ep: 3222, time 27.424851417541504, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3222
goal_identified
goal_identified
=== ep: 3223, time 27.208094358444214, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3223
goal_identified
=== ep: 3224, time 27.198364973068237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3224
=== ep: 3225, time 27.374168395996094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3225
=== ep: 3226, time 27.33624768257141, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3226
=== ep: 3227, time 27.28649091720581, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3227
goal_identified
goal_identified
=== ep: 3228, time 27.49330234527588, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3228
=== ep: 3229, time 35.84203481674194, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3229
=== ep: 3230, time 27.40319514274597, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3230
=== ep: 3231, time 27.481876850128174, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3231
=== ep: 3232, time 27.391032457351685, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3232
=== ep: 3233, time 27.32248878479004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3233
goal_identified
=== ep: 3234, time 27.200940132141113, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3234
goal_identified
goal_identified
=== ep: 3235, time 27.946919202804565, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3235
=== ep: 3236, time 27.133472442626953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3236
=== ep: 3237, time 27.043828010559082, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3237
goal_identified
=== ep: 3238, time 27.45364546775818, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3238
=== ep: 3239, time 35.57938742637634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3239
goal_identified
=== ep: 3240, time 27.024646997451782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3240
=== ep: 3241, time 27.375266790390015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3241
goal_identified
=== ep: 3242, time 27.495174169540405, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3242
goal_identified
=== ep: 3243, time 27.045170545578003, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3243
goal_identified
=== ep: 3244, time 27.484779357910156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3244
goal_identified
=== ep: 3245, time 26.977325439453125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3245
goal_identified
goal_identified
goal_identified
=== ep: 3246, time 27.701177835464478, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3246
goal_identified
=== ep: 3247, time 27.163193225860596, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3247
=== ep: 3248, time 27.50552773475647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3248
goal_identified
=== ep: 3249, time 35.85459756851196, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3249
goal_identified
=== ep: 3250, time 27.410725116729736, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3250
goal_identified
=== ep: 3251, time 27.448969841003418, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3251
=== ep: 3252, time 27.577126502990723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3252
=== ep: 3253, time 27.961399793624878, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3253
=== ep: 3254, time 27.640228033065796, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3254
goal_identified
=== ep: 3255, time 27.45137619972229, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3255
goal_identified
=== ep: 3256, time 27.5814847946167, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3256
=== ep: 3257, time 27.469254970550537, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3257
=== ep: 3258, time 27.299681901931763, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3258
=== ep: 3259, time 36.046340227127075, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3259
=== ep: 3260, time 27.068432331085205, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3260
goal_identified
=== ep: 3261, time 27.051448583602905, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3261
goal_identified
goal_identified
=== ep: 3262, time 27.136976718902588, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3262
goal_identified
=== ep: 3263, time 27.309221267700195, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3263
goal_identified
=== ep: 3264, time 27.72588276863098, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3264
=== ep: 3265, time 28.331490516662598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3265
=== ep: 3266, time 27.20110583305359, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3266
=== ep: 3267, time 27.479146003723145, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3267
=== ep: 3268, time 27.03703808784485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3268
=== ep: 3269, time 35.84243392944336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3269
=== ep: 3270, time 27.60824751853943, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3270
=== ep: 3271, time 27.174870491027832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3271
goal_identified
=== ep: 3272, time 27.341144323349, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3272
goal_identified
=== ep: 3273, time 27.624946117401123, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3273
goal_identified
=== ep: 3274, time 27.488993406295776, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3274
goal_identified
goal_identified
=== ep: 3275, time 27.436310529708862, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3275
=== ep: 3276, time 27.26589059829712, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3276
=== ep: 3277, time 27.578598022460938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3277
=== ep: 3278, time 27.59155583381653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3278
=== ep: 3279, time 35.97851228713989, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3279
=== ep: 3280, time 27.746746063232422, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3280
goal_identified
=== ep: 3281, time 27.488332271575928, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3281
=== ep: 3282, time 26.901249885559082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3282
goal_identified
=== ep: 3283, time 27.350895404815674, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3283
=== ep: 3284, time 27.241734266281128, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3284
goal_identified
=== ep: 3285, time 27.03962254524231, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3285
goal_identified
goal_identified
goal_identified
=== ep: 3286, time 27.259896755218506, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3286
=== ep: 3287, time 27.381956338882446, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3287
goal_identified
=== ep: 3288, time 27.045854330062866, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3288
=== ep: 3289, time 36.38391470909119, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3289
goal_identified
=== ep: 3290, time 27.032459497451782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3290
goal_identified
goal_identified
=== ep: 3291, time 27.49527359008789, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3291
goal_identified
=== ep: 3292, time 27.677128791809082, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3292
=== ep: 3293, time 27.722006797790527, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3293
goal_identified
goal_identified
=== ep: 3294, time 27.248637199401855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3294
=== ep: 3295, time 27.61983633041382, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3295
goal_identified
=== ep: 3296, time 27.322487592697144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3296
goal_identified
=== ep: 3297, time 27.634462356567383, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3297
goal_identified
=== ep: 3298, time 27.18711543083191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3298
goal_identified
=== ep: 3299, time 36.073368549346924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3299
goal_identified
=== ep: 3300, time 27.306110858917236, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3300
=== ep: 3301, time 27.44683861732483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3301
goal_identified
=== ep: 3302, time 27.147934198379517, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3302
=== ep: 3303, time 27.32212209701538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3303
=== ep: 3304, time 27.65893840789795, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3304
=== ep: 3305, time 27.63113236427307, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3305
goal_identified
=== ep: 3306, time 27.309717893600464, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3306
goal_identified
=== ep: 3307, time 27.42368793487549, eps 0.001, sum reward: 1, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3307
goal_identified
=== ep: 3308, time 27.757221221923828, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3308
goal_identified
=== ep: 3309, time 36.2406952381134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3309
=== ep: 3310, time 27.278228998184204, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3310
=== ep: 3311, time 27.715946197509766, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3311
goal_identified
=== ep: 3312, time 27.55042314529419, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3312
=== ep: 3313, time 27.892993211746216, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3313
goal_identified
=== ep: 3314, time 27.623464822769165, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3314
=== ep: 3315, time 27.294666290283203, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3315
=== ep: 3316, time 27.196808338165283, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3316
=== ep: 3317, time 27.553977012634277, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3317
goal_identified
=== ep: 3318, time 27.702152729034424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3318
=== ep: 3319, time 35.92765808105469, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3319
goal_identified
=== ep: 3320, time 27.23927664756775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3320
=== ep: 3321, time 27.43499779701233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3321
goal_identified
=== ep: 3322, time 27.859641075134277, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3322
goal_identified
goal_identified
=== ep: 3323, time 27.205129384994507, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3323
=== ep: 3324, time 27.67435383796692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3324
=== ep: 3325, time 27.10623836517334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3325
goal_identified
goal_identified
=== ep: 3326, time 27.222684621810913, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3326
=== ep: 3327, time 27.284627199172974, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3327
goal_identified
goal_identified
=== ep: 3328, time 27.359671115875244, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3328
=== ep: 3329, time 35.54240536689758, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3329
goal_identified
=== ep: 3330, time 27.078109741210938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3330
=== ep: 3331, time 27.53423237800598, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3331
=== ep: 3332, time 27.419827699661255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3332
goal_identified
=== ep: 3333, time 27.184821605682373, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3333
=== ep: 3334, time 27.553594827651978, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3334
goal_identified
=== ep: 3335, time 27.728785276412964, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3335
goal_identified
goal_identified
=== ep: 3336, time 27.24237036705017, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3336
goal_identified
=== ep: 3337, time 27.179916381835938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3337
=== ep: 3338, time 27.460851430892944, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3338
goal_identified
goal_identified
=== ep: 3339, time 36.19553089141846, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3339
goal_identified
=== ep: 3340, time 27.69712519645691, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3340
=== ep: 3341, time 27.596436738967896, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3341
goal_identified
=== ep: 3342, time 27.15221381187439, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3342
=== ep: 3343, time 27.42072081565857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3343
=== ep: 3344, time 27.221927165985107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3344
=== ep: 3345, time 27.64977502822876, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3345
goal_identified
goal_identified
goal_identified
=== ep: 3346, time 26.911414623260498, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
=== ep: 3347, time 27.148536920547485, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3347
goal_identified
=== ep: 3348, time 27.686941623687744, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3348
goal_identified
=== ep: 3349, time 36.848363399505615, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3349
goal_identified
=== ep: 3350, time 27.170916318893433, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3350
goal_identified
=== ep: 3351, time 27.69835114479065, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3351
goal_identified
=== ep: 3352, time 27.458006620407104, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3352
=== ep: 3353, time 26.938701629638672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3353
=== ep: 3354, time 27.352073431015015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3354
goal_identified
=== ep: 3355, time 27.02160143852234, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3355
=== ep: 3356, time 27.240832805633545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3356
=== ep: 3357, time 27.498082399368286, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3357
goal_identified
=== ep: 3358, time 27.313074350357056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3358
=== ep: 3359, time 36.27753162384033, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3359
goal_identified
=== ep: 3360, time 27.32322072982788, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3360
goal_identified
=== ep: 3361, time 27.35206913948059, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3361
=== ep: 3362, time 27.393569946289062, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3362
goal_identified
goal_identified
=== ep: 3363, time 27.100828409194946, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3363
=== ep: 3364, time 27.26910924911499, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3364
goal_identified
goal_identified
=== ep: 3365, time 27.461474418640137, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3365
goal_identified
=== ep: 3366, time 26.84796667098999, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3366
goal_identified
=== ep: 3367, time 27.450461864471436, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3367
=== ep: 3368, time 27.276353359222412, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3368
goal_identified
=== ep: 3369, time 36.57839107513428, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3369
goal_identified
goal_identified
=== ep: 3370, time 26.98607897758484, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3370
goal_identified
goal_identified
=== ep: 3371, time 27.333100080490112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3371
goal_identified
=== ep: 3372, time 27.41528630256653, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3372
goal_identified
=== ep: 3373, time 27.355908393859863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3373
goal_identified
=== ep: 3374, time 27.121882915496826, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3374
goal_identified
=== ep: 3375, time 27.124671459197998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3375
=== ep: 3376, time 27.602282285690308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3376
=== ep: 3377, time 27.413655281066895, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3377
goal_identified
=== ep: 3378, time 27.41866374015808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3378
=== ep: 3379, time 36.23469662666321, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3379
=== ep: 3380, time 27.32103204727173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3380
=== ep: 3381, time 27.28805708885193, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3381
=== ep: 3382, time 27.290411710739136, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3382
=== ep: 3383, time 27.46927309036255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3383
goal_identified
=== ep: 3384, time 26.797334671020508, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3384
=== ep: 3385, time 27.1840763092041, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3385
=== ep: 3386, time 27.237162351608276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3386
goal_identified
=== ep: 3387, time 27.121905088424683, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3387
=== ep: 3388, time 27.128517627716064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3388
goal_identified
=== ep: 3389, time 36.70556974411011, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3389
goal_identified
goal_identified
=== ep: 3390, time 27.544644832611084, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3390
goal_identified
=== ep: 3391, time 27.218728065490723, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3391
goal_identified
=== ep: 3392, time 27.27789545059204, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3392
goal_identified
=== ep: 3393, time 27.085843324661255, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3393
goal_identified
goal_identified
=== ep: 3394, time 27.266823530197144, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3394
goal_identified
=== ep: 3395, time 27.309417963027954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3395
goal_identified
=== ep: 3396, time 27.618733644485474, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3396
goal_identified
=== ep: 3397, time 26.988537073135376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3397
=== ep: 3398, time 27.56775188446045, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3398
=== ep: 3399, time 37.00025725364685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3399
=== ep: 3400, time 27.39959406852722, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3400
goal_identified
=== ep: 3401, time 27.26294207572937, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3401
goal_identified
=== ep: 3402, time 27.29778265953064, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3402
goal_identified
=== ep: 3403, time 27.570346117019653, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3403
=== ep: 3404, time 27.644816875457764, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3404
goal_identified
=== ep: 3405, time 26.85002851486206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3405
goal_identified
=== ep: 3406, time 27.162782430648804, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3406
=== ep: 3407, time 27.30726432800293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3407
=== ep: 3408, time 27.059446573257446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3408
goal_identified
=== ep: 3409, time 37.25814652442932, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3409
goal_identified
=== ep: 3410, time 27.154491662979126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3410
goal_identified
goal_identified
=== ep: 3411, time 27.582395553588867, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3411
=== ep: 3412, time 27.7164945602417, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3412
=== ep: 3413, time 26.784090757369995, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3413
goal_identified
=== ep: 3414, time 26.903384923934937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3414
goal_identified
=== ep: 3415, time 27.57860803604126, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3415
goal_identified
=== ep: 3416, time 27.07806921005249, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3416
goal_identified
=== ep: 3417, time 27.383235692977905, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3417
goal_identified
=== ep: 3418, time 27.583882570266724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3418
=== ep: 3419, time 37.52634143829346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3419
goal_identified
=== ep: 3420, time 26.998441457748413, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3420
=== ep: 3421, time 27.084106922149658, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3421
=== ep: 3422, time 27.049315214157104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3422
goal_identified
=== ep: 3423, time 27.5076687335968, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3423
goal_identified
goal_identified
=== ep: 3424, time 27.538525819778442, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3424
goal_identified
=== ep: 3425, time 27.38059163093567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3425
goal_identified
=== ep: 3426, time 27.047852516174316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3426
goal_identified
goal_identified
=== ep: 3427, time 27.4362473487854, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3427
goal_identified
=== ep: 3428, time 27.296215295791626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3428
=== ep: 3429, time 36.000274896621704, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3429
goal_identified
=== ep: 3430, time 27.523618936538696, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3430
goal_identified
=== ep: 3431, time 27.392868280410767, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3431
=== ep: 3432, time 27.609942197799683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3432
=== ep: 3433, time 27.102733373641968, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3433
=== ep: 3434, time 27.263216018676758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3434
=== ep: 3435, time 27.18283462524414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3435
goal_identified
=== ep: 3436, time 27.383960008621216, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3436
=== ep: 3437, time 27.169939517974854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3437
=== ep: 3438, time 27.086395502090454, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3438
goal_identified
=== ep: 3439, time 35.87574315071106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3439
goal_identified
=== ep: 3440, time 27.017051696777344, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3440
=== ep: 3441, time 27.067520141601562, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3441
goal_identified
=== ep: 3442, time 27.182990312576294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3442
=== ep: 3443, time 27.731441736221313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3443
=== ep: 3444, time 27.064409971237183, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3444
goal_identified
goal_identified
=== ep: 3445, time 27.20862913131714, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3445
=== ep: 3446, time 27.300338983535767, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3446
goal_identified
=== ep: 3447, time 27.47644352912903, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 27.054819107055664, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3448
=== ep: 3449, time 36.07155466079712, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3449
=== ep: 3450, time 27.331806182861328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3450
=== ep: 3451, time 27.44991183280945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3451
goal_identified
=== ep: 3452, time 27.44264030456543, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3452
=== ep: 3453, time 27.403242588043213, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3453
goal_identified
=== ep: 3454, time 26.861912727355957, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3454
=== ep: 3455, time 27.418358087539673, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3455
goal_identified
goal_identified
=== ep: 3456, time 27.224650144577026, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3456
goal_identified
=== ep: 3457, time 27.160777807235718, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3457
goal_identified
goal_identified
=== ep: 3458, time 27.54704189300537, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3458
goal_identified
goal_identified
goal_identified
=== ep: 3459, time 36.385334491729736, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3459
goal_identified
=== ep: 3460, time 26.842286109924316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3460
goal_identified
=== ep: 3461, time 27.10520601272583, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3461
goal_identified
=== ep: 3462, time 27.319242477416992, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3462
=== ep: 3463, time 27.04734706878662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3463
=== ep: 3464, time 27.290667057037354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3464
goal_identified
goal_identified
goal_identified
=== ep: 3465, time 27.120953798294067, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3465
=== ep: 3466, time 27.523568391799927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3466
goal_identified
=== ep: 3467, time 27.264456033706665, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3467
=== ep: 3468, time 27.10192561149597, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3468
=== ep: 3469, time 42.09315037727356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3469
goal_identified
=== ep: 3470, time 27.49653196334839, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3470
=== ep: 3471, time 27.542454481124878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3471
=== ep: 3472, time 27.401883125305176, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3472
goal_identified
=== ep: 3473, time 27.14264464378357, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3473
=== ep: 3474, time 26.83235812187195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3474
=== ep: 3475, time 27.31066584587097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3475
=== ep: 3476, time 27.111162900924683, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3476
goal_identified
=== ep: 3477, time 27.27430272102356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3477
goal_identified
=== ep: 3478, time 27.505549430847168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3478
goal_identified
=== ep: 3479, time 35.985963106155396, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3479
goal_identified
=== ep: 3480, time 27.70135498046875, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3480
goal_identified
=== ep: 3481, time 27.199368476867676, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3481
goal_identified
=== ep: 3482, time 27.380664110183716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3482
=== ep: 3483, time 27.376639366149902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3483
=== ep: 3484, time 27.473780870437622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3484
=== ep: 3485, time 27.24851393699646, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3485
=== ep: 3486, time 27.088078498840332, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3486
goal_identified
=== ep: 3487, time 27.440504550933838, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3487
goal_identified
=== ep: 3488, time 27.68699836730957, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3488
goal_identified
=== ep: 3489, time 36.473828077316284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3489
=== ep: 3490, time 27.551390409469604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3490
=== ep: 3491, time 27.432512283325195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3491
=== ep: 3492, time 27.50053310394287, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3492
goal_identified
goal_identified
goal_identified
=== ep: 3493, time 26.912729024887085, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3493
=== ep: 3494, time 27.647402048110962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3494
=== ep: 3495, time 27.21092176437378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3495
goal_identified
=== ep: 3496, time 27.273518085479736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3496
goal_identified
goal_identified
=== ep: 3497, time 27.397242069244385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3497
=== ep: 3498, time 27.292282819747925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3498
goal_identified
=== ep: 3499, time 36.166940450668335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3499
goal_identified
=== ep: 3500, time 27.242561101913452, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3500
goal_identified
=== ep: 3501, time 27.21089458465576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3501
goal_identified
goal_identified
goal_identified
=== ep: 3502, time 27.088167905807495, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1706
=== ep: 3503, time 27.3101863861084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3503
=== ep: 3504, time 26.827972650527954, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3504
goal_identified
=== ep: 3505, time 27.49010419845581, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3505
=== ep: 3506, time 27.129862308502197, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3506
goal_identified
=== ep: 3507, time 27.66387438774109, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3507
goal_identified
goal_identified
=== ep: 3508, time 26.991743087768555, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3508
goal_identified
goal_identified
=== ep: 3509, time 35.67677688598633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3509
=== ep: 3510, time 27.38627791404724, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3510
goal_identified
=== ep: 3511, time 27.319605350494385, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3511
goal_identified
goal_identified
goal_identified
=== ep: 3512, time 27.151256322860718, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3512
goal_identified
=== ep: 3513, time 27.389847993850708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3513
goal_identified
goal_identified
=== ep: 3514, time 27.514664888381958, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3514
goal_identified
=== ep: 3515, time 26.977795839309692, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3515
goal_identified
=== ep: 3516, time 26.96023726463318, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3516
goal_identified
=== ep: 3517, time 27.40175771713257, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3517
goal_identified
=== ep: 3518, time 27.59809374809265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3518
goal_identified
=== ep: 3519, time 35.68393278121948, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3519
goal_identified
=== ep: 3520, time 26.854185581207275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3520
goal_identified
=== ep: 3521, time 27.501575469970703, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3521
=== ep: 3522, time 27.721651554107666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3522
goal_identified
=== ep: 3523, time 27.44775676727295, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3523
goal_identified
=== ep: 3524, time 27.381402015686035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3524
goal_identified
=== ep: 3525, time 26.99520707130432, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3525
=== ep: 3526, time 27.417625665664673, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3526
=== ep: 3527, time 27.361977338790894, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3527
=== ep: 3528, time 27.533706426620483, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3528
goal_identified
=== ep: 3529, time 36.377257108688354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3529
=== ep: 3530, time 26.803312063217163, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3530
goal_identified
=== ep: 3531, time 27.193262577056885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3531
=== ep: 3532, time 27.260578155517578, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3532
=== ep: 3533, time 27.563807725906372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3533
=== ep: 3534, time 27.40641736984253, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3534
goal_identified
=== ep: 3535, time 27.30635404586792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3535
goal_identified
=== ep: 3536, time 27.281606674194336, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3536
=== ep: 3537, time 27.353960514068604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3537
goal_identified
goal_identified
=== ep: 3538, time 27.435186862945557, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3538
goal_identified
goal_identified
=== ep: 3539, time 37.30144023895264, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3539
=== ep: 3540, time 28.07003164291382, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3540
=== ep: 3541, time 27.60555863380432, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3541
=== ep: 3542, time 27.254392385482788, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3542
=== ep: 3543, time 27.22398543357849, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3543
=== ep: 3544, time 27.263695240020752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3544
=== ep: 3545, time 27.41516900062561, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3545
goal_identified
=== ep: 3546, time 27.361160278320312, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3546
=== ep: 3547, time 27.472943782806396, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3547
goal_identified
=== ep: 3548, time 27.316527128219604, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3548
goal_identified
=== ep: 3549, time 37.22534704208374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3549
goal_identified
=== ep: 3550, time 27.513671875, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3550
goal_identified
goal_identified
=== ep: 3551, time 27.70268702507019, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3551
goal_identified
goal_identified
=== ep: 3552, time 27.282106161117554, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3552
goal_identified
=== ep: 3553, time 27.340898513793945, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3553
goal_identified
=== ep: 3554, time 27.39615750312805, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3554
goal_identified
=== ep: 3555, time 28.38457703590393, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3555
goal_identified
=== ep: 3556, time 27.55169439315796, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3556
goal_identified
=== ep: 3557, time 27.451091527938843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3557
goal_identified
=== ep: 3558, time 27.33908987045288, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3558
=== ep: 3559, time 36.012839555740356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3559
=== ep: 3560, time 27.443520545959473, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3560
goal_identified
=== ep: 3561, time 27.494856357574463, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3561
=== ep: 3562, time 27.084391832351685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3562
=== ep: 3563, time 27.346127033233643, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3563
=== ep: 3564, time 27.119024991989136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3564
=== ep: 3565, time 27.182081699371338, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3565
=== ep: 3566, time 27.60199499130249, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3566
goal_identified
goal_identified
=== ep: 3567, time 29.700015544891357, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3567
goal_identified
goal_identified
=== ep: 3568, time 27.393067121505737, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3568
=== ep: 3569, time 35.97008275985718, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3569
=== ep: 3570, time 27.10554552078247, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3570
goal_identified
goal_identified
=== ep: 3571, time 27.515690088272095, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3571
=== ep: 3572, time 27.240339756011963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3572
goal_identified
goal_identified
=== ep: 3573, time 27.224722146987915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3573
=== ep: 3574, time 27.208000659942627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3574
goal_identified
=== ep: 3575, time 26.95261311531067, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3575
=== ep: 3576, time 27.814186811447144, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3576
goal_identified
=== ep: 3577, time 27.54570960998535, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3577
goal_identified
goal_identified
=== ep: 3578, time 27.09262490272522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3578
=== ep: 3579, time 36.37846064567566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3579
=== ep: 3580, time 27.38375449180603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3580
=== ep: 3581, time 27.514906883239746, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3581
=== ep: 3582, time 26.874337673187256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3582
=== ep: 3583, time 27.933927059173584, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3583
=== ep: 3584, time 27.5737783908844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3584
=== ep: 3585, time 27.19036626815796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3585
=== ep: 3586, time 27.546875715255737, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3586
=== ep: 3587, time 27.285717964172363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3587
=== ep: 3588, time 27.657543420791626, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3588
goal_identified
=== ep: 3589, time 36.348278522491455, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3589
=== ep: 3590, time 27.65725302696228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3590
=== ep: 3591, time 27.269022941589355, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3591
=== ep: 3592, time 27.63965606689453, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3592
=== ep: 3593, time 27.723962545394897, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3593
goal_identified
=== ep: 3594, time 27.268426179885864, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3594
=== ep: 3595, time 27.636183261871338, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3595
goal_identified
=== ep: 3596, time 27.355265378952026, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3596
=== ep: 3597, time 27.161736965179443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3597
=== ep: 3598, time 27.470695972442627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3598
=== ep: 3599, time 36.331469774246216, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3599
goal_identified
=== ep: 3600, time 27.10840630531311, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3600
=== ep: 3601, time 27.619922876358032, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3601
=== ep: 3602, time 27.458932161331177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3602
=== ep: 3603, time 27.108144283294678, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3603
goal_identified
goal_identified
=== ep: 3604, time 27.35369086265564, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3604
goal_identified
goal_identified
=== ep: 3605, time 27.203341007232666, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3605
goal_identified
=== ep: 3606, time 26.737589359283447, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3606
goal_identified
=== ep: 3607, time 27.068001985549927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3607
goal_identified
=== ep: 3608, time 27.08262538909912, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3608
goal_identified
=== ep: 3609, time 36.148213386535645, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3609
=== ep: 3610, time 27.07696223258972, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3610
goal_identified
goal_identified
=== ep: 3611, time 27.53205943107605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3611
goal_identified
=== ep: 3612, time 27.470463275909424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3612
goal_identified
=== ep: 3613, time 27.457972049713135, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3613
goal_identified
=== ep: 3614, time 27.09406590461731, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3614
goal_identified
goal_identified
=== ep: 3615, time 27.03973937034607, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3615
=== ep: 3616, time 27.05382537841797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3616
goal_identified
=== ep: 3617, time 27.65316605567932, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3617
=== ep: 3618, time 27.398067235946655, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3618
goal_identified
=== ep: 3619, time 36.12224340438843, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3619
=== ep: 3620, time 27.458605527877808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3620
goal_identified
goal_identified
=== ep: 3621, time 27.533262968063354, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3621
goal_identified
goal_identified
=== ep: 3622, time 27.270848035812378, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3622
goal_identified
=== ep: 3623, time 27.353439331054688, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3623
=== ep: 3624, time 27.6806161403656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3624
goal_identified
=== ep: 3625, time 27.267584562301636, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3625
goal_identified
=== ep: 3626, time 27.26956820487976, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3626
goal_identified
=== ep: 3627, time 26.88853669166565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3627
goal_identified
goal_identified
=== ep: 3628, time 26.9074490070343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3628
=== ep: 3629, time 35.52179026603699, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3629
=== ep: 3630, time 27.141483783721924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3630
goal_identified
goal_identified
=== ep: 3631, time 26.920551776885986, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3631
goal_identified
=== ep: 3632, time 27.990227937698364, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3632
goal_identified
=== ep: 3633, time 27.103477716445923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3633
=== ep: 3634, time 33.931073904037476, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3634
goal_identified
goal_identified
=== ep: 3635, time 27.376344203948975, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3635
goal_identified
=== ep: 3636, time 27.48953652381897, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3636
=== ep: 3637, time 27.508686304092407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3637
=== ep: 3638, time 28.2313175201416, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3638
=== ep: 3639, time 36.609771490097046, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3639
goal_identified
goal_identified
=== ep: 3640, time 27.174818992614746, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3640
=== ep: 3641, time 27.275295972824097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3641
goal_identified
=== ep: 3642, time 27.166492700576782, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3642
=== ep: 3643, time 27.304856300354004, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3643
=== ep: 3644, time 26.8518967628479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3644
goal_identified
=== ep: 3645, time 27.33955430984497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3645
goal_identified
=== ep: 3646, time 27.21794557571411, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3646
=== ep: 3647, time 27.428556442260742, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3647
goal_identified
=== ep: 3648, time 27.320728063583374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3648
=== ep: 3649, time 35.98651051521301, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3649
=== ep: 3650, time 27.534525632858276, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3650
=== ep: 3651, time 27.483861684799194, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3651
=== ep: 3652, time 27.41198468208313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3652
=== ep: 3653, time 27.033490896224976, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3653
goal_identified
goal_identified
=== ep: 3654, time 27.480279445648193, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3654
=== ep: 3655, time 27.073416471481323, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3655
goal_identified
=== ep: 3656, time 27.346197366714478, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3656
goal_identified
=== ep: 3657, time 27.27171015739441, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3657
=== ep: 3658, time 27.45909881591797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3658
=== ep: 3659, time 36.11126756668091, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3659
goal_identified
=== ep: 3660, time 27.504809856414795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3660
goal_identified
=== ep: 3661, time 27.37772488594055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3661
=== ep: 3662, time 27.565152645111084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3662
goal_identified
goal_identified
=== ep: 3663, time 27.373299837112427, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3663
=== ep: 3664, time 27.883763313293457, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3664
=== ep: 3665, time 33.662641286849976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3665
=== ep: 3666, time 27.406256675720215, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3666
=== ep: 3667, time 27.400426387786865, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3667
=== ep: 3668, time 27.17034888267517, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3668
=== ep: 3669, time 36.43861508369446, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3669
goal_identified
goal_identified
=== ep: 3670, time 27.387918710708618, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3670
=== ep: 3671, time 27.155960083007812, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3671
=== ep: 3672, time 27.414607763290405, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3672
goal_identified
=== ep: 3673, time 27.667149543762207, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3673
=== ep: 3674, time 27.37986445426941, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3674
goal_identified
=== ep: 3675, time 27.483129501342773, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3675
=== ep: 3676, time 27.010411262512207, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3676
=== ep: 3677, time 27.626973628997803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3677
=== ep: 3678, time 27.393213272094727, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3678
=== ep: 3679, time 36.42108082771301, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3679
goal_identified
=== ep: 3680, time 26.924429416656494, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3680
=== ep: 3681, time 27.504621982574463, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3681
=== ep: 3682, time 27.57846760749817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3682
=== ep: 3683, time 27.67934250831604, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3683
=== ep: 3684, time 27.26827907562256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3684
=== ep: 3685, time 27.22893190383911, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3685
goal_identified
=== ep: 3686, time 27.754727840423584, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3686
goal_identified
=== ep: 3687, time 27.7748863697052, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3687
=== ep: 3688, time 27.34990167617798, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3688
goal_identified
=== ep: 3689, time 36.34123468399048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3689
goal_identified
goal_identified
=== ep: 3690, time 27.44244384765625, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3690
goal_identified
=== ep: 3691, time 27.443037748336792, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3691
goal_identified
=== ep: 3692, time 27.1891987323761, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3692
=== ep: 3693, time 27.329521417617798, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3693
=== ep: 3694, time 27.188498973846436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3694
goal_identified
=== ep: 3695, time 27.37343955039978, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3695
goal_identified
=== ep: 3696, time 27.41072368621826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3696
=== ep: 3697, time 27.067155599594116, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3697
goal_identified
=== ep: 3698, time 27.5949285030365, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3698
=== ep: 3699, time 36.29305815696716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3699
=== ep: 3700, time 27.32512927055359, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3700
=== ep: 3701, time 26.92273712158203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3701
goal_identified
=== ep: 3702, time 27.199596405029297, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3702
goal_identified
goal_identified
=== ep: 3703, time 27.61683201789856, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3703
=== ep: 3704, time 26.92995047569275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3704
goal_identified
=== ep: 3705, time 27.168514490127563, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3705
goal_identified
goal_identified
goal_identified
=== ep: 3706, time 27.314271688461304, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3706
=== ep: 3707, time 27.153772354125977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3707
goal_identified
=== ep: 3708, time 27.52482032775879, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3708
goal_identified
=== ep: 3709, time 36.10090231895447, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3709
=== ep: 3710, time 27.651200532913208, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3710
goal_identified
=== ep: 3711, time 27.166802883148193, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3711
=== ep: 3712, time 27.368694067001343, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3712
goal_identified
=== ep: 3713, time 27.472909927368164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3713
goal_identified
=== ep: 3714, time 27.208815574645996, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3714
goal_identified
goal_identified
=== ep: 3715, time 27.326861143112183, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3715
=== ep: 3716, time 27.54388928413391, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3716
=== ep: 3717, time 27.84981060028076, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3717
=== ep: 3718, time 27.34877324104309, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3718
=== ep: 3719, time 36.25092530250549, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3719
goal_identified
=== ep: 3720, time 27.48056387901306, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3720
goal_identified
=== ep: 3721, time 27.129448652267456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3721
=== ep: 3722, time 27.57276940345764, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3722
=== ep: 3723, time 26.95019841194153, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3723
goal_identified
=== ep: 3724, time 27.008089303970337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3724
goal_identified
=== ep: 3725, time 27.309918642044067, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3725
goal_identified
=== ep: 3726, time 27.489489316940308, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3726
=== ep: 3727, time 26.844359874725342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3727
goal_identified
=== ep: 3728, time 26.959259510040283, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3728
=== ep: 3729, time 36.25302195549011, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3729
goal_identified
=== ep: 3730, time 27.253117561340332, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3730
=== ep: 3731, time 27.248663187026978, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3731
goal_identified
=== ep: 3732, time 27.399667024612427, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3732
goal_identified
goal_identified
=== ep: 3733, time 27.595908164978027, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3733
goal_identified
=== ep: 3734, time 27.324074745178223, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3734
=== ep: 3735, time 27.240485191345215, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3735
goal_identified
=== ep: 3736, time 27.247199296951294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3736
=== ep: 3737, time 27.364834547042847, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3737
goal_identified
=== ep: 3738, time 27.41191291809082, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3738
=== ep: 3739, time 36.033857107162476, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3739
=== ep: 3740, time 27.51521921157837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3740
=== ep: 3741, time 27.909709215164185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3741
goal_identified
=== ep: 3742, time 27.54169201850891, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3742
=== ep: 3743, time 27.30842661857605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3743
=== ep: 3744, time 27.18503427505493, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3744
=== ep: 3745, time 26.841274976730347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3745
=== ep: 3746, time 27.514137744903564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3746
=== ep: 3747, time 27.3682382106781, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 27.22160816192627, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3748
=== ep: 3749, time 36.46527719497681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3749
goal_identified
goal_identified
=== ep: 3750, time 27.26054358482361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 27.45025086402893, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3751
goal_identified
=== ep: 3752, time 27.467658758163452, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3752
=== ep: 3753, time 27.443291902542114, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3753
goal_identified
=== ep: 3754, time 27.002150774002075, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3754
=== ep: 3755, time 27.490941286087036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3755
=== ep: 3756, time 27.547231674194336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3756
goal_identified
=== ep: 3757, time 27.23007869720459, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3757
goal_identified
=== ep: 3758, time 26.982174396514893, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3758
goal_identified
=== ep: 3759, time 37.686856269836426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3759
goal_identified
=== ep: 3760, time 27.292239665985107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3760
=== ep: 3761, time 27.75220799446106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3761
goal_identified
=== ep: 3762, time 26.977569580078125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3762
goal_identified
=== ep: 3763, time 26.932748556137085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3763
=== ep: 3764, time 26.634334087371826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3764
=== ep: 3765, time 27.097887992858887, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3765
goal_identified
goal_identified
=== ep: 3766, time 27.39599585533142, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3766
goal_identified
=== ep: 3767, time 27.28234362602234, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3767
=== ep: 3768, time 27.256075859069824, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3768
=== ep: 3769, time 35.95852303504944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3769
goal_identified
goal_identified
=== ep: 3770, time 27.33326745033264, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3770
goal_identified
=== ep: 3771, time 27.22860026359558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3771
goal_identified
goal_identified
=== ep: 3772, time 26.690062761306763, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3772
=== ep: 3773, time 27.055526971817017, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3773
goal_identified
=== ep: 3774, time 26.929471015930176, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3774
goal_identified
=== ep: 3775, time 27.36820650100708, eps 0.001, sum reward: 1, score_diff -5, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3775
=== ep: 3776, time 27.39430284500122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3776
goal_identified
=== ep: 3777, time 27.32545495033264, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3777
goal_identified
=== ep: 3778, time 27.397956609725952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3778
goal_identified
goal_identified
=== ep: 3779, time 35.68320083618164, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 27.13231873512268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3780
=== ep: 3781, time 27.412437200546265, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3781
=== ep: 3782, time 27.071537256240845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3782
=== ep: 3783, time 27.129127025604248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3783
goal_identified
=== ep: 3784, time 26.982952117919922, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3784
=== ep: 3785, time 26.847402572631836, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3785
goal_identified
=== ep: 3786, time 27.05759334564209, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3786
goal_identified
=== ep: 3787, time 26.848849773406982, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3787
goal_identified
goal_identified
=== ep: 3788, time 26.803279638290405, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3788
goal_identified
=== ep: 3789, time 35.55620098114014, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3789
goal_identified
=== ep: 3790, time 26.855634927749634, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3790
=== ep: 3791, time 27.10050129890442, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3791
goal_identified
goal_identified
=== ep: 3792, time 27.24468469619751, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3792
=== ep: 3793, time 27.71490240097046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3793
goal_identified
goal_identified
goal_identified
=== ep: 3794, time 27.413235187530518, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3794
=== ep: 3795, time 27.50237202644348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3795
=== ep: 3796, time 27.25141978263855, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3796
goal_identified
goal_identified
=== ep: 3797, time 27.334417581558228, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3797
goal_identified
goal_identified
=== ep: 3798, time 27.145835876464844, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3798
=== ep: 3799, time 36.02539849281311, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3799
goal_identified
goal_identified
=== ep: 3800, time 27.20588755607605, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3800
goal_identified
=== ep: 3801, time 27.45352077484131, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3801
=== ep: 3802, time 27.46314525604248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3802
=== ep: 3803, time 27.670872449874878, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3803
goal_identified
=== ep: 3804, time 26.963157415390015, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3804
=== ep: 3805, time 27.308055639266968, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3805
goal_identified
=== ep: 3806, time 27.354917287826538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3806
goal_identified
=== ep: 3807, time 27.415370225906372, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3807
goal_identified
goal_identified
=== ep: 3808, time 27.19325065612793, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3808
=== ep: 3809, time 35.759238481521606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3809
goal_identified
=== ep: 3810, time 27.071285247802734, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3810
goal_identified
=== ep: 3811, time 27.129831314086914, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3811
goal_identified
=== ep: 3812, time 27.156537294387817, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3812
=== ep: 3813, time 27.351654529571533, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3813
=== ep: 3814, time 27.097289323806763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3814
goal_identified
=== ep: 3815, time 27.403175592422485, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3815
=== ep: 3816, time 27.02142071723938, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3816
goal_identified
=== ep: 3817, time 27.049487113952637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3817
=== ep: 3818, time 27.216458797454834, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3818
=== ep: 3819, time 35.78868532180786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3819
=== ep: 3820, time 27.21057438850403, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3820
goal_identified
=== ep: 3821, time 27.243535041809082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3821
=== ep: 3822, time 27.3754723072052, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3822
goal_identified
=== ep: 3823, time 27.190390825271606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3823
=== ep: 3824, time 26.948903560638428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3824
goal_identified
=== ep: 3825, time 27.121050119400024, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3825
goal_identified
=== ep: 3826, time 27.57249641418457, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3826
=== ep: 3827, time 26.905617475509644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3827
=== ep: 3828, time 27.208999156951904, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3828
=== ep: 3829, time 35.61652708053589, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3829
goal_identified
=== ep: 3830, time 26.966411590576172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3830
=== ep: 3831, time 27.21265482902527, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3831
goal_identified
=== ep: 3832, time 27.084934949874878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3832
goal_identified
=== ep: 3833, time 27.114972591400146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3833
goal_identified
=== ep: 3834, time 27.285497665405273, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3834
goal_identified
=== ep: 3835, time 27.21665358543396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3835
goal_identified
=== ep: 3836, time 27.063304901123047, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3836
goal_identified
=== ep: 3837, time 27.425861358642578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3837
goal_identified
goal_identified
=== ep: 3838, time 27.502964735031128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3838
=== ep: 3839, time 36.21337604522705, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3839
goal_identified
=== ep: 3840, time 26.824673891067505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3840
=== ep: 3841, time 27.031595468521118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 26.62103509902954, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2082
=== ep: 3843, time 27.098583698272705, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3843
=== ep: 3844, time 27.193359851837158, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3844
=== ep: 3845, time 26.896242380142212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3845
=== ep: 3846, time 27.220961809158325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3846
=== ep: 3847, time 27.10680317878723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3847
=== ep: 3848, time 26.9794442653656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3848
goal_identified
goal_identified
goal_identified
=== ep: 3849, time 36.17119240760803, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3849
=== ep: 3850, time 27.087965488433838, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3850
=== ep: 3851, time 27.403883695602417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3851
=== ep: 3852, time 26.916622400283813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3852
goal_identified
=== ep: 3853, time 27.640129566192627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3853
=== ep: 3854, time 26.978726148605347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3854
=== ep: 3855, time 27.19527268409729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3855
=== ep: 3856, time 27.12881112098694, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3856
goal_identified
=== ep: 3857, time 27.20911169052124, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3857
=== ep: 3858, time 27.277401447296143, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3858
=== ep: 3859, time 36.10350465774536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3859
=== ep: 3860, time 27.352030277252197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3860
goal_identified
=== ep: 3861, time 27.32728886604309, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3861
goal_identified
=== ep: 3862, time 27.011963605880737, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3862
goal_identified
=== ep: 3863, time 26.599027395248413, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3863
goal_identified
=== ep: 3864, time 27.464436292648315, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3864
=== ep: 3865, time 27.46001935005188, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3865
=== ep: 3866, time 27.6658353805542, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3866
=== ep: 3867, time 27.47694683074951, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3867
goal_identified
goal_identified
=== ep: 3868, time 26.941985607147217, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3868
goal_identified
=== ep: 3869, time 35.958274364471436, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3869
=== ep: 3870, time 27.78994631767273, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3870
=== ep: 3871, time 27.2897367477417, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3871
goal_identified
=== ep: 3872, time 27.26859164237976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3872
=== ep: 3873, time 28.107292890548706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3873
=== ep: 3874, time 27.319421768188477, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3874
goal_identified
=== ep: 3875, time 27.04444909095764, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3875
=== ep: 3876, time 26.769518613815308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3876
goal_identified
=== ep: 3877, time 27.863945484161377, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3877
=== ep: 3878, time 27.331406116485596, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3878
goal_identified
=== ep: 3879, time 45.475581645965576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3879
goal_identified
=== ep: 3880, time 27.20587658882141, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3880
=== ep: 3881, time 26.913249015808105, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3881
goal_identified
goal_identified
=== ep: 3882, time 27.318156480789185, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3882
=== ep: 3883, time 27.550773859024048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3883
goal_identified
=== ep: 3884, time 27.090153455734253, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3884
=== ep: 3885, time 27.49625825881958, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3885
goal_identified
=== ep: 3886, time 27.121305465698242, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3886
=== ep: 3887, time 27.17461347579956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3887
goal_identified
=== ep: 3888, time 27.014520168304443, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3888
goal_identified
goal_identified
=== ep: 3889, time 37.834818840026855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3889
=== ep: 3890, time 27.43935775756836, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3890
=== ep: 3891, time 27.56096339225769, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3891
goal_identified
=== ep: 3892, time 27.20193099975586, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3892
=== ep: 3893, time 27.73124670982361, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3893
=== ep: 3894, time 27.2261323928833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3894
=== ep: 3895, time 26.923848628997803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3895
=== ep: 3896, time 27.410178661346436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3896
goal_identified
=== ep: 3897, time 27.100666522979736, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3897
goal_identified
=== ep: 3898, time 27.37026071548462, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3898
=== ep: 3899, time 43.60274124145508, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3899
=== ep: 3900, time 27.529946327209473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3900
=== ep: 3901, time 27.222389221191406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3901
goal_identified
=== ep: 3902, time 27.046301126480103, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3902
goal_identified
goal_identified
=== ep: 3903, time 27.050426959991455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3903
goal_identified
=== ep: 3904, time 26.983576774597168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3904
=== ep: 3905, time 27.265662908554077, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3905
=== ep: 3906, time 26.678754568099976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3906
goal_identified
=== ep: 3907, time 27.36091160774231, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3907
goal_identified
=== ep: 3908, time 27.412662029266357, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3908
goal_identified
=== ep: 3909, time 36.04861760139465, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3909
goal_identified
=== ep: 3910, time 27.285448789596558, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3910
goal_identified
=== ep: 3911, time 26.7264187335968, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3911
goal_identified
=== ep: 3912, time 26.627259016036987, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3912
goal_identified
goal_identified
=== ep: 3913, time 27.2315616607666, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3913
=== ep: 3914, time 27.128204107284546, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3914
=== ep: 3915, time 27.049947261810303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3915
=== ep: 3916, time 27.085402965545654, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3916
=== ep: 3917, time 26.899882555007935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3917
=== ep: 3918, time 26.655447959899902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3918
goal_identified
=== ep: 3919, time 35.62629294395447, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3919
=== ep: 3920, time 27.295913457870483, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3920
goal_identified
goal_identified
=== ep: 3921, time 26.643396615982056, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3921
=== ep: 3922, time 26.95202350616455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3922
goal_identified
=== ep: 3923, time 27.132378339767456, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3923
goal_identified
=== ep: 3924, time 27.125409603118896, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3924
=== ep: 3925, time 26.988001108169556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3925
goal_identified
=== ep: 3926, time 26.677440643310547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3926
goal_identified
=== ep: 3927, time 26.958727598190308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3927
goal_identified
=== ep: 3928, time 27.021717309951782, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3928
goal_identified
=== ep: 3929, time 35.52435922622681, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3929
goal_identified
=== ep: 3930, time 26.96196460723877, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3930
=== ep: 3931, time 26.976993560791016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3931
goal_identified
=== ep: 3932, time 26.901305675506592, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3932
goal_identified
=== ep: 3933, time 26.771631956100464, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3933
=== ep: 3934, time 27.123235940933228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3934
goal_identified
=== ep: 3935, time 27.708048343658447, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3935
=== ep: 3936, time 27.139319896697998, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3936
goal_identified
goal_identified
=== ep: 3937, time 27.23175573348999, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3937
=== ep: 3938, time 27.335046768188477, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3938
goal_identified
goal_identified
=== ep: 3939, time 35.10511875152588, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3939
goal_identified
=== ep: 3940, time 26.729670763015747, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3940
goal_identified
goal_identified
=== ep: 3941, time 26.887747049331665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3941
goal_identified
=== ep: 3942, time 26.936518669128418, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3942
goal_identified
=== ep: 3943, time 26.787859678268433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3943
=== ep: 3944, time 27.345338582992554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3944
goal_identified
=== ep: 3945, time 27.478222846984863, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3945
=== ep: 3946, time 26.961161851882935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3946
=== ep: 3947, time 27.12056303024292, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3947
goal_identified
=== ep: 3948, time 27.110188007354736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3948
goal_identified
=== ep: 3949, time 36.120676040649414, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3949
=== ep: 3950, time 27.013782262802124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3950
goal_identified
=== ep: 3951, time 26.931739807128906, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3951
=== ep: 3952, time 27.009947776794434, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3952
goal_identified
=== ep: 3953, time 26.79224920272827, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3953
goal_identified
=== ep: 3954, time 27.02455973625183, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3954
=== ep: 3955, time 27.638137102127075, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3955
=== ep: 3956, time 26.814647674560547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3956
=== ep: 3957, time 27.317041873931885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3957
goal_identified
=== ep: 3958, time 27.212021827697754, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3958
=== ep: 3959, time 35.962987184524536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3959
=== ep: 3960, time 27.55593228340149, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3960
goal_identified
goal_identified
=== ep: 3961, time 26.645323514938354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3961
=== ep: 3962, time 27.020448207855225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3962
goal_identified
goal_identified
goal_identified
=== ep: 3963, time 27.206620931625366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2127
goal_identified
=== ep: 3964, time 27.37710928916931, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3964
goal_identified
=== ep: 3965, time 26.9022319316864, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3965
goal_identified
goal_identified
=== ep: 3966, time 27.365309715270996, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3966
goal_identified
=== ep: 3967, time 27.097513437271118, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3967
=== ep: 3968, time 27.052095890045166, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3968
goal_identified
goal_identified
goal_identified
=== ep: 3969, time 35.9226930141449, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3969
=== ep: 3970, time 27.22815704345703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3970
goal_identified
goal_identified
=== ep: 3971, time 26.770103931427002, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3971
=== ep: 3972, time 27.53022789955139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3972
goal_identified
=== ep: 3973, time 27.465189218521118, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3973
goal_identified
=== ep: 3974, time 27.244960069656372, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3974
=== ep: 3975, time 26.979702472686768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3975
=== ep: 3976, time 27.520193815231323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3976
=== ep: 3977, time 27.234357357025146, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3977
=== ep: 3978, time 27.063873767852783, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3978
=== ep: 3979, time 35.26852011680603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3979
goal_identified
=== ep: 3980, time 27.458666801452637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3980
goal_identified
=== ep: 3981, time 27.0635883808136, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3981
=== ep: 3982, time 27.376190423965454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3982
=== ep: 3983, time 26.88996386528015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3983
=== ep: 3984, time 26.913703680038452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3984
goal_identified
=== ep: 3985, time 26.609885931015015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3985
goal_identified
=== ep: 3986, time 26.838006734848022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3986
goal_identified
=== ep: 3987, time 27.215265035629272, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3987
=== ep: 3988, time 27.700502395629883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3988
goal_identified
=== ep: 3989, time 35.667065143585205, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3989
goal_identified
=== ep: 3990, time 27.25532054901123, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3990
goal_identified
goal_identified
=== ep: 3991, time 27.289958715438843, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3991
=== ep: 3992, time 27.486154317855835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 27.035030841827393, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3993
goal_identified
=== ep: 3994, time 27.4916353225708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3994
=== ep: 3995, time 27.641456365585327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3995
goal_identified
=== ep: 3996, time 26.96230173110962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3996
goal_identified
goal_identified
goal_identified
=== ep: 3997, time 26.872864961624146, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2622
=== ep: 3998, time 27.710055828094482, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3998
=== ep: 3999, time 34.6618447303772, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
