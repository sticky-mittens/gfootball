==> Playing in 11_vs_11_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 6 layers and 500 hidden units.
goal_identified
=== ep: 0, time 25.93751859664917, eps 0.9, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
=== ep: 1, time 25.177648067474365, eps 0.8561552526261419, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
=== ep: 2, time 25.53821086883545, eps 0.8144488388143276, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
=== ep: 3, time 24.968920469284058, eps 0.774776470806127, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
goal_identified
=== ep: 4, time 25.25948429107666, eps 0.7370389470171057, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
goal_identified
=== ep: 5, time 25.558870553970337, eps 0.701141903981193, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
=== ep: 6, time 25.861781358718872, eps 0.6669955803928644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
goal_identified
goal_identified
=== ep: 7, time 25.96295189857483, eps 0.6345145926571234, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
goal_identified
=== ep: 8, time 25.618740797042847, eps 0.6036177213860398, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
goal_identified
goal_identified
goal_identified
=== ep: 9, time 27.877105712890625, eps 0.5742277083079742, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
=== ep: 10, time 26.133041620254517, eps 0.5462710630816575, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 11, time 25.76243019104004, eps 0.5196778795320575, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
goal_identified
=== ep: 12, time 28.443806648254395, eps 0.49438166084852986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 13, time 26.078447818756104, eps 0.47031915330815344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
goal_identified
=== ep: 14, time 28.059933185577393, eps 0.4474301881084772, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
=== ep: 15, time 30.623555898666382, eps 0.42565753091417224, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
=== ep: 16, time 26.135520696640015, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
=== ep: 17, time 25.77345633506775, eps 0.3852460238219053, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 18, time 26.104328870773315, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
goal_identified
goal_identified
=== ep: 19, time 29.629194498062134, eps 0.3486801800855966, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
=== ep: 20, time 26.262763023376465, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 21, time 25.77392268180847, eps 0.31559403645092865, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
goal_identified
=== ep: 22, time 25.564232110977173, eps 0.3002511042445735, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
=== ep: 23, time 25.799673318862915, eps 0.2856564556717689, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
goal_identified
=== ep: 24, time 26.024248600006104, eps 0.27177359650906974, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
=== ep: 25, time 25.793955087661743, eps 0.2585678123773109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
=== ep: 26, time 26.13237977027893, eps 0.24600608193757734, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
goal_identified
goal_identified
=== ep: 27, time 25.809163808822632, eps 0.23405699432065646, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
=== ep: 28, time 25.84894824028015, eps 0.22269067058350425, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
=== ep: 29, time 31.954688787460327, eps 0.2118786889963241, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
goal_identified
=== ep: 30, time 25.714698553085327, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
goal_identified
=== ep: 31, time 25.65519618988037, eps 0.191810928470242, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
=== ep: 32, time 25.780579566955566, eps 0.1825049696771952, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
goal_identified
=== ep: 33, time 25.635666370391846, eps 0.17365286785005798, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
=== ep: 34, time 26.116541862487793, eps 0.16523248812340846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
=== ep: 35, time 25.697546243667603, eps 0.15722277516195018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
goal_identified
goal_identified
=== ep: 36, time 25.899971961975098, eps 0.1496037005112063, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 37, time 25.734259843826294, eps 0.14235621251595124, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
=== ep: 38, time 25.849388122558594, eps 0.13546218868114893, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
=== ep: 39, time 28.681437730789185, eps 0.1289043903562757, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
=== ep: 40, time 26.209319829940796, eps 0.12266641962971482, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 41, time 25.987611293792725, eps 0.116732678325436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
goal_identified
=== ep: 42, time 25.47833490371704, eps 0.11108832899943073, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
=== ep: 43, time 25.884691953659058, eps 0.10571925783837377, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
=== ep: 44, time 30.262938737869263, eps 0.10061203936773815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
=== ep: 45, time 25.531294345855713, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
=== ep: 46, time 26.111796855926514, eps 0.09113270050680057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 47, time 25.368953466415405, eps 0.08673687683177911, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
goal_identified
=== ep: 48, time 26.0378201007843, eps 0.08255544000718185, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
goal_identified
=== ep: 49, time 28.872178554534912, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
goal_identified
=== ep: 50, time 26.010430097579956, eps 0.07479441376288502, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
goal_identified
=== ep: 51, time 25.875396251678467, eps 0.0711954177350367, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
goal_identified
=== ep: 52, time 26.145352840423584, eps 0.06777194681468615, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 53, time 30.08451509475708, eps 0.06451544054132621, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 54, time 23.30666971206665, eps 0.06141775595303503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 26.189652681350708, eps 0.05847114722483011, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 56, time 25.749654293060303, eps 0.05566824630007096, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 26.280404329299927, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 26.549318552017212, eps 0.050465874830710106, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
goal_identified
=== ep: 59, time 28.642534494400024, eps 0.04805339564764071, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
goal_identified
=== ep: 60, time 25.6738440990448, eps 0.045758574462709686, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
goal_identified
goal_identified
=== ep: 61, time 26.109169483184814, eps 0.043575673027635695, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
=== ep: 62, time 26.449269771575928, eps 0.04149923295180846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
goal_identified
=== ep: 63, time 26.46237850189209, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 64, time 26.127941846847534, eps 0.03764522137655123, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
=== ep: 65, time 26.032119035720825, eps 0.03585801284071809, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
goal_identified
=== ep: 66, time 26.316269159317017, eps 0.034157967493714775, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
goal_identified
goal_identified
=== ep: 67, time 26.03328537940979, eps 0.03254083433665968, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
=== ep: 68, time 26.181048154830933, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
goal_identified
=== ep: 69, time 28.752872943878174, eps 0.02953932710388308, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
=== ep: 70, time 26.028345584869385, eps 0.028147447696664333, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 25.90994119644165, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
goal_identified
=== ep: 72, time 26.30912971496582, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
goal_identified
=== ep: 73, time 26.209778308868408, eps 0.02436602477210106, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
=== ep: 74, time 26.451252460479736, eps 0.02322645029683511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 75, time 26.010376691818237, eps 0.02214245352455219, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 26.40121340751648, eps 0.02111132389869288, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 28.50390648841858, eps 0.020130483058101077, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 26.141807556152344, eps 0.019197478389778148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
goal_identified
=== ep: 79, time 29.569878101348877, eps 0.018309976896072843, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 80, time 26.184953451156616, eps 0.017465759360972027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
goal_identified
=== ep: 81, time 26.451159238815308, eps 0.01666271480090467, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
goal_identified
goal_identified
=== ep: 82, time 26.152596712112427, eps 0.015898835186183367, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
goal_identified
=== ep: 83, time 26.068551540374756, eps 0.015172210419884185, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
goal_identified
=== ep: 84, time 26.16704797744751, eps 0.014481023561609456, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
=== ep: 85, time 25.982162714004517, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
goal_identified
goal_identified
=== ep: 86, time 25.8216233253479, eps 0.013198134551968641, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
goal_identified
=== ep: 87, time 26.880125045776367, eps 0.012603224509851407, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
=== ep: 88, time 26.321921825408936, eps 0.012037328572858524, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
=== ep: 89, time 29.46909809112549, eps 0.011499031706385502, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 90, time 25.98718237876892, eps 0.010986987887879832, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 91, time 26.515864610671997, eps 0.010499916741083536, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
=== ep: 92, time 26.735811471939087, eps 0.010036600334425595, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 26.204009771347046, eps 0.00959588013555861, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
goal_identified
=== ep: 94, time 26.518807888031006, eps 0.009176654114424539, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
=== ep: 95, time 26.277361631393433, eps 0.00877787398760545, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
goal_identified
=== ep: 96, time 25.92067837715149, eps 0.008398542597069007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
goal_identified
=== ep: 97, time 26.11153793334961, eps 0.008037711416753971, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
=== ep: 98, time 26.222781658172607, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
goal_identified
goal_identified
=== ep: 99, time 29.643027782440186, eps 0.007367984627217855, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
goal_identified
=== ep: 100, time 26.48813796043396, eps 0.007057414352177835, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
goal_identified
=== ep: 101, time 26.20651149749756, eps 0.006761990768184489, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
=== ep: 102, time 26.538570642471313, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
goal_identified
goal_identified
=== ep: 103, time 26.006629943847656, eps 0.006213664849431085, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
=== ep: 104, time 31.27554750442505, eps 0.005959391414263934, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
=== ep: 105, time 27.00007152557373, eps 0.005717519040864065, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
goal_identified
=== ep: 106, time 26.936991214752197, eps 0.005487442922312285, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
goal_identified
=== ep: 107, time 26.173418760299683, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
=== ep: 108, time 26.301490306854248, eps 0.005060406267408787, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
=== ep: 109, time 29.499919652938843, eps 0.004862377916986354, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 110, time 27.00524640083313, eps 0.004674007523179196, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 26.796412706375122, eps 0.004494824061885041, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
goal_identified
=== ep: 112, time 26.339280366897583, eps 0.0043243794811181555, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
goal_identified
=== ep: 113, time 26.005794763565063, eps 0.0041622475806460035, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 26.038914680480957, eps 0.0040080229462666735, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 26.574511289596558, eps 0.0038613199360621906, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
=== ep: 116, time 26.634947776794434, eps 0.003721771716092858, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 26.456803560256958, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
goal_identified
=== ep: 118, time 26.260746717453003, eps 0.0034627608920727634, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
=== ep: 119, time 29.840222597122192, eps 0.00334265062604924, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 26.397189140319824, eps 0.0032283982068230565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 27.59499216079712, eps 0.0031197179438347193, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 26.257992029190063, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
goal_identified
=== ep: 123, time 28.46788763999939, eps 0.0029180001112638996, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 26.308290481567383, eps 0.002824458142029865, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 26.30782151222229, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
=== ep: 126, time 26.40691065788269, eps 0.0026508379945489875, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
=== ep: 127, time 26.280574321746826, eps 0.0025703256754987464, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
goal_identified
=== ep: 128, time 26.201196908950806, eps 0.0024937399885833667, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
=== ep: 129, time 29.621699810028076, eps 0.0024208894296938593, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
goal_identified
goal_identified
goal_identified
=== ep: 130, time 26.65287733078003, eps 0.0023515918344868374, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
goal_identified
=== ep: 131, time 26.53261137008667, eps 0.002285673922878779, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
goal_identified
=== ep: 132, time 26.301473379135132, eps 0.0022229708657555565, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 26.95867919921875, eps 0.0021633258728137976, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
goal_identified
=== ep: 134, time 25.960089206695557, eps 0.0021065898005034594, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 29.15796709060669, eps 0.002052620779091266, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 26.763418436050415, eps 0.0020012838579124784, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
=== ep: 137, time 26.17923331260681, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
goal_identified
=== ep: 138, time 26.11431622505188, eps 0.001905999100714611, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 29.665303230285645, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 26.546267986297607, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
goal_identified
goal_identified
goal_identified
=== ep: 141, time 26.39195728302002, eps 0.0017798006526189953, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 142, time 29.789475202560425, eps 0.0017417693260160481, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 26.355204820632935, eps 0.0017055928090985275, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 26.37633514404297, eps 0.0016711806417306348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
goal_identified
goal_identified
goal_identified
=== ep: 145, time 26.443486213684082, eps 0.0016384467755694515, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
goal_identified
=== ep: 146, time 26.479859590530396, eps 0.0016073093588992661, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 26.709295511245728, eps 0.0015776905319596466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
=== ep: 148, time 26.431995391845703, eps 0.0015495162322554856, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 30.387553691864014, eps 0.0015227160093621863, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
=== ep: 150, time 26.651283740997314, eps 0.0014972228487629025, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
goal_identified
goal_identified
=== ep: 151, time 26.698851585388184, eps 0.0014729730042773413, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 26.541324138641357, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 26.62708067893982, eps 0.00142796367199102, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
=== ep: 154, time 26.56708812713623, eps 0.0014070916374152305, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
goal_identified
goal_identified
=== ep: 155, time 26.439099311828613, eps 0.001387237543977543, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 26.671531915664673, eps 0.0013683517461028282, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 26.505884408950806, eps 0.0013503870194592265, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
=== ep: 158, time 26.630239248275757, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
=== ep: 159, time 29.012968063354492, eps 0.001317043286000802, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
goal_identified
=== ep: 160, time 26.595934867858887, eps 0.0013015809024843582, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 29.060077667236328, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 27.169612169265747, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 163, time 26.901945114135742, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
goal_identified
=== ep: 164, time 26.484598875045776, eps 0.001246913559404956, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
=== ep: 165, time 26.535029411315918, eps 0.0012348714430141991, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
=== ep: 166, time 26.423678159713745, eps 0.0012234166275700486, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
=== ep: 167, time 26.837012767791748, eps 0.001212520470067348, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
=== ep: 168, time 26.815523386001587, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
=== ep: 169, time 29.67676043510437, eps 0.0011922964734155277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
goal_identified
=== ep: 170, time 26.761377096176147, eps 0.001182918063740569, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
goal_identified
goal_identified
=== ep: 171, time 26.723036766052246, eps 0.0011739970445027263, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
goal_identified
goal_identified
=== ep: 172, time 26.800683975219727, eps 0.0011655111085071537, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
goal_identified
=== ep: 173, time 26.124860286712646, eps 0.001157439036493735, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
goal_identified
=== ep: 174, time 26.560994625091553, eps 0.0011497606440778825, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
goal_identified
=== ep: 175, time 26.9840247631073, eps 0.0011424567312790603, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
=== ep: 176, time 26.844351291656494, eps 0.0011355090345108335, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 26.944446086883545, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
goal_identified
=== ep: 178, time 26.91474223136902, eps 0.0011226136449073282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
=== ep: 179, time 29.73654842376709, eps 0.001116633706881133, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 26.86055588722229, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 26.65095591545105, eps 0.001105534542190287, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
=== ep: 182, time 26.708322048187256, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
goal_identified
=== ep: 183, time 27.08301305770874, eps 0.0010954916026690664, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
goal_identified
=== ep: 184, time 26.36326551437378, eps 0.001090834422251547, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
goal_identified
=== ep: 185, time 26.939875602722168, eps 0.0010864043752031938, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
goal_identified
=== ep: 186, time 26.924472093582153, eps 0.0010821903840988777, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 187, time 27.31491184234619, eps 0.0010781819117658682, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
=== ep: 188, time 26.779299020767212, eps 0.0010743689349354123, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 29.38595962524414, eps 0.0010707419191793434, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
goal_identified
=== ep: 190, time 27.00423550605774, eps 0.0010672917950690429, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 191, time 26.947970390319824, eps 0.0010640099354971456, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
=== ep: 192, time 27.235999822616577, eps 0.0010608881341052777, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
=== ep: 193, time 27.235549688339233, eps 0.0010579185847638855, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
goal_identified
goal_identified
=== ep: 194, time 26.910253763198853, eps 0.0010550938620528466, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
goal_identified
=== ep: 195, time 26.39611792564392, eps 0.001052406902694051, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 26.695749282836914, eps 0.001049850987889527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 26.51508855819702, eps 0.0010474197265209469, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
goal_identified
=== ep: 198, time 26.732444286346436, eps 0.0010451070391685015, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
=== ep: 199, time 31.524909257888794, eps 0.001042907142909185, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
goal_identified
=== ep: 200, time 27.363389492034912, eps 0.001040814536856474, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
goal_identified
goal_identified
=== ep: 201, time 26.75004553794861, eps 0.0010388239884052469, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 26.758702993392944, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
=== ep: 203, time 26.591002225875854, eps 0.0010351293974264616, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
=== ep: 204, time 26.987948656082153, eps 0.00103341611649703, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
=== ep: 205, time 26.915884256362915, eps 0.0010317863932645186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 26.956302642822266, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
goal_identified
goal_identified
=== ep: 207, time 26.542569875717163, eps 0.0010287615180101426, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 208, time 27.26076602935791, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
goal_identified
=== ep: 209, time 29.82679796218872, eps 0.0010260244976950921, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 210, time 27.213891744613647, eps 0.0010247552679654227, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
goal_identified
goal_identified
=== ep: 211, time 27.265581846237183, eps 0.00102354793930011, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 212, time 27.114908695220947, eps 0.0010223994927486214, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
=== ep: 213, time 26.79992365837097, eps 0.001021307056596379, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 214, time 26.847959280014038, eps 0.0010202678991839778, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 215, time 27.05492377281189, eps 0.0010192794220766138, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
=== ep: 216, time 32.867732763290405, eps 0.0010183391535666436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
goal_identified
=== ep: 217, time 26.402815341949463, eps 0.0010174447424930286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 218, time 26.99288845062256, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 219, time 29.902507066726685, eps 0.0010157846557556941, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 220, time 27.050525903701782, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 26.37857151031494, eps 0.0010142825471585687, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
goal_identified
=== ep: 222, time 26.439396142959595, eps 0.0010135859791140496, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 223, time 26.759984016418457, eps 0.0010129233830939361, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
goal_identified
=== ep: 224, time 26.88783860206604, eps 0.0010122931022630473, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
goal_identified
=== ep: 225, time 26.702040433883667, eps 0.001011693560591007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
goal_identified
=== ep: 226, time 26.995836973190308, eps 0.0010111232589113477, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
goal_identified
goal_identified
=== ep: 227, time 26.78545570373535, eps 0.0010105807711728136, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
goal_identified
goal_identified
=== ep: 228, time 26.573179483413696, eps 0.0010100647408734893, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 29.858104944229126, eps 0.001009573877668838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 26.683544635772705, eps 0.001009106954145169, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 26.827375650405884, eps 0.0010086628027504636, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
goal_identified
=== ep: 232, time 26.4569890499115, eps 0.0010082403128748867, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
goal_identified
goal_identified
=== ep: 233, time 26.369723081588745, eps 0.0010078384280736842, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
goal_identified
=== ep: 234, time 26.74306607246399, eps 0.001007456143425521, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
=== ep: 235, time 29.596062898635864, eps 0.001007092503019653, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
goal_identified
=== ep: 236, time 30.690772771835327, eps 0.001006746597565654, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
goal_identified
=== ep: 237, time 26.995456218719482, eps 0.001006417562119715, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
goal_identified
goal_identified
=== ep: 238, time 26.85747265815735, eps 0.0010061045739218342, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
=== ep: 239, time 31.076486110687256, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
=== ep: 240, time 26.739744424819946, eps 0.001005523646905642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
=== ep: 241, time 26.80164098739624, eps 0.001005254255467199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 26.988574028015137, eps 0.0010049980024042435, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 26.856706619262695, eps 0.0010047542469506416, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
goal_identified
=== ep: 244, time 26.63987970352173, eps 0.0010045223795907931, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 245, time 26.866924047470093, eps 0.001004301820535524, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
=== ep: 246, time 26.86730170249939, eps 0.0010040920182723119, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
=== ep: 247, time 32.76830291748047, eps 0.0010038924481862177, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
goal_identified
=== ep: 248, time 27.042067050933838, eps 0.0010037026112480747, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
=== ep: 249, time 30.426589965820312, eps 0.0010035220327666559, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
goal_identified
goal_identified
=== ep: 250, time 26.982030868530273, eps 0.0010033502612016988, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
=== ep: 251, time 26.913503408432007, eps 0.001003186867034819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
goal_identified
=== ep: 252, time 27.134714126586914, eps 0.001003031441695491, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
goal_identified
=== ep: 253, time 26.576956272125244, eps 0.0010028835965394094, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
=== ep: 254, time 26.7706778049469, eps 0.0010027429618766747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
=== ep: 255, time 26.583892345428467, eps 0.0010026091860473767, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 27.061792850494385, eps 0.0010024819345422614, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
goal_identified
=== ep: 257, time 27.019562244415283, eps 0.0010023608891662839, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 27.35005784034729, eps 0.001002245747242954, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
goal_identified
=== ep: 259, time 30.261202812194824, eps 0.0010021362208574892, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
goal_identified
=== ep: 260, time 27.02286458015442, eps 0.001002032036136876, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
goal_identified
=== ep: 261, time 26.947609901428223, eps 0.0010019329325650452, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 26.72271990776062, eps 0.0010018386623314465, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 27.478222608566284, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 264, time 27.063320636749268, eps 0.0010016636904766263, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
=== ep: 265, time 26.918408393859863, eps 0.0010015825513346283, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
goal_identified
goal_identified
=== ep: 266, time 27.012734174728394, eps 0.0010015053693952815, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 27.181901216506958, eps 0.0010014319516635345, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 30.565616846084595, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 29.88280439376831, eps 0.0010012956834459848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 27.075182914733887, eps 0.0010012324922186594, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
=== ep: 271, time 27.053507089614868, eps 0.001001172382863857, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
=== ep: 272, time 26.361035108566284, eps 0.0010011152050768812, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 26.886503219604492, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 27.107560873031616, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
=== ep: 275, time 26.857361316680908, eps 0.0010009598659050213, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 276, time 27.300350666046143, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 26.82806158065796, eps 0.0010008685225871602, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 27.09310507774353, eps 0.0010008261642407504, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
goal_identified
goal_identified
=== ep: 279, time 29.766855001449585, eps 0.001000785871735272, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
=== ep: 280, time 26.51593017578125, eps 0.0010007475443184742, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
goal_identified
=== ep: 281, time 27.020531177520752, eps 0.001000711086151851, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 26.9764666557312, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 29.268734216690063, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
goal_identified
=== ep: 284, time 26.860803365707397, eps 0.0010006120375228235, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
goal_identified
=== ep: 285, time 26.807644367218018, eps 0.0010005821881006083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 286, time 26.823835849761963, eps 0.0010005537944518927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 26.7729070186615, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
=== ep: 288, time 27.11752414703369, eps 0.0010005010939419733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
goal_identified
=== ep: 289, time 33.437028646469116, eps 0.001000476655302044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
=== ep: 290, time 26.704052686691284, eps 0.0010004534085486486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 26.97452998161316, eps 0.0010004312955527947, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 32.511656522750854, eps 0.0010004102610204745, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
=== ep: 293, time 27.229246616363525, eps 0.0010003902523544011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
=== ep: 294, time 29.362146854400635, eps 0.0010003712195224871, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
=== ep: 295, time 27.008428812026978, eps 0.0010003531149327387, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 296, time 27.27601456642151, eps 0.0010003358933142518, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
=== ep: 297, time 27.07390522956848, eps 0.0010003195116040093, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 26.95125722885132, eps 0.0010003039288392032, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 30.014987468719482, eps 0.0010002891060548044, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
goal_identified
=== ep: 300, time 27.06019401550293, eps 0.0010002750061861312, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
=== ep: 301, time 27.24889874458313, eps 0.0010002615939761676, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
goal_identified
=== ep: 302, time 27.08337163925171, eps 0.001000248835887403, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 27.328845024108887, eps 0.0010002367000179694, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
=== ep: 304, time 27.15757131576538, eps 0.0010002251560218723, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
=== ep: 305, time 31.01686978340149, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
goal_identified
goal_identified
=== ep: 306, time 26.93057942390442, eps 0.0010002037295934862, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
=== ep: 307, time 26.856339693069458, eps 0.0010001937935839656, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 27.255318880081177, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 30.811396598815918, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
=== ep: 310, time 27.386841535568237, eps 0.0010001667996834991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 27.322880029678345, eps 0.001000158664766942, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
goal_identified
=== ep: 312, time 28.255266904830933, eps 0.0010001509265949466, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 313, time 26.882184267044067, eps 0.001000143565818053, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
=== ep: 314, time 26.5907564163208, eps 0.0010001365640304844, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 27.51712942123413, eps 0.0010001299037241253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
goal_identified
goal_identified
goal_identified
=== ep: 316, time 26.52385187149048, eps 0.0010001235682447402, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
=== ep: 317, time 32.35979676246643, eps 0.0010001175417503308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 27.094393730163574, eps 0.0010001118091715218, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 30.14923644065857, eps 0.0010001063561738807, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
=== ep: 320, time 27.169981002807617, eps 0.0010001011691220727, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
goal_identified
=== ep: 321, time 27.387516260147095, eps 0.0010000962350457665, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
goal_identified
goal_identified
=== ep: 322, time 29.554755687713623, eps 0.0010000915416072012, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
goal_identified
=== ep: 323, time 26.9000301361084, eps 0.0010000870770703358, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 27.082921266555786, eps 0.0010000828302715028, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 27.223376274108887, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 27.109257698059082, eps 0.0010000749479290019, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
goal_identified
=== ep: 327, time 26.907368421554565, eps 0.001000071292675372, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 26.885162115097046, eps 0.001000067815690565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
=== ep: 329, time 32.04877853393555, eps 0.0010000645082803084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
=== ep: 330, time 27.20333743095398, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 27.13290047645569, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 27.494810104370117, eps 0.0010000555227914069, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 26.97892451286316, eps 0.0010000528149129166, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
=== ep: 334, time 27.151914596557617, eps 0.0010000502390992187, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 335, time 30.933419466018677, eps 0.0010000477889094373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
goal_identified
=== ep: 336, time 27.242583990097046, eps 0.0010000454582168217, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
goal_identified
goal_identified
=== ep: 337, time 27.3441321849823, eps 0.001000043241193426, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 32.9021475315094, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
=== ep: 339, time 30.23416757583618, eps 0.0010000391262498123, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 27.620903491973877, eps 0.001000037218040092, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 26.896093368530273, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
goal_identified
=== ep: 342, time 27.568954944610596, eps 0.0010000336762753012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 27.488330364227295, eps 0.001000032033863974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 344, time 27.28479552268982, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
=== ep: 345, time 30.886508464813232, eps 0.001000028985438768, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
goal_identified
goal_identified
=== ep: 346, time 26.660356521606445, eps 0.001000027571802238, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
goal_identified
goal_identified
=== ep: 347, time 26.927274227142334, eps 0.0010000262271095755, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
goal_identified
=== ep: 348, time 26.929872751235962, eps 0.0010000249479983478, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
=== ep: 349, time 32.461076974868774, eps 0.0010000237312701107, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 350, time 32.913987159729004, eps 0.00100002257388241, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
goal_identified
=== ep: 351, time 26.838648557662964, eps 0.0010000214729411737, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 27.12196135520935, eps 0.0010000204256934752, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 26.908607244491577, eps 0.0010000194295206493, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
=== ep: 354, time 27.184683084487915, eps 0.0010000184819317455, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
goal_identified
=== ep: 355, time 27.581355810165405, eps 0.001000017580557298, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
=== ep: 356, time 27.309815406799316, eps 0.001000016723143401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
goal_identified
goal_identified
goal_identified
=== ep: 357, time 27.016697883605957, eps 0.0010000159075460732, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
goal_identified
=== ep: 358, time 27.250796794891357, eps 0.0010000151317258964, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
goal_identified
=== ep: 359, time 30.25836753845215, eps 0.0010000143937429161, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 27.306663513183594, eps 0.0010000136917517905, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
=== ep: 361, time 26.8235023021698, eps 0.001000013023997176, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 362, time 27.140406370162964, eps 0.0010000123888093385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
goal_identified
goal_identified
goal_identified
=== ep: 363, time 27.194096565246582, eps 0.0010000117845999773, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
=== ep: 364, time 27.595304012298584, eps 0.0010000112098582543, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
goal_identified
=== ep: 365, time 26.812787294387817, eps 0.001000010663147016, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
goal_identified
=== ep: 366, time 27.353744745254517, eps 0.0010000101430991996, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
goal_identified
=== ep: 367, time 29.186424493789673, eps 0.0010000096484144142, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 27.526947498321533, eps 0.0010000091778556905, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 30.320364236831665, eps 0.0010000087302463867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
=== ep: 370, time 27.1675763130188, eps 0.001000008304467246, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 27.124136924743652, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
=== ep: 372, time 29.816587209701538, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
goal_identified
=== ep: 373, time 26.93088698387146, eps 0.0010000071477211988, sum reward: 2, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
=== ep: 374, time 27.058878183364868, eps 0.0010000067991227223, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
=== ep: 375, time 26.773078441619873, eps 0.0010000064675255943, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 376, time 27.36015820503235, eps 0.001000006152100649, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 27.194767236709595, eps 0.0010000058520591598, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
goal_identified
goal_identified
=== ep: 378, time 27.068893909454346, eps 0.0010000055666508666, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 379, time 29.093706369400024, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 26.86320161819458, eps 0.0010000050369139975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
goal_identified
=== ep: 381, time 27.052492380142212, eps 0.001000004791260803, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
goal_identified
goal_identified
=== ep: 382, time 26.8297336101532, eps 0.0010000045575882562, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
=== ep: 383, time 27.27238941192627, eps 0.001000004335312054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
=== ep: 384, time 27.132420778274536, eps 0.0010000041238763903, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 27.242280960083008, eps 0.0010000039227525655, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 30.134780883789062, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
=== ep: 387, time 27.01905393600464, eps 0.001000003549453303, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
goal_identified
=== ep: 388, time 27.311405897140503, eps 0.0010000033763444226, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 389, time 29.75063681602478, eps 0.001000003211678162, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 390, time 26.816316843032837, eps 0.0010000030550427698, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 27.126991748809814, eps 0.0010000029060465757, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 27.561206579208374, eps 0.0010000027643170119, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
goal_identified
=== ep: 393, time 27.10736060142517, eps 0.0010000026294996803, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
=== ep: 394, time 27.200272798538208, eps 0.0010000025012574677, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
=== ep: 395, time 27.46155595779419, eps 0.0010000023792697014, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
goal_identified
goal_identified
=== ep: 396, time 27.28227925300598, eps 0.0010000022632313489, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 27.21104121208191, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 26.878116607666016, eps 0.00100000204785641, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 30.72506856918335, eps 0.0010000019479812744, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 27.015692710876465, eps 0.0010000018529771066, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
goal_identified
=== ep: 401, time 27.097718000411987, eps 0.0010000017626063467, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
goal_identified
goal_identified
=== ep: 402, time 27.117456436157227, eps 0.0010000016766430208, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 27.92318058013916, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
=== ep: 404, time 27.35189962387085, eps 0.001000001517089342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
goal_identified
=== ep: 405, time 32.429004192352295, eps 0.0010000014431000217, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
goal_identified
goal_identified
goal_identified
=== ep: 406, time 26.870604038238525, eps 0.001000001372719203, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
=== ep: 407, time 27.585159301757812, eps 0.0010000013057708975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 27.360021352767944, eps 0.0010000012420876994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
goal_identified
goal_identified
=== ep: 409, time 30.294182300567627, eps 0.0010000011815103674, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
goal_identified
=== ep: 410, time 27.402653455734253, eps 0.001000001123887427, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 411, time 27.228586435317993, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
goal_identified
goal_identified
=== ep: 412, time 27.358819246292114, eps 0.0010000010169353975, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 27.31697916984558, eps 0.0010000009673388729, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
goal_identified
=== ep: 414, time 27.586218118667603, eps 0.0010000009201611994, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
goal_identified
goal_identified
=== ep: 415, time 27.381617307662964, eps 0.0010000008752844081, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 27.469982385635376, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
goal_identified
=== ep: 417, time 27.42150640487671, eps 0.001000000791990084, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
goal_identified
=== ep: 418, time 27.36527180671692, eps 0.0010000007533642718, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
=== ep: 419, time 29.174591064453125, eps 0.0010000007166222626, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 27.367443561553955, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
goal_identified
=== ep: 421, time 27.389628171920776, eps 0.001000000648426638, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 27.249523401260376, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
goal_identified
=== ep: 423, time 27.346737146377563, eps 0.0010000005867206849, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
goal_identified
goal_identified
=== ep: 424, time 27.36958074569702, eps 0.0010000005581059794, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
goal_identified
goal_identified
goal_identified
=== ep: 425, time 27.53473687171936, eps 0.0010000005308868295, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
goal_identified
=== ep: 426, time 27.676772356033325, eps 0.0010000005049951733, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 27.239399433135986, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
goal_identified
=== ep: 428, time 27.75222134590149, eps 0.0010000004569385287, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 30.1475191116333, eps 0.0010000004346533736, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
=== ep: 430, time 27.00953459739685, eps 0.0010000004134550786, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 27.222060441970825, eps 0.0010000003932906364, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
=== ep: 432, time 27.186020135879517, eps 0.0010000003741096257, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 27.029255867004395, eps 0.001000000355864084, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 27.29412031173706, eps 0.0010000003385083878, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
=== ep: 435, time 26.99944496154785, eps 0.001000000321999139, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 27.447469234466553, eps 0.0010000003062950555, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
goal_identified
=== ep: 437, time 27.011701822280884, eps 0.0010000002913568694, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 27.01807713508606, eps 0.0010000002771472273, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
=== ep: 439, time 32.19413232803345, eps 0.0010000002636305976, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
=== ep: 440, time 27.444547653198242, eps 0.0010000002507731815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 27.136974096298218, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
=== ep: 442, time 27.32714581489563, eps 0.0010000002269089582, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
=== ep: 443, time 27.13628888130188, eps 0.0010000002158424776, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
goal_identified
=== ep: 444, time 26.416715145111084, eps 0.0010000002053157158, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 27.11540937423706, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
=== ep: 446, time 27.110965967178345, eps 0.001000000185777342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 447, time 26.98181700706482, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
=== ep: 448, time 27.030674934387207, eps 0.0010000001680982905, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
goal_identified
=== ep: 449, time 30.243475914001465, eps 0.0010000001599000403, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 27.056854009628296, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
=== ep: 451, time 27.32152557373047, eps 0.0010000001446835395, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 27.327508449554443, eps 0.0010000001376272401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
goal_identified
=== ep: 453, time 27.44745373725891, eps 0.0010000001309150804, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
=== ep: 454, time 27.603365182876587, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
goal_identified
=== ep: 455, time 27.533915281295776, eps 0.0010000001184568633, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
=== ep: 456, time 26.959568977355957, eps 0.0010000001126796538, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
=== ep: 457, time 27.329605102539062, eps 0.0010000001071842023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 27.479946851730347, eps 0.001000000101956767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 30.44828510284424, eps 0.001000000096984277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
=== ep: 460, time 27.305222272872925, eps 0.001000000092254298, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
=== ep: 461, time 27.13844132423401, eps 0.0010000000877550027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 27.168641328811646, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
goal_identified
=== ep: 463, time 27.240363836288452, eps 0.00100000007940401, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
goal_identified
=== ep: 464, time 27.445709228515625, eps 0.0010000000755314307, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
=== ep: 465, time 27.235610485076904, eps 0.0010000000718477194, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
goal_identified
=== ep: 466, time 27.649641513824463, eps 0.0010000000683436647, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
goal_identified
=== ep: 467, time 27.23128843307495, eps 0.001000000065010505, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
=== ep: 468, time 27.45235848426819, eps 0.0010000000618399052, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 31.78115701675415, eps 0.0010000000588239375, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
goal_identified
=== ep: 470, time 27.602887868881226, eps 0.0010000000559550603, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
=== ep: 471, time 27.31159234046936, eps 0.0010000000532260998, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
goal_identified
=== ep: 472, time 27.395151376724243, eps 0.0010000000506302322, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 473, time 27.09671640396118, eps 0.0010000000481609666, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
=== ep: 474, time 27.38680601119995, eps 0.0010000000458121286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
goal_identified
goal_identified
goal_identified
=== ep: 475, time 26.725952863693237, eps 0.0010000000435778447, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
goal_identified
=== ep: 476, time 27.995460987091064, eps 0.001000000041452528, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
goal_identified
=== ep: 477, time 27.37412452697754, eps 0.0010000000394308644, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 27.111778020858765, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 479, time 31.246670961380005, eps 0.0010000000356785216, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 27.753173112869263, eps 0.0010000000339384595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 27.23640465736389, eps 0.0010000000322832614, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 27.396712064743042, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 27.056872129440308, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
goal_identified
=== ep: 484, time 27.216060400009155, eps 0.0010000000277864607, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
=== ep: 485, time 27.462754726409912, eps 0.0010000000264312988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
=== ep: 486, time 26.7285578250885, eps 0.0010000000251422292, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
goal_identified
=== ep: 487, time 26.996187686920166, eps 0.0010000000239160282, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
=== ep: 488, time 27.51236581802368, eps 0.00100000002274963, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
goal_identified
=== ep: 489, time 29.741414070129395, eps 0.0010000000216401172, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
=== ep: 490, time 27.472163438796997, eps 0.0010000000205847162, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 27.67086410522461, eps 0.0010000000195807877, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 26.92050838470459, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 27.278446674346924, eps 0.0010000000177174295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
goal_identified
=== ep: 494, time 27.222280502319336, eps 0.0010000000168533404, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
goal_identified
goal_identified
=== ep: 495, time 27.29869818687439, eps 0.0010000000160313932, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 27.191096305847168, eps 0.001000000015249533, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
goal_identified
=== ep: 497, time 27.24478268623352, eps 0.0010000000145058043, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 26.947636127471924, eps 0.001000000013798348, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 499, time 34.99580788612366, eps 0.0010000000131253947, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 500, time 27.626521348953247, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 27.386691093444824, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 27.393025875091553, eps 0.0010000000112971319, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 27.50512194633484, eps 0.0010000000107461642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 26.922515869140625, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
goal_identified
=== ep: 505, time 27.30015254020691, eps 0.0010000000097235315, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
goal_identified
goal_identified
=== ep: 506, time 26.7472665309906, eps 0.0010000000092493092, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
goal_identified
=== ep: 507, time 26.99727964401245, eps 0.0010000000087982152, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
goal_identified
=== ep: 508, time 27.281582832336426, eps 0.0010000000083691212, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
goal_identified
=== ep: 509, time 31.53568720817566, eps 0.0010000000079609542, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
goal_identified
=== ep: 510, time 27.62740206718445, eps 0.001000000007572694, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
goal_identified
goal_identified
=== ep: 511, time 27.241225957870483, eps 0.0010000000072033692, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 27.44610857963562, eps 0.001000000006852057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
=== ep: 513, time 27.4616596698761, eps 0.001000000006517878, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
goal_identified
goal_identified
=== ep: 514, time 27.19065761566162, eps 0.0010000000061999974, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 27.286919593811035, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
=== ep: 516, time 27.145769596099854, eps 0.0010000000056099897, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
goal_identified
=== ep: 517, time 28.848802089691162, eps 0.0010000000053363872, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
goal_identified
=== ep: 518, time 27.61968445777893, eps 0.0010000000050761286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
goal_identified
=== ep: 519, time 29.71461844444275, eps 0.001000000004828563, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 27.09376549720764, eps 0.001000000004593071, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 26.902425527572632, eps 0.0010000000043690644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 27.277596473693848, eps 0.0010000000041559827, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
=== ep: 523, time 27.66689109802246, eps 0.0010000000039532928, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
goal_identified
=== ep: 524, time 27.34771156311035, eps 0.0010000000037604885, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
=== ep: 525, time 27.775460720062256, eps 0.0010000000035770874, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 30.46968173980713, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 27.599456071853638, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 27.371302127838135, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
=== ep: 529, time 30.53136658668518, eps 0.0010000000029286714, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
=== ep: 530, time 27.064292430877686, eps 0.0010000000027858384, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 27.098265886306763, eps 0.0010000000026499714, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
goal_identified
=== ep: 532, time 27.185855388641357, eps 0.0010000000025207308, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
goal_identified
=== ep: 533, time 27.08415412902832, eps 0.0010000000023977934, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
goal_identified
goal_identified
=== ep: 534, time 27.45146894454956, eps 0.0010000000022808515, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
=== ep: 535, time 27.796249628067017, eps 0.0010000000021696133, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
goal_identified
=== ep: 536, time 27.890109062194824, eps 0.0010000000020637999, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
goal_identified
=== ep: 537, time 27.129260301589966, eps 0.0010000000019631471, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
=== ep: 538, time 27.323864221572876, eps 0.0010000000018674034, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 539, time 29.60058879852295, eps 0.001000000001776329, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
goal_identified
=== ep: 540, time 27.310229301452637, eps 0.0010000000016896964, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
=== ep: 541, time 27.178826570510864, eps 0.001000000001607289, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
=== ep: 542, time 27.783716678619385, eps 0.0010000000015289005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
=== ep: 543, time 27.141191959381104, eps 0.0010000000014543352, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
goal_identified
goal_identified
=== ep: 544, time 27.618367195129395, eps 0.0010000000013834064, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 27.422205686569214, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 27.20506262779236, eps 0.0010000000012517578, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
goal_identified
goal_identified
goal_identified
=== ep: 547, time 27.081706762313843, eps 0.001000000001190709, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
goal_identified
goal_identified
=== ep: 548, time 27.03602886199951, eps 0.0010000000011326374, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
goal_identified
=== ep: 549, time 31.929529190063477, eps 0.001000000001077398, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
goal_identified
=== ep: 550, time 27.381421327590942, eps 0.0010000000010248527, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
=== ep: 551, time 27.765225887298584, eps 0.00100000000097487, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 27.678643465042114, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 553, time 27.2284893989563, eps 0.0010000000008820989, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
=== ep: 554, time 27.55814480781555, eps 0.0010000000008390784, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
goal_identified
=== ep: 555, time 27.84852623939514, eps 0.001000000000798156, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
=== ep: 556, time 27.04532027244568, eps 0.0010000000007592295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
goal_identified
goal_identified
=== ep: 557, time 27.038949966430664, eps 0.0010000000007222014, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 27.229472398757935, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
=== ep: 559, time 31.38685131072998, eps 0.001000000000653475, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
goal_identified
=== ep: 560, time 30.571507453918457, eps 0.0010000000006216046, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 27.00428605079651, eps 0.0010000000005912885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
=== ep: 562, time 28.29913067817688, eps 0.0010000000005624511, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
=== ep: 563, time 27.629231214523315, eps 0.00100000000053502, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 27.244521141052246, eps 0.001000000000508927, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
=== ep: 565, time 32.901305198669434, eps 0.001000000000484106, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 566, time 27.369888067245483, eps 0.001000000000460496, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 27.235494136810303, eps 0.0010000000004380374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 33.81884241104126, eps 0.001000000000416674, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
=== ep: 569, time 30.30232858657837, eps 0.0010000000003963527, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
goal_identified
=== ep: 570, time 27.733370780944824, eps 0.0010000000003770222, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
=== ep: 571, time 31.88839316368103, eps 0.0010000000003586346, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 27.157091856002808, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
=== ep: 573, time 27.525294065475464, eps 0.001000000000324506, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
goal_identified
=== ep: 574, time 27.4857759475708, eps 0.0010000000003086798, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 27.930363416671753, eps 0.0010000000002936252, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
goal_identified
goal_identified
=== ep: 576, time 27.643482208251953, eps 0.001000000000279305, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
goal_identified
goal_identified
=== ep: 577, time 27.410441637039185, eps 0.0010000000002656831, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 27.613986253738403, eps 0.0010000000002527256, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
=== ep: 579, time 33.30402755737305, eps 0.0010000000002404, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 27.3546142578125, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
goal_identified
goal_identified
=== ep: 581, time 27.040746450424194, eps 0.0010000000002175229, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
=== ep: 582, time 27.53543472290039, eps 0.0010000000002069142, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 583, time 27.036828994750977, eps 0.0010000000001968228, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 584, time 27.781912565231323, eps 0.0010000000001872237, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
goal_identified
=== ep: 585, time 27.306760549545288, eps 0.0010000000001780928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 27.562437772750854, eps 0.001000000000169407, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 27.38651204109192, eps 0.001000000000161145, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
goal_identified
goal_identified
=== ep: 588, time 27.01235604286194, eps 0.0010000000001532858, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
goal_identified
goal_identified
=== ep: 589, time 30.264415979385376, eps 0.00100000000014581, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
=== ep: 590, time 27.667968034744263, eps 0.0010000000001386988, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 27.844345808029175, eps 0.0010000000001319344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
=== ep: 592, time 28.11531162261963, eps 0.0010000000001255, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 26.952951431274414, eps 0.0010000000001193791, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
goal_identified
=== ep: 594, time 27.070516109466553, eps 0.001000000000113557, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
goal_identified
=== ep: 595, time 28.001798391342163, eps 0.0010000000001080186, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
goal_identified
=== ep: 596, time 27.605557918548584, eps 0.0010000000001027505, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
goal_identified
=== ep: 597, time 27.47413659095764, eps 0.0010000000000977393, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 27.66434907913208, eps 0.0010000000000929725, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
=== ep: 599, time 30.244983434677124, eps 0.0010000000000884382, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
goal_identified
goal_identified
=== ep: 600, time 27.71655535697937, eps 0.001000000000084125, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
=== ep: 601, time 27.192214488983154, eps 0.0010000000000800222, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
=== ep: 602, time 27.60644793510437, eps 0.0010000000000761195, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 603, time 27.368281841278076, eps 0.0010000000000724072, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
goal_identified
=== ep: 604, time 27.40619421005249, eps 0.0010000000000688757, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
goal_identified
=== ep: 605, time 27.662208557128906, eps 0.0010000000000655166, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 606, time 27.847352504730225, eps 0.0010000000000623215, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
goal_identified
=== ep: 607, time 27.35734796524048, eps 0.001000000000059282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
goal_identified
=== ep: 608, time 27.585992574691772, eps 0.0010000000000563907, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
=== ep: 609, time 31.754732131958008, eps 0.0010000000000536405, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 27.355900049209595, eps 0.0010000000000510245, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
goal_identified
=== ep: 611, time 26.983365535736084, eps 0.0010000000000485358, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
goal_identified
goal_identified
goal_identified
=== ep: 612, time 27.519189834594727, eps 0.0010000000000461688, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 27.54449200630188, eps 0.0010000000000439171, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
goal_identified
=== ep: 614, time 27.933354139328003, eps 0.0010000000000417752, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 615, time 27.21962261199951, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
goal_identified
=== ep: 616, time 28.325332641601562, eps 0.0010000000000377999, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
=== ep: 617, time 27.58983325958252, eps 0.0010000000000359563, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
=== ep: 618, time 27.578415393829346, eps 0.0010000000000342027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
goal_identified
goal_identified
=== ep: 619, time 31.314603805541992, eps 0.0010000000000325345, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 620, time 27.47830820083618, eps 0.001000000000030948, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
=== ep: 621, time 27.252995491027832, eps 0.0010000000000294385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
=== ep: 622, time 27.647958040237427, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 27.659276723861694, eps 0.0010000000000266371, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
=== ep: 624, time 27.8485689163208, eps 0.001000000000025338, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 27.259289026260376, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
goal_identified
=== ep: 626, time 27.379394054412842, eps 0.0010000000000229268, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 627, time 26.17852783203125, eps 0.0010000000000218085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
=== ep: 628, time 27.189776182174683, eps 0.001000000000020745, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
=== ep: 629, time 30.566828966140747, eps 0.0010000000000197332, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
=== ep: 630, time 27.878106355667114, eps 0.0010000000000187708, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
=== ep: 631, time 27.51292395591736, eps 0.0010000000000178553, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
goal_identified
goal_identified
=== ep: 632, time 27.382968425750732, eps 0.0010000000000169845, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 633, time 27.8269259929657, eps 0.0010000000000161562, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
=== ep: 634, time 27.02137517929077, eps 0.0010000000000153684, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
goal_identified
goal_identified
=== ep: 635, time 27.369754314422607, eps 0.0010000000000146188, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
=== ep: 636, time 27.81715154647827, eps 0.0010000000000139058, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
=== ep: 637, time 27.50226330757141, eps 0.0010000000000132275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 27.371406316757202, eps 0.0010000000000125824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 35.11245059967041, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 27.410264253616333, eps 0.001000000000011385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
goal_identified
=== ep: 641, time 27.640100955963135, eps 0.00100000000001083, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
=== ep: 642, time 27.327976942062378, eps 0.0010000000000103017, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
=== ep: 643, time 27.454350471496582, eps 0.0010000000000097993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
goal_identified
goal_identified
=== ep: 644, time 27.336399793624878, eps 0.0010000000000093213, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
goal_identified
goal_identified
=== ep: 645, time 27.474889993667603, eps 0.0010000000000088666, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
goal_identified
=== ep: 646, time 27.482352018356323, eps 0.0010000000000084342, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 27.404335021972656, eps 0.001000000000008023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
=== ep: 648, time 27.155329704284668, eps 0.0010000000000076317, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 30.435340642929077, eps 0.0010000000000072594, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
goal_identified
goal_identified
=== ep: 650, time 27.253072261810303, eps 0.0010000000000069055, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 27.215238332748413, eps 0.0010000000000065686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
goal_identified
goal_identified
=== ep: 652, time 27.574994325637817, eps 0.0010000000000062483, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 27.474954843521118, eps 0.0010000000000059436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
=== ep: 654, time 27.390881538391113, eps 0.0010000000000056537, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
=== ep: 655, time 27.006261110305786, eps 0.0010000000000053779, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 27.08840274810791, eps 0.0010000000000051157, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
goal_identified
=== ep: 657, time 27.09303116798401, eps 0.0010000000000048661, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
=== ep: 658, time 27.633049726486206, eps 0.001000000000004629, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
=== ep: 659, time 31.160470962524414, eps 0.0010000000000044032, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
=== ep: 660, time 27.79692578315735, eps 0.0010000000000041883, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
goal_identified
=== ep: 661, time 27.459243059158325, eps 0.001000000000003984, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
=== ep: 662, time 27.444172382354736, eps 0.0010000000000037897, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 26.991827249526978, eps 0.001000000000003605, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 27.497690677642822, eps 0.0010000000000034291, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
goal_identified
goal_identified
goal_identified
=== ep: 665, time 26.95575737953186, eps 0.001000000000003262, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
=== ep: 666, time 27.20948815345764, eps 0.0010000000000031028, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
=== ep: 667, time 27.762382984161377, eps 0.0010000000000029514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
goal_identified
=== ep: 668, time 27.575081825256348, eps 0.0010000000000028075, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 31.949707508087158, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
goal_identified
=== ep: 670, time 27.496981620788574, eps 0.0010000000000025403, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 671, time 27.744006156921387, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
goal_identified
goal_identified
=== ep: 672, time 26.85924005508423, eps 0.0010000000000022985, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
=== ep: 673, time 27.50291395187378, eps 0.0010000000000021864, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 27.27225923538208, eps 0.00100000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 27.49368381500244, eps 0.0010000000000019785, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
=== ep: 676, time 27.609201192855835, eps 0.001000000000001882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
=== ep: 677, time 27.354585647583008, eps 0.0010000000000017903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
goal_identified
=== ep: 678, time 27.66312575340271, eps 0.0010000000000017029, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
goal_identified
=== ep: 679, time 29.263943433761597, eps 0.0010000000000016198, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 27.470454931259155, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 27.526432514190674, eps 0.0010000000000014656, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
goal_identified
=== ep: 682, time 27.240694284439087, eps 0.0010000000000013943, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
=== ep: 683, time 27.773805379867554, eps 0.0010000000000013262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
=== ep: 684, time 27.344422340393066, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
goal_identified
=== ep: 685, time 27.64465856552124, eps 0.0010000000000012, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
=== ep: 686, time 27.341955184936523, eps 0.0010000000000011415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
goal_identified
=== ep: 687, time 29.757906913757324, eps 0.0010000000000010857, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
=== ep: 688, time 27.88863968849182, eps 0.0010000000000010328, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
goal_identified
goal_identified
=== ep: 689, time 31.39403462409973, eps 0.0010000000000009825, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
goal_identified
=== ep: 690, time 27.393077611923218, eps 0.0010000000000009346, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 27.58822512626648, eps 0.001000000000000889, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
goal_identified
goal_identified
=== ep: 692, time 27.07144522666931, eps 0.0010000000000008457, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 27.689149379730225, eps 0.0010000000000008045, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
=== ep: 694, time 27.52585482597351, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
goal_identified
=== ep: 695, time 27.360385417938232, eps 0.0010000000000007277, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
goal_identified
=== ep: 696, time 27.291903257369995, eps 0.0010000000000006924, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
=== ep: 697, time 27.832518815994263, eps 0.0010000000000006586, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
goal_identified
=== ep: 698, time 27.165146350860596, eps 0.0010000000000006265, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 30.28395676612854, eps 0.001000000000000596, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
goal_identified
goal_identified
=== ep: 700, time 27.2723708152771, eps 0.0010000000000005668, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
goal_identified
goal_identified
=== ep: 701, time 27.552835941314697, eps 0.0010000000000005393, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
goal_identified
=== ep: 702, time 27.71413254737854, eps 0.0010000000000005128, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 27.75006413459778, eps 0.001000000000000488, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
=== ep: 704, time 27.751750469207764, eps 0.001000000000000464, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
goal_identified
goal_identified
=== ep: 705, time 26.99752902984619, eps 0.0010000000000004415, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
goal_identified
=== ep: 706, time 27.48408007621765, eps 0.00100000000000042, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
=== ep: 707, time 27.44293522834778, eps 0.0010000000000003994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
goal_identified
goal_identified
goal_identified
=== ep: 708, time 27.30723762512207, eps 0.00100000000000038, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
goal_identified
=== ep: 709, time 30.46502447128296, eps 0.0010000000000003615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
goal_identified
=== ep: 710, time 27.19122338294983, eps 0.0010000000000003437, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
=== ep: 711, time 27.617734909057617, eps 0.001000000000000327, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
=== ep: 712, time 27.422541618347168, eps 0.0010000000000003112, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 27.653456449508667, eps 0.001000000000000296, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
=== ep: 714, time 27.4863703250885, eps 0.0010000000000002815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
=== ep: 715, time 27.708603143692017, eps 0.0010000000000002678, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
goal_identified
=== ep: 716, time 27.40502905845642, eps 0.0010000000000002548, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
goal_identified
goal_identified
=== ep: 717, time 27.86494731903076, eps 0.0010000000000002422, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
=== ep: 718, time 27.181233406066895, eps 0.0010000000000002305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
=== ep: 719, time 31.41997981071472, eps 0.0010000000000002192, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
goal_identified
goal_identified
=== ep: 720, time 27.53395128250122, eps 0.0010000000000002086, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
=== ep: 721, time 27.220024824142456, eps 0.0010000000000001984, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
goal_identified
=== ep: 722, time 27.369304656982422, eps 0.0010000000000001887, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
=== ep: 723, time 27.682075023651123, eps 0.0010000000000001796, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
goal_identified
=== ep: 724, time 27.435131788253784, eps 0.0010000000000001707, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 27.47109293937683, eps 0.0010000000000001624, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
goal_identified
=== ep: 726, time 27.197797060012817, eps 0.0010000000000001544, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 28.983335971832275, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 27.189252853393555, eps 0.0010000000000001399, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
goal_identified
=== ep: 729, time 31.18827486038208, eps 0.001000000000000133, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
=== ep: 730, time 27.577372312545776, eps 0.0010000000000001264, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
=== ep: 731, time 27.818063020706177, eps 0.0010000000000001204, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
=== ep: 732, time 27.761924982070923, eps 0.0010000000000001145, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
=== ep: 733, time 27.43461585044861, eps 0.0010000000000001089, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
goal_identified
=== ep: 734, time 27.68072199821472, eps 0.0010000000000001037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
=== ep: 735, time 27.359927892684937, eps 0.0010000000000000985, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
goal_identified
goal_identified
goal_identified
=== ep: 736, time 27.213238954544067, eps 0.0010000000000000937, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
=== ep: 737, time 27.540204286575317, eps 0.0010000000000000891, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
=== ep: 738, time 28.393513679504395, eps 0.0010000000000000848, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
=== ep: 739, time 30.903329372406006, eps 0.0010000000000000807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
goal_identified
=== ep: 740, time 27.80646586418152, eps 0.0010000000000000768, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 29.611705780029297, eps 0.001000000000000073, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
=== ep: 742, time 27.493895530700684, eps 0.0010000000000000694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
goal_identified
goal_identified
goal_identified
=== ep: 743, time 27.53832721710205, eps 0.001000000000000066, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
goal_identified
goal_identified
=== ep: 744, time 27.333894968032837, eps 0.001000000000000063, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 745, time 27.38437008857727, eps 0.0010000000000000599, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
=== ep: 746, time 27.16277551651001, eps 0.0010000000000000568, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
=== ep: 747, time 27.20332622528076, eps 0.001000000000000054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 27.723716259002686, eps 0.0010000000000000514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
=== ep: 749, time 30.395371198654175, eps 0.001000000000000049, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
goal_identified
goal_identified
=== ep: 750, time 26.929282903671265, eps 0.0010000000000000466, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 26.501453161239624, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 26.409331798553467, eps 0.001000000000000042, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
goal_identified
=== ep: 753, time 26.441272020339966, eps 0.0010000000000000401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
=== ep: 754, time 31.102253198623657, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
=== ep: 755, time 26.587740182876587, eps 0.0010000000000000362, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 756, time 26.254056930541992, eps 0.0010000000000000345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
goal_identified
goal_identified
goal_identified
=== ep: 757, time 27.030864477157593, eps 0.0010000000000000328, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 26.272549629211426, eps 0.0010000000000000312, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
goal_identified
=== ep: 759, time 30.39565920829773, eps 0.0010000000000000297, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
goal_identified
=== ep: 760, time 26.140910148620605, eps 0.0010000000000000282, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
=== ep: 761, time 26.718544721603394, eps 0.001000000000000027, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
=== ep: 762, time 26.436651706695557, eps 0.0010000000000000256, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
goal_identified
=== ep: 763, time 26.770492792129517, eps 0.0010000000000000243, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 26.969007968902588, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
goal_identified
=== ep: 765, time 26.697893619537354, eps 0.001000000000000022, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
=== ep: 766, time 26.371326446533203, eps 0.0010000000000000208, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
=== ep: 767, time 26.605786323547363, eps 0.00100000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
goal_identified
goal_identified
goal_identified
=== ep: 768, time 26.11868906021118, eps 0.0010000000000000189, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
goal_identified
=== ep: 769, time 29.497546672821045, eps 0.001000000000000018, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
goal_identified
=== ep: 770, time 26.16796875, eps 0.0010000000000000172, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
goal_identified
=== ep: 771, time 26.23551607131958, eps 0.0010000000000000163, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
=== ep: 772, time 26.856230974197388, eps 0.0010000000000000154, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
goal_identified
=== ep: 773, time 26.89747190475464, eps 0.0010000000000000148, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
goal_identified
=== ep: 774, time 29.379814863204956, eps 0.0010000000000000141, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
goal_identified
=== ep: 775, time 26.717435836791992, eps 0.0010000000000000132, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
goal_identified
goal_identified
=== ep: 776, time 26.619494438171387, eps 0.0010000000000000126, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
=== ep: 777, time 26.842005014419556, eps 0.0010000000000000122, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
goal_identified
=== ep: 778, time 26.960071802139282, eps 0.0010000000000000115, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 29.500166177749634, eps 0.0010000000000000109, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
=== ep: 780, time 26.103269577026367, eps 0.0010000000000000104, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
goal_identified
=== ep: 781, time 25.05345392227173, eps 0.00100000000000001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
=== ep: 782, time 26.685949087142944, eps 0.0010000000000000093, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
goal_identified
goal_identified
=== ep: 783, time 26.34296703338623, eps 0.001000000000000009, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
goal_identified
goal_identified
=== ep: 784, time 26.7140953540802, eps 0.0010000000000000085, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
=== ep: 785, time 26.60710334777832, eps 0.001000000000000008, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
=== ep: 786, time 26.34378409385681, eps 0.0010000000000000076, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 25.24515676498413, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
goal_identified
=== ep: 788, time 32.281365156173706, eps 0.001000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
goal_identified
=== ep: 789, time 30.612942695617676, eps 0.0010000000000000067, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
goal_identified
goal_identified
=== ep: 790, time 27.176473379135132, eps 0.0010000000000000063, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
goal_identified
goal_identified
=== ep: 791, time 26.451088905334473, eps 0.001000000000000006, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
goal_identified
=== ep: 792, time 26.733174562454224, eps 0.0010000000000000057, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
goal_identified
goal_identified
=== ep: 793, time 26.57876205444336, eps 0.0010000000000000054, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
=== ep: 794, time 26.464757919311523, eps 0.0010000000000000052, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 26.8121817111969, eps 0.001000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
goal_identified
=== ep: 796, time 26.721147775650024, eps 0.0010000000000000048, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 797, time 26.522448778152466, eps 0.0010000000000000044, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
=== ep: 798, time 26.65928602218628, eps 0.0010000000000000041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
goal_identified
=== ep: 799, time 29.320871353149414, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
=== ep: 800, time 26.810717582702637, eps 0.001000000000000004, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 26.986226320266724, eps 0.0010000000000000037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
=== ep: 802, time 27.199844360351562, eps 0.0010000000000000035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
=== ep: 803, time 27.98678159713745, eps 0.0010000000000000033, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
=== ep: 804, time 26.715027332305908, eps 0.001000000000000003, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
=== ep: 805, time 26.10422444343567, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
goal_identified
goal_identified
=== ep: 806, time 26.354624271392822, eps 0.0010000000000000028, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
=== ep: 807, time 26.758077383041382, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 808, time 26.370333433151245, eps 0.0010000000000000026, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
goal_identified
goal_identified
=== ep: 809, time 29.332624912261963, eps 0.0010000000000000024, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
goal_identified
=== ep: 810, time 26.837219715118408, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
=== ep: 811, time 26.797433376312256, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
=== ep: 812, time 26.981890439987183, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
=== ep: 813, time 26.70931053161621, eps 0.001000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
goal_identified
=== ep: 814, time 26.925902366638184, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
goal_identified
=== ep: 815, time 26.848297119140625, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
=== ep: 816, time 26.676512479782104, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 26.931544065475464, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
=== ep: 818, time 32.284889698028564, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
goal_identified
=== ep: 819, time 30.103794813156128, eps 0.0010000000000000015, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
=== ep: 820, time 26.825709581375122, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
goal_identified
=== ep: 821, time 26.419404983520508, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 26.104583978652954, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 26.46052312850952, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
=== ep: 824, time 28.099090337753296, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
goal_identified
=== ep: 825, time 26.531105995178223, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
=== ep: 826, time 26.66808581352234, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
=== ep: 827, time 27.419761419296265, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
=== ep: 828, time 26.42623209953308, eps 0.0010000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
=== ep: 829, time 30.3707435131073, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
goal_identified
goal_identified
=== ep: 830, time 26.685421466827393, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
goal_identified
=== ep: 831, time 26.905659675598145, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
=== ep: 832, time 27.021162509918213, eps 0.0010000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
=== ep: 833, time 26.748774766921997, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
goal_identified
=== ep: 834, time 26.7197368144989, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
goal_identified
=== ep: 835, time 26.427563190460205, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
goal_identified
goal_identified
=== ep: 836, time 26.078123569488525, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
goal_identified
goal_identified
=== ep: 837, time 26.532118797302246, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
goal_identified
=== ep: 838, time 26.798081636428833, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
goal_identified
=== ep: 839, time 29.66063666343689, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
goal_identified
=== ep: 840, time 26.490979433059692, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
goal_identified
goal_identified
=== ep: 841, time 26.344244718551636, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
=== ep: 842, time 26.381465673446655, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
=== ep: 843, time 26.948853254318237, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
=== ep: 844, time 26.794496297836304, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
goal_identified
goal_identified
=== ep: 845, time 26.494247436523438, eps 0.0010000000000000005, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
goal_identified
=== ep: 846, time 26.482879400253296, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
=== ep: 847, time 29.589266061782837, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
goal_identified
goal_identified
=== ep: 848, time 26.747970819473267, eps 0.0010000000000000005, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
goal_identified
goal_identified
goal_identified
=== ep: 849, time 29.59127402305603, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 850, time 26.727627515792847, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 26.3302001953125, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
goal_identified
=== ep: 852, time 26.783098697662354, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
goal_identified
=== ep: 853, time 30.4480197429657, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
=== ep: 854, time 26.588318347930908, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
=== ep: 855, time 26.847759246826172, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
goal_identified
=== ep: 856, time 26.431190967559814, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
=== ep: 857, time 26.80863332748413, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
goal_identified
=== ep: 858, time 26.13980531692505, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
=== ep: 859, time 31.43904757499695, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
=== ep: 860, time 26.98753523826599, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
=== ep: 861, time 26.478580951690674, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
goal_identified
=== ep: 862, time 26.547112703323364, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
goal_identified
=== ep: 863, time 26.925697565078735, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
=== ep: 864, time 26.626269340515137, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
goal_identified
=== ep: 865, time 27.005156993865967, eps 0.0010000000000000002, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
goal_identified
goal_identified
=== ep: 866, time 26.75773596763611, eps 0.0010000000000000002, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
=== ep: 867, time 26.439078092575073, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
=== ep: 868, time 32.113282680511475, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
=== ep: 869, time 28.337735414505005, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
=== ep: 870, time 35.890859603881836, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
goal_identified
=== ep: 871, time 26.71764612197876, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
goal_identified
goal_identified
=== ep: 872, time 26.602173805236816, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
=== ep: 873, time 27.170531749725342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
goal_identified
=== ep: 874, time 26.944071054458618, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
=== ep: 875, time 27.504920959472656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
goal_identified
=== ep: 876, time 26.551137685775757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 27.01061725616455, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
goal_identified
=== ep: 878, time 27.63728427886963, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
goal_identified
goal_identified
=== ep: 879, time 29.32282280921936, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
=== ep: 880, time 26.60761833190918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
goal_identified
=== ep: 881, time 26.370785236358643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
goal_identified
goal_identified
=== ep: 882, time 27.301826238632202, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
=== ep: 883, time 27.489454984664917, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
=== ep: 884, time 26.538864374160767, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
goal_identified
=== ep: 885, time 26.871111392974854, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
=== ep: 886, time 26.8617582321167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
goal_identified
goal_identified
goal_identified
=== ep: 887, time 26.64608335494995, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
goal_identified
=== ep: 888, time 26.81464147567749, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
=== ep: 889, time 31.347187757492065, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 27.03075122833252, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
=== ep: 891, time 26.34222912788391, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
goal_identified
=== ep: 892, time 26.65747594833374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
goal_identified
=== ep: 893, time 26.456156969070435, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
goal_identified
goal_identified
goal_identified
=== ep: 894, time 26.856789350509644, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
=== ep: 895, time 26.62934422492981, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
=== ep: 896, time 26.913867473602295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
goal_identified
=== ep: 897, time 26.780500411987305, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
goal_identified
=== ep: 898, time 26.851404666900635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
goal_identified
goal_identified
=== ep: 899, time 28.538114070892334, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
goal_identified
=== ep: 900, time 26.423061847686768, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
goal_identified
goal_identified
=== ep: 901, time 26.52793836593628, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
goal_identified
=== ep: 902, time 27.099543809890747, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
goal_identified
=== ep: 903, time 26.434040546417236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
=== ep: 904, time 27.18305468559265, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
=== ep: 905, time 27.082812786102295, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
goal_identified
=== ep: 906, time 26.174792528152466, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
goal_identified
goal_identified
=== ep: 907, time 26.639677047729492, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
goal_identified
goal_identified
=== ep: 908, time 26.42927885055542, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
goal_identified
goal_identified
=== ep: 909, time 30.052314519882202, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
=== ep: 910, time 26.771583318710327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
=== ep: 911, time 27.021175622940063, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
=== ep: 912, time 26.749629497528076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
=== ep: 913, time 26.85100817680359, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 26.659345865249634, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 915, time 26.44930934906006, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
=== ep: 916, time 27.0663001537323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
goal_identified
=== ep: 917, time 26.849347829818726, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
goal_identified
=== ep: 918, time 26.712647676467896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
goal_identified
goal_identified
=== ep: 919, time 31.637157201766968, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
=== ep: 920, time 26.708759307861328, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
goal_identified
=== ep: 921, time 26.628074884414673, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
goal_identified
=== ep: 922, time 26.709686279296875, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 26.223839044570923, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
=== ep: 924, time 26.706193685531616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
goal_identified
=== ep: 925, time 30.322941303253174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
=== ep: 926, time 27.276172637939453, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
goal_identified
=== ep: 927, time 26.866204738616943, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
=== ep: 928, time 26.51287055015564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
=== ep: 929, time 29.09208846092224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
goal_identified
=== ep: 930, time 26.68987536430359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
=== ep: 931, time 27.209038257598877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
goal_identified
=== ep: 932, time 27.214922666549683, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
goal_identified
=== ep: 933, time 27.167476892471313, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 934, time 27.528770208358765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
goal_identified
goal_identified
=== ep: 935, time 27.08365774154663, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
=== ep: 936, time 27.071035861968994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
goal_identified
=== ep: 937, time 27.074476718902588, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
=== ep: 938, time 26.7356595993042, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
=== ep: 939, time 38.94608807563782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
=== ep: 940, time 26.800323009490967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
goal_identified
=== ep: 941, time 26.01405954360962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
=== ep: 942, time 27.39067244529724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
=== ep: 943, time 26.567787170410156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
=== ep: 944, time 26.897543907165527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
goal_identified
=== ep: 945, time 26.741820096969604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
=== ep: 946, time 26.984825611114502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
goal_identified
=== ep: 947, time 26.631909608840942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
=== ep: 948, time 27.16562271118164, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
=== ep: 949, time 30.16519522666931, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
goal_identified
=== ep: 950, time 26.848398208618164, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
goal_identified
=== ep: 951, time 29.149806022644043, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
=== ep: 952, time 26.855425119400024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 25/25)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
=== ep: 953, time 26.833444118499756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
=== ep: 954, time 26.82629418373108, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
goal_identified
=== ep: 955, time 26.61164426803589, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
=== ep: 956, time 26.54056191444397, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
goal_identified
=== ep: 957, time 26.53756856918335, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 26.59293484687805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
=== ep: 959, time 29.38706374168396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
goal_identified
goal_identified
=== ep: 960, time 26.736525535583496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
goal_identified
=== ep: 961, time 26.876653909683228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
=== ep: 962, time 26.402867794036865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
=== ep: 963, time 27.019348621368408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
goal_identified
=== ep: 964, time 27.07924723625183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
goal_identified
=== ep: 965, time 26.36646580696106, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
goal_identified
=== ep: 966, time 26.55037832260132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 26.766924142837524, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 26.896836042404175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
=== ep: 969, time 28.41608428955078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
goal_identified
=== ep: 970, time 26.74733304977417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
goal_identified
goal_identified
=== ep: 971, time 26.960168600082397, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
goal_identified
=== ep: 972, time 26.85989022254944, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
goal_identified
=== ep: 973, time 26.82323718070984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
goal_identified
goal_identified
=== ep: 974, time 26.77078676223755, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
=== ep: 975, time 26.871495962142944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
=== ep: 976, time 32.26182246208191, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
goal_identified
=== ep: 977, time 25.4498291015625, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
goal_identified
goal_identified
=== ep: 978, time 26.763193368911743, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
=== ep: 979, time 30.005021810531616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
=== ep: 980, time 26.63020658493042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
goal_identified
=== ep: 981, time 26.493924140930176, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
=== ep: 982, time 29.616397619247437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 26.298729419708252, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
=== ep: 984, time 26.73378348350525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
=== ep: 985, time 26.963481903076172, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
=== ep: 986, time 26.588849306106567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
=== ep: 987, time 26.675621271133423, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
goal_identified
goal_identified
=== ep: 988, time 26.990163803100586, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
goal_identified
=== ep: 989, time 30.034239530563354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
=== ep: 990, time 26.82551670074463, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
goal_identified
=== ep: 991, time 26.840686321258545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
goal_identified
goal_identified
=== ep: 992, time 27.114124059677124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
goal_identified
=== ep: 993, time 26.059948682785034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
=== ep: 994, time 27.836612701416016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
goal_identified
=== ep: 995, time 26.660381317138672, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
=== ep: 996, time 27.105592727661133, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
=== ep: 997, time 26.525631427764893, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
goal_identified
goal_identified
=== ep: 998, time 26.786913871765137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
goal_identified
=== ep: 999, time 30.409531593322754, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
=== ep: 1000, time 27.493175268173218, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
goal_identified
=== ep: 1001, time 26.593390703201294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
=== ep: 1002, time 26.493491411209106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
=== ep: 1003, time 26.995970726013184, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
goal_identified
goal_identified
=== ep: 1004, time 26.53502893447876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
=== ep: 1005, time 26.702316522598267, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
goal_identified
=== ep: 1006, time 26.468981504440308, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
goal_identified
goal_identified
=== ep: 1007, time 26.885281562805176, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
goal_identified
=== ep: 1008, time 26.257487058639526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
goal_identified
=== ep: 1009, time 29.31752872467041, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
=== ep: 1010, time 26.975468158721924, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1011, time 26.650222778320312, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
=== ep: 1012, time 26.623400449752808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
=== ep: 1013, time 26.706242322921753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
goal_identified
=== ep: 1014, time 26.704559564590454, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
goal_identified
=== ep: 1015, time 26.805033445358276, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
