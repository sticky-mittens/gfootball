==> Playing in 11_vs_11_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 6 layers and 500 hidden units.
=== ep: 0, time 28.393404006958008, eps 0.9, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
goal_identified
=== ep: 1, time 27.18098783493042, eps 0.8561552526261419, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
=== ep: 2, time 27.4925057888031, eps 0.8144488388143276, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3, time 27.226546049118042, eps 0.774776470806127, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
goal_identified
=== ep: 4, time 27.526366472244263, eps 0.7370389470171057, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 5, time 27.569098711013794, eps 0.701141903981193, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
goal_identified
goal_identified
=== ep: 6, time 27.29920506477356, eps 0.6669955803928644, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
goal_identified
goal_identified
goal_identified
=== ep: 7, time 27.907525062561035, eps 0.6345145926571234, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
=== ep: 8, time 27.619938135147095, eps 0.6036177213860398, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
goal_identified
=== ep: 9, time 31.148282527923584, eps 0.5742277083079742, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
goal_identified
goal_identified
=== ep: 10, time 27.984530925750732, eps 0.5462710630816575, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
=== ep: 11, time 27.374605655670166, eps 0.5196778795320575, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 12, time 28.25569200515747, eps 0.49438166084852986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
=== ep: 13, time 27.883089780807495, eps 0.47031915330815344, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
goal_identified
goal_identified
=== ep: 14, time 29.10679864883423, eps 0.4474301881084772, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 15, time 27.839609622955322, eps 0.42565753091417224, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
=== ep: 16, time 27.60399556159973, eps 0.4049467387413822, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
goal_identified
goal_identified
=== ep: 17, time 27.960622549057007, eps 0.3852460238219053, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 18, time 28.051475524902344, eps 0.3665061241067986, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
=== ep: 19, time 30.887924671173096, eps 0.3486801800855966, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
goal_identified
goal_identified
=== ep: 20, time 28.114898443222046, eps 0.3317236176131267, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
goal_identified
goal_identified
=== ep: 21, time 28.449381351470947, eps 0.31559403645092865, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
=== ep: 22, time 27.75817084312439, eps 0.3002511042445735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 23, time 27.940147638320923, eps 0.2856564556717689, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
goal_identified
goal_identified
=== ep: 24, time 28.11314845085144, eps 0.27177359650906974, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 25, time 28.262264251708984, eps 0.2585678123773109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 26, time 28.035036087036133, eps 0.24600608193757734, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 27, time 28.25263476371765, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
=== ep: 28, time 27.859258890151978, eps 0.22269067058350425, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
goal_identified
=== ep: 29, time 30.755917072296143, eps 0.2118786889963241, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 30, time 28.366296768188477, eps 0.2015940139734384, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
goal_identified
=== ep: 31, time 27.79565191268921, eps 0.191810928470242, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
goal_identified
goal_identified
=== ep: 32, time 28.212968111038208, eps 0.1825049696771952, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 33, time 27.855902433395386, eps 0.17365286785005798, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
=== ep: 34, time 28.048938989639282, eps 0.16523248812340846, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
goal_identified
goal_identified
goal_identified
=== ep: 35, time 28.017022132873535, eps 0.15722277516195018, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
goal_identified
goal_identified
=== ep: 36, time 28.086933374404907, eps 0.1496037005112063, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 37, time 28.48131275177002, eps 0.14235621251595124, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 38, time 27.874882459640503, eps 0.13546218868114893, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 39, time 32.095335960388184, eps 0.1289043903562757, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
goal_identified
goal_identified
goal_identified
=== ep: 40, time 27.947032928466797, eps 0.12266641962971482, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
goal_identified
goal_identified
=== ep: 41, time 27.967201948165894, eps 0.116732678325436, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 42, time 28.20129632949829, eps 0.11108832899943073, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
goal_identified
goal_identified
=== ep: 43, time 27.86738419532776, eps 0.10571925783837377, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 44, time 28.350956678390503, eps 0.10061203936773815, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
goal_identified
=== ep: 45, time 27.90394377708435, eps 0.09575390288111604, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
goal_identified
=== ep: 46, time 27.829437732696533, eps 0.09113270050680057, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
goal_identified
goal_identified
=== ep: 47, time 27.84165048599243, eps 0.08673687683177911, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 48, time 28.164893627166748, eps 0.08255544000718185, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
goal_identified
goal_identified
goal_identified
=== ep: 49, time 32.100353956222534, eps 0.07857793426293408, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 50, time 27.96332097053528, eps 0.07479441376288502, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
goal_identified
goal_identified
=== ep: 51, time 28.1500186920166, eps 0.0711954177350367, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
goal_identified
goal_identified
=== ep: 52, time 28.53772521018982, eps 0.06777194681468615, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 53, time 27.986438035964966, eps 0.06451544054132621, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
goal_identified
goal_identified
=== ep: 54, time 28.26213049888611, eps 0.06141775595303503, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 28.482574701309204, eps 0.05847114722483011, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
goal_identified
goal_identified
=== ep: 56, time 28.391367197036743, eps 0.05566824630007096, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
goal_identified
goal_identified
=== ep: 57, time 28.23514413833618, eps 0.05300204446647978, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 26.93209409713745, eps 0.050465874830710106, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
goal_identified
goal_identified
=== ep: 59, time 29.671793699264526, eps 0.04805339564764071, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
goal_identified
goal_identified
=== ep: 60, time 24.91879415512085, eps 0.045758574462709686, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 25.2204008102417, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 62, time 24.763123035430908, eps 0.04149923295180846, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
goal_identified
goal_identified
goal_identified
=== ep: 63, time 24.84370446205139, eps 0.03952406205346913, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
=== ep: 64, time 25.17448091506958, eps 0.03764522137655123, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 25.189470529556274, eps 0.03585801284071809, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
goal_identified
goal_identified
=== ep: 66, time 25.16225528717041, eps 0.034157967493714775, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
goal_identified
goal_identified
goal_identified
=== ep: 67, time 25.00977849960327, eps 0.03254083433665968, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 68, time 24.866631269454956, eps 0.031002569694333147, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
goal_identified
=== ep: 69, time 28.335232496261597, eps 0.02953932710388308, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 70, time 25.136263608932495, eps 0.028147447696664333, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 25.102649211883545, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
goal_identified
goal_identified
=== ep: 72, time 25.134284019470215, eps 0.025564026480116013, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
goal_identified
goal_identified
goal_identified
=== ep: 73, time 24.61186170578003, eps 0.02436602477210106, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
goal_identified
goal_identified
=== ep: 74, time 24.155211448669434, eps 0.02322645029683511, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 75, time 24.142627000808716, eps 0.02214245352455219, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
goal_identified
=== ep: 76, time 24.468865394592285, eps 0.02111132389869288, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
goal_identified
goal_identified
goal_identified
=== ep: 77, time 24.21808934211731, eps 0.020130483058101077, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 24.597777605056763, eps 0.019197478389778148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
goal_identified
goal_identified
=== ep: 79, time 27.80849862098694, eps 0.018309976896072843, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
goal_identified
goal_identified
goal_identified
=== ep: 80, time 24.339054822921753, eps 0.017465759360972027, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
=== ep: 81, time 24.31183886528015, eps 0.01666271480090467, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
goal_identified
=== ep: 82, time 24.632192134857178, eps 0.015898835186183367, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
goal_identified
goal_identified
=== ep: 83, time 24.120375871658325, eps 0.015172210419884185, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 84, time 24.32235360145569, eps 0.014481023561609456, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
goal_identified
goal_identified
=== ep: 85, time 24.2977933883667, eps 0.01382354628419033, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 86, time 24.289738655090332, eps 0.013198134551968641, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 87, time 24.210715293884277, eps 0.012603224509851407, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
goal_identified
goal_identified
=== ep: 88, time 24.4073383808136, eps 0.012037328572858524, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
goal_identified
=== ep: 89, time 28.001660108566284, eps 0.011499031706385502, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
goal_identified
goal_identified
=== ep: 90, time 24.28981924057007, eps 0.010986987887879832, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 91, time 24.30700945854187, eps 0.010499916741083536, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
goal_identified
=== ep: 92, time 24.423874139785767, eps 0.010036600334425595, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 24.265122890472412, eps 0.00959588013555861, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
=== ep: 94, time 24.377684354782104, eps 0.009176654114424539, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
goal_identified
=== ep: 95, time 24.182804822921753, eps 0.00877787398760545, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
goal_identified
goal_identified
goal_identified
=== ep: 96, time 24.804553270339966, eps 0.008398542597069007, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
=== ep: 97, time 24.391701221466064, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
goal_identified
goal_identified
=== ep: 98, time 24.643605947494507, eps 0.00769447818076098, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
goal_identified
goal_identified
goal_identified
=== ep: 99, time 27.682754278182983, eps 0.007367984627217855, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 100, time 24.366698026657104, eps 0.007057414352177835, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 101, time 24.529487133026123, eps 0.006761990768184489, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 102, time 24.37027096748352, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
goal_identified
goal_identified
=== ep: 103, time 24.504070281982422, eps 0.006213664849431085, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
goal_identified
goal_identified
=== ep: 104, time 24.62765145301819, eps 0.005959391414263934, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
goal_identified
goal_identified
=== ep: 105, time 24.35809350013733, eps 0.005717519040864065, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
=== ep: 106, time 24.5696222782135, eps 0.005487442922312285, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 107, time 24.32008147239685, eps 0.005268587748470919, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 108, time 24.415218353271484, eps 0.005060406267408787, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
=== ep: 109, time 28.18572449684143, eps 0.004862377916986354, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
goal_identified
goal_identified
goal_identified
=== ep: 110, time 24.380649089813232, eps 0.004674007523179196, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 111, time 24.786158084869385, eps 0.004494824061885041, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 24.40643072128296, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 24.644598245620728, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 24.70851492881775, eps 0.0040080229462666735, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 24.53034472465515, eps 0.0038613199360621906, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
goal_identified
goal_identified
goal_identified
=== ep: 116, time 24.459005117416382, eps 0.003721771716092858, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
goal_identified
goal_identified
=== ep: 117, time 24.323870182037354, eps 0.0035890293431213305, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 118, time 24.451738357543945, eps 0.0034627608920727634, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
goal_identified
goal_identified
goal_identified
=== ep: 119, time 28.50676417350769, eps 0.00334265062604924, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
goal_identified
=== ep: 120, time 24.795626401901245, eps 0.0032283982068230565, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 121, time 24.42640972137451, eps 0.0031197179438347193, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
goal_identified
goal_identified
=== ep: 122, time 24.256325721740723, eps 0.0030163380798177374, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 123, time 24.555371522903442, eps 0.0029180001112638996, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 24.6983380317688, eps 0.002824458142029865, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
goal_identified
goal_identified
=== ep: 125, time 24.7520751953125, eps 0.0027354782684687108, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
goal_identified
=== ep: 126, time 24.70738697052002, eps 0.0026508379945489875, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 127, time 24.27167534828186, eps 0.0025703256754987464, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
goal_identified
goal_identified
=== ep: 128, time 24.624160289764404, eps 0.0024937399885833667, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 129, time 28.262468338012695, eps 0.0024208894296938593, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 130, time 24.694583654403687, eps 0.0023515918344868374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
goal_identified
goal_identified
goal_identified
=== ep: 131, time 24.612908124923706, eps 0.002285673922878779, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
goal_identified
=== ep: 132, time 24.49873375892639, eps 0.0022229708657555565, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
goal_identified
goal_identified
goal_identified
=== ep: 133, time 24.559894561767578, eps 0.0021633258728137976, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 24.598472356796265, eps 0.0021065898005034594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
goal_identified
goal_identified
goal_identified
=== ep: 135, time 24.871794939041138, eps 0.002052620779091266, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
goal_identified
=== ep: 136, time 24.148821592330933, eps 0.0020012838579124784, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
goal_identified
goal_identified
goal_identified
=== ep: 137, time 24.465641021728516, eps 0.0019524506679239415, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
goal_identified
goal_identified
=== ep: 138, time 24.73014807701111, eps 0.001905999100714611, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
goal_identified
=== ep: 139, time 27.993102312088013, eps 0.001861813003170924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 24.615694284439087, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 24.764749765396118, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 24.769145250320435, eps 0.0017417693260160481, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
goal_identified
goal_identified
=== ep: 143, time 24.497312545776367, eps 0.0017055928090985275, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
goal_identified
goal_identified
=== ep: 144, time 24.281595706939697, eps 0.0016711806417306348, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
goal_identified
goal_identified
goal_identified
=== ep: 145, time 24.49502921104431, eps 0.0016384467755694515, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 24.55823302268982, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 24.362063884735107, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 148, time 24.70513105392456, eps 0.0015495162322554856, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 28.54938578605652, eps 0.0015227160093621863, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
goal_identified
goal_identified
goal_identified
=== ep: 150, time 24.68184542655945, eps 0.0014972228487629025, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
goal_identified
goal_identified
=== ep: 151, time 24.53522300720215, eps 0.0014729730042773413, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
goal_identified
goal_identified
goal_identified
=== ep: 152, time 24.752132177352905, eps 0.001449905838663109, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
goal_identified
goal_identified
=== ep: 153, time 25.05526566505432, eps 0.00142796367199102, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 154, time 24.622727870941162, eps 0.0014070916374152305, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
goal_identified
=== ep: 155, time 24.451793432235718, eps 0.001387237543977543, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
=== ep: 156, time 24.756911516189575, eps 0.0013683517461028282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 24.735763788223267, eps 0.0013503870194592265, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 24.631016969680786, eps 0.0013332984428727204, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
goal_identified
goal_identified
=== ep: 159, time 28.336990356445312, eps 0.001317043286000802, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 160, time 24.547343254089355, eps 0.0013015809024843582, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 24.340052127838135, eps 0.0012868726283106018, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
goal_identified
=== ep: 162, time 24.327282905578613, eps 0.0012728816851329014, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
goal_identified
goal_identified
=== ep: 163, time 23.599149703979492, eps 0.0012595730883057546, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
goal_identified
goal_identified
=== ep: 164, time 23.676485538482666, eps 0.001246913559404956, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
=== ep: 165, time 23.97232174873352, eps 0.0012348714430141991, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
=== ep: 166, time 23.817806005477905, eps 0.0012234166275700486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
goal_identified
=== ep: 167, time 23.69249153137207, eps 0.001212520470067348, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 168, time 23.826248168945312, eps 0.0012021557244367845, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
goal_identified
goal_identified
=== ep: 169, time 27.814305543899536, eps 0.0011922964734155277, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
goal_identified
goal_identified
goal_identified
=== ep: 170, time 24.059635639190674, eps 0.001182918063740569, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 23.698578596115112, eps 0.0011739970445027263, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
goal_identified
goal_identified
=== ep: 172, time 23.70470404624939, eps 0.0011655111085071537, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 23.745842218399048, eps 0.001157439036493735, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 174, time 23.80812120437622, eps 0.0011497606440778825, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 175, time 23.829282522201538, eps 0.0011424567312790603, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
goal_identified
goal_identified
goal_identified
=== ep: 176, time 23.638171672821045, eps 0.0011355090345108335, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
goal_identified
goal_identified
=== ep: 177, time 24.1456196308136, eps 0.0011289001809123877, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 178, time 24.059677600860596, eps 0.0011226136449073282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
goal_identified
goal_identified
=== ep: 179, time 28.05097985267639, eps 0.001116633706881133, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
goal_identified
=== ep: 180, time 23.812239408493042, eps 0.001110945413873925, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
goal_identified
=== ep: 181, time 23.94700026512146, eps 0.001105534542190287, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 182, time 23.795701026916504, eps 0.0011003875618326132, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 23.96791934967041, eps 0.0010954916026690664, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
goal_identified
=== ep: 184, time 24.02656102180481, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 185, time 23.95394015312195, eps 0.0010864043752031938, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
=== ep: 186, time 23.905067205429077, eps 0.0010821903840988777, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
goal_identified
=== ep: 187, time 23.80643606185913, eps 0.0010781819117658682, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
=== ep: 188, time 23.906139612197876, eps 0.0010743689349354123, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 27.840527772903442, eps 0.0010707419191793434, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
goal_identified
goal_identified
=== ep: 190, time 24.07868480682373, eps 0.0010672917950690429, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
goal_identified
goal_identified
goal_identified
=== ep: 191, time 23.762587308883667, eps 0.0010640099354971456, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
goal_identified
=== ep: 192, time 23.960763931274414, eps 0.0010608881341052777, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 193, time 24.016332626342773, eps 0.0010579185847638855, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
=== ep: 194, time 23.747379541397095, eps 0.0010550938620528466, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
goal_identified
goal_identified
goal_identified
=== ep: 195, time 23.85891056060791, eps 0.001052406902694051, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
goal_identified
goal_identified
goal_identified
=== ep: 196, time 23.938369035720825, eps 0.001049850987889527, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
goal_identified
goal_identified
goal_identified
=== ep: 197, time 24.04586386680603, eps 0.0010474197265209469, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 198, time 23.88563561439514, eps 0.0010451070391685015, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 199, time 27.99445104598999, eps 0.001042907142909185, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 24.192753791809082, eps 0.001040814536856474, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 201, time 23.87370276451111, eps 0.0010388239884052469, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
goal_identified
goal_identified
=== ep: 202, time 24.036308526992798, eps 0.0010369305201475454, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
goal_identified
=== ep: 203, time 24.03220820426941, eps 0.0010351293974264616, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
goal_identified
=== ep: 204, time 23.938090085983276, eps 0.00103341611649703, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 24.18714928627014, eps 0.0010317863932645186, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
goal_identified
goal_identified
=== ep: 206, time 24.22532892227173, eps 0.0010302361525719613, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
goal_identified
goal_identified
=== ep: 207, time 24.115052223205566, eps 0.0010287615180101426, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
goal_identified
goal_identified
goal_identified
=== ep: 208, time 23.890828371047974, eps 0.001027358802224555, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
goal_identified
goal_identified
=== ep: 209, time 27.952464818954468, eps 0.0010260244976950921, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 210, time 24.27985668182373, eps 0.0010247552679654227, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
goal_identified
goal_identified
goal_identified
=== ep: 211, time 23.89245295524597, eps 0.00102354793930011, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 212, time 24.066450834274292, eps 0.0010223994927486214, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
goal_identified
=== ep: 213, time 24.171168565750122, eps 0.001021307056596379, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 214, time 23.694919109344482, eps 0.0010202678991839778, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 215, time 24.172805070877075, eps 0.0010192794220766138, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
goal_identified
goal_identified
=== ep: 216, time 24.237043142318726, eps 0.0010183391535666436, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
goal_identified
goal_identified
=== ep: 217, time 24.185222148895264, eps 0.0010174447424930286, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 138/138)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
goal_identified
goal_identified
=== ep: 218, time 24.069626331329346, eps 0.0010165939523622068, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
goal_identified
goal_identified
goal_identified
=== ep: 219, time 28.720070600509644, eps 0.0010157846557556941, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
goal_identified
goal_identified
goal_identified
=== ep: 220, time 24.22458004951477, eps 0.001015014829010431, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
=== ep: 221, time 24.302180767059326, eps 0.0010142825471585687, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
goal_identified
=== ep: 222, time 25.327011823654175, eps 0.0010135859791140496, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
goal_identified
goal_identified
goal_identified
=== ep: 223, time 23.886675596237183, eps 0.0010129233830939361, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
goal_identified
goal_identified
goal_identified
=== ep: 224, time 24.095296382904053, eps 0.0010122931022630473, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
goal_identified
goal_identified
=== ep: 225, time 23.839400053024292, eps 0.001011693560591007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
goal_identified
goal_identified
=== ep: 226, time 24.25956082344055, eps 0.0010111232589113477, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
goal_identified
=== ep: 227, time 23.881603479385376, eps 0.0010105807711728136, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 228, time 24.301563501358032, eps 0.0010100647408734893, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
goal_identified
=== ep: 229, time 29.01423740386963, eps 0.001009573877668838, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
goal_identified
=== ep: 230, time 25.168652296066284, eps 0.001009106954145169, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
goal_identified
=== ep: 231, time 24.166043996810913, eps 0.0010086628027504636, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 23.81898808479309, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
goal_identified
goal_identified
goal_identified
=== ep: 233, time 24.253275156021118, eps 0.0010078384280736842, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
goal_identified
goal_identified
goal_identified
=== ep: 234, time 24.426170349121094, eps 0.001007456143425521, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
goal_identified
goal_identified
=== ep: 235, time 24.33358669281006, eps 0.001007092503019653, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
goal_identified
=== ep: 236, time 23.889812707901, eps 0.001006746597565654, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
goal_identified
goal_identified
=== ep: 237, time 24.31087827682495, eps 0.001006417562119715, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 238, time 24.874589920043945, eps 0.0010061045739218342, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 239, time 28.617988109588623, eps 0.0010058068503384884, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
goal_identified
goal_identified
=== ep: 240, time 24.452017068862915, eps 0.001005523646905642, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
=== ep: 241, time 24.221099138259888, eps 0.001005254255467199, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 24.33954167366028, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
goal_identified
goal_identified
=== ep: 243, time 24.010745763778687, eps 0.0010047542469506416, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
goal_identified
goal_identified
=== ep: 244, time 24.44542145729065, eps 0.0010045223795907931, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
goal_identified
=== ep: 245, time 23.8769690990448, eps 0.001004301820535524, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 24.360504150390625, eps 0.0010040920182723119, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
goal_identified
goal_identified
=== ep: 247, time 23.80992865562439, eps 0.0010038924481862177, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
=== ep: 248, time 24.462658166885376, eps 0.0010037026112480747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
goal_identified
=== ep: 249, time 28.791072368621826, eps 0.0010035220327666559, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
goal_identified
goal_identified
=== ep: 250, time 24.28706407546997, eps 0.0010033502612016988, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
goal_identified
=== ep: 251, time 24.636951446533203, eps 0.001003186867034819, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 252, time 24.045114755630493, eps 0.001003031441695491, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
goal_identified
=== ep: 253, time 24.359093189239502, eps 0.0010028835965394094, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
goal_identified
goal_identified
=== ep: 254, time 24.36931085586548, eps 0.0010027429618766747, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
goal_identified
=== ep: 255, time 24.49899959564209, eps 0.0010026091860473767, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 24.383558988571167, eps 0.0010024819345422614, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
goal_identified
goal_identified
=== ep: 257, time 24.41787838935852, eps 0.0010023608891662839, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
goal_identified
=== ep: 258, time 24.145063400268555, eps 0.001002245747242954, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
goal_identified
=== ep: 259, time 28.467209339141846, eps 0.0010021362208574892, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 24.51915192604065, eps 0.001002032036136876, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
goal_identified
goal_identified
=== ep: 261, time 24.13941264152527, eps 0.0010019329325650452, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 262, time 24.545098066329956, eps 0.0010018386623314465, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
goal_identified
=== ep: 263, time 24.47905707359314, eps 0.0010017489897113931, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
goal_identified
goal_identified
goal_identified
=== ep: 264, time 24.105502367019653, eps 0.0010016636904766263, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
goal_identified
goal_identified
=== ep: 265, time 24.42310929298401, eps 0.0010015825513346283, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
goal_identified
=== ep: 266, time 24.42258381843567, eps 0.0010015053693952815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
=== ep: 267, time 24.213634252548218, eps 0.0010014319516635345, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 24.490089893341064, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
goal_identified
goal_identified
goal_identified
=== ep: 269, time 29.426878213882446, eps 0.0010012956834459848, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 24.446948289871216, eps 0.0010012324922186594, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
goal_identified
goal_identified
=== ep: 271, time 24.35087513923645, eps 0.001001172382863857, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
=== ep: 272, time 24.39938735961914, eps 0.0010011152050768812, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
goal_identified
goal_identified
=== ep: 273, time 24.256877183914185, eps 0.0010010608158834819, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
goal_identified
=== ep: 274, time 24.20729398727417, eps 0.0010010090792823456, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
goal_identified
goal_identified
=== ep: 275, time 24.563782691955566, eps 0.0010009598659050213, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
goal_identified
goal_identified
=== ep: 276, time 23.931020259857178, eps 0.0010009130526924313, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 24.545636653900146, eps 0.0010008685225871602, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 24.37614417076111, eps 0.0010008261642407504, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 30.822784900665283, eps 0.001000785871735272, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
goal_identified
goal_identified
=== ep: 280, time 24.330833435058594, eps 0.0010007475443184742, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
goal_identified
goal_identified
goal_identified
=== ep: 281, time 24.175153017044067, eps 0.001000711086151851, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
goal_identified
goal_identified
=== ep: 282, time 24.357349395751953, eps 0.0010006764060709957, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 283, time 24.195462942123413, eps 0.001000643417357642, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
goal_identified
goal_identified
=== ep: 284, time 24.41872763633728, eps 0.0010006120375228235, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
goal_identified
=== ep: 285, time 24.423744916915894, eps 0.0010005821881006083, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
goal_identified
goal_identified
goal_identified
=== ep: 286, time 24.31710195541382, eps 0.0010005537944518927, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 24.300355195999146, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 288, time 24.35414409637451, eps 0.0010005010939419733, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
goal_identified
goal_identified
goal_identified
=== ep: 289, time 28.466035842895508, eps 0.001000476655302044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
=== ep: 290, time 24.524776935577393, eps 0.0010004534085486486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 291, time 24.404781818389893, eps 0.0010004312955527947, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 292, time 24.510531663894653, eps 0.0010004102610204745, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
goal_identified
goal_identified
=== ep: 293, time 24.231531143188477, eps 0.0010003902523544011, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 294, time 24.537025690078735, eps 0.0010003712195224871, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
goal_identified
goal_identified
=== ep: 295, time 24.05108952522278, eps 0.0010003531149327387, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
goal_identified
=== ep: 296, time 24.494813442230225, eps 0.0010003358933142518, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
goal_identified
goal_identified
=== ep: 297, time 24.433644771575928, eps 0.0010003195116040093, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
goal_identified
goal_identified
=== ep: 298, time 24.085209369659424, eps 0.0010003039288392032, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
goal_identified
=== ep: 299, time 28.975472450256348, eps 0.0010002891060548044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
goal_identified
=== ep: 300, time 24.44253921508789, eps 0.0010002750061861312, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
goal_identified
goal_identified
=== ep: 301, time 24.447030544281006, eps 0.0010002615939761676, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
=== ep: 302, time 24.60708785057068, eps 0.001000248835887403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
goal_identified
goal_identified
=== ep: 303, time 24.4323251247406, eps 0.0010002367000179694, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 304, time 24.146424293518066, eps 0.0010002251560218723, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
goal_identified
=== ep: 305, time 24.361051082611084, eps 0.0010002141750331084, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
goal_identified
=== ep: 306, time 26.904120206832886, eps 0.0010002037295934862, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
goal_identified
=== ep: 307, time 24.233012199401855, eps 0.0010001937935839656, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 24.494613885879517, eps 0.0010001843421593476, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 29.007673501968384, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
goal_identified
goal_identified
goal_identified
=== ep: 310, time 24.396719217300415, eps 0.0010001667996834991, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 311, time 24.724504470825195, eps 0.001000158664766942, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
goal_identified
goal_identified
=== ep: 312, time 24.592868089675903, eps 0.0010001509265949466, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
=== ep: 313, time 24.441152095794678, eps 0.001000143565818053, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 314, time 24.437159776687622, eps 0.0010001365640304844, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
goal_identified
=== ep: 315, time 24.40264868736267, eps 0.0010001299037241253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
goal_identified
=== ep: 316, time 24.287880659103394, eps 0.0010001235682447402, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
goal_identified
goal_identified
=== ep: 317, time 24.54861855506897, eps 0.0010001175417503308, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
goal_identified
goal_identified
goal_identified
=== ep: 318, time 24.485332250595093, eps 0.0010001118091715218, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
goal_identified
goal_identified
goal_identified
=== ep: 319, time 28.78767228126526, eps 0.0010001063561738807, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
=== ep: 320, time 24.4634690284729, eps 0.0010001011691220727, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
goal_identified
=== ep: 321, time 24.364897966384888, eps 0.0010000962350457665, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
goal_identified
goal_identified
goal_identified
=== ep: 322, time 24.474733352661133, eps 0.0010000915416072012, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
goal_identified
goal_identified
goal_identified
=== ep: 323, time 24.57897710800171, eps 0.0010000870770703358, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
goal_identified
=== ep: 324, time 24.574544429779053, eps 0.0010000828302715028, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
goal_identified
=== ep: 325, time 24.4533908367157, eps 0.0010000787905914928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
goal_identified
=== ep: 326, time 24.58417582511902, eps 0.0010000749479290019, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
goal_identified
goal_identified
=== ep: 327, time 24.547484874725342, eps 0.001000071292675372, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 24.93397068977356, eps 0.001000067815690565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
goal_identified
goal_identified
goal_identified
=== ep: 329, time 28.907354831695557, eps 0.0010000645082803084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
goal_identified
goal_identified
=== ep: 330, time 25.846259355545044, eps 0.0010000613621743532, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
goal_identified
goal_identified
=== ep: 331, time 24.093807697296143, eps 0.0010000583695057963, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
goal_identified
=== ep: 332, time 24.4436252117157, eps 0.0010000555227914069, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 24.83126425743103, eps 0.0010000528149129166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
goal_identified
=== ep: 334, time 24.69587254524231, eps 0.0010000502390992187, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 335, time 24.783957958221436, eps 0.0010000477889094373, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 336, time 24.643242359161377, eps 0.0010000454582168217, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 24.308838367462158, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
goal_identified
goal_identified
goal_identified
=== ep: 338, time 24.878801345825195, eps 0.0010000411322955373, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
=== ep: 339, time 28.8086519241333, eps 0.0010000391262498123, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
goal_identified
=== ep: 340, time 24.150477170944214, eps 0.001000037218040092, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 341, time 24.694110870361328, eps 0.0010000354028948577, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
goal_identified
=== ep: 342, time 24.94568657875061, eps 0.0010000336762753012, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 343, time 24.48242974281311, eps 0.001000032033863974, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
goal_identified
goal_identified
goal_identified
=== ep: 344, time 24.68052053451538, eps 0.0010000304715539925, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
goal_identified
=== ep: 345, time 24.679296731948853, eps 0.001000028985438768, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
goal_identified
=== ep: 346, time 24.285640001296997, eps 0.001000027571802238, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 24.431593894958496, eps 0.0010000262271095755, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
goal_identified
goal_identified
goal_identified
=== ep: 348, time 24.32342219352722, eps 0.0010000249479983478, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
goal_identified
=== ep: 349, time 28.69107723236084, eps 0.0010000237312701107, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 24.4874050617218, eps 0.00100002257388241, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
goal_identified
goal_identified
=== ep: 351, time 24.299479246139526, eps 0.0010000214729411737, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
goal_identified
goal_identified
=== ep: 352, time 24.494733095169067, eps 0.0010000204256934752, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 25.141175031661987, eps 0.0010000194295206493, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
goal_identified
goal_identified
goal_identified
=== ep: 354, time 24.307734727859497, eps 0.0010000184819317455, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
goal_identified
=== ep: 355, time 24.38931655883789, eps 0.001000017580557298, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
goal_identified
goal_identified
=== ep: 356, time 24.644523859024048, eps 0.001000016723143401, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
goal_identified
=== ep: 357, time 24.36844778060913, eps 0.0010000159075460732, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
goal_identified
goal_identified
goal_identified
=== ep: 358, time 24.853322982788086, eps 0.0010000151317258964, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 29.1516010761261, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
goal_identified
=== ep: 360, time 24.498915672302246, eps 0.0010000136917517905, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
goal_identified
goal_identified
=== ep: 361, time 24.748701095581055, eps 0.001000013023997176, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 24.46885895729065, eps 0.0010000123888093385, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
goal_identified
goal_identified
=== ep: 363, time 24.895963191986084, eps 0.0010000117845999773, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 24.76876711845398, eps 0.0010000112098582543, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
goal_identified
goal_identified
=== ep: 365, time 24.42991065979004, eps 0.001000010663147016, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 366, time 24.503431797027588, eps 0.0010000101430991996, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
goal_identified
goal_identified
=== ep: 367, time 24.32785153388977, eps 0.0010000096484144142, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
goal_identified
=== ep: 368, time 24.652931213378906, eps 0.0010000091778556905, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 28.386542320251465, eps 0.0010000087302463867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
goal_identified
=== ep: 370, time 24.577823638916016, eps 0.001000008304467246, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 25.09799361228943, eps 0.0010000078994535993, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 24.77302074432373, eps 0.0010000075141927012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
=== ep: 373, time 24.606263637542725, eps 0.0010000071477211988, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
goal_identified
=== ep: 374, time 24.733468055725098, eps 0.0010000067991227223, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
=== ep: 375, time 24.64300012588501, eps 0.0010000064675255943, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 376, time 24.593826293945312, eps 0.001000006152100649, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
goal_identified
=== ep: 377, time 25.082473278045654, eps 0.0010000058520591598, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
goal_identified
goal_identified
goal_identified
=== ep: 378, time 24.3866229057312, eps 0.0010000055666508666, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
goal_identified
goal_identified
=== ep: 379, time 28.995660543441772, eps 0.0010000052951621003, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
goal_identified
=== ep: 380, time 24.57707715034485, eps 0.0010000050369139975, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
goal_identified
goal_identified
=== ep: 381, time 24.7948579788208, eps 0.001000004791260803, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
goal_identified
=== ep: 382, time 24.710469722747803, eps 0.0010000045575882562, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
goal_identified
goal_identified
=== ep: 383, time 24.66277503967285, eps 0.001000004335312054, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
goal_identified
=== ep: 384, time 24.92424201965332, eps 0.0010000041238763903, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
goal_identified
goal_identified
=== ep: 385, time 24.736618280410767, eps 0.0010000039227525655, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 386, time 24.96313786506653, eps 0.0010000037314376652, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 24.837352991104126, eps 0.001000003549453303, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 388, time 24.71781086921692, eps 0.0010000033763444226, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 29.09063982963562, eps 0.001000003211678162, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 390, time 24.611174821853638, eps 0.0010000030550427698, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
=== ep: 391, time 25.01568627357483, eps 0.0010000029060465757, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
goal_identified
goal_identified
=== ep: 392, time 24.97546696662903, eps 0.0010000027643170119, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 393, time 24.89292001724243, eps 0.0010000026294996803, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
goal_identified
=== ep: 394, time 24.69022297859192, eps 0.0010000025012574677, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
goal_identified
=== ep: 395, time 24.692808151245117, eps 0.0010000023792697014, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
goal_identified
goal_identified
goal_identified
=== ep: 396, time 24.808334827423096, eps 0.0010000022632313489, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 24.650243520736694, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 24.803693532943726, eps 0.00100000204785641, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
goal_identified
goal_identified
goal_identified
=== ep: 399, time 29.23522663116455, eps 0.0010000019479812744, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
goal_identified
goal_identified
goal_identified
=== ep: 400, time 24.55421280860901, eps 0.0010000018529771066, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 401, time 24.874018907546997, eps 0.0010000017626063467, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 402, time 24.82191777229309, eps 0.0010000016766430208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
goal_identified
goal_identified
goal_identified
=== ep: 403, time 24.592710971832275, eps 0.0010000015948721758, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 24.671202898025513, eps 0.001000001517089342, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
goal_identified
=== ep: 405, time 24.989694356918335, eps 0.0010000014431000217, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
goal_identified
goal_identified
goal_identified
=== ep: 406, time 25.15034818649292, eps 0.001000001372719203, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
goal_identified
goal_identified
goal_identified
=== ep: 407, time 24.568742275238037, eps 0.0010000013057708975, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
goal_identified
goal_identified
goal_identified
=== ep: 408, time 25.052298307418823, eps 0.0010000012420876994, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
goal_identified
goal_identified
goal_identified
=== ep: 409, time 28.458749294281006, eps 0.0010000011815103674, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 410, time 24.90376377105713, eps 0.001000001123887427, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
goal_identified
goal_identified
goal_identified
=== ep: 411, time 24.73306131362915, eps 0.0010000010690747903, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
goal_identified
goal_identified
=== ep: 412, time 24.651326656341553, eps 0.0010000010169353975, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
goal_identified
goal_identified
=== ep: 413, time 24.672804355621338, eps 0.0010000009673388729, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
goal_identified
goal_identified
goal_identified
=== ep: 414, time 24.607861757278442, eps 0.0010000009201611994, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
goal_identified
=== ep: 415, time 24.64140558242798, eps 0.0010000008752844081, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 416, time 24.786873817443848, eps 0.0010000008325962838, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
goal_identified
=== ep: 417, time 24.98940396308899, eps 0.001000000791990084, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
goal_identified
goal_identified
=== ep: 418, time 24.778051137924194, eps 0.0010000007533642718, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
goal_identified
=== ep: 419, time 28.631221532821655, eps 0.0010000007166222626, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
goal_identified
=== ep: 420, time 24.595678329467773, eps 0.0010000006816721825, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 24.844383239746094, eps 0.001000000648426638, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
goal_identified
=== ep: 422, time 24.991076231002808, eps 0.0010000006168024976, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
goal_identified
goal_identified
=== ep: 423, time 24.867944717407227, eps 0.0010000005867206849, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
goal_identified
goal_identified
=== ep: 424, time 24.695815324783325, eps 0.0010000005581059794, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
goal_identified
goal_identified
=== ep: 425, time 24.787537574768066, eps 0.0010000005308868295, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
goal_identified
goal_identified
goal_identified
=== ep: 426, time 24.618736028671265, eps 0.0010000005049951733, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
goal_identified
goal_identified
goal_identified
=== ep: 427, time 24.76033043861389, eps 0.001000000480366268, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
goal_identified
goal_identified
=== ep: 428, time 25.241934537887573, eps 0.0010000004569385287, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
goal_identified
goal_identified
goal_identified
=== ep: 429, time 28.738285779953003, eps 0.0010000004346533736, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
=== ep: 430, time 25.028140544891357, eps 0.0010000004134550786, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 24.68881869316101, eps 0.0010000003932906364, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
goal_identified
goal_identified
goal_identified
=== ep: 432, time 25.013314485549927, eps 0.0010000003741096257, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 25.08332657814026, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 434, time 25.52778458595276, eps 0.0010000003385083878, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
=== ep: 435, time 25.070853233337402, eps 0.001000000321999139, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
goal_identified
=== ep: 436, time 24.705907583236694, eps 0.0010000003062950555, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
goal_identified
goal_identified
goal_identified
=== ep: 437, time 24.681776523590088, eps 0.0010000002913568694, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 24.842251539230347, eps 0.0010000002771472273, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
goal_identified
=== ep: 439, time 28.860584497451782, eps 0.0010000002636305976, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
=== ep: 440, time 25.047388076782227, eps 0.0010000002507731815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
goal_identified
goal_identified
goal_identified
=== ep: 441, time 24.948864459991455, eps 0.0010000002385428292, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 442, time 25.2410831451416, eps 0.0010000002269089582, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
goal_identified
=== ep: 443, time 25.02269434928894, eps 0.0010000002158424776, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
=== ep: 444, time 24.61254620552063, eps 0.0010000002053157158, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
goal_identified
goal_identified
=== ep: 445, time 24.775909185409546, eps 0.0010000001953023503, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
goal_identified
goal_identified
=== ep: 446, time 25.226401329040527, eps 0.001000000185777342, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 447, time 24.88878059387207, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
goal_identified
=== ep: 448, time 24.859484672546387, eps 0.0010000001680982905, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 449, time 29.083026885986328, eps 0.0010000001599000403, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 24.836275100708008, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
goal_identified
=== ep: 451, time 24.982450008392334, eps 0.0010000001446835395, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
goal_identified
=== ep: 452, time 24.9938805103302, eps 0.0010000001376272401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
goal_identified
goal_identified
=== ep: 453, time 25.09268546104431, eps 0.0010000001309150804, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
goal_identified
goal_identified
goal_identified
=== ep: 454, time 24.820563316345215, eps 0.0010000001245302765, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
goal_identified
=== ep: 455, time 25.05767250061035, eps 0.0010000001184568633, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
goal_identified
=== ep: 456, time 25.08678412437439, eps 0.0010000001126796538, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
=== ep: 457, time 25.152708768844604, eps 0.0010000001071842023, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
goal_identified
goal_identified
goal_identified
=== ep: 458, time 25.141924619674683, eps 0.001000000101956767, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
goal_identified
goal_identified
=== ep: 459, time 28.838849306106567, eps 0.001000000096984277, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
=== ep: 460, time 24.937882900238037, eps 0.001000000092254298, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
=== ep: 461, time 25.013870000839233, eps 0.0010000000877550027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
goal_identified
goal_identified
=== ep: 462, time 24.686426877975464, eps 0.0010000000834751407, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
goal_identified
=== ep: 463, time 24.797433137893677, eps 0.00100000007940401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
goal_identified
goal_identified
goal_identified
=== ep: 464, time 25.22262215614319, eps 0.0010000000755314307, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
goal_identified
goal_identified
=== ep: 465, time 25.17223310470581, eps 0.0010000000718477194, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
goal_identified
=== ep: 466, time 24.79666543006897, eps 0.0010000000683436647, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
goal_identified
goal_identified
=== ep: 467, time 24.94630527496338, eps 0.001000000065010505, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
goal_identified
goal_identified
=== ep: 468, time 24.69262433052063, eps 0.0010000000618399052, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 29.250741958618164, eps 0.0010000000588239375, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
goal_identified
goal_identified
=== ep: 470, time 25.12739372253418, eps 0.0010000000559550603, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
goal_identified
=== ep: 471, time 25.21421766281128, eps 0.0010000000532260998, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
goal_identified
=== ep: 472, time 24.913811445236206, eps 0.0010000000506302322, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 473, time 24.811970710754395, eps 0.0010000000481609666, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
=== ep: 474, time 25.054863691329956, eps 0.0010000000458121286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
goal_identified
goal_identified
=== ep: 475, time 25.55562949180603, eps 0.0010000000435778447, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
goal_identified
=== ep: 476, time 25.142112970352173, eps 0.001000000041452528, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
goal_identified
goal_identified
=== ep: 477, time 24.96855926513672, eps 0.0010000000394308644, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 24.657832145690918, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
goal_identified
goal_identified
=== ep: 479, time 28.500494956970215, eps 0.0010000000356785216, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 480, time 25.020503997802734, eps 0.0010000000339384595, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 25.165910243988037, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
goal_identified
goal_identified
=== ep: 482, time 24.95568609237671, eps 0.0010000000307087882, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 25.00691294670105, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 25.339219570159912, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
goal_identified
goal_identified
=== ep: 485, time 24.942660331726074, eps 0.0010000000264312988, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
goal_identified
goal_identified
=== ep: 486, time 24.92411160469055, eps 0.0010000000251422292, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 24.27539587020874, eps 0.0010000000239160282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 488, time 25.02512502670288, eps 0.00100000002274963, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 489, time 28.620580673217773, eps 0.0010000000216401172, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
goal_identified
=== ep: 490, time 24.85340714454651, eps 0.0010000000205847162, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
goal_identified
goal_identified
goal_identified
=== ep: 491, time 24.767879486083984, eps 0.0010000000195807877, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
goal_identified
goal_identified
=== ep: 492, time 24.633803129196167, eps 0.0010000000186258216, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 24.683074235916138, eps 0.0010000000177174295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
goal_identified
goal_identified
goal_identified
=== ep: 494, time 24.683468341827393, eps 0.0010000000168533404, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
=== ep: 495, time 24.79325294494629, eps 0.0010000000160313932, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 24.82979106903076, eps 0.001000000015249533, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
goal_identified
goal_identified
goal_identified
=== ep: 497, time 24.65890073776245, eps 0.0010000000145058043, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
goal_identified
goal_identified
=== ep: 498, time 24.40105628967285, eps 0.001000000013798348, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
goal_identified
=== ep: 499, time 28.80338215827942, eps 0.0010000000131253947, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
goal_identified
=== ep: 500, time 25.15511679649353, eps 0.0010000000124852615, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
goal_identified
=== ep: 501, time 24.558069467544556, eps 0.0010000000118763482, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
goal_identified
=== ep: 502, time 24.725549936294556, eps 0.0010000000112971319, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
goal_identified
goal_identified
=== ep: 503, time 24.624537229537964, eps 0.0010000000107461642, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 24.702094554901123, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
goal_identified
goal_identified
=== ep: 505, time 24.94586706161499, eps 0.0010000000097235315, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
goal_identified
=== ep: 506, time 24.70805335044861, eps 0.0010000000092493092, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
goal_identified
=== ep: 507, time 24.82314133644104, eps 0.0010000000087982152, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
goal_identified
goal_identified
=== ep: 508, time 24.808887004852295, eps 0.0010000000083691212, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 509, time 28.832489728927612, eps 0.0010000000079609542, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
goal_identified
=== ep: 510, time 24.81084179878235, eps 0.001000000007572694, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
goal_identified
goal_identified
goal_identified
=== ep: 511, time 24.967819929122925, eps 0.0010000000072033692, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
goal_identified
goal_identified
goal_identified
=== ep: 512, time 24.635906219482422, eps 0.001000000006852057, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
goal_identified
=== ep: 513, time 24.917906045913696, eps 0.001000000006517878, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
goal_identified
=== ep: 514, time 24.80397868156433, eps 0.0010000000061999974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
goal_identified
goal_identified
goal_identified
=== ep: 515, time 24.576021194458008, eps 0.0010000000058976199, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
goal_identified
goal_identified
=== ep: 516, time 24.71863889694214, eps 0.0010000000056099897, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
goal_identified
=== ep: 517, time 24.423163414001465, eps 0.0010000000053363872, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
goal_identified
goal_identified
goal_identified
=== ep: 518, time 24.69234323501587, eps 0.0010000000050761286, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
=== ep: 519, time 28.706590175628662, eps 0.001000000004828563, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
=== ep: 520, time 24.672341108322144, eps 0.001000000004593071, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 24.91217041015625, eps 0.0010000000043690644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
goal_identified
goal_identified
=== ep: 522, time 24.49569344520569, eps 0.0010000000041559827, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
=== ep: 523, time 24.585380792617798, eps 0.0010000000039532928, sum reward: 3, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
goal_identified
=== ep: 524, time 24.721185445785522, eps 0.0010000000037604885, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
goal_identified
=== ep: 525, time 24.817915201187134, eps 0.0010000000035770874, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
goal_identified
goal_identified
goal_identified
=== ep: 526, time 24.808462619781494, eps 0.0010000000034026306, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
goal_identified
goal_identified
goal_identified
=== ep: 527, time 24.71340036392212, eps 0.0010000000032366824, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 528, time 24.56479835510254, eps 0.0010000000030788276, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
goal_identified
goal_identified
goal_identified
=== ep: 529, time 28.76951241493225, eps 0.0010000000029286714, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
goal_identified
goal_identified
goal_identified
=== ep: 530, time 24.784021377563477, eps 0.0010000000027858384, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
goal_identified
goal_identified
=== ep: 531, time 24.935070514678955, eps 0.0010000000026499714, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
goal_identified
goal_identified
=== ep: 532, time 24.956687211990356, eps 0.0010000000025207308, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
=== ep: 533, time 25.010759592056274, eps 0.0010000000023977934, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 534, time 24.815103769302368, eps 0.0010000000022808515, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
goal_identified
goal_identified
goal_identified
=== ep: 535, time 25.02776026725769, eps 0.0010000000021696133, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
goal_identified
goal_identified
=== ep: 536, time 25.129180669784546, eps 0.0010000000020637999, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 25.094360828399658, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 538, time 24.883562088012695, eps 0.0010000000018674034, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
goal_identified
goal_identified
goal_identified
=== ep: 539, time 28.995169162750244, eps 0.001000000001776329, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
goal_identified
=== ep: 540, time 24.932119607925415, eps 0.0010000000016896964, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
=== ep: 541, time 24.72763681411743, eps 0.001000000001607289, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
goal_identified
=== ep: 542, time 24.786001205444336, eps 0.0010000000015289005, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
=== ep: 543, time 25.037160634994507, eps 0.0010000000014543352, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
goal_identified
goal_identified
=== ep: 544, time 25.182722806930542, eps 0.0010000000013834064, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
goal_identified
=== ep: 545, time 24.679030656814575, eps 0.001000000001315937, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 546, time 25.13398003578186, eps 0.0010000000012517578, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 24.79413604736328, eps 0.001000000001190709, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
=== ep: 548, time 24.584481239318848, eps 0.0010000000011326374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 29.09397268295288, eps 0.001000000001077398, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 24.58027744293213, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 551, time 24.764533519744873, eps 0.00100000000097487, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
goal_identified
=== ep: 552, time 24.7088463306427, eps 0.001000000000927325, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
goal_identified
=== ep: 553, time 25.191990852355957, eps 0.0010000000008820989, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
goal_identified
=== ep: 554, time 24.68417978286743, eps 0.0010000000008390784, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
goal_identified
=== ep: 555, time 25.104954957962036, eps 0.001000000000798156, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
goal_identified
goal_identified
goal_identified
=== ep: 556, time 25.02334713935852, eps 0.0010000000007592295, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 24.74761199951172, eps 0.0010000000007222014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 25.059510469436646, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
goal_identified
goal_identified
goal_identified
=== ep: 559, time 28.979696035385132, eps 0.001000000000653475, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
goal_identified
=== ep: 560, time 25.013774633407593, eps 0.0010000000006216046, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
goal_identified
goal_identified
goal_identified
=== ep: 561, time 24.57576608657837, eps 0.0010000000005912885, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
goal_identified
goal_identified
goal_identified
=== ep: 562, time 24.555141925811768, eps 0.0010000000005624511, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
goal_identified
goal_identified
=== ep: 563, time 25.01298213005066, eps 0.00100000000053502, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 24.714564323425293, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
goal_identified
=== ep: 565, time 24.716814041137695, eps 0.001000000000484106, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 566, time 24.827685832977295, eps 0.001000000000460496, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 25.213286876678467, eps 0.0010000000004380374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
goal_identified
goal_identified
=== ep: 568, time 24.546013355255127, eps 0.001000000000416674, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 569, time 28.909734964370728, eps 0.0010000000003963527, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 24.968178749084473, eps 0.0010000000003770222, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
goal_identified
=== ep: 571, time 24.66154432296753, eps 0.0010000000003586346, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
goal_identified
goal_identified
=== ep: 572, time 24.68046259880066, eps 0.0010000000003411438, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
=== ep: 573, time 24.705697059631348, eps 0.001000000000324506, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
goal_identified
=== ep: 574, time 24.602843523025513, eps 0.0010000000003086798, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
goal_identified
=== ep: 575, time 24.949782133102417, eps 0.0010000000002936252, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 576, time 24.82737112045288, eps 0.001000000000279305, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
goal_identified
=== ep: 577, time 24.740428924560547, eps 0.0010000000002656831, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
goal_identified
goal_identified
=== ep: 578, time 25.30722999572754, eps 0.0010000000002527256, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 579, time 29.194374561309814, eps 0.0010000000002404, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
goal_identified
goal_identified
goal_identified
=== ep: 580, time 25.170931100845337, eps 0.0010000000002286756, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
goal_identified
goal_identified
=== ep: 581, time 24.8903591632843, eps 0.0010000000002175229, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 582, time 24.760928869247437, eps 0.0010000000002069142, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 583, time 25.09202289581299, eps 0.0010000000001968228, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 584, time 24.58411169052124, eps 0.0010000000001872237, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
goal_identified
goal_identified
goal_identified
=== ep: 585, time 24.802681922912598, eps 0.0010000000001780928, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 24.79959273338318, eps 0.001000000000169407, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
=== ep: 587, time 24.93146276473999, eps 0.001000000000161145, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
=== ep: 588, time 24.518009185791016, eps 0.0010000000001532858, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
=== ep: 589, time 28.86115002632141, eps 0.00100000000014581, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
goal_identified
goal_identified
=== ep: 590, time 24.56087875366211, eps 0.0010000000001386988, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 591, time 24.465875387191772, eps 0.0010000000001319344, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 592, time 24.37284564971924, eps 0.0010000000001255, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
goal_identified
=== ep: 593, time 24.676079273223877, eps 0.0010000000001193791, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
goal_identified
goal_identified
goal_identified
=== ep: 594, time 24.583574056625366, eps 0.001000000000113557, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
goal_identified
=== ep: 595, time 24.524546146392822, eps 0.0010000000001080186, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
goal_identified
goal_identified
=== ep: 596, time 24.503159523010254, eps 0.0010000000001027505, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 597, time 24.69171166419983, eps 0.0010000000000977393, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
goal_identified
=== ep: 598, time 24.29227304458618, eps 0.0010000000000929725, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
goal_identified
=== ep: 599, time 29.756901025772095, eps 0.0010000000000884382, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
=== ep: 600, time 24.170509338378906, eps 0.001000000000084125, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
goal_identified
goal_identified
=== ep: 601, time 23.876971006393433, eps 0.0010000000000800222, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 23.975536346435547, eps 0.0010000000000761195, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
goal_identified
goal_identified
=== ep: 603, time 23.76538610458374, eps 0.0010000000000724072, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
goal_identified
goal_identified
=== ep: 604, time 23.89725923538208, eps 0.0010000000000688757, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
goal_identified
goal_identified
=== ep: 605, time 24.03606367111206, eps 0.0010000000000655166, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
goal_identified
=== ep: 606, time 24.221343994140625, eps 0.0010000000000623215, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
goal_identified
=== ep: 607, time 23.845513105392456, eps 0.001000000000059282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
goal_identified
goal_identified
goal_identified
=== ep: 608, time 23.893312454223633, eps 0.0010000000000563907, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 609, time 27.930039882659912, eps 0.0010000000000536405, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
goal_identified
goal_identified
=== ep: 610, time 24.04737377166748, eps 0.0010000000000510245, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
goal_identified
=== ep: 611, time 24.21733331680298, eps 0.0010000000000485358, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
goal_identified
goal_identified
=== ep: 612, time 24.125732421875, eps 0.0010000000000461688, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 613, time 24.075021505355835, eps 0.0010000000000439171, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
goal_identified
=== ep: 614, time 24.013855457305908, eps 0.0010000000000417752, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 615, time 23.5075101852417, eps 0.0010000000000397378, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 616, time 24.29797601699829, eps 0.0010000000000377999, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 24.08070683479309, eps 0.0010000000000359563, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
goal_identified
goal_identified
=== ep: 618, time 24.251219511032104, eps 0.0010000000000342027, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 619, time 29.3052237033844, eps 0.0010000000000325345, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 620, time 24.107776165008545, eps 0.001000000000030948, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
goal_identified
=== ep: 621, time 23.988792419433594, eps 0.0010000000000294385, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
goal_identified
goal_identified
=== ep: 622, time 24.252793073654175, eps 0.0010000000000280028, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
=== ep: 623, time 24.152660369873047, eps 0.0010000000000266371, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
goal_identified
=== ep: 624, time 24.171980381011963, eps 0.001000000000025338, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 24.175921201705933, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
goal_identified
=== ep: 626, time 24.54444646835327, eps 0.0010000000000229268, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 627, time 23.971744775772095, eps 0.0010000000000218085, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
goal_identified
goal_identified
=== ep: 628, time 24.17170739173889, eps 0.001000000000020745, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
goal_identified
goal_identified
=== ep: 629, time 28.336016178131104, eps 0.0010000000000197332, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
goal_identified
=== ep: 630, time 23.8349130153656, eps 0.0010000000000187708, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
goal_identified
=== ep: 631, time 24.26828694343567, eps 0.0010000000000178553, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 632, time 24.293851375579834, eps 0.0010000000000169845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
=== ep: 633, time 24.26888418197632, eps 0.0010000000000161562, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
goal_identified
goal_identified
goal_identified
=== ep: 634, time 23.89201545715332, eps 0.0010000000000153684, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
goal_identified
goal_identified
goal_identified
=== ep: 635, time 24.242270469665527, eps 0.0010000000000146188, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
goal_identified
=== ep: 636, time 24.04409432411194, eps 0.0010000000000139058, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
goal_identified
goal_identified
goal_identified
=== ep: 637, time 23.868634462356567, eps 0.0010000000000132275, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 24.115123748779297, eps 0.0010000000000125824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
goal_identified
goal_identified
=== ep: 639, time 28.376184225082397, eps 0.0010000000000119687, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
=== ep: 640, time 23.85455346107483, eps 0.001000000000011385, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
goal_identified
=== ep: 641, time 24.487460136413574, eps 0.00100000000001083, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
goal_identified
goal_identified
=== ep: 642, time 24.211284160614014, eps 0.0010000000000103017, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
goal_identified
=== ep: 643, time 24.015889406204224, eps 0.0010000000000097993, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
goal_identified
=== ep: 644, time 24.65933132171631, eps 0.0010000000000093213, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
goal_identified
goal_identified
goal_identified
=== ep: 645, time 24.096495389938354, eps 0.0010000000000088666, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
goal_identified
goal_identified
goal_identified
=== ep: 646, time 24.274994611740112, eps 0.0010000000000084342, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 24.001545429229736, eps 0.001000000000008023, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
=== ep: 648, time 23.93339991569519, eps 0.0010000000000076317, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 649, time 28.0940363407135, eps 0.0010000000000072594, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
goal_identified
=== ep: 650, time 24.293906688690186, eps 0.0010000000000069055, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
goal_identified
goal_identified
=== ep: 651, time 24.342379808425903, eps 0.0010000000000065686, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 24.13010287284851, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 653, time 23.897541522979736, eps 0.0010000000000059436, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
goal_identified
goal_identified
goal_identified
=== ep: 654, time 24.544511079788208, eps 0.0010000000000056537, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
goal_identified
goal_identified
goal_identified
=== ep: 655, time 24.067317247390747, eps 0.0010000000000053779, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 23.97142791748047, eps 0.0010000000000051157, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
goal_identified
goal_identified
=== ep: 657, time 24.26401925086975, eps 0.0010000000000048661, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
goal_identified
=== ep: 658, time 24.198642015457153, eps 0.001000000000004629, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
goal_identified
goal_identified
=== ep: 659, time 28.432445287704468, eps 0.0010000000000044032, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 660, time 24.357701539993286, eps 0.0010000000000041883, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 24.434433460235596, eps 0.001000000000003984, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 24.348212718963623, eps 0.0010000000000037897, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 24.0263774394989, eps 0.001000000000003605, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
goal_identified
=== ep: 664, time 23.964470148086548, eps 0.0010000000000034291, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
goal_identified
goal_identified
=== ep: 665, time 23.892130613327026, eps 0.001000000000003262, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
goal_identified
goal_identified
=== ep: 666, time 23.98196053504944, eps 0.0010000000000031028, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 667, time 24.310784816741943, eps 0.0010000000000029514, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
goal_identified
=== ep: 668, time 24.402809381484985, eps 0.0010000000000028075, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
goal_identified
goal_identified
goal_identified
=== ep: 669, time 28.02943444252014, eps 0.0010000000000026706, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
goal_identified
=== ep: 670, time 24.469932079315186, eps 0.0010000000000025403, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 671, time 24.155715942382812, eps 0.0010000000000024165, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
goal_identified
goal_identified
=== ep: 672, time 24.01805591583252, eps 0.0010000000000022985, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
goal_identified
goal_identified
goal_identified
=== ep: 673, time 23.867471933364868, eps 0.0010000000000021864, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
goal_identified
=== ep: 674, time 24.23607301712036, eps 0.00100000000000208, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
goal_identified
goal_identified
goal_identified
=== ep: 675, time 24.05426597595215, eps 0.0010000000000019785, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
goal_identified
goal_identified
goal_identified
=== ep: 676, time 24.099818468093872, eps 0.001000000000001882, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
goal_identified
goal_identified
=== ep: 677, time 24.336959838867188, eps 0.0010000000000017903, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
goal_identified
goal_identified
goal_identified
=== ep: 678, time 24.43529725074768, eps 0.0010000000000017029, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
goal_identified
goal_identified
=== ep: 679, time 28.218194007873535, eps 0.0010000000000016198, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 24.20436382293701, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
goal_identified
=== ep: 681, time 24.77802038192749, eps 0.0010000000000014656, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
goal_identified
goal_identified
=== ep: 682, time 24.34353232383728, eps 0.0010000000000013943, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
goal_identified
goal_identified
=== ep: 683, time 24.167123317718506, eps 0.0010000000000013262, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
goal_identified
goal_identified
=== ep: 684, time 24.458309173583984, eps 0.0010000000000012616, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
goal_identified
=== ep: 685, time 24.097079038619995, eps 0.0010000000000012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
goal_identified
goal_identified
goal_identified
=== ep: 686, time 24.256349563598633, eps 0.0010000000000011415, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
goal_identified
goal_identified
=== ep: 687, time 24.508301734924316, eps 0.0010000000000010857, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
goal_identified
goal_identified
=== ep: 688, time 24.240973949432373, eps 0.0010000000000010328, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
goal_identified
goal_identified
=== ep: 689, time 28.095560550689697, eps 0.0010000000000009825, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
goal_identified
goal_identified
goal_identified
=== ep: 690, time 24.135451316833496, eps 0.0010000000000009346, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
goal_identified
=== ep: 691, time 24.371663331985474, eps 0.001000000000000889, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
=== ep: 692, time 24.091934204101562, eps 0.0010000000000008457, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
goal_identified
=== ep: 693, time 24.30167841911316, eps 0.0010000000000008045, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
goal_identified
goal_identified
=== ep: 694, time 24.649946928024292, eps 0.0010000000000007653, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
goal_identified
goal_identified
=== ep: 695, time 23.937991619110107, eps 0.0010000000000007277, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 696, time 24.216514587402344, eps 0.0010000000000006924, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 697, time 23.952212810516357, eps 0.0010000000000006586, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
goal_identified
goal_identified
=== ep: 698, time 24.209416389465332, eps 0.0010000000000006265, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
goal_identified
goal_identified
goal_identified
=== ep: 699, time 28.24889826774597, eps 0.001000000000000596, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 700, time 24.04529309272766, eps 0.0010000000000005668, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
goal_identified
goal_identified
=== ep: 701, time 24.365293741226196, eps 0.0010000000000005393, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 702, time 24.07164716720581, eps 0.0010000000000005128, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
=== ep: 703, time 24.18743658065796, eps 0.001000000000000488, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 704, time 24.235230445861816, eps 0.001000000000000464, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
goal_identified
=== ep: 705, time 24.43537473678589, eps 0.0010000000000004415, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
goal_identified
goal_identified
=== ep: 706, time 24.44653558731079, eps 0.00100000000000042, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
goal_identified
goal_identified
goal_identified
=== ep: 707, time 24.401680946350098, eps 0.0010000000000003994, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 708, time 24.305198431015015, eps 0.00100000000000038, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
goal_identified
=== ep: 709, time 28.59092092514038, eps 0.0010000000000003615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
goal_identified
=== ep: 710, time 24.876800775527954, eps 0.0010000000000003437, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 711, time 24.498288869857788, eps 0.001000000000000327, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 712, time 24.334596633911133, eps 0.0010000000000003112, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 24.278391122817993, eps 0.001000000000000296, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
goal_identified
=== ep: 714, time 23.913475513458252, eps 0.0010000000000002815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
goal_identified
goal_identified
=== ep: 715, time 24.478687286376953, eps 0.0010000000000002678, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
goal_identified
goal_identified
=== ep: 716, time 24.11278510093689, eps 0.0010000000000002548, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
goal_identified
goal_identified
goal_identified
=== ep: 717, time 24.730925798416138, eps 0.0010000000000002422, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
goal_identified
=== ep: 718, time 24.347466707229614, eps 0.0010000000000002305, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
goal_identified
goal_identified
=== ep: 719, time 28.62068200111389, eps 0.0010000000000002192, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
goal_identified
=== ep: 720, time 24.485337734222412, eps 0.0010000000000002086, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 721, time 24.18206024169922, eps 0.0010000000000001984, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
goal_identified
=== ep: 722, time 24.41936159133911, eps 0.0010000000000001887, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
=== ep: 723, time 25.023723363876343, eps 0.0010000000000001796, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
goal_identified
=== ep: 724, time 24.059858083724976, eps 0.0010000000000001707, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 725, time 24.332148790359497, eps 0.0010000000000001624, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
goal_identified
goal_identified
=== ep: 726, time 23.989177465438843, eps 0.0010000000000001544, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 24.994468450546265, eps 0.001000000000000147, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 24.40409564971924, eps 0.0010000000000001399, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 729, time 28.19232416152954, eps 0.001000000000000133, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 730, time 24.102795600891113, eps 0.0010000000000001264, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
goal_identified
goal_identified
=== ep: 731, time 24.199862480163574, eps 0.0010000000000001204, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
goal_identified
goal_identified
=== ep: 732, time 24.14783787727356, eps 0.0010000000000001145, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
=== ep: 733, time 24.2704861164093, eps 0.0010000000000001089, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
=== ep: 734, time 24.457744359970093, eps 0.0010000000000001037, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
goal_identified
=== ep: 735, time 24.45695734024048, eps 0.0010000000000000985, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 736, time 24.433059215545654, eps 0.0010000000000000937, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
goal_identified
=== ep: 737, time 24.675745964050293, eps 0.0010000000000000891, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
goal_identified
goal_identified
=== ep: 738, time 24.149664878845215, eps 0.0010000000000000848, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
goal_identified
goal_identified
=== ep: 739, time 28.510898113250732, eps 0.0010000000000000807, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
=== ep: 740, time 24.100603580474854, eps 0.0010000000000000768, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
goal_identified
goal_identified
=== ep: 741, time 24.320334911346436, eps 0.001000000000000073, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 742, time 24.13612389564514, eps 0.0010000000000000694, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
goal_identified
goal_identified
goal_identified
=== ep: 743, time 24.613012075424194, eps 0.001000000000000066, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
goal_identified
goal_identified
=== ep: 744, time 24.402217149734497, eps 0.001000000000000063, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
goal_identified
goal_identified
=== ep: 745, time 24.41961097717285, eps 0.0010000000000000599, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
goal_identified
goal_identified
=== ep: 746, time 24.4752995967865, eps 0.0010000000000000568, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
goal_identified
goal_identified
goal_identified
=== ep: 747, time 24.5846164226532, eps 0.001000000000000054, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
goal_identified
goal_identified
goal_identified
=== ep: 748, time 24.46848750114441, eps 0.0010000000000000514, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 749, time 28.358768224716187, eps 0.001000000000000049, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
=== ep: 750, time 24.287670850753784, eps 0.0010000000000000466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
goal_identified
=== ep: 751, time 24.333133935928345, eps 0.0010000000000000443, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 24.42268943786621, eps 0.001000000000000042, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
goal_identified
=== ep: 753, time 24.677392721176147, eps 0.0010000000000000401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
goal_identified
goal_identified
=== ep: 754, time 24.507498025894165, eps 0.0010000000000000382, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 755, time 24.581167221069336, eps 0.0010000000000000362, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
goal_identified
=== ep: 756, time 24.304595470428467, eps 0.0010000000000000345, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
goal_identified
=== ep: 757, time 24.433025121688843, eps 0.0010000000000000328, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 24.703284978866577, eps 0.0010000000000000312, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
goal_identified
=== ep: 759, time 28.28508162498474, eps 0.0010000000000000297, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
goal_identified
goal_identified
=== ep: 760, time 24.65999460220337, eps 0.0010000000000000282, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
goal_identified
=== ep: 761, time 24.39103603363037, eps 0.001000000000000027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
=== ep: 762, time 24.87468981742859, eps 0.0010000000000000256, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 763, time 24.382015228271484, eps 0.0010000000000000243, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
goal_identified
=== ep: 764, time 24.38282608985901, eps 0.0010000000000000232, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
goal_identified
goal_identified
=== ep: 765, time 24.44659924507141, eps 0.001000000000000022, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
goal_identified
=== ep: 766, time 24.42530131340027, eps 0.0010000000000000208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
goal_identified
goal_identified
=== ep: 767, time 24.297673225402832, eps 0.00100000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
goal_identified
goal_identified
=== ep: 768, time 24.228978633880615, eps 0.0010000000000000189, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
goal_identified
goal_identified
goal_identified
=== ep: 769, time 28.581807136535645, eps 0.001000000000000018, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
goal_identified
=== ep: 770, time 24.40214204788208, eps 0.0010000000000000172, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
goal_identified
=== ep: 771, time 24.754507303237915, eps 0.0010000000000000163, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
goal_identified
goal_identified
=== ep: 772, time 24.569387435913086, eps 0.0010000000000000154, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
goal_identified
goal_identified
goal_identified
=== ep: 773, time 24.55594825744629, eps 0.0010000000000000148, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
goal_identified
goal_identified
=== ep: 774, time 24.77488923072815, eps 0.0010000000000000141, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 775, time 24.462162494659424, eps 0.0010000000000000132, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 776, time 24.452909469604492, eps 0.0010000000000000126, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
goal_identified
goal_identified
goal_identified
=== ep: 777, time 24.540196657180786, eps 0.0010000000000000122, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
goal_identified
=== ep: 778, time 24.4773850440979, eps 0.0010000000000000115, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 28.195768117904663, eps 0.0010000000000000109, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
goal_identified
goal_identified
=== ep: 780, time 24.694109678268433, eps 0.0010000000000000104, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 781, time 24.584094762802124, eps 0.00100000000000001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
goal_identified
=== ep: 782, time 24.309009075164795, eps 0.0010000000000000093, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
goal_identified
goal_identified
=== ep: 783, time 24.69820761680603, eps 0.001000000000000009, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
goal_identified
=== ep: 784, time 24.704018592834473, eps 0.0010000000000000085, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 24.284743309020996, eps 0.001000000000000008, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
goal_identified
goal_identified
=== ep: 786, time 24.319494485855103, eps 0.0010000000000000076, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
goal_identified
goal_identified
goal_identified
=== ep: 787, time 24.643551349639893, eps 0.0010000000000000074, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
goal_identified
goal_identified
=== ep: 788, time 24.27989411354065, eps 0.001000000000000007, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
=== ep: 789, time 30.732775926589966, eps 0.0010000000000000067, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
goal_identified
goal_identified
goal_identified
=== ep: 790, time 24.80117392539978, eps 0.0010000000000000063, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
goal_identified
goal_identified
goal_identified
=== ep: 791, time 24.699581384658813, eps 0.001000000000000006, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
goal_identified
=== ep: 792, time 24.74595808982849, eps 0.0010000000000000057, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 793, time 24.51763892173767, eps 0.0010000000000000054, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
goal_identified
goal_identified
goal_identified
=== ep: 794, time 24.748003005981445, eps 0.0010000000000000052, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
goal_identified
goal_identified
=== ep: 795, time 24.387200832366943, eps 0.001000000000000005, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 796, time 24.558881044387817, eps 0.0010000000000000048, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
goal_identified
goal_identified
=== ep: 797, time 24.483506202697754, eps 0.0010000000000000044, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
goal_identified
=== ep: 798, time 24.638535976409912, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
goal_identified
goal_identified
goal_identified
=== ep: 799, time 28.76112723350525, eps 0.0010000000000000041, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 800, time 24.53450894355774, eps 0.001000000000000004, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
goal_identified
goal_identified
goal_identified
=== ep: 801, time 24.646721839904785, eps 0.0010000000000000037, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
goal_identified
goal_identified
=== ep: 802, time 24.470824718475342, eps 0.0010000000000000035, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
goal_identified
goal_identified
goal_identified
=== ep: 803, time 24.546111583709717, eps 0.0010000000000000033, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
goal_identified
goal_identified
=== ep: 804, time 24.526308298110962, eps 0.001000000000000003, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
goal_identified
goal_identified
=== ep: 805, time 24.967580556869507, eps 0.001000000000000003, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
goal_identified
goal_identified
=== ep: 806, time 24.93081307411194, eps 0.0010000000000000028, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 807, time 24.584936380386353, eps 0.0010000000000000026, sum reward: 6, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
goal_identified
goal_identified
=== ep: 808, time 24.393531560897827, eps 0.0010000000000000026, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 809, time 28.170304775238037, eps 0.0010000000000000024, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 810, time 24.765258073806763, eps 0.0010000000000000024, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
=== ep: 811, time 24.948967695236206, eps 0.0010000000000000022, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
goal_identified
goal_identified
goal_identified
=== ep: 812, time 24.400597095489502, eps 0.0010000000000000022, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
goal_identified
=== ep: 813, time 24.69119691848755, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
goal_identified
goal_identified
goal_identified
=== ep: 814, time 24.59526300430298, eps 0.001000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
goal_identified
goal_identified
goal_identified
=== ep: 815, time 24.609159231185913, eps 0.0010000000000000018, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
goal_identified
=== ep: 816, time 24.80769920349121, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 24.91610860824585, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
goal_identified
goal_identified
=== ep: 818, time 25.638208150863647, eps 0.0010000000000000015, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
=== ep: 819, time 28.43043088912964, eps 0.0010000000000000015, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 820, time 24.86237621307373, eps 0.0010000000000000013, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
goal_identified
goal_identified
goal_identified
=== ep: 821, time 24.635622024536133, eps 0.0010000000000000013, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 24.539520502090454, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 24.6073215007782, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
goal_identified
=== ep: 824, time 24.89827036857605, eps 0.001000000000000001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
goal_identified
goal_identified
goal_identified
=== ep: 825, time 24.660310983657837, eps 0.001000000000000001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
goal_identified
goal_identified
=== ep: 826, time 24.321244955062866, eps 0.001000000000000001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
goal_identified
goal_identified
goal_identified
=== ep: 827, time 24.5181245803833, eps 0.001000000000000001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 828, time 24.53630042076111, eps 0.0010000000000000009, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 829, time 28.559508323669434, eps 0.0010000000000000009, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
=== ep: 830, time 24.69967222213745, eps 0.0010000000000000009, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
goal_identified
goal_identified
=== ep: 831, time 24.59927773475647, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 832, time 24.659979581832886, eps 0.0010000000000000009, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
goal_identified
=== ep: 833, time 24.541085720062256, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
goal_identified
goal_identified
=== ep: 834, time 24.605298280715942, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
goal_identified
goal_identified
=== ep: 835, time 24.85886240005493, eps 0.0010000000000000007, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
goal_identified
goal_identified
goal_identified
=== ep: 836, time 24.697376012802124, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 837, time 24.34452509880066, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 838, time 24.53816032409668, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
goal_identified
goal_identified
=== ep: 839, time 28.433695793151855, eps 0.0010000000000000007, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
goal_identified
=== ep: 840, time 24.636348724365234, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 841, time 24.99808382987976, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
goal_identified
goal_identified
goal_identified
=== ep: 842, time 24.837316513061523, eps 0.0010000000000000005, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 843, time 25.050209283828735, eps 0.0010000000000000005, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 844, time 25.082947731018066, eps 0.0010000000000000005, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
goal_identified
goal_identified
goal_identified
=== ep: 845, time 24.999398946762085, eps 0.0010000000000000005, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
goal_identified
goal_identified
goal_identified
=== ep: 846, time 24.93452525138855, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
goal_identified
goal_identified
goal_identified
=== ep: 847, time 25.145060062408447, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
goal_identified
goal_identified
goal_identified
=== ep: 848, time 25.29650068283081, eps 0.0010000000000000005, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 849, time 28.934571504592896, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
goal_identified
=== ep: 850, time 25.14614987373352, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 25.205031394958496, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
goal_identified
=== ep: 852, time 25.439460039138794, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
=== ep: 853, time 25.1876060962677, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
goal_identified
goal_identified
goal_identified
=== ep: 854, time 24.644794940948486, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
goal_identified
goal_identified
=== ep: 855, time 25.09449577331543, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
goal_identified
goal_identified
goal_identified
=== ep: 856, time 24.276219129562378, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
goal_identified
goal_identified
goal_identified
=== ep: 857, time 24.70605778694153, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
=== ep: 858, time 24.822998762130737, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
goal_identified
=== ep: 859, time 28.894683122634888, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 860, time 25.409314393997192, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 861, time 25.77003526687622, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
goal_identified
goal_identified
=== ep: 862, time 26.13312864303589, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
goal_identified
goal_identified
goal_identified
=== ep: 863, time 25.803793907165527, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
goal_identified
=== ep: 864, time 25.91597270965576, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
goal_identified
goal_identified
=== ep: 865, time 25.986592292785645, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
goal_identified
goal_identified
=== ep: 866, time 25.816282987594604, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
goal_identified
goal_identified
goal_identified
=== ep: 867, time 25.719662189483643, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
goal_identified
goal_identified
=== ep: 868, time 26.249368906021118, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 869, time 30.286964178085327, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 870, time 26.23127794265747, eps 0.0010000000000000002, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 25.768686056137085, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
goal_identified
goal_identified
goal_identified
=== ep: 872, time 25.77566409111023, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
goal_identified
goal_identified
goal_identified
=== ep: 873, time 25.678659915924072, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
goal_identified
goal_identified
=== ep: 874, time 26.13109588623047, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
goal_identified
=== ep: 875, time 25.68071460723877, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 876, time 25.756561756134033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 26.047547340393066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
goal_identified
goal_identified
goal_identified
=== ep: 878, time 26.22397208213806, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
goal_identified
=== ep: 879, time 30.08201003074646, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
goal_identified
goal_identified
=== ep: 880, time 25.466033935546875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
goal_identified
goal_identified
goal_identified
=== ep: 881, time 25.533506870269775, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
goal_identified
goal_identified
=== ep: 882, time 26.104673385620117, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
=== ep: 883, time 25.748476266860962, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 884, time 25.726882696151733, eps 0.001, sum reward: 4, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
goal_identified
=== ep: 885, time 25.902901887893677, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
goal_identified
goal_identified
=== ep: 886, time 26.189311265945435, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
=== ep: 887, time 25.83780813217163, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
goal_identified
goal_identified
=== ep: 888, time 25.69852590560913, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
goal_identified
goal_identified
=== ep: 889, time 30.192901134490967, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 25.765539169311523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
goal_identified
goal_identified
goal_identified
=== ep: 891, time 25.70679473876953, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
goal_identified
=== ep: 892, time 25.8113911151886, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
goal_identified
goal_identified
=== ep: 893, time 25.892229080200195, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
goal_identified
goal_identified
=== ep: 894, time 25.93321180343628, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
goal_identified
=== ep: 895, time 25.770623922348022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
goal_identified
goal_identified
goal_identified
=== ep: 896, time 25.804876804351807, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 897, time 25.90822434425354, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
=== ep: 898, time 25.880547761917114, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
goal_identified
=== ep: 899, time 30.220999479293823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
goal_identified
goal_identified
=== ep: 900, time 25.850972890853882, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
goal_identified
=== ep: 901, time 25.973804712295532, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
=== ep: 902, time 25.71980333328247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
goal_identified
goal_identified
=== ep: 903, time 25.745004415512085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
goal_identified
goal_identified
=== ep: 904, time 25.784326791763306, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
goal_identified
goal_identified
=== ep: 905, time 25.857842206954956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
=== ep: 906, time 25.73227596282959, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
goal_identified
=== ep: 907, time 25.721691608428955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
goal_identified
=== ep: 908, time 25.98178267478943, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 909, time 30.424709796905518, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
goal_identified
goal_identified
=== ep: 910, time 25.978214263916016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
goal_identified
=== ep: 911, time 25.944350242614746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
goal_identified
=== ep: 912, time 25.750437021255493, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
goal_identified
=== ep: 913, time 25.960373640060425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
goal_identified
goal_identified
goal_identified
=== ep: 914, time 25.72531819343567, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
goal_identified
goal_identified
goal_identified
=== ep: 915, time 25.92864680290222, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 915
goal_identified
goal_identified
=== ep: 916, time 25.719639539718628, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 25.89285683631897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
goal_identified
goal_identified
goal_identified
=== ep: 918, time 25.984413862228394, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
goal_identified
goal_identified
goal_identified
=== ep: 919, time 30.221396684646606, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
goal_identified
=== ep: 920, time 26.326666831970215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
goal_identified
=== ep: 921, time 26.030788898468018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
goal_identified
goal_identified
=== ep: 922, time 26.089616775512695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 26.1832275390625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
goal_identified
goal_identified
goal_identified
=== ep: 924, time 26.16589641571045, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 925, time 26.046565532684326, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
goal_identified
goal_identified
goal_identified
=== ep: 926, time 25.861023902893066, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
goal_identified
goal_identified
goal_identified
=== ep: 927, time 25.75010395050049, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
=== ep: 928, time 26.29680585861206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
goal_identified
=== ep: 929, time 30.249685525894165, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 930, time 26.011196851730347, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 931, time 26.053699493408203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
goal_identified
goal_identified
=== ep: 932, time 25.958732843399048, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
=== ep: 933, time 25.784422397613525, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
goal_identified
=== ep: 934, time 26.276555061340332, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
goal_identified
goal_identified
goal_identified
=== ep: 935, time 25.901187658309937, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
goal_identified
goal_identified
=== ep: 936, time 26.21895456314087, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
goal_identified
=== ep: 937, time 25.933013200759888, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
goal_identified
goal_identified
goal_identified
=== ep: 938, time 25.686281442642212, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
goal_identified
=== ep: 939, time 30.37799048423767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
goal_identified
goal_identified
=== ep: 940, time 26.133633613586426, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 941, time 26.134456396102905, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
goal_identified
goal_identified
=== ep: 942, time 26.097465991973877, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 943, time 25.646483898162842, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
goal_identified
=== ep: 944, time 25.85256004333496, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
goal_identified
=== ep: 945, time 26.04830837249756, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
=== ep: 946, time 25.86825728416443, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 947, time 25.959614992141724, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
goal_identified
=== ep: 948, time 25.844280004501343, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
goal_identified
goal_identified
goal_identified
=== ep: 949, time 29.62113356590271, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 950, time 25.294918060302734, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
goal_identified
goal_identified
goal_identified
=== ep: 951, time 25.261610507965088, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
goal_identified
goal_identified
=== ep: 952, time 25.14784264564514, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
goal_identified
goal_identified
goal_identified
=== ep: 953, time 25.19624900817871, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
goal_identified
=== ep: 954, time 25.011276483535767, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
goal_identified
goal_identified
=== ep: 955, time 25.055721759796143, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
=== ep: 956, time 25.131268739700317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
goal_identified
=== ep: 957, time 25.481051445007324, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
goal_identified
goal_identified
goal_identified
=== ep: 958, time 24.850589752197266, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
=== ep: 959, time 29.396002054214478, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
goal_identified
goal_identified
goal_identified
=== ep: 960, time 25.171284437179565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
goal_identified
goal_identified
goal_identified
=== ep: 961, time 24.913978099822998, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
goal_identified
goal_identified
goal_identified
=== ep: 962, time 24.916982412338257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
goal_identified
goal_identified
goal_identified
=== ep: 963, time 24.808010816574097, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
=== ep: 964, time 25.25432324409485, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
goal_identified
goal_identified
=== ep: 965, time 24.671051263809204, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 966, time 25.140876293182373, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
goal_identified
goal_identified
goal_identified
=== ep: 967, time 24.950226306915283, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
goal_identified
=== ep: 968, time 26.1371328830719, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
goal_identified
goal_identified
goal_identified
=== ep: 969, time 30.202921152114868, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
goal_identified
=== ep: 970, time 26.202991247177124, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
=== ep: 971, time 25.978113651275635, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 972, time 26.219964504241943, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 973, time 25.912096738815308, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
goal_identified
=== ep: 974, time 26.166629791259766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
goal_identified
=== ep: 975, time 26.34528684616089, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
goal_identified
goal_identified
=== ep: 976, time 26.07159948348999, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 977, time 25.95566487312317, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
goal_identified
goal_identified
=== ep: 978, time 25.967076539993286, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
goal_identified
goal_identified
goal_identified
=== ep: 979, time 30.73139762878418, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
goal_identified
=== ep: 980, time 25.891687154769897, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
goal_identified
goal_identified
=== ep: 981, time 25.941242694854736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
goal_identified
goal_identified
goal_identified
=== ep: 982, time 25.743509769439697, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 983, time 25.91265106201172, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
goal_identified
goal_identified
goal_identified
=== ep: 984, time 25.90873122215271, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
goal_identified
goal_identified
=== ep: 985, time 26.347622394561768, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
goal_identified
=== ep: 986, time 25.650870323181152, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
goal_identified
=== ep: 987, time 26.574641704559326, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
goal_identified
goal_identified
=== ep: 988, time 26.2604660987854, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
goal_identified
=== ep: 989, time 30.720489263534546, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
goal_identified
=== ep: 990, time 25.988445043563843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
goal_identified
goal_identified
=== ep: 991, time 25.76404857635498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
=== ep: 992, time 26.153181314468384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
=== ep: 993, time 26.282180786132812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
goal_identified
goal_identified
goal_identified
=== ep: 994, time 26.070717811584473, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
goal_identified
=== ep: 995, time 25.77641201019287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
goal_identified
goal_identified
=== ep: 996, time 26.52937126159668, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
goal_identified
=== ep: 997, time 26.143117904663086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
goal_identified
=== ep: 998, time 26.497619152069092, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
goal_identified
goal_identified
=== ep: 999, time 31.342437028884888, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1000, time 27.161268711090088, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
goal_identified
goal_identified
=== ep: 1001, time 27.294040203094482, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
goal_identified
goal_identified
goal_identified
=== ep: 1002, time 26.763038158416748, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
goal_identified
=== ep: 1003, time 26.870462656021118, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1004, time 27.012991666793823, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
goal_identified
=== ep: 1005, time 27.074959993362427, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
goal_identified
goal_identified
=== ep: 1006, time 27.440680027008057, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
goal_identified
goal_identified
=== ep: 1007, time 27.045098066329956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
goal_identified
goal_identified
=== ep: 1008, time 27.49803614616394, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
goal_identified
goal_identified
=== ep: 1009, time 32.06306791305542, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
goal_identified
=== ep: 1010, time 27.257509469985962, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
goal_identified
goal_identified
=== ep: 1011, time 27.167370796203613, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
goal_identified
=== ep: 1012, time 27.242079257965088, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
goal_identified
=== ep: 1013, time 27.90050458908081, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
goal_identified
goal_identified
goal_identified
=== ep: 1014, time 27.276647567749023, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
goal_identified
goal_identified
goal_identified
=== ep: 1015, time 27.324881076812744, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
goal_identified
=== ep: 1016, time 27.048903703689575, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
goal_identified
=== ep: 1017, time 27.047916412353516, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
goal_identified
goal_identified
goal_identified
=== ep: 1018, time 27.381067752838135, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
goal_identified
=== ep: 1019, time 32.85901117324829, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
goal_identified
=== ep: 1020, time 28.186823844909668, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
goal_identified
goal_identified
=== ep: 1021, time 27.323290586471558, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
goal_identified
goal_identified
goal_identified
=== ep: 1022, time 27.926013469696045, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
goal_identified
goal_identified
=== ep: 1023, time 26.076626777648926, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
goal_identified
goal_identified
=== ep: 1024, time 25.58671545982361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
goal_identified
=== ep: 1025, time 25.36172080039978, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
goal_identified
goal_identified
=== ep: 1026, time 24.967255353927612, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
goal_identified
=== ep: 1027, time 24.306145191192627, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
goal_identified
=== ep: 1028, time 24.59404945373535, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
goal_identified
goal_identified
goal_identified
=== ep: 1029, time 28.87108278274536, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
goal_identified
goal_identified
goal_identified
=== ep: 1030, time 24.79692268371582, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
goal_identified
goal_identified
goal_identified
=== ep: 1031, time 25.302582263946533, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
goal_identified
=== ep: 1032, time 24.807496786117554, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
=== ep: 1033, time 24.281479120254517, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
goal_identified
=== ep: 1034, time 23.98661708831787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
goal_identified
goal_identified
goal_identified
=== ep: 1035, time 24.353038549423218, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
goal_identified
goal_identified
goal_identified
=== ep: 1036, time 26.84627938270569, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
goal_identified
=== ep: 1037, time 26.69986653327942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
goal_identified
goal_identified
goal_identified
=== ep: 1038, time 26.614537954330444, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
=== ep: 1039, time 31.968076467514038, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
goal_identified
goal_identified
=== ep: 1040, time 26.110743761062622, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1041, time 26.71340298652649, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1041
goal_identified
=== ep: 1042, time 26.55943512916565, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
goal_identified
=== ep: 1043, time 26.7608380317688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
goal_identified
goal_identified
=== ep: 1044, time 26.922744274139404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
goal_identified
=== ep: 1045, time 26.852010250091553, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1046, time 26.850074291229248, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
goal_identified
goal_identified
=== ep: 1047, time 26.679782390594482, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
=== ep: 1048, time 26.765427827835083, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1049, time 33.452831506729126, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
goal_identified
goal_identified
goal_identified
=== ep: 1050, time 26.952495574951172, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
goal_identified
=== ep: 1051, time 27.281888246536255, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
goal_identified
=== ep: 1052, time 27.14458680152893, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
goal_identified
=== ep: 1053, time 26.98015570640564, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
goal_identified
goal_identified
=== ep: 1054, time 26.614864826202393, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
goal_identified
=== ep: 1055, time 26.668572425842285, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
goal_identified
=== ep: 1056, time 27.480494737625122, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
=== ep: 1057, time 26.53873562812805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
goal_identified
=== ep: 1058, time 27.203540802001953, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
goal_identified
goal_identified
goal_identified
=== ep: 1059, time 31.515998125076294, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
goal_identified
goal_identified
=== ep: 1060, time 27.46752119064331, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
goal_identified
goal_identified
=== ep: 1061, time 27.542788982391357, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
goal_identified
goal_identified
goal_identified
=== ep: 1062, time 27.076053857803345, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1063, time 26.923532962799072, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
goal_identified
goal_identified
goal_identified
=== ep: 1064, time 27.32452940940857, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
goal_identified
goal_identified
=== ep: 1065, time 27.21383023262024, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
goal_identified
=== ep: 1066, time 27.291574239730835, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
goal_identified
goal_identified
=== ep: 1067, time 27.408014059066772, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
goal_identified
goal_identified
=== ep: 1068, time 27.749001502990723, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
goal_identified
goal_identified
goal_identified
=== ep: 1069, time 32.32064366340637, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
goal_identified
goal_identified
=== ep: 1070, time 27.329268217086792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
goal_identified
goal_identified
goal_identified
=== ep: 1071, time 27.07834029197693, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
goal_identified
goal_identified
=== ep: 1072, time 27.404747247695923, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
goal_identified
goal_identified
=== ep: 1073, time 26.98905920982361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
goal_identified
goal_identified
=== ep: 1074, time 26.935243368148804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1075, time 27.334079027175903, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
=== ep: 1076, time 27.198961973190308, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
goal_identified
=== ep: 1077, time 27.733225345611572, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
goal_identified
goal_identified
=== ep: 1078, time 27.221064805984497, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1079, time 32.365862131118774, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
goal_identified
goal_identified
goal_identified
=== ep: 1080, time 27.553232431411743, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1081, time 27.473451852798462, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
goal_identified
goal_identified
=== ep: 1082, time 27.32029366493225, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
goal_identified
goal_identified
=== ep: 1083, time 27.52857232093811, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
goal_identified
goal_identified
=== ep: 1084, time 27.41890239715576, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
=== ep: 1085, time 27.3367862701416, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
goal_identified
=== ep: 1086, time 27.409414291381836, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
goal_identified
goal_identified
=== ep: 1087, time 27.46632695198059, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
goal_identified
=== ep: 1088, time 27.685043334960938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
goal_identified
=== ep: 1089, time 32.13716411590576, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
goal_identified
goal_identified
goal_identified
=== ep: 1090, time 27.78421688079834, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1091, time 27.856658697128296, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
goal_identified
goal_identified
goal_identified
=== ep: 1092, time 27.708470344543457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
goal_identified
goal_identified
goal_identified
=== ep: 1093, time 27.793184757232666, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
goal_identified
=== ep: 1094, time 27.62231683731079, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1095, time 27.458842277526855, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
goal_identified
goal_identified
goal_identified
=== ep: 1096, time 27.85965633392334, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
goal_identified
=== ep: 1097, time 28.000337839126587, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
goal_identified
goal_identified
=== ep: 1098, time 27.885655403137207, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
goal_identified
goal_identified
=== ep: 1099, time 32.3474326133728, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
=== ep: 1100, time 27.88807201385498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
goal_identified
goal_identified
=== ep: 1101, time 27.875460863113403, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
goal_identified
=== ep: 1102, time 27.924150466918945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
=== ep: 1103, time 27.955947160720825, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
goal_identified
goal_identified
=== ep: 1104, time 28.036908626556396, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
goal_identified
=== ep: 1105, time 27.99972105026245, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1106, time 27.467021703720093, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
goal_identified
goal_identified
=== ep: 1107, time 27.9092800617218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1108, time 27.634933948516846, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
goal_identified
goal_identified
=== ep: 1109, time 32.733282804489136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
goal_identified
goal_identified
=== ep: 1110, time 27.824265718460083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
=== ep: 1111, time 27.920423984527588, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
goal_identified
goal_identified
=== ep: 1112, time 28.03468608856201, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
goal_identified
goal_identified
goal_identified
=== ep: 1113, time 27.635889291763306, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
goal_identified
=== ep: 1114, time 27.599408864974976, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
goal_identified
=== ep: 1115, time 27.610708951950073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
goal_identified
goal_identified
goal_identified
=== ep: 1116, time 27.915244817733765, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
goal_identified
goal_identified
=== ep: 1117, time 27.885629415512085, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
goal_identified
=== ep: 1118, time 27.848137378692627, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
goal_identified
=== ep: 1119, time 32.58913493156433, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
goal_identified
goal_identified
=== ep: 1120, time 27.98595404624939, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
goal_identified
goal_identified
goal_identified
=== ep: 1121, time 27.90582585334778, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1122, time 28.062145471572876, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
goal_identified
=== ep: 1123, time 27.71416425704956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
goal_identified
goal_identified
=== ep: 1124, time 27.455032110214233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
goal_identified
goal_identified
=== ep: 1125, time 27.798966646194458, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
=== ep: 1126, time 27.465355157852173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
goal_identified
goal_identified
goal_identified
=== ep: 1127, time 26.91853952407837, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
goal_identified
goal_identified
=== ep: 1128, time 26.868348836898804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1129, time 31.682548999786377, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
goal_identified
goal_identified
goal_identified
=== ep: 1130, time 26.793853759765625, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
goal_identified
goal_identified
=== ep: 1131, time 26.97390365600586, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1132, time 27.377572774887085, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
goal_identified
goal_identified
=== ep: 1133, time 26.92585563659668, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1134, time 26.880281686782837, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1135, time 26.574983835220337, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
goal_identified
goal_identified
=== ep: 1136, time 26.773139238357544, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
goal_identified
=== ep: 1137, time 26.931321620941162, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
goal_identified
goal_identified
=== ep: 1138, time 27.083451747894287, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
goal_identified
goal_identified
=== ep: 1139, time 31.75775718688965, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
goal_identified
goal_identified
goal_identified
=== ep: 1140, time 26.815258026123047, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
goal_identified
=== ep: 1141, time 27.109621286392212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
=== ep: 1142, time 27.28091311454773, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
goal_identified
goal_identified
goal_identified
=== ep: 1143, time 26.961496591567993, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
goal_identified
=== ep: 1144, time 26.86118721961975, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
goal_identified
goal_identified
goal_identified
=== ep: 1145, time 27.543801307678223, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
=== ep: 1146, time 26.87571883201599, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
goal_identified
=== ep: 1147, time 27.577003240585327, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
=== ep: 1148, time 27.03704571723938, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
=== ep: 1149, time 31.88921356201172, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1150, time 26.876712560653687, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
goal_identified
goal_identified
goal_identified
=== ep: 1151, time 27.110716342926025, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
goal_identified
goal_identified
=== ep: 1152, time 26.716338396072388, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
goal_identified
goal_identified
goal_identified
=== ep: 1153, time 26.677202701568604, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
goal_identified
goal_identified
=== ep: 1154, time 27.199130535125732, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
goal_identified
=== ep: 1155, time 26.596026182174683, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
goal_identified
goal_identified
goal_identified
=== ep: 1156, time 26.867719650268555, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1157, time 27.11919140815735, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
goal_identified
goal_identified
goal_identified
=== ep: 1158, time 27.058125495910645, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
goal_identified
goal_identified
goal_identified
=== ep: 1159, time 31.508767127990723, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
goal_identified
goal_identified
goal_identified
=== ep: 1160, time 26.707523345947266, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
goal_identified
goal_identified
goal_identified
=== ep: 1161, time 26.99886727333069, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
=== ep: 1162, time 27.151549339294434, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
goal_identified
goal_identified
goal_identified
=== ep: 1163, time 27.1579270362854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
goal_identified
goal_identified
=== ep: 1164, time 27.14289116859436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
goal_identified
goal_identified
goal_identified
=== ep: 1165, time 27.077181339263916, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1166, time 26.89902973175049, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
goal_identified
goal_identified
goal_identified
=== ep: 1167, time 26.455941677093506, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
goal_identified
goal_identified
=== ep: 1168, time 26.538953065872192, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
goal_identified
goal_identified
goal_identified
=== ep: 1169, time 31.413024425506592, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
goal_identified
goal_identified
goal_identified
=== ep: 1170, time 26.094828844070435, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
=== ep: 1171, time 25.18495273590088, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
goal_identified
goal_identified
=== ep: 1172, time 25.03084373474121, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
goal_identified
goal_identified
goal_identified
=== ep: 1173, time 24.991302967071533, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 25.042871713638306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
goal_identified
goal_identified
=== ep: 1175, time 24.902185440063477, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
goal_identified
goal_identified
goal_identified
=== ep: 1176, time 25.151873350143433, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
goal_identified
goal_identified
=== ep: 1177, time 25.245144367218018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
=== ep: 1178, time 25.36062479019165, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
goal_identified
=== ep: 1179, time 30.36860179901123, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
goal_identified
=== ep: 1180, time 25.67970037460327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1181, time 25.81409764289856, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
goal_identified
goal_identified
goal_identified
=== ep: 1182, time 25.57392907142639, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
goal_identified
=== ep: 1183, time 25.65351963043213, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1184, time 25.381255388259888, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
goal_identified
goal_identified
=== ep: 1185, time 25.534340381622314, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
goal_identified
goal_identified
goal_identified
=== ep: 1186, time 25.75474715232849, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
goal_identified
goal_identified
goal_identified
=== ep: 1187, time 25.59101963043213, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
goal_identified
=== ep: 1188, time 25.47407627105713, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
=== ep: 1189, time 30.208667516708374, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1190, time 25.462586164474487, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
goal_identified
goal_identified
=== ep: 1191, time 25.475597381591797, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
goal_identified
goal_identified
goal_identified
=== ep: 1192, time 25.562377452850342, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
goal_identified
=== ep: 1193, time 26.073086977005005, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
goal_identified
goal_identified
=== ep: 1194, time 25.857574224472046, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 25.90552020072937, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
goal_identified
goal_identified
=== ep: 1196, time 25.656784296035767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
goal_identified
goal_identified
goal_identified
=== ep: 1197, time 25.848182678222656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
goal_identified
goal_identified
=== ep: 1198, time 25.795294523239136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
goal_identified
goal_identified
goal_identified
=== ep: 1199, time 30.84399175643921, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
goal_identified
goal_identified
=== ep: 1200, time 25.828589916229248, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
goal_identified
goal_identified
=== ep: 1201, time 26.1852707862854, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
goal_identified
goal_identified
goal_identified
=== ep: 1202, time 26.23105525970459, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
goal_identified
goal_identified
=== ep: 1203, time 25.874656915664673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
goal_identified
=== ep: 1204, time 26.2458655834198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
=== ep: 1205, time 26.374731302261353, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
goal_identified
goal_identified
goal_identified
=== ep: 1206, time 25.558794498443604, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
goal_identified
=== ep: 1207, time 25.85032844543457, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1208, time 25.946739435195923, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
goal_identified
goal_identified
goal_identified
=== ep: 1209, time 30.177514791488647, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
goal_identified
goal_identified
=== ep: 1210, time 25.945387363433838, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
goal_identified
goal_identified
=== ep: 1211, time 27.72732162475586, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
goal_identified
goal_identified
=== ep: 1212, time 25.73429250717163, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
=== ep: 1213, time 26.25685143470764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1214, time 25.982577562332153, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
goal_identified
goal_identified
goal_identified
=== ep: 1215, time 25.937589406967163, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
goal_identified
goal_identified
=== ep: 1216, time 25.8939368724823, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
goal_identified
goal_identified
goal_identified
=== ep: 1217, time 25.839231491088867, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
goal_identified
goal_identified
=== ep: 1218, time 24.93782377243042, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1219, time 28.928054809570312, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
goal_identified
goal_identified
=== ep: 1220, time 25.914829969406128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
=== ep: 1221, time 25.720649480819702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
goal_identified
goal_identified
goal_identified
=== ep: 1222, time 26.12998914718628, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
goal_identified
goal_identified
=== ep: 1223, time 25.92324733734131, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 26.035247564315796, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
goal_identified
goal_identified
=== ep: 1225, time 25.8574161529541, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
goal_identified
=== ep: 1226, time 25.83158540725708, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
goal_identified
=== ep: 1227, time 25.775835037231445, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1228, time 25.61318874359131, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
goal_identified
goal_identified
=== ep: 1229, time 30.794374227523804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
goal_identified
goal_identified
=== ep: 1230, time 26.126305103302002, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
goal_identified
goal_identified
goal_identified
=== ep: 1231, time 26.19614315032959, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 25.84964895248413, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
goal_identified
goal_identified
goal_identified
=== ep: 1233, time 26.210137128829956, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
goal_identified
=== ep: 1234, time 25.983996152877808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
=== ep: 1235, time 25.609785318374634, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1235
goal_identified
goal_identified
goal_identified
=== ep: 1236, time 25.760789155960083, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
goal_identified
goal_identified
=== ep: 1237, time 25.501391172409058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
goal_identified
goal_identified
goal_identified
=== ep: 1238, time 25.99823307991028, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
=== ep: 1239, time 30.629969596862793, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
goal_identified
goal_identified
goal_identified
=== ep: 1240, time 25.858076095581055, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1241, time 26.106120824813843, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1242, time 25.95812177658081, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
goal_identified
goal_identified
=== ep: 1243, time 25.858617305755615, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
goal_identified
goal_identified
=== ep: 1244, time 25.82092261314392, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1245, time 25.503161430358887, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
goal_identified
goal_identified
goal_identified
=== ep: 1246, time 26.486170530319214, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
goal_identified
goal_identified
goal_identified
=== ep: 1247, time 25.76443123817444, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1248, time 26.187896013259888, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
goal_identified
goal_identified
goal_identified
=== ep: 1249, time 30.176857948303223, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
goal_identified
goal_identified
goal_identified
=== ep: 1250, time 25.756619691848755, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
goal_identified
=== ep: 1251, time 25.687444925308228, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
goal_identified
goal_identified
goal_identified
=== ep: 1252, time 25.63039994239807, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
goal_identified
=== ep: 1253, time 25.31686496734619, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
=== ep: 1254, time 25.82901954650879, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
goal_identified
goal_identified
=== ep: 1255, time 26.236011028289795, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
goal_identified
goal_identified
=== ep: 1256, time 25.929919004440308, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
goal_identified
goal_identified
goal_identified
=== ep: 1257, time 25.799243450164795, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1258, time 26.0558762550354, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
goal_identified
=== ep: 1259, time 30.54268169403076, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
goal_identified
goal_identified
goal_identified
=== ep: 1260, time 25.536267518997192, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
goal_identified
goal_identified
goal_identified
=== ep: 1261, time 25.956045150756836, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
goal_identified
goal_identified
=== ep: 1262, time 25.680328607559204, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
goal_identified
=== ep: 1263, time 25.877241611480713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 26.093035459518433, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
goal_identified
goal_identified
=== ep: 1265, time 26.114516019821167, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
goal_identified
goal_identified
=== ep: 1266, time 25.709615230560303, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1267, time 26.041177988052368, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1268, time 25.89073395729065, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
goal_identified
=== ep: 1269, time 30.27031707763672, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
goal_identified
goal_identified
=== ep: 1270, time 26.17233371734619, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
=== ep: 1271, time 25.684314966201782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
goal_identified
goal_identified
goal_identified
=== ep: 1272, time 25.628347158432007, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
goal_identified
goal_identified
goal_identified
=== ep: 1273, time 25.70364022254944, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
=== ep: 1274, time 26.17253303527832, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
goal_identified
=== ep: 1275, time 26.052058219909668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1276, time 25.599637031555176, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
goal_identified
goal_identified
=== ep: 1277, time 25.79339289665222, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
goal_identified
=== ep: 1278, time 26.335094928741455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
goal_identified
goal_identified
goal_identified
=== ep: 1279, time 30.734943628311157, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
goal_identified
=== ep: 1280, time 26.31548571586609, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
goal_identified
=== ep: 1281, time 26.072133541107178, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1282, time 25.656562089920044, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
goal_identified
goal_identified
goal_identified
=== ep: 1283, time 25.65468192100525, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
goal_identified
goal_identified
goal_identified
=== ep: 1284, time 26.03390860557556, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
goal_identified
=== ep: 1285, time 26.207953691482544, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
goal_identified
goal_identified
goal_identified
=== ep: 1286, time 26.062952041625977, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
goal_identified
=== ep: 1287, time 26.038387775421143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
goal_identified
=== ep: 1288, time 25.88695740699768, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1289, time 30.651550769805908, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1290, time 26.21695613861084, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
goal_identified
=== ep: 1291, time 25.971601724624634, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
goal_identified
goal_identified
=== ep: 1292, time 25.740496397018433, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
goal_identified
goal_identified
=== ep: 1293, time 25.825762033462524, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
goal_identified
goal_identified
goal_identified
=== ep: 1294, time 25.877286195755005, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1295, time 25.834587335586548, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
=== ep: 1296, time 26.04305338859558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
goal_identified
goal_identified
goal_identified
=== ep: 1297, time 26.161324977874756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1298, time 25.862339973449707, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 30.86501717567444, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1300, time 25.934104442596436, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
goal_identified
goal_identified
=== ep: 1301, time 26.067928791046143, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
goal_identified
goal_identified
=== ep: 1302, time 26.082776308059692, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
goal_identified
goal_identified
goal_identified
=== ep: 1303, time 25.800304174423218, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
goal_identified
goal_identified
goal_identified
=== ep: 1304, time 25.841691732406616, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
goal_identified
=== ep: 1305, time 25.871991395950317, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
=== ep: 1306, time 25.887341737747192, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
goal_identified
=== ep: 1307, time 25.64178490638733, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
goal_identified
goal_identified
goal_identified
=== ep: 1308, time 25.75214409828186, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
goal_identified
=== ep: 1309, time 30.790533542633057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
goal_identified
goal_identified
goal_identified
=== ep: 1310, time 25.66392707824707, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
goal_identified
=== ep: 1311, time 26.015165328979492, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
=== ep: 1312, time 26.096817016601562, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
=== ep: 1313, time 26.064934015274048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
goal_identified
goal_identified
=== ep: 1314, time 26.098681926727295, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
goal_identified
=== ep: 1315, time 26.19367551803589, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
goal_identified
goal_identified
=== ep: 1316, time 25.7329523563385, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
goal_identified
=== ep: 1317, time 25.98639178276062, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
goal_identified
goal_identified
=== ep: 1318, time 26.638277053833008, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
goal_identified
=== ep: 1319, time 30.935304641723633, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
goal_identified
goal_identified
=== ep: 1320, time 26.35084319114685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
goal_identified
=== ep: 1321, time 25.732515335083008, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
=== ep: 1322, time 25.939221143722534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1323, time 26.34656596183777, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
=== ep: 1324, time 25.932793378829956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
goal_identified
goal_identified
=== ep: 1325, time 26.06883478164673, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
goal_identified
=== ep: 1326, time 25.80132484436035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
goal_identified
goal_identified
=== ep: 1327, time 26.159767866134644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
goal_identified
goal_identified
goal_identified
=== ep: 1328, time 26.189375400543213, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
goal_identified
goal_identified
goal_identified
=== ep: 1329, time 30.51594114303589, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
goal_identified
=== ep: 1330, time 26.209656476974487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
goal_identified
goal_identified
=== ep: 1331, time 25.90369701385498, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1332, time 25.597297430038452, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
goal_identified
goal_identified
goal_identified
=== ep: 1333, time 26.059026956558228, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
goal_identified
goal_identified
=== ep: 1334, time 26.143638610839844, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
goal_identified
goal_identified
=== ep: 1335, time 26.40318250656128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
=== ep: 1336, time 26.026626110076904, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
goal_identified
goal_identified
=== ep: 1337, time 25.85897660255432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
goal_identified
goal_identified
=== ep: 1338, time 26.153740406036377, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
goal_identified
goal_identified
goal_identified
=== ep: 1339, time 30.917609691619873, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
goal_identified
goal_identified
goal_identified
=== ep: 1340, time 26.179884672164917, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
goal_identified
goal_identified
=== ep: 1341, time 26.060873985290527, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
goal_identified
goal_identified
=== ep: 1342, time 26.25372076034546, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
goal_identified
goal_identified
=== ep: 1343, time 26.01962447166443, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1344, time 26.368226766586304, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
goal_identified
goal_identified
=== ep: 1345, time 25.59937071800232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
goal_identified
goal_identified
goal_identified
=== ep: 1346, time 26.389719247817993, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
goal_identified
=== ep: 1347, time 26.13003921508789, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1348, time 26.210618257522583, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
goal_identified
goal_identified
=== ep: 1349, time 31.31380343437195, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
goal_identified
=== ep: 1350, time 26.56799030303955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
goal_identified
=== ep: 1351, time 26.911227703094482, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
goal_identified
goal_identified
goal_identified
=== ep: 1352, time 26.612709283828735, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
goal_identified
goal_identified
goal_identified
=== ep: 1353, time 26.847413778305054, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
goal_identified
goal_identified
goal_identified
=== ep: 1354, time 26.86263418197632, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
goal_identified
goal_identified
=== ep: 1355, time 26.43710422515869, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
goal_identified
goal_identified
=== ep: 1356, time 26.79659104347229, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
goal_identified
goal_identified
=== ep: 1357, time 27.04558801651001, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
goal_identified
goal_identified
=== ep: 1358, time 26.45972990989685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
goal_identified
goal_identified
goal_identified
=== ep: 1359, time 31.397083044052124, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1360, time 26.7205867767334, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
goal_identified
goal_identified
goal_identified
=== ep: 1361, time 26.863027334213257, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
goal_identified
=== ep: 1362, time 26.602930545806885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
goal_identified
goal_identified
=== ep: 1363, time 26.398548364639282, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
goal_identified
goal_identified
=== ep: 1364, time 26.586756467819214, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
=== ep: 1365, time 27.15596842765808, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
goal_identified
=== ep: 1366, time 26.55844807624817, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
goal_identified
=== ep: 1367, time 26.720823764801025, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
goal_identified
=== ep: 1368, time 26.75140619277954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
goal_identified
=== ep: 1369, time 31.25003409385681, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
goal_identified
goal_identified
goal_identified
=== ep: 1370, time 26.841047048568726, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
goal_identified
goal_identified
=== ep: 1371, time 27.091771364212036, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
goal_identified
goal_identified
=== ep: 1372, time 26.700897455215454, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1373, time 26.857149600982666, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
goal_identified
goal_identified
goal_identified
=== ep: 1374, time 26.929675579071045, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
goal_identified
goal_identified
=== ep: 1375, time 26.81182098388672, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
goal_identified
goal_identified
=== ep: 1376, time 26.948211431503296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1377, time 26.716043949127197, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
goal_identified
goal_identified
=== ep: 1378, time 26.501386404037476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
goal_identified
goal_identified
goal_identified
=== ep: 1379, time 31.95204496383667, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1380, time 26.829739809036255, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1381, time 26.62467098236084, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
goal_identified
=== ep: 1382, time 26.86018705368042, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
goal_identified
goal_identified
=== ep: 1383, time 26.478526830673218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
goal_identified
goal_identified
goal_identified
=== ep: 1384, time 27.14803671836853, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
goal_identified
goal_identified
goal_identified
=== ep: 1385, time 26.648985147476196, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
goal_identified
goal_identified
=== ep: 1386, time 26.366463661193848, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
goal_identified
=== ep: 1387, time 26.977437019348145, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1388, time 26.992352962493896, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
goal_identified
goal_identified
goal_identified
=== ep: 1389, time 31.79431414604187, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1390, time 26.8993661403656, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
goal_identified
=== ep: 1391, time 26.695255994796753, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
goal_identified
goal_identified
goal_identified
=== ep: 1392, time 26.946653842926025, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
goal_identified
=== ep: 1393, time 26.611992120742798, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
goal_identified
goal_identified
=== ep: 1394, time 26.76957058906555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
goal_identified
goal_identified
goal_identified
=== ep: 1395, time 26.710701942443848, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1396, time 26.87321925163269, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
goal_identified
=== ep: 1397, time 27.04108428955078, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
goal_identified
=== ep: 1398, time 26.69710612297058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
goal_identified
goal_identified
goal_identified
=== ep: 1399, time 31.123449563980103, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
goal_identified
=== ep: 1400, time 26.671867847442627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
=== ep: 1401, time 26.666956424713135, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
goal_identified
goal_identified
=== ep: 1402, time 27.154749870300293, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
goal_identified
goal_identified
goal_identified
=== ep: 1403, time 26.57410979270935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
goal_identified
goal_identified
goal_identified
=== ep: 1404, time 26.417160749435425, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
goal_identified
=== ep: 1405, time 26.829723358154297, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
goal_identified
goal_identified
=== ep: 1406, time 26.904216289520264, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
goal_identified
=== ep: 1407, time 26.57673215866089, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
=== ep: 1408, time 26.76924705505371, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1409, time 31.703094244003296, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
=== ep: 1410, time 26.618892431259155, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
=== ep: 1411, time 27.07892370223999, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1412, time 26.816396236419678, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
goal_identified
goal_identified
=== ep: 1413, time 26.746950149536133, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
goal_identified
=== ep: 1414, time 26.9105167388916, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
goal_identified
=== ep: 1415, time 27.055463075637817, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1415
goal_identified
goal_identified
=== ep: 1416, time 26.87112832069397, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
goal_identified
goal_identified
goal_identified
=== ep: 1417, time 26.324081897735596, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
goal_identified
goal_identified
goal_identified
=== ep: 1418, time 26.948396921157837, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
goal_identified
goal_identified
=== ep: 1419, time 31.639968156814575, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
goal_identified
goal_identified
=== ep: 1420, time 26.736611366271973, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 26.719311475753784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
goal_identified
goal_identified
goal_identified
=== ep: 1422, time 26.91886615753174, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
goal_identified
=== ep: 1423, time 26.71624755859375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 26.760631322860718, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
goal_identified
goal_identified
=== ep: 1425, time 26.899213790893555, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
goal_identified
goal_identified
=== ep: 1426, time 27.19538187980652, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
goal_identified
=== ep: 1427, time 26.67065119743347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1428, time 26.81592869758606, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
goal_identified
goal_identified
=== ep: 1429, time 31.62673544883728, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1430, time 26.620431900024414, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
=== ep: 1431, time 27.3933367729187, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
goal_identified
goal_identified
goal_identified
=== ep: 1432, time 26.87624716758728, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
goal_identified
goal_identified
=== ep: 1433, time 26.72329020500183, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
goal_identified
goal_identified
=== ep: 1434, time 26.36590552330017, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
goal_identified
goal_identified
goal_identified
=== ep: 1435, time 26.63301920890808, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1436, time 26.63273048400879, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
goal_identified
goal_identified
=== ep: 1437, time 26.878069400787354, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
=== ep: 1438, time 26.699411153793335, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
=== ep: 1439, time 32.01093912124634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
goal_identified
goal_identified
=== ep: 1440, time 26.927979469299316, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
goal_identified
=== ep: 1441, time 26.535815954208374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
goal_identified
goal_identified
=== ep: 1442, time 26.899105072021484, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
goal_identified
goal_identified
=== ep: 1443, time 26.722184419631958, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
goal_identified
goal_identified
=== ep: 1444, time 27.046833992004395, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
goal_identified
goal_identified
goal_identified
=== ep: 1445, time 26.520633697509766, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
goal_identified
goal_identified
=== ep: 1446, time 26.696390867233276, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
goal_identified
goal_identified
=== ep: 1447, time 26.76618456840515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1448, time 26.759562730789185, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
goal_identified
goal_identified
goal_identified
=== ep: 1449, time 32.9659743309021, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
goal_identified
goal_identified
=== ep: 1450, time 26.857462882995605, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
goal_identified
=== ep: 1451, time 26.882838487625122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
goal_identified
=== ep: 1452, time 26.923227310180664, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
goal_identified
goal_identified
=== ep: 1453, time 27.253488063812256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
=== ep: 1454, time 27.003323078155518, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
=== ep: 1455, time 26.97525429725647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1456, time 26.675424814224243, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1457, time 26.598347902297974, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
goal_identified
goal_identified
=== ep: 1458, time 26.75356674194336, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
goal_identified
=== ep: 1459, time 32.043031215667725, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
goal_identified
goal_identified
goal_identified
=== ep: 1460, time 26.909685611724854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
=== ep: 1461, time 26.889052391052246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
goal_identified
goal_identified
goal_identified
=== ep: 1462, time 26.906327724456787, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
goal_identified
goal_identified
=== ep: 1463, time 27.165015935897827, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1464, time 27.035757780075073, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
goal_identified
goal_identified
goal_identified
=== ep: 1465, time 26.912904977798462, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
goal_identified
goal_identified
=== ep: 1466, time 26.958890199661255, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1467, time 26.929381132125854, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
goal_identified
goal_identified
=== ep: 1468, time 26.506595849990845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
goal_identified
goal_identified
=== ep: 1469, time 31.61327576637268, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
goal_identified
goal_identified
=== ep: 1470, time 26.633568286895752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
goal_identified
=== ep: 1471, time 26.682075023651123, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
goal_identified
=== ep: 1472, time 26.744615077972412, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
goal_identified
goal_identified
=== ep: 1473, time 27.112603902816772, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
goal_identified
goal_identified
goal_identified
=== ep: 1474, time 27.154284477233887, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1475, time 26.652065753936768, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
goal_identified
goal_identified
goal_identified
=== ep: 1476, time 26.634982109069824, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1477, time 26.621401071548462, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
=== ep: 1478, time 26.87275266647339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1479, time 32.108359813690186, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
goal_identified
goal_identified
=== ep: 1480, time 26.574984073638916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
goal_identified
=== ep: 1481, time 26.26583194732666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
goal_identified
goal_identified
goal_identified
=== ep: 1482, time 26.923614740371704, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1482
goal_identified
goal_identified
=== ep: 1483, time 26.56831932067871, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
goal_identified
=== ep: 1484, time 26.700685739517212, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
goal_identified
=== ep: 1485, time 26.843963384628296, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
=== ep: 1486, time 26.90944194793701, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
goal_identified
goal_identified
=== ep: 1487, time 26.636924028396606, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
goal_identified
goal_identified
=== ep: 1488, time 27.017694234848022, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
goal_identified
goal_identified
=== ep: 1489, time 31.356457471847534, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
goal_identified
goal_identified
goal_identified
=== ep: 1490, time 26.634963274002075, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
goal_identified
=== ep: 1491, time 27.2225923538208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
goal_identified
goal_identified
=== ep: 1492, time 27.018024921417236, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
goal_identified
goal_identified
=== ep: 1493, time 26.707038164138794, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 26.84265637397766, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
=== ep: 1495, time 26.864201068878174, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
goal_identified
goal_identified
goal_identified
=== ep: 1496, time 26.69355058670044, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1496
goal_identified
goal_identified
=== ep: 1497, time 26.8039448261261, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1498, time 26.955847024917603, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
goal_identified
=== ep: 1499, time 31.212434768676758, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
goal_identified
goal_identified
goal_identified
=== ep: 1500, time 26.641037464141846, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
=== ep: 1501, time 26.68500542640686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
goal_identified
goal_identified
=== ep: 1502, time 26.589617252349854, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 26.89482045173645, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
goal_identified
goal_identified
goal_identified
=== ep: 1504, time 26.93484377861023, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
goal_identified
goal_identified
=== ep: 1505, time 26.586904287338257, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
goal_identified
goal_identified
=== ep: 1506, time 26.875688076019287, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
=== ep: 1507, time 26.656779050827026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1507
goal_identified
goal_identified
goal_identified
=== ep: 1508, time 26.558486461639404, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
goal_identified
goal_identified
goal_identified
=== ep: 1509, time 31.395851850509644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1510, time 27.083460092544556, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1511, time 27.110342979431152, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1512, time 26.709571838378906, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
goal_identified
goal_identified
=== ep: 1513, time 27.33814287185669, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1514, time 26.703402042388916, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
goal_identified
=== ep: 1515, time 26.884764432907104, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
goal_identified
goal_identified
=== ep: 1516, time 27.09048342704773, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
goal_identified
=== ep: 1517, time 27.04616641998291, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
goal_identified
goal_identified
=== ep: 1518, time 26.522407293319702, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
goal_identified
goal_identified
goal_identified
=== ep: 1519, time 31.50348997116089, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
goal_identified
=== ep: 1520, time 26.869149208068848, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
goal_identified
=== ep: 1521, time 26.78562617301941, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
goal_identified
goal_identified
=== ep: 1522, time 27.021644353866577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
goal_identified
goal_identified
goal_identified
=== ep: 1523, time 26.785563945770264, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
goal_identified
goal_identified
=== ep: 1524, time 26.696879148483276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
goal_identified
goal_identified
goal_identified
=== ep: 1525, time 26.918930053710938, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
goal_identified
=== ep: 1526, time 26.8141872882843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
goal_identified
goal_identified
=== ep: 1527, time 26.606414318084717, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1527
goal_identified
goal_identified
=== ep: 1528, time 26.697728872299194, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
goal_identified
goal_identified
goal_identified
=== ep: 1529, time 32.45609736442566, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
goal_identified
goal_identified
goal_identified
=== ep: 1530, time 26.971077919006348, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
goal_identified
goal_identified
goal_identified
=== ep: 1531, time 26.925400733947754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
goal_identified
goal_identified
=== ep: 1532, time 26.798495531082153, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
goal_identified
goal_identified
=== ep: 1533, time 27.02664279937744, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
goal_identified
goal_identified
=== ep: 1534, time 27.265891551971436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
goal_identified
=== ep: 1535, time 26.95676851272583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
goal_identified
goal_identified
goal_identified
=== ep: 1536, time 26.861599683761597, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
goal_identified
goal_identified
=== ep: 1537, time 26.65074062347412, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
goal_identified
goal_identified
=== ep: 1538, time 26.540186405181885, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
goal_identified
goal_identified
goal_identified
=== ep: 1539, time 31.65536665916443, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
goal_identified
goal_identified
=== ep: 1540, time 26.74432682991028, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
goal_identified
goal_identified
goal_identified
=== ep: 1541, time 26.83318591117859, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
goal_identified
goal_identified
=== ep: 1542, time 26.840210676193237, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
goal_identified
goal_identified
=== ep: 1543, time 26.975588083267212, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
goal_identified
goal_identified
goal_identified
=== ep: 1544, time 26.938593864440918, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
goal_identified
goal_identified
goal_identified
=== ep: 1545, time 27.161522388458252, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
goal_identified
goal_identified
goal_identified
=== ep: 1546, time 26.94565987586975, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1547, time 26.73382830619812, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
goal_identified
=== ep: 1548, time 27.197964906692505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
goal_identified
goal_identified
=== ep: 1549, time 31.751816034317017, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
=== ep: 1550, time 27.485896348953247, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
goal_identified
=== ep: 1551, time 27.059258699417114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
goal_identified
goal_identified
goal_identified
=== ep: 1552, time 26.796974897384644, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
goal_identified
goal_identified
goal_identified
=== ep: 1553, time 26.754353046417236, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
goal_identified
=== ep: 1554, time 26.946499347686768, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
goal_identified
=== ep: 1555, time 27.308873891830444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1556, time 26.869713306427002, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
goal_identified
goal_identified
goal_identified
=== ep: 1557, time 27.1287899017334, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
goal_identified
goal_identified
=== ep: 1558, time 26.77694272994995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
goal_identified
goal_identified
=== ep: 1559, time 31.418681621551514, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
goal_identified
=== ep: 1560, time 26.668296337127686, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
goal_identified
goal_identified
goal_identified
=== ep: 1561, time 26.706674575805664, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
goal_identified
goal_identified
=== ep: 1562, time 26.668442726135254, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
goal_identified
=== ep: 1563, time 26.93779158592224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
goal_identified
goal_identified
=== ep: 1564, time 26.527998208999634, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
goal_identified
goal_identified
goal_identified
=== ep: 1565, time 26.704989671707153, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
goal_identified
=== ep: 1566, time 26.832332611083984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
goal_identified
=== ep: 1567, time 26.82236909866333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
goal_identified
goal_identified
=== ep: 1568, time 26.762510538101196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
=== ep: 1569, time 31.042202949523926, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
goal_identified
goal_identified
goal_identified
=== ep: 1570, time 26.54358959197998, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1571, time 26.90006947517395, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
goal_identified
goal_identified
=== ep: 1572, time 26.94959330558777, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
=== ep: 1573, time 26.906890869140625, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1574, time 26.905224800109863, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
goal_identified
goal_identified
=== ep: 1575, time 26.797937393188477, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
goal_identified
goal_identified
goal_identified
=== ep: 1576, time 27.000678062438965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
goal_identified
goal_identified
=== ep: 1577, time 27.043710470199585, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
goal_identified
=== ep: 1578, time 26.705247163772583, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
=== ep: 1579, time 31.492408752441406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1579
goal_identified
goal_identified
=== ep: 1580, time 26.86976981163025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1581, time 26.562629222869873, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1582, time 26.852949857711792, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1547
goal_identified
goal_identified
goal_identified
=== ep: 1583, time 26.320396900177002, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 27.482208013534546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1584
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1585, time 27.47601842880249, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
goal_identified
goal_identified
=== ep: 1586, time 27.484118223190308, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
goal_identified
=== ep: 1587, time 27.442549228668213, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1588, time 26.844958543777466, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
goal_identified
goal_identified
=== ep: 1589, time 31.446717977523804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
goal_identified
goal_identified
goal_identified
=== ep: 1590, time 27.73276162147522, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
goal_identified
goal_identified
goal_identified
=== ep: 1591, time 27.582754850387573, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
goal_identified
goal_identified
=== ep: 1592, time 27.257009029388428, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
goal_identified
=== ep: 1593, time 27.30766272544861, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
goal_identified
=== ep: 1594, time 27.623196363449097, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
goal_identified
=== ep: 1595, time 27.37813138961792, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1596, time 26.87372589111328, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
goal_identified
=== ep: 1597, time 27.22900915145874, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
goal_identified
goal_identified
goal_identified
=== ep: 1598, time 27.50366997718811, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1599, time 31.994407415390015, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
goal_identified
goal_identified
goal_identified
=== ep: 1600, time 27.576384782791138, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1600
goal_identified
=== ep: 1601, time 26.871951580047607, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
goal_identified
=== ep: 1602, time 27.00163722038269, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
goal_identified
goal_identified
=== ep: 1603, time 27.442956686019897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
goal_identified
goal_identified
goal_identified
=== ep: 1604, time 27.240435361862183, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
goal_identified
goal_identified
=== ep: 1605, time 27.31509494781494, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
goal_identified
goal_identified
=== ep: 1606, time 27.22699761390686, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
goal_identified
=== ep: 1607, time 26.98448610305786, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
=== ep: 1608, time 27.179468870162964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
goal_identified
=== ep: 1609, time 31.618484258651733, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
goal_identified
=== ep: 1610, time 26.689446687698364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1611, time 27.339943647384644, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
goal_identified
goal_identified
goal_identified
=== ep: 1612, time 27.148099660873413, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
=== ep: 1613, time 27.202876567840576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1614, time 28.26883554458618, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
goal_identified
goal_identified
goal_identified
=== ep: 1615, time 27.427049160003662, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
goal_identified
goal_identified
=== ep: 1616, time 27.19933819770813, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1617, time 27.124969244003296, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
goal_identified
goal_identified
goal_identified
=== ep: 1618, time 27.350285053253174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1619, time 31.499771118164062, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
goal_identified
=== ep: 1620, time 27.2496440410614, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
goal_identified
goal_identified
goal_identified
=== ep: 1621, time 27.531291007995605, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1622, time 27.62437915802002, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
goal_identified
goal_identified
=== ep: 1623, time 27.238072395324707, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
goal_identified
goal_identified
goal_identified
=== ep: 1624, time 27.293275356292725, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
goal_identified
goal_identified
goal_identified
=== ep: 1625, time 27.433056116104126, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
goal_identified
goal_identified
goal_identified
=== ep: 1626, time 27.291014909744263, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1627, time 27.40714120864868, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
goal_identified
=== ep: 1628, time 26.621657609939575, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 32.27926015853882, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
goal_identified
=== ep: 1630, time 27.283244371414185, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
goal_identified
goal_identified
goal_identified
=== ep: 1631, time 27.434723615646362, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1632, time 27.130308866500854, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
goal_identified
goal_identified
=== ep: 1633, time 27.456209659576416, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
=== ep: 1634, time 27.51044225692749, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
goal_identified
goal_identified
goal_identified
=== ep: 1635, time 27.072171449661255, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 27.392916679382324, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
goal_identified
goal_identified
=== ep: 1637, time 27.720118522644043, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
goal_identified
=== ep: 1638, time 27.656518697738647, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1639, time 31.859637022018433, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
goal_identified
goal_identified
=== ep: 1640, time 27.657365322113037, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
goal_identified
goal_identified
goal_identified
=== ep: 1641, time 27.040359020233154, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
goal_identified
goal_identified
=== ep: 1642, time 27.483296632766724, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
goal_identified
goal_identified
=== ep: 1643, time 27.397842168807983, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
goal_identified
=== ep: 1644, time 27.08356022834778, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1645, time 27.08847951889038, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
goal_identified
=== ep: 1646, time 27.435341358184814, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
goal_identified
goal_identified
goal_identified
=== ep: 1647, time 27.424311876296997, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1648, time 27.10391116142273, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1649, time 31.879589080810547, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
goal_identified
goal_identified
goal_identified
=== ep: 1650, time 27.400253295898438, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
=== ep: 1651, time 27.394553661346436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 27.01952075958252, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
goal_identified
=== ep: 1653, time 27.555630207061768, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1654, time 27.17931628227234, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
goal_identified
goal_identified
=== ep: 1655, time 27.582056522369385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
goal_identified
=== ep: 1656, time 27.673118829727173, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
=== ep: 1657, time 27.538812160491943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
=== ep: 1658, time 27.126299142837524, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
goal_identified
goal_identified
=== ep: 1659, time 32.252564668655396, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
goal_identified
goal_identified
=== ep: 1660, time 27.091633081436157, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
goal_identified
goal_identified
goal_identified
=== ep: 1661, time 27.778949737548828, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
goal_identified
goal_identified
=== ep: 1662, time 27.25299859046936, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
goal_identified
goal_identified
=== ep: 1663, time 27.325054168701172, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
=== ep: 1664, time 27.321149110794067, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1665, time 27.521281957626343, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
goal_identified
goal_identified
=== ep: 1666, time 27.04904317855835, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
goal_identified
goal_identified
=== ep: 1667, time 27.015197038650513, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
goal_identified
goal_identified
goal_identified
=== ep: 1668, time 27.208751440048218, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
goal_identified
goal_identified
=== ep: 1669, time 31.728415727615356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
goal_identified
=== ep: 1670, time 27.206549644470215, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
goal_identified
goal_identified
goal_identified
=== ep: 1671, time 27.40812110900879, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
goal_identified
goal_identified
goal_identified
=== ep: 1672, time 27.477180004119873, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
goal_identified
goal_identified
goal_identified
=== ep: 1673, time 27.358306884765625, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
goal_identified
=== ep: 1674, time 27.362571477890015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 27.347318410873413, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1676, time 26.998976469039917, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
goal_identified
goal_identified
=== ep: 1677, time 27.412370443344116, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
goal_identified
goal_identified
goal_identified
=== ep: 1678, time 27.232179880142212, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
=== ep: 1679, time 34.74362254142761, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
goal_identified
=== ep: 1680, time 27.556817293167114, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
goal_identified
=== ep: 1681, time 27.386828899383545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1682, time 27.478044748306274, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
goal_identified
goal_identified
=== ep: 1683, time 27.42329502105713, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1684, time 26.97096872329712, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
goal_identified
goal_identified
=== ep: 1685, time 27.390361070632935, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
goal_identified
goal_identified
goal_identified
=== ep: 1686, time 27.264439582824707, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
goal_identified
goal_identified
=== ep: 1687, time 27.37423872947693, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
goal_identified
goal_identified
goal_identified
=== ep: 1688, time 27.476338148117065, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
=== ep: 1689, time 32.33714842796326, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
goal_identified
goal_identified
goal_identified
=== ep: 1690, time 27.289947271347046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1691, time 27.625352382659912, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
goal_identified
=== ep: 1692, time 27.518172025680542, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
goal_identified
=== ep: 1693, time 27.21045422554016, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
goal_identified
=== ep: 1694, time 27.408963680267334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
goal_identified
goal_identified
=== ep: 1695, time 27.34138512611389, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 27.242499113082886, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
goal_identified
goal_identified
goal_identified
=== ep: 1697, time 27.270926237106323, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
goal_identified
=== ep: 1698, time 27.410602807998657, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
goal_identified
goal_identified
=== ep: 1699, time 32.09815216064453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 27.408854246139526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
goal_identified
goal_identified
=== ep: 1701, time 27.07430863380432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1702, time 27.224452257156372, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1702
goal_identified
goal_identified
goal_identified
=== ep: 1703, time 27.297161102294922, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1703
goal_identified
goal_identified
=== ep: 1704, time 27.603070974349976, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1704
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1705, time 27.6174533367157, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1705
goal_identified
goal_identified
=== ep: 1706, time 27.085009813308716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1706
goal_identified
=== ep: 1707, time 27.276921272277832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1707
goal_identified
goal_identified
goal_identified
=== ep: 1708, time 27.62334680557251, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1709, time 32.26337671279907, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1709
goal_identified
goal_identified
=== ep: 1710, time 27.189783811569214, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1710
goal_identified
goal_identified
=== ep: 1711, time 27.55928111076355, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1711
goal_identified
goal_identified
=== ep: 1712, time 27.16124176979065, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1712
goal_identified
goal_identified
=== ep: 1713, time 27.249349117279053, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1713
goal_identified
goal_identified
goal_identified
=== ep: 1714, time 26.58461833000183, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1714
=== ep: 1715, time 27.17200756072998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1715
goal_identified
goal_identified
goal_identified
=== ep: 1716, time 27.369272232055664, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1716
goal_identified
goal_identified
=== ep: 1717, time 27.2752468585968, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1717
goal_identified
=== ep: 1718, time 27.59458565711975, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1718
goal_identified
goal_identified
=== ep: 1719, time 31.737205743789673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1719
goal_identified
goal_identified
goal_identified
=== ep: 1720, time 27.165103435516357, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1720
goal_identified
goal_identified
=== ep: 1721, time 27.134680032730103, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1721
goal_identified
goal_identified
goal_identified
=== ep: 1722, time 27.494538068771362, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1722
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1723, time 26.63524317741394, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1723
goal_identified
goal_identified
=== ep: 1724, time 27.49297571182251, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1724
goal_identified
goal_identified
goal_identified
=== ep: 1725, time 27.516883850097656, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1725
goal_identified
goal_identified
goal_identified
=== ep: 1726, time 26.940226793289185, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1726
goal_identified
goal_identified
goal_identified
=== ep: 1727, time 27.315760850906372, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1727
goal_identified
goal_identified
goal_identified
=== ep: 1728, time 27.51904797554016, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1728
goal_identified
goal_identified
goal_identified
=== ep: 1729, time 32.15014863014221, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1729
goal_identified
=== ep: 1730, time 27.450459003448486, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1730
goal_identified
=== ep: 1731, time 27.242039680480957, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1731
goal_identified
goal_identified
goal_identified
=== ep: 1732, time 27.19600248336792, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1732
goal_identified
goal_identified
goal_identified
=== ep: 1733, time 27.130502223968506, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1733
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1734, time 27.303149938583374, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1734
goal_identified
=== ep: 1735, time 27.67912769317627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1735
goal_identified
=== ep: 1736, time 26.97519016265869, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1736
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1737, time 27.131877660751343, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1737
=== ep: 1738, time 27.204175233840942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1738
goal_identified
goal_identified
goal_identified
=== ep: 1739, time 32.17882299423218, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1739
=== ep: 1740, time 26.826753616333008, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1740
goal_identified
goal_identified
=== ep: 1741, time 27.110098123550415, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1741
goal_identified
goal_identified
=== ep: 1742, time 27.529261112213135, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1742
=== ep: 1743, time 27.497876167297363, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1743
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1744, time 27.404235363006592, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1744
goal_identified
goal_identified
goal_identified
=== ep: 1745, time 27.064560174942017, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1745
goal_identified
goal_identified
goal_identified
=== ep: 1746, time 27.339943885803223, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1746
goal_identified
goal_identified
=== ep: 1747, time 27.122589349746704, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1747
goal_identified
=== ep: 1748, time 27.67948293685913, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1748
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1749, time 32.02999782562256, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1749
goal_identified
goal_identified
=== ep: 1750, time 26.992360591888428, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1750
goal_identified
=== ep: 1751, time 27.174564123153687, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1751
goal_identified
goal_identified
goal_identified
=== ep: 1752, time 27.230347633361816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1752
goal_identified
goal_identified
=== ep: 1753, time 27.714471578598022, eps 0.001, sum reward: 2, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1753
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1754, time 27.052008867263794, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
goal_identified
goal_identified
goal_identified
=== ep: 1755, time 26.705121755599976, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1755
goal_identified
goal_identified
goal_identified
=== ep: 1756, time 27.405033349990845, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1756
goal_identified
=== ep: 1757, time 27.287065744400024, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1757
goal_identified
goal_identified
goal_identified
=== ep: 1758, time 27.23808741569519, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1758
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1759, time 31.690228700637817, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1759
goal_identified
=== ep: 1760, time 27.596134424209595, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1761, time 27.43656086921692, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1762, time 27.246582508087158, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1762
goal_identified
goal_identified
goal_identified
=== ep: 1763, time 26.818777084350586, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1763
goal_identified
=== ep: 1764, time 26.954763412475586, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1764
goal_identified
goal_identified
=== ep: 1765, time 27.293100118637085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1765
goal_identified
goal_identified
goal_identified
=== ep: 1766, time 27.37956690788269, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1766
goal_identified
goal_identified
=== ep: 1767, time 27.467431783676147, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1767
goal_identified
goal_identified
=== ep: 1768, time 27.117361545562744, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1768
goal_identified
goal_identified
=== ep: 1769, time 32.473289012908936, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1769
goal_identified
goal_identified
=== ep: 1770, time 27.480223894119263, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1770
goal_identified
=== ep: 1771, time 27.326111316680908, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1771
goal_identified
goal_identified
=== ep: 1772, time 26.962133407592773, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1772
goal_identified
goal_identified
=== ep: 1773, time 27.262943744659424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1773
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1774, time 27.560672521591187, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1774
goal_identified
=== ep: 1775, time 27.109875202178955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1775
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1776, time 26.79794955253601, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1776
goal_identified
goal_identified
goal_identified
=== ep: 1777, time 27.350877046585083, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1777
goal_identified
goal_identified
=== ep: 1778, time 26.899760723114014, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1778
goal_identified
goal_identified
=== ep: 1779, time 32.714481353759766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1779
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1780, time 27.189871311187744, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1781, time 27.212560176849365, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1781
goal_identified
goal_identified
goal_identified
=== ep: 1782, time 27.358397722244263, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1782
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1783, time 27.74505066871643, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1783
goal_identified
goal_identified
=== ep: 1784, time 27.355980396270752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1784
goal_identified
=== ep: 1785, time 27.232864379882812, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1785
goal_identified
goal_identified
=== ep: 1786, time 27.184689044952393, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1786
goal_identified
=== ep: 1787, time 27.152793169021606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1787
goal_identified
goal_identified
goal_identified
=== ep: 1788, time 27.354487895965576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 33.902769804000854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1789
goal_identified
goal_identified
=== ep: 1790, time 27.311033248901367, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1790
goal_identified
=== ep: 1791, time 27.37385630607605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1791
goal_identified
goal_identified
=== ep: 1792, time 27.170332670211792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1792
goal_identified
goal_identified
=== ep: 1793, time 27.158995151519775, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1793
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1794, time 26.82712721824646, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1794
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1795, time 27.629186868667603, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1795
goal_identified
=== ep: 1796, time 27.49544358253479, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1796
goal_identified
goal_identified
goal_identified
=== ep: 1797, time 27.924710035324097, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1797
goal_identified
goal_identified
=== ep: 1798, time 27.131880044937134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1798
=== ep: 1799, time 32.751211404800415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1799
goal_identified
goal_identified
goal_identified
=== ep: 1800, time 27.48190689086914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 27.167502880096436, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1801
=== ep: 1802, time 27.187119960784912, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1802
goal_identified
goal_identified
goal_identified
=== ep: 1803, time 27.27088212966919, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1803
goal_identified
goal_identified
=== ep: 1804, time 27.51712393760681, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1804
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1805, time 26.953396797180176, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1805
goal_identified
goal_identified
=== ep: 1806, time 27.27971839904785, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1806
goal_identified
goal_identified
=== ep: 1807, time 27.037024974822998, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1807
goal_identified
goal_identified
=== ep: 1808, time 27.142789602279663, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 32.27605700492859, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1809
goal_identified
goal_identified
=== ep: 1810, time 27.292855262756348, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1810
goal_identified
goal_identified
=== ep: 1811, time 27.312582969665527, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1811
goal_identified
goal_identified
=== ep: 1812, time 27.28923273086548, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1812
goal_identified
goal_identified
=== ep: 1813, time 27.22934651374817, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1813
goal_identified
goal_identified
goal_identified
=== ep: 1814, time 27.17013669013977, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1814
goal_identified
goal_identified
=== ep: 1815, time 27.76151704788208, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1816, time 26.76576066017151, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1816
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1817, time 27.110127687454224, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1817
=== ep: 1818, time 27.251873254776, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1818
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1819, time 32.04009032249451, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1819
goal_identified
=== ep: 1820, time 26.861387252807617, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1820
goal_identified
goal_identified
=== ep: 1821, time 27.463031768798828, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1821
goal_identified
goal_identified
goal_identified
=== ep: 1822, time 27.617061138153076, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1822
goal_identified
goal_identified
goal_identified
=== ep: 1823, time 26.900363206863403, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1823
goal_identified
goal_identified
goal_identified
=== ep: 1824, time 27.535476207733154, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1824
goal_identified
=== ep: 1825, time 26.766216278076172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1826, time 27.384803533554077, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1826
goal_identified
goal_identified
=== ep: 1827, time 27.174793481826782, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1827
goal_identified
=== ep: 1828, time 27.14443802833557, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1828
goal_identified
goal_identified
=== ep: 1829, time 32.036699056625366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1829
goal_identified
goal_identified
=== ep: 1830, time 27.374781131744385, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1830
goal_identified
goal_identified
goal_identified
=== ep: 1831, time 27.8154034614563, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1831
goal_identified
goal_identified
=== ep: 1832, time 26.925634622573853, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1832
goal_identified
goal_identified
=== ep: 1833, time 27.023951053619385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1833
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1834, time 27.22104024887085, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1834
goal_identified
goal_identified
goal_identified
=== ep: 1835, time 27.45393657684326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1836, time 27.36357569694519, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1836
=== ep: 1837, time 27.325502157211304, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1837
goal_identified
=== ep: 1838, time 27.34803295135498, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1838
goal_identified
goal_identified
goal_identified
=== ep: 1839, time 31.909030199050903, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1839
goal_identified
goal_identified
=== ep: 1840, time 27.393945932388306, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1840
goal_identified
=== ep: 1841, time 27.66340446472168, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1841
goal_identified
goal_identified
=== ep: 1842, time 27.32465124130249, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1842
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1843, time 26.86468505859375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1843
goal_identified
goal_identified
=== ep: 1844, time 27.41421937942505, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1844
goal_identified
goal_identified
=== ep: 1845, time 27.789710760116577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1845
goal_identified
=== ep: 1846, time 27.130585432052612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1846
goal_identified
goal_identified
=== ep: 1847, time 27.634289979934692, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1847
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1848, time 26.677488803863525, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1848
goal_identified
goal_identified
=== ep: 1849, time 32.50011706352234, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1849
=== ep: 1850, time 27.243963479995728, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1850
goal_identified
=== ep: 1851, time 27.545315504074097, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1851
goal_identified
goal_identified
=== ep: 1852, time 26.691547393798828, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1852
goal_identified
goal_identified
=== ep: 1853, time 27.313424825668335, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1853
=== ep: 1854, time 27.700255870819092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1854
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1855, time 27.06747531890869, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1855
goal_identified
goal_identified
goal_identified
=== ep: 1856, time 27.57466983795166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1856
goal_identified
goal_identified
=== ep: 1857, time 27.281591176986694, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1857
goal_identified
goal_identified
=== ep: 1858, time 27.560888528823853, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1858
goal_identified
goal_identified
goal_identified
=== ep: 1859, time 32.02287173271179, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1859
goal_identified
=== ep: 1860, time 27.346445560455322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1860
=== ep: 1861, time 27.588484048843384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1861
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 26.92599630355835, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1862
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1863, time 27.49342393875122, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1863
goal_identified
goal_identified
=== ep: 1864, time 27.458569765090942, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1864
goal_identified
goal_identified
goal_identified
=== ep: 1865, time 27.39477801322937, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1865
=== ep: 1866, time 26.864476919174194, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1866
goal_identified
=== ep: 1867, time 27.14242458343506, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1867
=== ep: 1868, time 27.57774591445923, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1869, time 32.328673124313354, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1869
goal_identified
=== ep: 1870, time 26.672264575958252, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1870
goal_identified
goal_identified
goal_identified
=== ep: 1871, time 27.002413988113403, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1871
goal_identified
=== ep: 1872, time 27.537369966506958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1872
goal_identified
goal_identified
goal_identified
=== ep: 1873, time 27.334513664245605, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1873
goal_identified
goal_identified
goal_identified
=== ep: 1874, time 27.1394681930542, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1874
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1875, time 27.03075337409973, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1875
goal_identified
=== ep: 1876, time 27.735792636871338, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1876
goal_identified
=== ep: 1877, time 27.452815532684326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1877
goal_identified
goal_identified
=== ep: 1878, time 27.36409282684326, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1878
goal_identified
goal_identified
=== ep: 1879, time 32.289477586746216, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1879
goal_identified
=== ep: 1880, time 27.131797790527344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1881, time 27.52054500579834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1881
goal_identified
=== ep: 1882, time 27.033978700637817, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1882
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1883, time 27.14900517463684, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1883
goal_identified
=== ep: 1884, time 27.4618399143219, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1884
goal_identified
=== ep: 1885, time 26.748620748519897, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1885
goal_identified
goal_identified
=== ep: 1886, time 27.264297485351562, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1886
=== ep: 1887, time 27.29969024658203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1887
goal_identified
=== ep: 1888, time 27.064916372299194, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1888
goal_identified
=== ep: 1889, time 31.756837129592896, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1889
goal_identified
goal_identified
=== ep: 1890, time 27.20854377746582, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1890
goal_identified
goal_identified
=== ep: 1891, time 27.22158455848694, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1891
goal_identified
goal_identified
goal_identified
=== ep: 1892, time 27.627514839172363, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1892
goal_identified
goal_identified
=== ep: 1893, time 26.977009296417236, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1893
goal_identified
=== ep: 1894, time 27.317707538604736, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1894
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1895, time 27.212403297424316, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1895
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1896, time 27.547738790512085, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1896
goal_identified
=== ep: 1897, time 27.149954319000244, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1897
goal_identified
goal_identified
goal_identified
=== ep: 1898, time 27.20577049255371, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1898
goal_identified
goal_identified
goal_identified
=== ep: 1899, time 31.857901334762573, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1899
=== ep: 1900, time 27.592987298965454, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1900
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1901, time 27.372278213500977, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1901
goal_identified
goal_identified
=== ep: 1902, time 27.748185396194458, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1902
=== ep: 1903, time 27.3405499458313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1903
goal_identified
goal_identified
goal_identified
=== ep: 1904, time 27.273253440856934, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1904
goal_identified
goal_identified
goal_identified
=== ep: 1905, time 27.328797101974487, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1905
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1906, time 27.76009440422058, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1906
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1907, time 27.477166652679443, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1907
goal_identified
goal_identified
goal_identified
=== ep: 1908, time 26.963238954544067, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1908
goal_identified
goal_identified
=== ep: 1909, time 32.44780111312866, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1909
goal_identified
=== ep: 1910, time 27.68716287612915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1910
goal_identified
goal_identified
=== ep: 1911, time 27.354180812835693, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1911
=== ep: 1912, time 27.29761505126953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1912
goal_identified
goal_identified
=== ep: 1913, time 27.26241636276245, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1913
goal_identified
=== ep: 1914, time 27.414043426513672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1914
goal_identified
goal_identified
=== ep: 1915, time 27.145703315734863, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1915
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1916, time 27.399771690368652, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1916
=== ep: 1917, time 27.176406383514404, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1917
=== ep: 1918, time 27.633869171142578, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1918
goal_identified
=== ep: 1919, time 32.32931399345398, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1919
goal_identified
=== ep: 1920, time 27.54162049293518, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1920
goal_identified
goal_identified
=== ep: 1921, time 27.7107675075531, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1921
goal_identified
goal_identified
=== ep: 1922, time 27.237075328826904, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1922
goal_identified
=== ep: 1923, time 27.463071823120117, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1923
goal_identified
goal_identified
=== ep: 1924, time 27.1801278591156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1924
goal_identified
goal_identified
=== ep: 1925, time 27.058988094329834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1925
goal_identified
goal_identified
goal_identified
=== ep: 1926, time 27.29443669319153, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1926
goal_identified
=== ep: 1927, time 26.885903358459473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1927
goal_identified
goal_identified
=== ep: 1928, time 27.615628957748413, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1929, time 32.62699103355408, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
goal_identified
=== ep: 1930, time 27.235500812530518, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1930
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1931, time 26.967241525650024, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1931
=== ep: 1932, time 27.183234930038452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1932
goal_identified
goal_identified
goal_identified
=== ep: 1933, time 27.4588680267334, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1933
=== ep: 1934, time 27.501421451568604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1934
goal_identified
=== ep: 1935, time 27.64951252937317, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1935
goal_identified
goal_identified
goal_identified
=== ep: 1936, time 27.067429542541504, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1936
goal_identified
goal_identified
goal_identified
=== ep: 1937, time 27.648267030715942, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1937
=== ep: 1938, time 27.651386737823486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1938
goal_identified
goal_identified
=== ep: 1939, time 32.09999680519104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1939
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1940, time 26.91714358329773, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1940
goal_identified
=== ep: 1941, time 27.33635973930359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1941
goal_identified
=== ep: 1942, time 27.67525029182434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1942
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1943, time 27.23822259902954, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1943
goal_identified
goal_identified
goal_identified
=== ep: 1944, time 27.3023362159729, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1944
goal_identified
goal_identified
=== ep: 1945, time 27.531466960906982, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1945
goal_identified
goal_identified
goal_identified
=== ep: 1946, time 28.23891520500183, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1946
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1947, time 27.386237144470215, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1947
goal_identified
=== ep: 1948, time 27.42839813232422, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1948
goal_identified
=== ep: 1949, time 31.81353998184204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1949
goal_identified
goal_identified
=== ep: 1950, time 27.35706353187561, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1950
goal_identified
goal_identified
goal_identified
=== ep: 1951, time 27.393181085586548, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1951
goal_identified
goal_identified
=== ep: 1952, time 27.03313946723938, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1952
goal_identified
goal_identified
=== ep: 1953, time 27.36070680618286, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1953
goal_identified
goal_identified
=== ep: 1954, time 27.26629662513733, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1954
goal_identified
=== ep: 1955, time 27.332943201065063, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1955
goal_identified
=== ep: 1956, time 27.301198482513428, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1956
goal_identified
=== ep: 1957, time 27.631131887435913, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1957
=== ep: 1958, time 27.11508083343506, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1958
goal_identified
goal_identified
=== ep: 1959, time 31.769319534301758, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1959
=== ep: 1960, time 27.45544672012329, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1960
goal_identified
goal_identified
goal_identified
=== ep: 1961, time 27.271469593048096, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1961
goal_identified
goal_identified
=== ep: 1962, time 27.210866928100586, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1962
goal_identified
goal_identified
goal_identified
=== ep: 1963, time 26.76755928993225, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1963
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1964, time 27.095764636993408, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1964
goal_identified
goal_identified
=== ep: 1965, time 27.311453104019165, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1965
=== ep: 1966, time 27.606045722961426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1966
goal_identified
goal_identified
=== ep: 1967, time 27.17838764190674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1967
goal_identified
goal_identified
=== ep: 1968, time 26.435070514678955, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1968
goal_identified
goal_identified
goal_identified
=== ep: 1969, time 32.23825645446777, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1969
goal_identified
goal_identified
=== ep: 1970, time 27.033204078674316, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1970
goal_identified
goal_identified
goal_identified
=== ep: 1971, time 27.426743984222412, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1971
=== ep: 1972, time 27.075225353240967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1972
goal_identified
goal_identified
goal_identified
=== ep: 1973, time 26.73602557182312, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1973
=== ep: 1974, time 27.42717933654785, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1974
goal_identified
goal_identified
=== ep: 1975, time 27.116552591323853, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1976, time 27.270357847213745, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1976
=== ep: 1977, time 26.807424545288086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1977
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1978, time 27.019278526306152, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1978
goal_identified
=== ep: 1979, time 32.28280782699585, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1979
goal_identified
goal_identified
goal_identified
=== ep: 1980, time 27.40662717819214, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1980
goal_identified
goal_identified
goal_identified
=== ep: 1981, time 27.1426362991333, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1981
=== ep: 1982, time 26.779120206832886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1982
goal_identified
goal_identified
goal_identified
=== ep: 1983, time 27.158069849014282, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1983
=== ep: 1984, time 27.317881107330322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1984
goal_identified
=== ep: 1985, time 27.33113670349121, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1985
goal_identified
goal_identified
goal_identified
=== ep: 1986, time 26.74450445175171, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1986
goal_identified
=== ep: 1987, time 27.147871017456055, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1987
goal_identified
goal_identified
=== ep: 1988, time 26.881853342056274, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1988
goal_identified
goal_identified
goal_identified
=== ep: 1989, time 32.24191451072693, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1989
goal_identified
=== ep: 1990, time 27.583644151687622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1990
goal_identified
goal_identified
goal_identified
=== ep: 1991, time 26.931056261062622, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1991
goal_identified
=== ep: 1992, time 27.51197648048401, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1992
goal_identified
goal_identified
goal_identified
=== ep: 1993, time 27.535319089889526, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1993
goal_identified
=== ep: 1994, time 27.61776113510132, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1994
=== ep: 1995, time 27.36735463142395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1995
=== ep: 1996, time 27.106377601623535, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1996
goal_identified
goal_identified
goal_identified
=== ep: 1997, time 27.156442642211914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1997
goal_identified
goal_identified
=== ep: 1998, time 27.720027208328247, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1999, time 32.55911350250244, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1999
goal_identified
=== ep: 2000, time 27.13876700401306, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2000
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2001, time 27.10002112388611, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2001
goal_identified
goal_identified
=== ep: 2002, time 27.409605741500854, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2002
=== ep: 2003, time 27.49058699607849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2003
goal_identified
goal_identified
=== ep: 2004, time 27.330488443374634, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2004
goal_identified
goal_identified
=== ep: 2005, time 26.820903539657593, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2005
goal_identified
=== ep: 2006, time 27.246440887451172, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2006
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2007, time 27.482040405273438, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2007
goal_identified
=== ep: 2008, time 27.124616146087646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2008
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2009, time 32.30542516708374, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2009
goal_identified
goal_identified
=== ep: 2010, time 27.1563081741333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2010
goal_identified
=== ep: 2011, time 27.516605854034424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2011
=== ep: 2012, time 27.510935068130493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2012
goal_identified
=== ep: 2013, time 27.517202138900757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2013
goal_identified
=== ep: 2014, time 27.125584363937378, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2014
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2015, time 27.26983404159546, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2015
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2016, time 27.53155827522278, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2016
=== ep: 2017, time 27.35089349746704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2017
goal_identified
goal_identified
=== ep: 2018, time 27.46048617362976, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2018
=== ep: 2019, time 31.730062007904053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2019
goal_identified
=== ep: 2020, time 27.4383327960968, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2020
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2021, time 27.33959984779358, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2021
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2022, time 27.52802085876465, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
goal_identified
goal_identified
goal_identified
=== ep: 2023, time 26.950239658355713, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2023
goal_identified
goal_identified
=== ep: 2024, time 26.956498622894287, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2024
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2025, time 27.164682388305664, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2025
goal_identified
goal_identified
=== ep: 2026, time 27.241387128829956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2026
=== ep: 2027, time 27.27330780029297, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2027
goal_identified
goal_identified
goal_identified
=== ep: 2028, time 27.420973300933838, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2028
=== ep: 2029, time 31.99731159210205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2029
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2030, time 27.239702463150024, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2030
goal_identified
goal_identified
goal_identified
=== ep: 2031, time 27.607342004776, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2031
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2032, time 27.123939752578735, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2032
goal_identified
goal_identified
goal_identified
=== ep: 2033, time 26.95514678955078, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2033
goal_identified
=== ep: 2034, time 26.77664828300476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2034
goal_identified
goal_identified
goal_identified
=== ep: 2035, time 27.588002920150757, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2035
goal_identified
=== ep: 2036, time 27.471988677978516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2036
goal_identified
=== ep: 2037, time 27.370375394821167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2037
goal_identified
goal_identified
=== ep: 2038, time 26.836087703704834, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2038
goal_identified
goal_identified
=== ep: 2039, time 33.73414444923401, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2039
goal_identified
goal_identified
=== ep: 2040, time 27.227498292922974, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2040
goal_identified
=== ep: 2041, time 27.51218891143799, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2041
goal_identified
goal_identified
=== ep: 2042, time 27.551384925842285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2042
=== ep: 2043, time 26.67724871635437, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2043
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2044, time 27.278536558151245, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2044
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2045, time 27.354726552963257, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2045
goal_identified
goal_identified
=== ep: 2046, time 27.114431381225586, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2046
goal_identified
=== ep: 2047, time 26.898454666137695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2047
goal_identified
=== ep: 2048, time 27.37504744529724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2048
goal_identified
goal_identified
goal_identified
=== ep: 2049, time 34.47146940231323, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2049
goal_identified
goal_identified
goal_identified
=== ep: 2050, time 27.51238703727722, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2050
=== ep: 2051, time 27.149047136306763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2051
goal_identified
goal_identified
=== ep: 2052, time 26.58968997001648, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2052
goal_identified
=== ep: 2053, time 27.396012544631958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2053
goal_identified
=== ep: 2054, time 27.539031267166138, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2054
goal_identified
goal_identified
=== ep: 2055, time 27.165320873260498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2055
goal_identified
goal_identified
=== ep: 2056, time 27.313417196273804, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2056
goal_identified
=== ep: 2057, time 27.20047116279602, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2057
goal_identified
goal_identified
goal_identified
=== ep: 2058, time 26.996016263961792, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2058
goal_identified
=== ep: 2059, time 33.08968472480774, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2059
goal_identified
goal_identified
=== ep: 2060, time 27.44737195968628, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2060
goal_identified
goal_identified
goal_identified
=== ep: 2061, time 26.879795789718628, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2061
=== ep: 2062, time 27.44201421737671, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2062
=== ep: 2063, time 27.262327432632446, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2063
goal_identified
goal_identified
goal_identified
=== ep: 2064, time 27.801719903945923, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2064
goal_identified
goal_identified
=== ep: 2065, time 27.518970251083374, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2065
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2066, time 27.171592473983765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2066
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2067, time 27.04684829711914, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2067
goal_identified
goal_identified
goal_identified
=== ep: 2068, time 27.26004195213318, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2068
=== ep: 2069, time 32.40568399429321, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2069
goal_identified
=== ep: 2070, time 27.87795925140381, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2070
goal_identified
=== ep: 2071, time 27.70236611366272, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2071
goal_identified
goal_identified
goal_identified
=== ep: 2072, time 26.251481771469116, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2072
goal_identified
goal_identified
goal_identified
=== ep: 2073, time 27.237942934036255, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2073
goal_identified
goal_identified
goal_identified
=== ep: 2074, time 27.011090755462646, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2074
goal_identified
=== ep: 2075, time 27.047903537750244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2075
goal_identified
goal_identified
=== ep: 2076, time 27.36000347137451, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2076
goal_identified
goal_identified
goal_identified
=== ep: 2077, time 27.298418045043945, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2077
goal_identified
goal_identified
=== ep: 2078, time 27.533655405044556, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2078
goal_identified
goal_identified
=== ep: 2079, time 31.910572052001953, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2079
goal_identified
goal_identified
=== ep: 2080, time 27.360271692276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2080
goal_identified
=== ep: 2081, time 26.95023226737976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2081
goal_identified
=== ep: 2082, time 27.293220281600952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2082
goal_identified
=== ep: 2083, time 27.221576929092407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2083
goal_identified
goal_identified
goal_identified
=== ep: 2084, time 26.890727519989014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2084
=== ep: 2085, time 27.368896484375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2085
goal_identified
goal_identified
=== ep: 2086, time 27.284308195114136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2086
goal_identified
=== ep: 2087, time 27.190181493759155, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2087
goal_identified
=== ep: 2088, time 27.286638975143433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2088
goal_identified
goal_identified
goal_identified
=== ep: 2089, time 32.28684425354004, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2089
goal_identified
goal_identified
goal_identified
=== ep: 2090, time 27.488272428512573, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2090
goal_identified
goal_identified
=== ep: 2091, time 27.43187117576599, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2091
goal_identified
goal_identified
goal_identified
=== ep: 2092, time 27.606909036636353, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2092
goal_identified
goal_identified
=== ep: 2093, time 26.802735328674316, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2093
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2094, time 28.798336267471313, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2094
goal_identified
goal_identified
=== ep: 2095, time 27.38423800468445, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2095
goal_identified
=== ep: 2096, time 27.596054077148438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2096
goal_identified
goal_identified
goal_identified
=== ep: 2097, time 27.543179750442505, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2097
goal_identified
goal_identified
goal_identified
=== ep: 2098, time 27.070478439331055, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2098
=== ep: 2099, time 32.00380849838257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2099
goal_identified
goal_identified
=== ep: 2100, time 27.222394227981567, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2100
goal_identified
goal_identified
=== ep: 2101, time 27.09548830986023, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2101
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2102, time 27.023573637008667, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2102
goal_identified
goal_identified
goal_identified
=== ep: 2103, time 26.92911982536316, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2103
=== ep: 2104, time 27.39575219154358, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2104
goal_identified
goal_identified
=== ep: 2105, time 27.463542938232422, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2105
goal_identified
goal_identified
=== ep: 2106, time 26.939106225967407, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2106
goal_identified
goal_identified
goal_identified
=== ep: 2107, time 27.24952745437622, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2107
goal_identified
=== ep: 2108, time 27.78644061088562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2108
goal_identified
=== ep: 2109, time 32.43560218811035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2110, time 27.11991834640503, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2110
goal_identified
=== ep: 2111, time 27.39535903930664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2111
goal_identified
goal_identified
=== ep: 2112, time 26.775312900543213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2112
goal_identified
=== ep: 2113, time 27.703391790390015, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2113
=== ep: 2114, time 27.41765856742859, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2114
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2115, time 27.2660710811615, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2115
goal_identified
goal_identified
goal_identified
=== ep: 2116, time 27.086763620376587, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2116
=== ep: 2117, time 27.515169858932495, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2117
goal_identified
goal_identified
goal_identified
=== ep: 2118, time 27.14259934425354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2118
goal_identified
goal_identified
goal_identified
=== ep: 2119, time 32.411943197250366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2119
=== ep: 2120, time 27.090348958969116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2120
goal_identified
goal_identified
goal_identified
=== ep: 2121, time 26.842045783996582, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2121
goal_identified
goal_identified
=== ep: 2122, time 26.932076930999756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2122
goal_identified
goal_identified
=== ep: 2123, time 27.617352724075317, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2123
goal_identified
goal_identified
goal_identified
=== ep: 2124, time 27.251948833465576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2124
goal_identified
=== ep: 2125, time 27.483537673950195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2125
goal_identified
goal_identified
=== ep: 2126, time 27.308294773101807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 27.355910301208496, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2127
goal_identified
goal_identified
goal_identified
=== ep: 2128, time 27.460850715637207, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2128
goal_identified
goal_identified
=== ep: 2129, time 32.10374212265015, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2129
goal_identified
goal_identified
goal_identified
=== ep: 2130, time 27.242125511169434, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2130
goal_identified
goal_identified
=== ep: 2131, time 27.340067625045776, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2131
goal_identified
goal_identified
=== ep: 2132, time 27.721248626708984, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2132
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2133, time 27.204835891723633, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2133
goal_identified
=== ep: 2134, time 27.154701948165894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2134
goal_identified
goal_identified
=== ep: 2135, time 27.00479769706726, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2135
goal_identified
goal_identified
goal_identified
=== ep: 2136, time 27.39961814880371, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2136
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2137, time 27.465789794921875, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2137
goal_identified
goal_identified
=== ep: 2138, time 27.338101148605347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2139, time 31.675819396972656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2139
goal_identified
=== ep: 2140, time 27.713828802108765, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2140
goal_identified
=== ep: 2141, time 27.475942850112915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2141
goal_identified
goal_identified
goal_identified
=== ep: 2142, time 27.13668131828308, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2142
goal_identified
goal_identified
goal_identified
=== ep: 2143, time 27.07525086402893, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2143
goal_identified
goal_identified
=== ep: 2144, time 27.478651762008667, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2144
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2145, time 27.23437190055847, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2145
goal_identified
goal_identified
goal_identified
=== ep: 2146, time 27.423846006393433, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2146
=== ep: 2147, time 27.260735750198364, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2147
=== ep: 2148, time 27.078380346298218, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2148
goal_identified
goal_identified
goal_identified
=== ep: 2149, time 32.21255898475647, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2149
goal_identified
=== ep: 2150, time 27.5752432346344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2150
goal_identified
=== ep: 2151, time 27.28918218612671, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2151
goal_identified
goal_identified
=== ep: 2152, time 27.093918085098267, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2152
goal_identified
=== ep: 2153, time 27.13117027282715, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2154, time 27.072543621063232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2154
goal_identified
goal_identified
goal_identified
=== ep: 2155, time 27.574522256851196, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2155
goal_identified
goal_identified
goal_identified
=== ep: 2156, time 27.456456661224365, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2156
goal_identified
goal_identified
goal_identified
=== ep: 2157, time 26.970705032348633, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2157
goal_identified
goal_identified
=== ep: 2158, time 27.193012952804565, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2158
goal_identified
goal_identified
goal_identified
=== ep: 2159, time 32.188817501068115, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2160, time 27.356300354003906, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2160
goal_identified
=== ep: 2161, time 27.387821435928345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2162, time 26.85874605178833, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2162
goal_identified
=== ep: 2163, time 27.04876208305359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2163
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2164, time 27.140878438949585, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2164
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2165, time 27.216163396835327, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2165
goal_identified
goal_identified
goal_identified
=== ep: 2166, time 28.413983821868896, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2166
goal_identified
goal_identified
=== ep: 2167, time 26.805134057998657, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2167
=== ep: 2168, time 27.46397852897644, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2168
goal_identified
=== ep: 2169, time 32.20435357093811, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2169
=== ep: 2170, time 27.46869969367981, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2170
goal_identified
goal_identified
=== ep: 2171, time 26.81908082962036, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2171
goal_identified
=== ep: 2172, time 27.424513578414917, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2172
=== ep: 2173, time 27.46660828590393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2173
goal_identified
goal_identified
=== ep: 2174, time 27.271732330322266, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2174
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2175, time 27.156471252441406, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2175
goal_identified
=== ep: 2176, time 27.091228723526, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2176
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2177, time 27.514657497406006, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2177
goal_identified
goal_identified
=== ep: 2178, time 27.20676875114441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2178
goal_identified
goal_identified
goal_identified
=== ep: 2179, time 32.16762733459473, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2179
goal_identified
goal_identified
=== ep: 2180, time 26.84881353378296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2180
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2181, time 27.40921640396118, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2181
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2182, time 27.456063270568848, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2182
goal_identified
goal_identified
=== ep: 2183, time 27.426852703094482, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2183
=== ep: 2184, time 27.03126573562622, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2184
goal_identified
=== ep: 2185, time 26.794389247894287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2185
goal_identified
goal_identified
=== ep: 2186, time 27.232858419418335, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2186
goal_identified
goal_identified
=== ep: 2187, time 27.351258754730225, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2187
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2188, time 26.96572208404541, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2188
goal_identified
=== ep: 2189, time 32.1621630191803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2189
goal_identified
=== ep: 2190, time 27.258910655975342, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2190
goal_identified
goal_identified
=== ep: 2191, time 27.275192737579346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2191
goal_identified
=== ep: 2192, time 27.619450092315674, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2192
goal_identified
goal_identified
=== ep: 2193, time 26.926374673843384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2193
goal_identified
=== ep: 2194, time 26.710686683654785, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2194
=== ep: 2195, time 27.34999966621399, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2195
goal_identified
goal_identified
=== ep: 2196, time 27.244218587875366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2196
goal_identified
goal_identified
=== ep: 2197, time 27.263059616088867, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2197
goal_identified
goal_identified
=== ep: 2198, time 27.295121669769287, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2198
=== ep: 2199, time 31.668210744857788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2199
goal_identified
goal_identified
=== ep: 2200, time 27.242981910705566, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2200
goal_identified
goal_identified
goal_identified
=== ep: 2201, time 27.109386444091797, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2201
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2202, time 27.136656045913696, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2202
goal_identified
=== ep: 2203, time 26.821194648742676, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2203
=== ep: 2204, time 26.94894504547119, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2204
goal_identified
goal_identified
goal_identified
=== ep: 2205, time 27.06038498878479, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2205
goal_identified
=== ep: 2206, time 27.677621841430664, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2206
goal_identified
goal_identified
goal_identified
=== ep: 2207, time 27.79618239402771, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2207
=== ep: 2208, time 27.68246293067932, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2208
goal_identified
goal_identified
=== ep: 2209, time 32.34499955177307, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2209
goal_identified
=== ep: 2210, time 26.970921754837036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2210
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2211, time 27.280410289764404, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2211
goal_identified
goal_identified
goal_identified
=== ep: 2212, time 26.633148670196533, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2212
goal_identified
goal_identified
=== ep: 2213, time 26.900480270385742, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2213
goal_identified
goal_identified
=== ep: 2214, time 27.288889169692993, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2214
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2215, time 27.336669445037842, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2215
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2216, time 27.235378980636597, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2216
goal_identified
=== ep: 2217, time 27.26253080368042, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2217
goal_identified
=== ep: 2218, time 26.994865655899048, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2218
goal_identified
=== ep: 2219, time 31.89231252670288, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2219
goal_identified
goal_identified
=== ep: 2220, time 27.55363631248474, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2220
goal_identified
goal_identified
=== ep: 2221, time 27.135776042938232, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2221
goal_identified
goal_identified
goal_identified
=== ep: 2222, time 26.816392421722412, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2222
goal_identified
=== ep: 2223, time 27.618334531784058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2223
goal_identified
goal_identified
=== ep: 2224, time 27.529193878173828, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2224
goal_identified
=== ep: 2225, time 27.343610048294067, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2225
goal_identified
goal_identified
goal_identified
=== ep: 2226, time 27.322980642318726, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2226
goal_identified
=== ep: 2227, time 26.956798553466797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2227
goal_identified
goal_identified
=== ep: 2228, time 27.722233533859253, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2228
goal_identified
=== ep: 2229, time 32.37496089935303, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2229
goal_identified
=== ep: 2230, time 27.067893981933594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2230
goal_identified
goal_identified
=== ep: 2231, time 26.516014337539673, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2231
=== ep: 2232, time 27.311006784439087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2232
goal_identified
=== ep: 2233, time 27.08651351928711, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2233
goal_identified
goal_identified
goal_identified
=== ep: 2234, time 27.15877890586853, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2234
goal_identified
goal_identified
=== ep: 2235, time 27.13548469543457, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2235
goal_identified
=== ep: 2236, time 26.848239183425903, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2236
goal_identified
=== ep: 2237, time 26.53868055343628, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2237
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2238, time 27.327940225601196, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2238
goal_identified
=== ep: 2239, time 32.16724991798401, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2239
=== ep: 2240, time 23.91021466255188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 13/13)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2240
goal_identified
goal_identified
goal_identified
=== ep: 2241, time 27.175117254257202, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2241
goal_identified
goal_identified
goal_identified
=== ep: 2242, time 27.219851970672607, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2242
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2243, time 26.890281915664673, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2243
goal_identified
=== ep: 2244, time 27.17797088623047, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 27.336823225021362, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2246, time 27.201655387878418, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2246
goal_identified
goal_identified
goal_identified
=== ep: 2247, time 27.586999654769897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2247
goal_identified
goal_identified
goal_identified
=== ep: 2248, time 27.284471035003662, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2248
=== ep: 2249, time 33.3374764919281, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2249
goal_identified
=== ep: 2250, time 27.033493518829346, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2250
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2251, time 26.990437030792236, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2251
goal_identified
=== ep: 2252, time 27.25533628463745, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2252
goal_identified
goal_identified
goal_identified
=== ep: 2253, time 26.98339867591858, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2253
goal_identified
goal_identified
=== ep: 2254, time 27.133354663848877, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2254
=== ep: 2255, time 27.28638982772827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2255
goal_identified
goal_identified
goal_identified
=== ep: 2256, time 26.92277503013611, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2256
goal_identified
=== ep: 2257, time 26.908063173294067, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2257
goal_identified
goal_identified
=== ep: 2258, time 27.024518251419067, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2258
goal_identified
goal_identified
=== ep: 2259, time 32.22804546356201, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2259
goal_identified
goal_identified
=== ep: 2260, time 26.696178197860718, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2260
goal_identified
=== ep: 2261, time 27.579258918762207, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2261
goal_identified
goal_identified
goal_identified
=== ep: 2262, time 27.239620447158813, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2262
goal_identified
goal_identified
goal_identified
=== ep: 2263, time 27.18761157989502, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2263
goal_identified
goal_identified
=== ep: 2264, time 27.162031412124634, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2264
goal_identified
=== ep: 2265, time 27.581785440444946, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2265
goal_identified
goal_identified
=== ep: 2266, time 27.084766149520874, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2266
goal_identified
=== ep: 2267, time 27.304248332977295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2267
=== ep: 2268, time 26.9979727268219, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2268
goal_identified
goal_identified
goal_identified
=== ep: 2269, time 31.748263359069824, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2269
goal_identified
goal_identified
=== ep: 2270, time 27.765374898910522, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2270
goal_identified
=== ep: 2271, time 27.19701600074768, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2271
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2272, time 27.07183861732483, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2272
goal_identified
goal_identified
=== ep: 2273, time 26.993760585784912, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2273
goal_identified
goal_identified
goal_identified
=== ep: 2274, time 26.9253191947937, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2274
goal_identified
goal_identified
goal_identified
=== ep: 2275, time 27.436184406280518, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 27.609824895858765, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2276
=== ep: 2277, time 27.369009733200073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2277
goal_identified
goal_identified
goal_identified
=== ep: 2278, time 27.337701559066772, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2278
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2279, time 32.50158095359802, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2279
goal_identified
goal_identified
goal_identified
=== ep: 2280, time 27.26105570793152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2280
=== ep: 2281, time 27.36153483390808, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2281
goal_identified
goal_identified
goal_identified
=== ep: 2282, time 27.210816144943237, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2282
goal_identified
=== ep: 2283, time 26.904704570770264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2283
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2284, time 27.328615188598633, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2284
goal_identified
=== ep: 2285, time 27.132731676101685, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2285
goal_identified
goal_identified
goal_identified
=== ep: 2286, time 27.513185739517212, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2286
goal_identified
goal_identified
=== ep: 2287, time 27.415311813354492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2287
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2288, time 26.637428522109985, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2288
goal_identified
=== ep: 2289, time 32.18539500236511, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2289
goal_identified
goal_identified
=== ep: 2290, time 27.1954448223114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2290
goal_identified
=== ep: 2291, time 27.286391735076904, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2291
goal_identified
goal_identified
=== ep: 2292, time 26.886314153671265, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2292
goal_identified
goal_identified
=== ep: 2293, time 27.4105007648468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2293
goal_identified
goal_identified
goal_identified
=== ep: 2294, time 27.17539381980896, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2294
goal_identified
goal_identified
goal_identified
=== ep: 2295, time 27.210902214050293, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2295
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2296, time 27.03070616722107, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2296
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2297, time 27.063356399536133, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2297
goal_identified
goal_identified
=== ep: 2298, time 26.88707137107849, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2298
=== ep: 2299, time 32.24552869796753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 27.38154363632202, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2300
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2301, time 27.0224506855011, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2301
goal_identified
goal_identified
goal_identified
=== ep: 2302, time 26.796403169631958, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2302
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2303, time 27.28034281730652, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2303
goal_identified
goal_identified
=== ep: 2304, time 27.457311153411865, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2304
goal_identified
goal_identified
=== ep: 2305, time 27.483673810958862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2305
goal_identified
=== ep: 2306, time 27.160929203033447, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2306
goal_identified
goal_identified
=== ep: 2307, time 27.150951623916626, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2307
goal_identified
goal_identified
=== ep: 2308, time 27.316928148269653, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2308
=== ep: 2309, time 32.49034881591797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2309
goal_identified
goal_identified
goal_identified
=== ep: 2310, time 27.576608419418335, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2310
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2311, time 27.035993814468384, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2311
goal_identified
goal_identified
=== ep: 2312, time 27.248902797698975, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2312
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2313, time 27.39008378982544, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2313
goal_identified
=== ep: 2314, time 27.30813431739807, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2314
goal_identified
=== ep: 2315, time 27.362861156463623, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2315
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2316, time 26.906886100769043, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2316
goal_identified
goal_identified
goal_identified
=== ep: 2317, time 26.852216958999634, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2317
goal_identified
=== ep: 2318, time 27.135787963867188, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2318
goal_identified
goal_identified
goal_identified
=== ep: 2319, time 32.307520627975464, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2319
=== ep: 2320, time 26.993212938308716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2320
goal_identified
goal_identified
goal_identified
=== ep: 2321, time 27.084176063537598, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2321
goal_identified
goal_identified
=== ep: 2322, time 27.16676950454712, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2322
goal_identified
=== ep: 2323, time 27.376421689987183, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2323
=== ep: 2324, time 27.16001033782959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2324
goal_identified
=== ep: 2325, time 27.098471879959106, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2325
goal_identified
goal_identified
=== ep: 2326, time 27.17255163192749, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2326
goal_identified
=== ep: 2327, time 27.35590672492981, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2327
goal_identified
=== ep: 2328, time 27.801230907440186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2328
goal_identified
goal_identified
=== ep: 2329, time 32.286537408828735, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2329
goal_identified
goal_identified
=== ep: 2330, time 26.75420308113098, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2330
=== ep: 2331, time 27.47701644897461, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2331
goal_identified
goal_identified
=== ep: 2332, time 27.37001609802246, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2332
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2333, time 27.118290662765503, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2333
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2334, time 27.47118377685547, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2334
goal_identified
goal_identified
goal_identified
=== ep: 2335, time 27.3508243560791, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2335
goal_identified
goal_identified
=== ep: 2336, time 27.267481565475464, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2336
goal_identified
goal_identified
=== ep: 2337, time 27.053003311157227, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2338, time 27.4542453289032, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2338
goal_identified
goal_identified
=== ep: 2339, time 31.961440563201904, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2339
goal_identified
=== ep: 2340, time 27.318910121917725, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2340
goal_identified
=== ep: 2341, time 27.539154052734375, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2341
goal_identified
=== ep: 2342, time 31.401519298553467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2342
goal_identified
goal_identified
=== ep: 2343, time 27.235191106796265, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2343
goal_identified
goal_identified
goal_identified
=== ep: 2344, time 27.222232580184937, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2344
=== ep: 2345, time 27.390450716018677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2345
goal_identified
goal_identified
=== ep: 2346, time 27.008456230163574, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2346
goal_identified
goal_identified
goal_identified
=== ep: 2347, time 27.446029901504517, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2347
goal_identified
goal_identified
=== ep: 2348, time 27.324787378311157, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2348
goal_identified
goal_identified
=== ep: 2349, time 32.59872508049011, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2349
goal_identified
goal_identified
=== ep: 2350, time 27.2238826751709, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2350
goal_identified
=== ep: 2351, time 27.510778665542603, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2351
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2352, time 27.29997444152832, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2352
goal_identified
goal_identified
goal_identified
=== ep: 2353, time 27.204177141189575, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2353
=== ep: 2354, time 27.153653144836426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2354
goal_identified
=== ep: 2355, time 27.266746282577515, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2355
goal_identified
goal_identified
goal_identified
=== ep: 2356, time 27.11115336418152, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2356
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2357, time 27.333410024642944, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2357
=== ep: 2358, time 27.415332317352295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2358
goal_identified
=== ep: 2359, time 32.28807616233826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2359
goal_identified
=== ep: 2360, time 27.602015733718872, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2360
goal_identified
=== ep: 2361, time 27.34668469429016, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2361
goal_identified
goal_identified
goal_identified
=== ep: 2362, time 26.78452181816101, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2362
goal_identified
goal_identified
=== ep: 2363, time 27.89221739768982, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2363
goal_identified
goal_identified
=== ep: 2364, time 27.35391902923584, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2364
=== ep: 2365, time 27.111077070236206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2365
goal_identified
goal_identified
=== ep: 2366, time 27.594483137130737, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2366
goal_identified
=== ep: 2367, time 26.691758394241333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2367
=== ep: 2368, time 27.336955547332764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2368
goal_identified
goal_identified
goal_identified
=== ep: 2369, time 32.53497314453125, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2370, time 27.3970787525177, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2370
=== ep: 2371, time 27.336544513702393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2371
goal_identified
goal_identified
goal_identified
=== ep: 2372, time 26.78474712371826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2372
goal_identified
=== ep: 2373, time 27.43840193748474, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2373
goal_identified
=== ep: 2374, time 27.55597972869873, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2374
goal_identified
goal_identified
goal_identified
=== ep: 2375, time 27.207582712173462, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2375
goal_identified
goal_identified
=== ep: 2376, time 27.206936836242676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2376
goal_identified
goal_identified
goal_identified
=== ep: 2377, time 27.06445550918579, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2377
goal_identified
=== ep: 2378, time 27.5000581741333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2379, time 33.59729075431824, eps 0.001, sum reward: 5, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2379
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2380, time 27.307652711868286, eps 0.001, sum reward: 5, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2380
goal_identified
goal_identified
goal_identified
=== ep: 2381, time 26.69088053703308, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2381
=== ep: 2382, time 27.59183406829834, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2382
goal_identified
=== ep: 2383, time 27.214107036590576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2383
goal_identified
goal_identified
=== ep: 2384, time 27.23460030555725, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2384
goal_identified
=== ep: 2385, time 27.012767791748047, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2385
=== ep: 2386, time 27.268383502960205, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2386
goal_identified
goal_identified
goal_identified
=== ep: 2387, time 27.54086422920227, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2387
goal_identified
goal_identified
=== ep: 2388, time 27.100658178329468, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2388
=== ep: 2389, time 34.28758478164673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2389
goal_identified
goal_identified
=== ep: 2390, time 26.95479130744934, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2390
goal_identified
=== ep: 2391, time 27.234992265701294, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2391
goal_identified
goal_identified
=== ep: 2392, time 27.457667589187622, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2392
goal_identified
goal_identified
goal_identified
=== ep: 2393, time 27.442622423171997, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2393
goal_identified
goal_identified
goal_identified
=== ep: 2394, time 27.494508266448975, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2394
goal_identified
=== ep: 2395, time 26.852466821670532, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2395
goal_identified
=== ep: 2396, time 27.295515298843384, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2396
goal_identified
=== ep: 2397, time 27.819080591201782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2397
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2398, time 27.571547746658325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2398
goal_identified
goal_identified
goal_identified
=== ep: 2399, time 34.48608994483948, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2399
goal_identified
=== ep: 2400, time 27.257376194000244, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2400
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2401, time 27.411941289901733, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2402, time 27.108974933624268, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2402
goal_identified
goal_identified
goal_identified
=== ep: 2403, time 27.255693674087524, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2403
goal_identified
goal_identified
goal_identified
=== ep: 2404, time 27.079105615615845, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2404
goal_identified
goal_identified
=== ep: 2405, time 27.271149396896362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2405
goal_identified
goal_identified
goal_identified
=== ep: 2406, time 27.075312614440918, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2406
goal_identified
goal_identified
goal_identified
=== ep: 2407, time 27.338812351226807, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 27.314823627471924, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2409, time 34.03581929206848, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2409
goal_identified
goal_identified
=== ep: 2410, time 27.529228925704956, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2411, time 27.302971363067627, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 27.073193788528442, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2412
goal_identified
goal_identified
goal_identified
=== ep: 2413, time 27.291527271270752, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2413
goal_identified
=== ep: 2414, time 27.179320812225342, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2414
goal_identified
goal_identified
=== ep: 2415, time 27.049264669418335, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2415
goal_identified
goal_identified
=== ep: 2416, time 27.654168367385864, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 27.50089979171753, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2417
goal_identified
goal_identified
=== ep: 2418, time 27.289899587631226, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2418
goal_identified
goal_identified
=== ep: 2419, time 32.96961975097656, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2419
goal_identified
goal_identified
=== ep: 2420, time 27.484708309173584, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2420
=== ep: 2421, time 27.470921993255615, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2421
=== ep: 2422, time 27.626943588256836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2422
goal_identified
=== ep: 2423, time 27.110216856002808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2423
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2424, time 26.918338298797607, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2424
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2425, time 27.589527130126953, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2425
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2426, time 27.08641481399536, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2426
goal_identified
=== ep: 2427, time 27.536401510238647, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2427
goal_identified
=== ep: 2428, time 27.10343909263611, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2428
goal_identified
goal_identified
goal_identified
=== ep: 2429, time 32.25241017341614, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2429
=== ep: 2430, time 27.231472969055176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2430
goal_identified
goal_identified
=== ep: 2431, time 27.60028100013733, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2431
goal_identified
goal_identified
=== ep: 2432, time 26.850260972976685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2432
goal_identified
goal_identified
=== ep: 2433, time 27.665443897247314, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2433
goal_identified
goal_identified
=== ep: 2434, time 26.911877632141113, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2434
goal_identified
goal_identified
=== ep: 2435, time 27.26052212715149, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2435
goal_identified
=== ep: 2436, time 27.357571601867676, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2436
=== ep: 2437, time 27.533021450042725, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2437
goal_identified
goal_identified
=== ep: 2438, time 27.551990747451782, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2438
goal_identified
goal_identified
goal_identified
=== ep: 2439, time 31.96747088432312, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2439
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2440, time 27.768367290496826, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2440
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2441, time 27.307777881622314, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2441
=== ep: 2442, time 27.622499227523804, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2442
goal_identified
goal_identified
=== ep: 2443, time 26.654048681259155, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2443
goal_identified
goal_identified
=== ep: 2444, time 27.00607419013977, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2444
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2445, time 27.51908588409424, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2445
goal_identified
goal_identified
goal_identified
=== ep: 2446, time 27.63754391670227, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2446
goal_identified
goal_identified
=== ep: 2447, time 27.187246561050415, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2447
goal_identified
goal_identified
=== ep: 2448, time 27.208073139190674, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2448
goal_identified
=== ep: 2449, time 32.37300682067871, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2449
goal_identified
goal_identified
goal_identified
=== ep: 2450, time 27.41227698326111, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2450
goal_identified
=== ep: 2451, time 27.223772048950195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2451
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2452, time 27.22176957130432, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2452
goal_identified
=== ep: 2453, time 27.52259135246277, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2453
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2454, time 27.61313796043396, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2454
goal_identified
goal_identified
=== ep: 2455, time 27.209557056427002, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2455
goal_identified
goal_identified
=== ep: 2456, time 27.259075164794922, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2456
goal_identified
=== ep: 2457, time 27.103148221969604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2457
goal_identified
=== ep: 2458, time 27.014400959014893, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2458
goal_identified
goal_identified
=== ep: 2459, time 32.66066288948059, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2459
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2460, time 27.291736841201782, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2460
goal_identified
goal_identified
=== ep: 2461, time 27.220298290252686, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2461
goal_identified
=== ep: 2462, time 26.743304014205933, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2462
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2463, time 27.378178119659424, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2463
goal_identified
=== ep: 2464, time 27.56604814529419, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2464
goal_identified
goal_identified
goal_identified
=== ep: 2465, time 27.208210706710815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2465
goal_identified
goal_identified
=== ep: 2466, time 27.226959943771362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2466
goal_identified
goal_identified
=== ep: 2467, time 26.65282392501831, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2467
goal_identified
=== ep: 2468, time 27.540318727493286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2468
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2469, time 32.06335759162903, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2469
goal_identified
goal_identified
goal_identified
=== ep: 2470, time 27.48345685005188, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2470
goal_identified
goal_identified
=== ep: 2471, time 27.038657426834106, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2471
goal_identified
=== ep: 2472, time 27.201090574264526, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2472
goal_identified
goal_identified
=== ep: 2473, time 27.04356336593628, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2473
=== ep: 2474, time 27.565701961517334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2474
goal_identified
goal_identified
goal_identified
=== ep: 2475, time 27.242618083953857, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2475
goal_identified
=== ep: 2476, time 27.248668670654297, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2477, time 27.114477157592773, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2477
goal_identified
=== ep: 2478, time 27.732406616210938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2478
goal_identified
goal_identified
=== ep: 2479, time 32.31840181350708, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2479
goal_identified
goal_identified
=== ep: 2480, time 27.075971603393555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2480
goal_identified
=== ep: 2481, time 27.34314203262329, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2481
goal_identified
goal_identified
=== ep: 2482, time 27.466243982315063, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2482
goal_identified
goal_identified
=== ep: 2483, time 27.219062566757202, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2483
goal_identified
goal_identified
=== ep: 2484, time 27.18135380744934, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2485, time 27.550844430923462, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2485
goal_identified
=== ep: 2486, time 26.99027729034424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2486
=== ep: 2487, time 27.33413076400757, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2488, time 27.175886154174805, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2488
goal_identified
goal_identified
=== ep: 2489, time 32.31255865097046, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2489
goal_identified
goal_identified
=== ep: 2490, time 27.336368560791016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2490
=== ep: 2491, time 26.850531101226807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2491
goal_identified
=== ep: 2492, time 27.23306393623352, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2492
goal_identified
=== ep: 2493, time 27.374227285385132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2493
goal_identified
goal_identified
=== ep: 2494, time 27.64644145965576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2494
goal_identified
goal_identified
=== ep: 2495, time 27.402841806411743, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2495
goal_identified
=== ep: 2496, time 26.711894512176514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2496
=== ep: 2497, time 27.12290668487549, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2497
goal_identified
=== ep: 2498, time 27.249111890792847, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 32.137423276901245, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2499
goal_identified
goal_identified
goal_identified
=== ep: 2500, time 27.445396423339844, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2500
=== ep: 2501, time 27.097806692123413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2501
goal_identified
=== ep: 2502, time 27.29832434654236, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 27.042922258377075, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2503
goal_identified
goal_identified
goal_identified
=== ep: 2504, time 27.10204315185547, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2504
=== ep: 2505, time 26.8449285030365, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2505
goal_identified
goal_identified
=== ep: 2506, time 26.86056613922119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2506
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2507, time 27.37381362915039, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2507
goal_identified
goal_identified
goal_identified
=== ep: 2508, time 27.549580812454224, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2508
goal_identified
goal_identified
goal_identified
=== ep: 2509, time 32.053579807281494, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2509
goal_identified
goal_identified
goal_identified
=== ep: 2510, time 27.50377082824707, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2511, time 27.058285236358643, eps 0.001, sum reward: 6, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2511
goal_identified
=== ep: 2512, time 26.92109179496765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2512
goal_identified
goal_identified
=== ep: 2513, time 27.062375783920288, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2514, time 27.284165859222412, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2514
goal_identified
goal_identified
=== ep: 2515, time 27.245368719100952, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2515
goal_identified
goal_identified
goal_identified
=== ep: 2516, time 27.373461961746216, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2516
goal_identified
=== ep: 2517, time 27.126233339309692, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2517
=== ep: 2518, time 27.58970308303833, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2518
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2519, time 32.69926691055298, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2519
goal_identified
goal_identified
=== ep: 2520, time 26.578893899917603, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2520
goal_identified
goal_identified
goal_identified
=== ep: 2521, time 27.124538898468018, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2521
goal_identified
=== ep: 2522, time 27.298032760620117, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2522
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2523, time 27.153051614761353, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2523
goal_identified
=== ep: 2524, time 27.40601944923401, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2524
goal_identified
goal_identified
=== ep: 2525, time 27.240851402282715, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2525
=== ep: 2526, time 27.447315454483032, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2526
=== ep: 2527, time 27.64074158668518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2527
goal_identified
goal_identified
goal_identified
=== ep: 2528, time 27.654528617858887, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2528
goal_identified
=== ep: 2529, time 32.140941858291626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2529
goal_identified
=== ep: 2530, time 26.993409633636475, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2530
goal_identified
goal_identified
goal_identified
=== ep: 2531, time 27.357022047042847, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2531
=== ep: 2532, time 27.5654935836792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2532
=== ep: 2533, time 27.258262395858765, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2533
goal_identified
goal_identified
=== ep: 2534, time 26.93059492111206, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2534
goal_identified
goal_identified
=== ep: 2535, time 27.481153964996338, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2535
goal_identified
=== ep: 2536, time 27.532430171966553, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2536
=== ep: 2537, time 27.432689428329468, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2537
goal_identified
goal_identified
goal_identified
=== ep: 2538, time 27.23691415786743, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2538
goal_identified
=== ep: 2539, time 31.86390471458435, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2539
goal_identified
goal_identified
goal_identified
=== ep: 2540, time 27.368340492248535, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2540
goal_identified
goal_identified
goal_identified
=== ep: 2541, time 27.33566164970398, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2541
goal_identified
goal_identified
goal_identified
=== ep: 2542, time 27.370611906051636, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2542
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2543, time 26.80654525756836, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2543
=== ep: 2544, time 27.205857753753662, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2544
goal_identified
=== ep: 2545, time 27.489378690719604, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2545
goal_identified
goal_identified
goal_identified
=== ep: 2546, time 27.41176962852478, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2546
=== ep: 2547, time 27.518792629241943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2547
goal_identified
goal_identified
goal_identified
=== ep: 2548, time 26.89612317085266, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2548
goal_identified
goal_identified
=== ep: 2549, time 31.954448699951172, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2549
goal_identified
goal_identified
=== ep: 2550, time 27.315085649490356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2550
goal_identified
=== ep: 2551, time 27.03774619102478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2551
goal_identified
goal_identified
=== ep: 2552, time 27.17422366142273, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2552
=== ep: 2553, time 26.860179662704468, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2553
goal_identified
=== ep: 2554, time 27.04516339302063, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2554
goal_identified
=== ep: 2555, time 27.229430198669434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2555
goal_identified
goal_identified
goal_identified
=== ep: 2556, time 27.60517168045044, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2556
goal_identified
goal_identified
=== ep: 2557, time 27.18259310722351, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2557
goal_identified
=== ep: 2558, time 27.083834648132324, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2558
goal_identified
goal_identified
goal_identified
=== ep: 2559, time 32.59451150894165, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2559
=== ep: 2560, time 27.290454626083374, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2560
=== ep: 2561, time 27.1731960773468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2561
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2562, time 26.914588928222656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2562
=== ep: 2563, time 27.036339044570923, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2563
goal_identified
goal_identified
=== ep: 2564, time 26.960372924804688, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2564
goal_identified
goal_identified
=== ep: 2565, time 27.54541301727295, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2565
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2566, time 27.131292581558228, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2566
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2567, time 27.387719869613647, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2567
goal_identified
goal_identified
=== ep: 2568, time 26.66007709503174, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2568
=== ep: 2569, time 32.07441592216492, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 27.360336780548096, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2570
goal_identified
goal_identified
=== ep: 2571, time 27.449162483215332, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2571
goal_identified
goal_identified
=== ep: 2572, time 27.103528261184692, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2572
goal_identified
=== ep: 2573, time 26.909082174301147, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2573
goal_identified
=== ep: 2574, time 26.89319944381714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2574
goal_identified
=== ep: 2575, time 27.421144723892212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2575
goal_identified
=== ep: 2576, time 27.3958420753479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2576
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2577, time 26.78996467590332, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2577
goal_identified
goal_identified
=== ep: 2578, time 27.228074550628662, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2579, time 31.993036031723022, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2579
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2580, time 27.00369119644165, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2580
goal_identified
goal_identified
goal_identified
=== ep: 2581, time 27.168407440185547, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2581
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2582, time 27.114652156829834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2582
goal_identified
goal_identified
goal_identified
=== ep: 2583, time 26.981611251831055, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2583
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2584, time 27.02630043029785, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2584
goal_identified
goal_identified
goal_identified
=== ep: 2585, time 27.198939085006714, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2585
goal_identified
goal_identified
=== ep: 2586, time 27.130305528640747, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2587, time 27.346288442611694, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2587
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2588, time 27.132830142974854, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2588
goal_identified
goal_identified
=== ep: 2589, time 32.1393666267395, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2589
goal_identified
goal_identified
goal_identified
=== ep: 2590, time 27.482973098754883, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2590
goal_identified
=== ep: 2591, time 27.333857774734497, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2591
goal_identified
=== ep: 2592, time 27.02038598060608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2592
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2593, time 27.010854244232178, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2593
goal_identified
goal_identified
=== ep: 2594, time 27.486631870269775, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2594
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2595, time 27.20635962486267, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2595
goal_identified
goal_identified
=== ep: 2596, time 27.36624574661255, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2596
goal_identified
goal_identified
goal_identified
=== ep: 2597, time 27.24504566192627, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2597
goal_identified
=== ep: 2598, time 26.777869701385498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2599, time 32.13531184196472, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2599
goal_identified
goal_identified
=== ep: 2600, time 27.290119409561157, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2600
goal_identified
goal_identified
goal_identified
=== ep: 2601, time 27.204925298690796, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2601
=== ep: 2602, time 26.854473114013672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2602
goal_identified
goal_identified
goal_identified
=== ep: 2603, time 27.3687686920166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2603
goal_identified
goal_identified
=== ep: 2604, time 27.29471516609192, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2604
=== ep: 2605, time 27.130271911621094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2605
goal_identified
goal_identified
goal_identified
=== ep: 2606, time 27.096856594085693, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2606
goal_identified
goal_identified
=== ep: 2607, time 27.0479838848114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2607
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2608, time 27.413936138153076, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2608
goal_identified
goal_identified
=== ep: 2609, time 32.12343716621399, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2609
goal_identified
goal_identified
goal_identified
=== ep: 2610, time 27.424072742462158, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2610
goal_identified
=== ep: 2611, time 26.663742542266846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2611
goal_identified
=== ep: 2612, time 27.242021560668945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2612
goal_identified
goal_identified
goal_identified
=== ep: 2613, time 26.88402795791626, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2613
goal_identified
goal_identified
=== ep: 2614, time 27.52515459060669, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2614
goal_identified
goal_identified
goal_identified
=== ep: 2615, time 27.239088773727417, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2615
goal_identified
goal_identified
=== ep: 2616, time 26.652833461761475, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2616
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2617, time 27.49527668952942, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
goal_identified
goal_identified
=== ep: 2618, time 26.991798400878906, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2618
goal_identified
goal_identified
=== ep: 2619, time 32.938947916030884, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2619
goal_identified
=== ep: 2620, time 26.781562089920044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2620
goal_identified
=== ep: 2621, time 27.016566038131714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2621
goal_identified
goal_identified
goal_identified
=== ep: 2622, time 27.188464164733887, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2622
=== ep: 2623, time 27.435683250427246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2623
goal_identified
=== ep: 2624, time 27.569016933441162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2624
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2625, time 26.895885944366455, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2625
goal_identified
goal_identified
=== ep: 2626, time 27.0945303440094, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2626
goal_identified
=== ep: 2627, time 27.097681760787964, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2627
goal_identified
=== ep: 2628, time 27.651852130889893, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2628
goal_identified
goal_identified
goal_identified
=== ep: 2629, time 32.01974964141846, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2629
=== ep: 2630, time 27.209620714187622, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2630
goal_identified
goal_identified
goal_identified
=== ep: 2631, time 27.327367067337036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2631
=== ep: 2632, time 27.254207134246826, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2632
goal_identified
goal_identified
=== ep: 2633, time 27.134187936782837, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2633
goal_identified
goal_identified
=== ep: 2634, time 27.42347025871277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2634
goal_identified
goal_identified
=== ep: 2635, time 27.355572938919067, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2635
=== ep: 2636, time 27.2815682888031, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2637, time 27.339279174804688, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2637
goal_identified
goal_identified
=== ep: 2638, time 27.30982208251953, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2638
goal_identified
=== ep: 2639, time 32.36090850830078, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2639
goal_identified
goal_identified
=== ep: 2640, time 26.74858570098877, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2640
goal_identified
goal_identified
=== ep: 2641, time 27.030168533325195, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2641
goal_identified
=== ep: 2642, time 27.289860248565674, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2642
=== ep: 2643, time 27.577754497528076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2643
goal_identified
goal_identified
goal_identified
=== ep: 2644, time 27.017204761505127, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2645, time 27.362030029296875, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2645
goal_identified
goal_identified
goal_identified
=== ep: 2646, time 27.07223129272461, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2646
=== ep: 2647, time 27.24167776107788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2647
=== ep: 2648, time 27.414418697357178, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2648
goal_identified
goal_identified
goal_identified
=== ep: 2649, time 33.61275601387024, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2649
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2650, time 26.386150598526, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2650
goal_identified
=== ep: 2651, time 27.542540311813354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2651
goal_identified
=== ep: 2652, time 27.207865715026855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2652
goal_identified
goal_identified
goal_identified
=== ep: 2653, time 27.435400009155273, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2653
goal_identified
=== ep: 2654, time 27.43580389022827, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2654
goal_identified
goal_identified
=== ep: 2655, time 26.89822268486023, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2655
goal_identified
=== ep: 2656, time 27.12313413619995, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2656
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2657, time 27.38741421699524, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
goal_identified
=== ep: 2658, time 27.335399389266968, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2658
goal_identified
=== ep: 2659, time 32.71979331970215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2659
goal_identified
=== ep: 2660, time 27.100401639938354, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2660
goal_identified
=== ep: 2661, time 27.577481031417847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2661
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2662, time 27.70360803604126, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2662
goal_identified
goal_identified
=== ep: 2663, time 27.682281017303467, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2663
goal_identified
=== ep: 2664, time 27.40986156463623, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2664
goal_identified
=== ep: 2665, time 26.97978925704956, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2665
goal_identified
goal_identified
goal_identified
=== ep: 2666, time 27.118700981140137, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2666
goal_identified
=== ep: 2667, time 27.365145683288574, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2667
goal_identified
=== ep: 2668, time 27.909685134887695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2668
goal_identified
goal_identified
goal_identified
=== ep: 2669, time 32.513155460357666, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2669
goal_identified
goal_identified
=== ep: 2670, time 27.624687910079956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2670
goal_identified
goal_identified
=== ep: 2671, time 27.436715602874756, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2671
goal_identified
=== ep: 2672, time 27.441357612609863, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2672
goal_identified
goal_identified
=== ep: 2673, time 27.665809154510498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2673
goal_identified
goal_identified
goal_identified
=== ep: 2674, time 27.172861576080322, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2674
goal_identified
goal_identified
goal_identified
=== ep: 2675, time 27.183287620544434, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2675
goal_identified
goal_identified
=== ep: 2676, time 27.211082696914673, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2676
goal_identified
goal_identified
=== ep: 2677, time 27.242038249969482, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2677
goal_identified
goal_identified
goal_identified
=== ep: 2678, time 27.901050567626953, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2678
goal_identified
=== ep: 2679, time 32.06476306915283, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2679
=== ep: 2680, time 27.45020866394043, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2680
goal_identified
=== ep: 2681, time 27.528673887252808, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2681
goal_identified
=== ep: 2682, time 27.260315656661987, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2682
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2683, time 27.386626482009888, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2683
goal_identified
goal_identified
goal_identified
=== ep: 2684, time 26.888916492462158, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2684
goal_identified
goal_identified
=== ep: 2685, time 27.541736125946045, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2685
goal_identified
goal_identified
=== ep: 2686, time 27.360920906066895, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2686
goal_identified
goal_identified
=== ep: 2687, time 27.405190467834473, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2687
goal_identified
goal_identified
goal_identified
=== ep: 2688, time 27.134893655776978, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2688
goal_identified
=== ep: 2689, time 32.72015833854675, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2689
goal_identified
goal_identified
=== ep: 2690, time 27.373522520065308, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2691, time 27.581682443618774, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2691
goal_identified
goal_identified
goal_identified
=== ep: 2692, time 27.013498067855835, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2692
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2693, time 27.234598875045776, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2694, time 26.92648959159851, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2694
goal_identified
goal_identified
=== ep: 2695, time 27.54122281074524, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2695
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2696, time 27.592249631881714, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2696
goal_identified
goal_identified
=== ep: 2697, time 27.4237802028656, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2697
goal_identified
goal_identified
goal_identified
=== ep: 2698, time 27.099417686462402, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2698
goal_identified
goal_identified
=== ep: 2699, time 31.773574590682983, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2699
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2700, time 27.163539171218872, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2700
goal_identified
goal_identified
goal_identified
=== ep: 2701, time 27.64553475379944, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2701
goal_identified
goal_identified
=== ep: 2702, time 27.182233095169067, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2702
goal_identified
=== ep: 2703, time 27.67582869529724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2703
goal_identified
=== ep: 2704, time 27.225224256515503, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2704
goal_identified
=== ep: 2705, time 27.26538872718811, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2705
goal_identified
goal_identified
=== ep: 2706, time 27.547327518463135, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2706
=== ep: 2707, time 27.29661536216736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2707
goal_identified
=== ep: 2708, time 26.871968507766724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2708
goal_identified
goal_identified
=== ep: 2709, time 32.40288424491882, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2709
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2710, time 26.947141885757446, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2711, time 27.51330256462097, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2711
goal_identified
goal_identified
=== ep: 2712, time 27.28710150718689, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2712
goal_identified
=== ep: 2713, time 26.666274070739746, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2713
goal_identified
goal_identified
=== ep: 2714, time 27.33689260482788, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2714
goal_identified
goal_identified
=== ep: 2715, time 27.58937358856201, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2715
goal_identified
goal_identified
goal_identified
=== ep: 2716, time 26.97915530204773, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2716
goal_identified
goal_identified
=== ep: 2717, time 27.539609670639038, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2717
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2718, time 26.811803579330444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2718
goal_identified
goal_identified
=== ep: 2719, time 32.05461096763611, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2719
goal_identified
goal_identified
=== ep: 2720, time 27.405170917510986, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2720
goal_identified
goal_identified
=== ep: 2721, time 27.819021224975586, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2721
goal_identified
=== ep: 2722, time 27.34863519668579, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2722
goal_identified
=== ep: 2723, time 26.99228310585022, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2723
goal_identified
=== ep: 2724, time 27.472617387771606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2724
goal_identified
goal_identified
=== ep: 2725, time 27.378346920013428, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2725
goal_identified
goal_identified
goal_identified
=== ep: 2726, time 27.59892749786377, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2726
goal_identified
goal_identified
goal_identified
=== ep: 2727, time 27.21445918083191, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2727
goal_identified
=== ep: 2728, time 27.38079261779785, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2728
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2729, time 32.701303005218506, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2729
goal_identified
goal_identified
=== ep: 2730, time 27.527801036834717, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2730
goal_identified
goal_identified
=== ep: 2731, time 27.929744482040405, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2731
goal_identified
goal_identified
goal_identified
=== ep: 2732, time 27.17817187309265, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2732
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2733, time 27.211732387542725, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2733
goal_identified
goal_identified
goal_identified
=== ep: 2734, time 27.46769380569458, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2734
goal_identified
=== ep: 2735, time 25.938889265060425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 23/23)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2735
goal_identified
=== ep: 2736, time 27.357773780822754, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2736
goal_identified
goal_identified
=== ep: 2737, time 27.1857693195343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2737
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2738, time 26.73487114906311, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2738
=== ep: 2739, time 32.58954477310181, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2739
goal_identified
goal_identified
goal_identified
=== ep: 2740, time 27.50550103187561, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 27.272698402404785, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2741
goal_identified
goal_identified
=== ep: 2742, time 27.25772452354431, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 27.042489290237427, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2743
goal_identified
goal_identified
=== ep: 2744, time 27.512848615646362, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2744
goal_identified
goal_identified
=== ep: 2745, time 27.148775815963745, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2745
goal_identified
goal_identified
=== ep: 2746, time 27.653122425079346, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2746
goal_identified
goal_identified
=== ep: 2747, time 27.60926580429077, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2747
goal_identified
goal_identified
goal_identified
=== ep: 2748, time 26.959197759628296, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2748
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2749, time 32.563732624053955, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2749
goal_identified
=== ep: 2750, time 27.160700798034668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 27.29676055908203, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2751
goal_identified
goal_identified
=== ep: 2752, time 27.318281412124634, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2752
goal_identified
goal_identified
=== ep: 2753, time 27.168948888778687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2753
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2754, time 27.530569076538086, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2754
goal_identified
goal_identified
=== ep: 2755, time 27.410666465759277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2755
goal_identified
goal_identified
=== ep: 2756, time 27.21578335762024, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2756
goal_identified
=== ep: 2757, time 27.9144344329834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2757
goal_identified
=== ep: 2758, time 26.927650928497314, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2758
=== ep: 2759, time 32.331154584884644, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2759
goal_identified
goal_identified
=== ep: 2760, time 27.672412157058716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2761, time 27.458385944366455, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2761
goal_identified
=== ep: 2762, time 27.590572834014893, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2762
goal_identified
goal_identified
=== ep: 2763, time 26.830395936965942, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2763
goal_identified
goal_identified
=== ep: 2764, time 27.703848361968994, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2764
goal_identified
goal_identified
goal_identified
=== ep: 2765, time 27.212597846984863, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2765
goal_identified
goal_identified
goal_identified
=== ep: 2766, time 27.107090711593628, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2766
goal_identified
goal_identified
=== ep: 2767, time 27.609094381332397, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2767
goal_identified
goal_identified
goal_identified
=== ep: 2768, time 26.61669111251831, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2768
goal_identified
=== ep: 2769, time 32.370797872543335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2769
goal_identified
=== ep: 2770, time 28.13284182548523, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2770
goal_identified
goal_identified
=== ep: 2771, time 27.415783643722534, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2771
goal_identified
=== ep: 2772, time 27.111289978027344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2772
goal_identified
goal_identified
goal_identified
=== ep: 2773, time 27.107277870178223, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2773
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2774, time 27.39462685585022, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2774
goal_identified
goal_identified
=== ep: 2775, time 27.14136242866516, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2775
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2776, time 27.20036220550537, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2776
goal_identified
goal_identified
=== ep: 2777, time 26.83678364753723, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2777
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2778, time 27.037653923034668, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2778
=== ep: 2779, time 32.72933387756348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2779
goal_identified
=== ep: 2780, time 27.462143659591675, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2780
goal_identified
=== ep: 2781, time 27.39302372932434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2781
=== ep: 2782, time 27.144997358322144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2782
=== ep: 2783, time 27.7254958152771, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2783
goal_identified
=== ep: 2784, time 27.314573526382446, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2784
goal_identified
goal_identified
goal_identified
=== ep: 2785, time 27.206753253936768, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2785
goal_identified
goal_identified
=== ep: 2786, time 27.548320055007935, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2786
goal_identified
goal_identified
goal_identified
=== ep: 2787, time 26.916037797927856, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2787
goal_identified
=== ep: 2788, time 27.67393469810486, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2788
goal_identified
goal_identified
=== ep: 2789, time 31.877274990081787, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2789
goal_identified
goal_identified
goal_identified
=== ep: 2790, time 27.17516303062439, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2790
goal_identified
=== ep: 2791, time 27.46833300590515, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2792, time 27.054440021514893, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1754
goal_identified
=== ep: 2793, time 27.540120363235474, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2793
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2794, time 27.84020447731018, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2794
goal_identified
=== ep: 2795, time 27.80866837501526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2795
goal_identified
=== ep: 2796, time 27.506285429000854, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2796
goal_identified
=== ep: 2797, time 27.034865140914917, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2797
=== ep: 2798, time 27.726341247558594, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2798
goal_identified
goal_identified
=== ep: 2799, time 32.579065561294556, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2799
goal_identified
goal_identified
=== ep: 2800, time 27.57360529899597, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2800
goal_identified
=== ep: 2801, time 27.015217065811157, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2801
goal_identified
=== ep: 2802, time 27.144113779067993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2802
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2803, time 27.42904305458069, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2803
goal_identified
goal_identified
=== ep: 2804, time 27.38422465324402, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2804
goal_identified
goal_identified
=== ep: 2805, time 27.489861488342285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2805
=== ep: 2806, time 27.18243145942688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2806
goal_identified
=== ep: 2807, time 27.446272134780884, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2807
goal_identified
goal_identified
=== ep: 2808, time 27.571821689605713, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2808
goal_identified
goal_identified
goal_identified
=== ep: 2809, time 32.80203866958618, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2809
goal_identified
goal_identified
goal_identified
=== ep: 2810, time 27.37284541130066, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2810
goal_identified
goal_identified
=== ep: 2811, time 26.99174666404724, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2811
goal_identified
goal_identified
goal_identified
=== ep: 2812, time 27.23231792449951, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2812
goal_identified
goal_identified
=== ep: 2813, time 27.345873594284058, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2813
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2814, time 27.301841497421265, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2814
goal_identified
goal_identified
=== ep: 2815, time 27.309568405151367, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2815
goal_identified
=== ep: 2816, time 27.50096297264099, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2817, time 27.34039068222046, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2817
goal_identified
goal_identified
goal_identified
=== ep: 2818, time 27.29241442680359, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2818
goal_identified
goal_identified
=== ep: 2819, time 32.09536290168762, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2819
=== ep: 2820, time 27.168644905090332, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2820
goal_identified
goal_identified
=== ep: 2821, time 27.009883165359497, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2821
goal_identified
=== ep: 2822, time 27.733206748962402, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2822
goal_identified
goal_identified
=== ep: 2823, time 27.240845918655396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2823
goal_identified
goal_identified
=== ep: 2824, time 27.305286169052124, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2824
goal_identified
=== ep: 2825, time 27.565624237060547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2825
goal_identified
goal_identified
goal_identified
=== ep: 2826, time 27.20132613182068, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2826
goal_identified
goal_identified
=== ep: 2827, time 27.090265035629272, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2827
goal_identified
goal_identified
goal_identified
=== ep: 2828, time 27.594255685806274, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2828
goal_identified
goal_identified
goal_identified
=== ep: 2829, time 32.579813957214355, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2829
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2830, time 27.366868257522583, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2830
goal_identified
=== ep: 2831, time 26.82781982421875, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2831
goal_identified
goal_identified
=== ep: 2832, time 27.05490255355835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2832
goal_identified
goal_identified
=== ep: 2833, time 26.992248058319092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2833
goal_identified
goal_identified
goal_identified
=== ep: 2834, time 27.68223786354065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2834
goal_identified
=== ep: 2835, time 27.1777184009552, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2835
goal_identified
goal_identified
goal_identified
=== ep: 2836, time 27.10929584503174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2836
=== ep: 2837, time 27.667271614074707, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2837
goal_identified
goal_identified
goal_identified
=== ep: 2838, time 27.585597276687622, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2838
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2839, time 32.57838463783264, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2839
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2840, time 27.21521544456482, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2840
goal_identified
goal_identified
=== ep: 2841, time 27.435274600982666, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2841
goal_identified
goal_identified
goal_identified
=== ep: 2842, time 27.485969305038452, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2842
=== ep: 2843, time 27.35366702079773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 27.576082706451416, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2844
goal_identified
goal_identified
=== ep: 2845, time 27.161223888397217, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 27.382566690444946, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2846
goal_identified
goal_identified
=== ep: 2847, time 27.667049169540405, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2847
goal_identified
goal_identified
=== ep: 2848, time 27.688390493392944, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2848
goal_identified
goal_identified
=== ep: 2849, time 32.284788608551025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2849
goal_identified
=== ep: 2850, time 27.54094648361206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2850
goal_identified
goal_identified
=== ep: 2851, time 27.38980746269226, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2851
goal_identified
goal_identified
goal_identified
=== ep: 2852, time 27.334327459335327, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2852
goal_identified
=== ep: 2853, time 27.531336784362793, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2853
goal_identified
=== ep: 2854, time 27.360888957977295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2854
goal_identified
=== ep: 2855, time 27.070885181427002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2855
=== ep: 2856, time 27.42308282852173, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2856
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2857, time 27.532132148742676, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2857
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2858, time 27.230031967163086, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2858
goal_identified
goal_identified
=== ep: 2859, time 31.96273636817932, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2859
goal_identified
goal_identified
=== ep: 2860, time 27.007231950759888, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 27.034878969192505, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2861
goal_identified
goal_identified
=== ep: 2862, time 27.65953826904297, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2862
goal_identified
goal_identified
=== ep: 2863, time 27.402626037597656, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2863
goal_identified
goal_identified
goal_identified
=== ep: 2864, time 27.030306816101074, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2864
goal_identified
goal_identified
=== ep: 2865, time 27.717657804489136, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2865
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2866, time 27.366119861602783, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2866
goal_identified
goal_identified
=== ep: 2867, time 27.567381858825684, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2867
=== ep: 2868, time 27.564456462860107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2868
goal_identified
=== ep: 2869, time 32.2871196269989, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2869
=== ep: 2870, time 27.29852843284607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2870
goal_identified
=== ep: 2871, time 27.828582763671875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2871
goal_identified
goal_identified
=== ep: 2872, time 27.48043131828308, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2872
goal_identified
=== ep: 2873, time 27.248743772506714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2873
=== ep: 2874, time 27.107054471969604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2874
goal_identified
goal_identified
=== ep: 2875, time 27.372993230819702, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2875
goal_identified
=== ep: 2876, time 27.30128574371338, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2877, time 27.688997983932495, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2877
goal_identified
=== ep: 2878, time 27.579263925552368, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2878
goal_identified
goal_identified
=== ep: 2879, time 32.067652225494385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2879
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2880, time 27.38949418067932, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2880
goal_identified
goal_identified
goal_identified
=== ep: 2881, time 26.98725700378418, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2881
goal_identified
=== ep: 2882, time 27.749284029006958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2882
goal_identified
goal_identified
goal_identified
=== ep: 2883, time 27.288940906524658, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2883
goal_identified
=== ep: 2884, time 26.8037531375885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2884
goal_identified
goal_identified
=== ep: 2885, time 27.44440722465515, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2885
goal_identified
=== ep: 2886, time 27.307704210281372, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2886
goal_identified
goal_identified
goal_identified
=== ep: 2887, time 27.243701696395874, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2887
goal_identified
=== ep: 2888, time 27.029426097869873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2888
goal_identified
=== ep: 2889, time 31.623911380767822, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2889
goal_identified
goal_identified
=== ep: 2890, time 27.345092296600342, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2890
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2891, time 27.603463649749756, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2891
goal_identified
=== ep: 2892, time 27.140082597732544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2892
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2893, time 27.387348651885986, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2893
goal_identified
goal_identified
goal_identified
=== ep: 2894, time 27.112309217453003, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2894
goal_identified
goal_identified
goal_identified
=== ep: 2895, time 27.923224210739136, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2895
=== ep: 2896, time 27.328914642333984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2896
goal_identified
=== ep: 2897, time 27.241482734680176, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2897
goal_identified
goal_identified
=== ep: 2898, time 26.965524673461914, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2898
goal_identified
goal_identified
=== ep: 2899, time 31.803228855133057, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2899
goal_identified
=== ep: 2900, time 27.133168697357178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2900
goal_identified
goal_identified
=== ep: 2901, time 27.697969913482666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2901
goal_identified
=== ep: 2902, time 27.509366750717163, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2902
goal_identified
goal_identified
=== ep: 2903, time 27.402634143829346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2903
goal_identified
goal_identified
=== ep: 2904, time 27.393460750579834, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2904
goal_identified
goal_identified
goal_identified
=== ep: 2905, time 27.43508744239807, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2905
goal_identified
goal_identified
=== ep: 2906, time 27.5251407623291, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2906
goal_identified
goal_identified
=== ep: 2907, time 27.131635189056396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2907
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2908, time 27.141895532608032, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2908
=== ep: 2909, time 32.232611894607544, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2909
goal_identified
goal_identified
=== ep: 2910, time 27.118206024169922, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2910
goal_identified
goal_identified
goal_identified
=== ep: 2911, time 27.601057529449463, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2911
goal_identified
goal_identified
=== ep: 2912, time 27.531983852386475, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2912
goal_identified
goal_identified
=== ep: 2913, time 27.582606315612793, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2913
goal_identified
goal_identified
=== ep: 2914, time 27.366527557373047, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2914
goal_identified
goal_identified
=== ep: 2915, time 27.16796350479126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2915
goal_identified
=== ep: 2916, time 27.383747100830078, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 27.487040758132935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2917
=== ep: 2918, time 27.569263458251953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2918
goal_identified
=== ep: 2919, time 32.46021366119385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2919
goal_identified
goal_identified
goal_identified
=== ep: 2920, time 27.166886806488037, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2920
goal_identified
goal_identified
goal_identified
=== ep: 2921, time 27.266782522201538, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2921
goal_identified
goal_identified
goal_identified
=== ep: 2922, time 27.31973910331726, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2922
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2923, time 27.416017770767212, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2923
goal_identified
=== ep: 2924, time 27.046908140182495, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2924
goal_identified
=== ep: 2925, time 27.311151266098022, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2925
goal_identified
goal_identified
goal_identified
=== ep: 2926, time 27.482041358947754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2926
=== ep: 2927, time 27.32433533668518, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2927
goal_identified
=== ep: 2928, time 27.145226001739502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 33.505897998809814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2929
goal_identified
goal_identified
=== ep: 2930, time 27.360231399536133, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2930
goal_identified
goal_identified
=== ep: 2931, time 27.40978479385376, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2931
goal_identified
=== ep: 2932, time 27.023340702056885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2932
goal_identified
goal_identified
goal_identified
=== ep: 2933, time 27.230525732040405, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2933
goal_identified
goal_identified
=== ep: 2934, time 26.94703960418701, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2934
goal_identified
=== ep: 2935, time 27.58261203765869, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2935
goal_identified
goal_identified
=== ep: 2936, time 27.27259659767151, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2936
=== ep: 2937, time 27.71546721458435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2937
goal_identified
goal_identified
goal_identified
=== ep: 2938, time 27.48712944984436, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2938
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2939, time 31.861193418502808, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2939
=== ep: 2940, time 26.988455533981323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2940
=== ep: 2941, time 27.835965871810913, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2941
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2942, time 27.426628828048706, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2942
=== ep: 2943, time 27.402855396270752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2943
goal_identified
goal_identified
=== ep: 2944, time 26.97218346595764, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2944
goal_identified
=== ep: 2945, time 27.084930181503296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2945
goal_identified
=== ep: 2946, time 27.593361139297485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2946
goal_identified
=== ep: 2947, time 27.699400424957275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2947
goal_identified
goal_identified
goal_identified
=== ep: 2948, time 27.18978524208069, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2948
=== ep: 2949, time 32.19019889831543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2949
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2950, time 26.746798038482666, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2950
goal_identified
=== ep: 2951, time 27.68903946876526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2951
=== ep: 2952, time 27.581459760665894, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2952
goal_identified
=== ep: 2953, time 27.75416660308838, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2953
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2954, time 26.591426849365234, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2954
goal_identified
goal_identified
goal_identified
=== ep: 2955, time 27.35430121421814, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2955
=== ep: 2956, time 27.565910577774048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2956
goal_identified
goal_identified
goal_identified
=== ep: 2957, time 27.73384165763855, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2957
=== ep: 2958, time 27.415854692459106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2958
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2959, time 31.635509729385376, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2959
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2960, time 27.73741841316223, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2960
goal_identified
goal_identified
=== ep: 2961, time 27.251954555511475, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2961
goal_identified
goal_identified
goal_identified
=== ep: 2962, time 27.39218020439148, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2962
goal_identified
goal_identified
goal_identified
=== ep: 2963, time 27.377613306045532, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2963
goal_identified
goal_identified
goal_identified
=== ep: 2964, time 27.089157104492188, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2964
goal_identified
=== ep: 2965, time 27.233195066452026, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2965
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2966, time 27.571553945541382, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2966
=== ep: 2967, time 27.062232494354248, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2967
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2968, time 27.338188648223877, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2968
goal_identified
=== ep: 2969, time 31.535955905914307, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2969
goal_identified
goal_identified
=== ep: 2970, time 27.489182233810425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2970
goal_identified
goal_identified
goal_identified
=== ep: 2971, time 27.561481714248657, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2971
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2972, time 27.920531034469604, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2972
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2973, time 27.36318278312683, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2973
goal_identified
goal_identified
goal_identified
=== ep: 2974, time 26.65067434310913, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2974
goal_identified
goal_identified
=== ep: 2975, time 27.286147832870483, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2975
goal_identified
goal_identified
goal_identified
=== ep: 2976, time 27.316336393356323, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2976
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2977, time 27.37942886352539, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2977
=== ep: 2978, time 27.36458444595337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2978
goal_identified
=== ep: 2979, time 31.703754901885986, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2979
goal_identified
=== ep: 2980, time 27.684133291244507, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2980
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2981, time 27.38281798362732, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2981
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2982, time 27.815044403076172, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2982
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2983, time 27.106820106506348, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2983
goal_identified
goal_identified
goal_identified
=== ep: 2984, time 27.086244344711304, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2984
goal_identified
goal_identified
=== ep: 2985, time 27.245463132858276, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2985
goal_identified
goal_identified
goal_identified
=== ep: 2986, time 28.09796380996704, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2986
goal_identified
goal_identified
=== ep: 2987, time 27.30227541923523, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2987
goal_identified
goal_identified
=== ep: 2988, time 27.11041235923767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2988
=== ep: 2989, time 32.236034631729126, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2989
goal_identified
goal_identified
=== ep: 2990, time 27.323853969573975, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2990
goal_identified
goal_identified
=== ep: 2991, time 27.48031783103943, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2991
goal_identified
=== ep: 2992, time 27.422564029693604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2992
goal_identified
goal_identified
goal_identified
=== ep: 2993, time 27.905917644500732, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2993
goal_identified
goal_identified
=== ep: 2994, time 26.855548858642578, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2994
goal_identified
goal_identified
goal_identified
=== ep: 2995, time 27.474995613098145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2995
=== ep: 2996, time 27.069705486297607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2996
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2997, time 27.4327232837677, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2997
=== ep: 2998, time 27.535150051116943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2999, time 32.03096961975098, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2999
goal_identified
=== ep: 3000, time 27.598309993743896, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3000
goal_identified
goal_identified
goal_identified
=== ep: 3001, time 27.34730100631714, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3001
goal_identified
=== ep: 3002, time 27.412294626235962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3002
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3003, time 27.578327178955078, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3003
goal_identified
goal_identified
=== ep: 3004, time 27.215856552124023, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3004
goal_identified
goal_identified
=== ep: 3005, time 27.576306343078613, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3005
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3006, time 27.006641149520874, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1780
goal_identified
goal_identified
goal_identified
=== ep: 3007, time 27.25729274749756, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3007
goal_identified
goal_identified
=== ep: 3008, time 27.674269676208496, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3008
goal_identified
goal_identified
goal_identified
=== ep: 3009, time 32.532156229019165, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3009
goal_identified
goal_identified
=== ep: 3010, time 28.008065700531006, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3010
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3011, time 27.69852113723755, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3011
goal_identified
=== ep: 3012, time 27.495164394378662, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3012
goal_identified
goal_identified
goal_identified
=== ep: 3013, time 27.404315948486328, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3013
goal_identified
goal_identified
=== ep: 3014, time 26.627696990966797, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3014
goal_identified
goal_identified
=== ep: 3015, time 27.31937289237976, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3015
=== ep: 3016, time 27.161806344985962, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3016
goal_identified
goal_identified
=== ep: 3017, time 27.563673496246338, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3017
goal_identified
goal_identified
=== ep: 3018, time 27.183953046798706, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3018
goal_identified
goal_identified
=== ep: 3019, time 31.70299506187439, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3019
goal_identified
=== ep: 3020, time 27.457050800323486, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3020
goal_identified
goal_identified
goal_identified
=== ep: 3021, time 27.574665546417236, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3021
goal_identified
goal_identified
goal_identified
=== ep: 3022, time 27.355318546295166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3022
goal_identified
=== ep: 3023, time 27.860050439834595, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3023
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3024, time 26.937939405441284, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2022
goal_identified
goal_identified
=== ep: 3025, time 27.71603798866272, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3025
goal_identified
goal_identified
=== ep: 3026, time 27.730963706970215, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3026
goal_identified
goal_identified
goal_identified
=== ep: 3027, time 27.453014850616455, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3027
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3028, time 27.55975914001465, eps 0.001, sum reward: 4, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3028
goal_identified
goal_identified
=== ep: 3029, time 32.37432050704956, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3029
=== ep: 3030, time 27.535914421081543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3030
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3031, time 27.594755172729492, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3031
goal_identified
=== ep: 3032, time 27.4914767742157, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3032
goal_identified
goal_identified
goal_identified
=== ep: 3033, time 27.64736819267273, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3033
=== ep: 3034, time 27.637545585632324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3034
goal_identified
goal_identified
goal_identified
=== ep: 3035, time 27.32022976875305, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3035
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3036, time 27.385093927383423, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3036
goal_identified
=== ep: 3037, time 27.711527585983276, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3037
goal_identified
=== ep: 3038, time 27.09953236579895, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3038
goal_identified
goal_identified
=== ep: 3039, time 32.2097487449646, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3039
goal_identified
goal_identified
=== ep: 3040, time 27.26136088371277, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 27.505332708358765, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3041
goal_identified
=== ep: 3042, time 27.66241431236267, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3042
goal_identified
goal_identified
=== ep: 3043, time 27.369961261749268, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3043
goal_identified
goal_identified
=== ep: 3044, time 27.337708234786987, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3044
goal_identified
goal_identified
=== ep: 3045, time 27.565407514572144, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3045
goal_identified
goal_identified
=== ep: 3046, time 27.06259274482727, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3046
goal_identified
=== ep: 3047, time 27.353353023529053, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3047
goal_identified
goal_identified
=== ep: 3048, time 27.618890523910522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3048
goal_identified
goal_identified
=== ep: 3049, time 31.923245668411255, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3049
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3050, time 27.234556198120117, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3050
goal_identified
=== ep: 3051, time 27.6822452545166, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3051
goal_identified
goal_identified
=== ep: 3052, time 27.58280301094055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3052
=== ep: 3053, time 27.670915842056274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3053
goal_identified
=== ep: 3054, time 26.86417555809021, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3054
goal_identified
goal_identified
=== ep: 3055, time 27.059925079345703, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3055
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3056, time 27.307374000549316, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3056
goal_identified
goal_identified
goal_identified
=== ep: 3057, time 27.688822507858276, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3057
goal_identified
goal_identified
=== ep: 3058, time 27.191730499267578, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3058
goal_identified
goal_identified
goal_identified
=== ep: 3059, time 31.710415363311768, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3059
goal_identified
goal_identified
=== ep: 3060, time 27.33244252204895, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3060
goal_identified
goal_identified
goal_identified
=== ep: 3061, time 27.422034978866577, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3061
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3062, time 27.506237745285034, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3062
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3063, time 27.216901302337646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3063
goal_identified
goal_identified
goal_identified
=== ep: 3064, time 27.073769569396973, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3064
goal_identified
goal_identified
=== ep: 3065, time 26.765738010406494, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3065
goal_identified
=== ep: 3066, time 27.31209707260132, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3066
goal_identified
goal_identified
=== ep: 3067, time 27.60867404937744, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3067
goal_identified
goal_identified
=== ep: 3068, time 27.49303388595581, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3068
goal_identified
goal_identified
=== ep: 3069, time 32.29229426383972, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3069
goal_identified
=== ep: 3070, time 26.978426218032837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3070
goal_identified
=== ep: 3071, time 27.44987463951111, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3071
goal_identified
=== ep: 3072, time 27.443617582321167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3072
goal_identified
goal_identified
goal_identified
=== ep: 3073, time 27.482802867889404, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3073
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3074, time 27.122706413269043, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3074
goal_identified
goal_identified
=== ep: 3075, time 26.929632663726807, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3075
goal_identified
goal_identified
goal_identified
=== ep: 3076, time 26.727487564086914, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3076
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3077, time 27.28786540031433, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3077
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3078, time 27.20242977142334, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3078
goal_identified
goal_identified
=== ep: 3079, time 32.002551794052124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3079
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3080, time 27.36561632156372, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3080
goal_identified
goal_identified
goal_identified
=== ep: 3081, time 27.43496799468994, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3081
goal_identified
goal_identified
=== ep: 3082, time 27.64158844947815, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3082
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3083, time 27.55350947380066, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3083
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3084, time 27.2784903049469, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3084
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3085, time 27.308456659317017, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3085
goal_identified
goal_identified
=== ep: 3086, time 27.456968307495117, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3086
goal_identified
goal_identified
goal_identified
=== ep: 3087, time 27.30121684074402, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3087
=== ep: 3088, time 27.465463876724243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3088
=== ep: 3089, time 31.628854513168335, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3089
goal_identified
goal_identified
=== ep: 3090, time 27.498831033706665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3090
goal_identified
goal_identified
=== ep: 3091, time 27.244618892669678, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3091
goal_identified
=== ep: 3092, time 27.870983123779297, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3092
goal_identified
=== ep: 3093, time 27.46290946006775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3093
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3094, time 26.890378713607788, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3094
goal_identified
goal_identified
=== ep: 3095, time 27.459752321243286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3095
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3096, time 27.88395118713379, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3096
=== ep: 3097, time 28.01348638534546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 27.893624305725098, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3098
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3099, time 31.984976530075073, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3099
goal_identified
goal_identified
=== ep: 3100, time 27.334486722946167, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3100
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3101, time 27.73878264427185, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3101
=== ep: 3102, time 27.350905895233154, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3102
goal_identified
=== ep: 3103, time 27.31418466567993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3103
goal_identified
goal_identified
goal_identified
=== ep: 3104, time 26.872257232666016, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3104
goal_identified
goal_identified
=== ep: 3105, time 27.62620735168457, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3105
goal_identified
=== ep: 3106, time 27.363481521606445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3106
=== ep: 3107, time 27.5395188331604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3107
goal_identified
goal_identified
=== ep: 3108, time 27.1452419757843, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3108
goal_identified
goal_identified
=== ep: 3109, time 31.795433044433594, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3109
goal_identified
goal_identified
=== ep: 3110, time 27.756060361862183, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3110
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3111, time 27.61533498764038, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3111
=== ep: 3112, time 27.478445053100586, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3112
goal_identified
goal_identified
=== ep: 3113, time 27.58862066268921, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3113
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3114, time 26.962747812271118, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3114
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3115, time 27.21336603164673, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3115
=== ep: 3116, time 27.484422206878662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3116
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3117, time 27.354066610336304, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3117
goal_identified
goal_identified
=== ep: 3118, time 26.343139171600342, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3118
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3119, time 30.570762634277344, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3119
goal_identified
goal_identified
=== ep: 3120, time 26.22513437271118, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3120
goal_identified
goal_identified
=== ep: 3121, time 25.516260147094727, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3121
goal_identified
goal_identified
goal_identified
=== ep: 3122, time 25.752400398254395, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3122
goal_identified
goal_identified
=== ep: 3123, time 25.39094877243042, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3123
goal_identified
=== ep: 3124, time 25.505067586898804, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3124
goal_identified
=== ep: 3125, time 25.624041080474854, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3125
goal_identified
goal_identified
=== ep: 3126, time 25.414916276931763, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3126
goal_identified
goal_identified
=== ep: 3127, time 25.32837462425232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3127
goal_identified
goal_identified
goal_identified
=== ep: 3128, time 25.50317096710205, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3128
goal_identified
=== ep: 3129, time 30.444129943847656, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3129
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3130, time 25.73349952697754, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3130
goal_identified
=== ep: 3131, time 25.612087726593018, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3131
goal_identified
goal_identified
=== ep: 3132, time 25.756258010864258, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3132
goal_identified
=== ep: 3133, time 26.16055178642273, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3133
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3134, time 26.091920137405396, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3134
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3135, time 26.07565927505493, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3135
goal_identified
=== ep: 3136, time 26.084608793258667, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3136
goal_identified
=== ep: 3137, time 26.527308464050293, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3137
goal_identified
goal_identified
=== ep: 3138, time 26.824148893356323, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3138
goal_identified
goal_identified
=== ep: 3139, time 31.03309440612793, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3139
goal_identified
goal_identified
=== ep: 3140, time 26.375974893569946, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3140
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3141, time 27.13843846321106, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3141
goal_identified
goal_identified
=== ep: 3142, time 27.04696798324585, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 26.960782051086426, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3143
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3144, time 27.381818056106567, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3144
goal_identified
=== ep: 3145, time 27.30357336997986, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3145
goal_identified
goal_identified
=== ep: 3146, time 27.218470096588135, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3146
goal_identified
goal_identified
goal_identified
=== ep: 3147, time 26.795359134674072, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3147
goal_identified
goal_identified
goal_identified
=== ep: 3148, time 26.853793621063232, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3148
goal_identified
=== ep: 3149, time 32.08270812034607, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3149
goal_identified
goal_identified
=== ep: 3150, time 27.106910705566406, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3150
goal_identified
goal_identified
goal_identified
=== ep: 3151, time 27.274711847305298, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3151
goal_identified
=== ep: 3152, time 26.935505390167236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3152
goal_identified
goal_identified
goal_identified
=== ep: 3153, time 27.184364557266235, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3153
goal_identified
goal_identified
=== ep: 3154, time 27.01236128807068, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3154
goal_identified
goal_identified
=== ep: 3155, time 27.194836139678955, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3155
goal_identified
=== ep: 3156, time 27.44196915626526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3156
goal_identified
goal_identified
goal_identified
=== ep: 3157, time 27.012371063232422, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3157
goal_identified
goal_identified
=== ep: 3158, time 26.786505460739136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3158
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3159, time 32.056809425354004, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2617
goal_identified
goal_identified
=== ep: 3160, time 27.35143804550171, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3160
=== ep: 3161, time 27.58450174331665, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3162, time 27.247293710708618, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2657
goal_identified
goal_identified
goal_identified
=== ep: 3163, time 27.22761297225952, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3163
goal_identified
goal_identified
goal_identified
=== ep: 3164, time 27.167991876602173, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3164
goal_identified
=== ep: 3165, time 27.333802461624146, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3165
goal_identified
goal_identified
goal_identified
=== ep: 3166, time 27.306145191192627, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 27.18533444404602, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3167
goal_identified
goal_identified
goal_identified
=== ep: 3168, time 27.23467493057251, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3168
goal_identified
=== ep: 3169, time 32.1361722946167, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3169
goal_identified
goal_identified
=== ep: 3170, time 27.19745922088623, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3170
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3171, time 27.42381453514099, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3171
goal_identified
=== ep: 3172, time 27.09941601753235, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3172
=== ep: 3173, time 27.480777502059937, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3173
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3174, time 27.16591715812683, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3174
goal_identified
goal_identified
=== ep: 3175, time 27.712793111801147, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3175
goal_identified
=== ep: 3176, time 27.282577753067017, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3176
goal_identified
goal_identified
goal_identified
=== ep: 3177, time 27.347880125045776, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3177
goal_identified
goal_identified
=== ep: 3178, time 27.211998224258423, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3178
goal_identified
=== ep: 3179, time 31.648271799087524, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3179
goal_identified
goal_identified
goal_identified
=== ep: 3180, time 26.850775957107544, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3180
=== ep: 3181, time 27.290855646133423, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3181
goal_identified
goal_identified
goal_identified
=== ep: 3182, time 27.1380033493042, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3182
goal_identified
goal_identified
=== ep: 3183, time 27.44518756866455, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3183
goal_identified
goal_identified
=== ep: 3184, time 27.410083532333374, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3184
goal_identified
=== ep: 3185, time 27.475086212158203, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3185
goal_identified
goal_identified
=== ep: 3186, time 27.62954330444336, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3186
goal_identified
goal_identified
goal_identified
=== ep: 3187, time 27.549076318740845, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3187
goal_identified
goal_identified
goal_identified
=== ep: 3188, time 27.598414182662964, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3188
goal_identified
goal_identified
goal_identified
=== ep: 3189, time 31.609960317611694, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3189
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3190, time 27.22443175315857, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3190
goal_identified
goal_identified
=== ep: 3191, time 26.732913494110107, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3191
=== ep: 3192, time 27.42525815963745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3192
goal_identified
=== ep: 3193, time 27.428363800048828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3193
goal_identified
goal_identified
=== ep: 3194, time 27.36337685585022, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3194
goal_identified
goal_identified
goal_identified
=== ep: 3195, time 27.338056564331055, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3195
goal_identified
=== ep: 3196, time 26.86447525024414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3196
goal_identified
=== ep: 3197, time 26.979591131210327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3197
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3198, time 26.995811700820923, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 31.94519329071045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3199
goal_identified
goal_identified
goal_identified
=== ep: 3200, time 27.422730445861816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3200
goal_identified
goal_identified
=== ep: 3201, time 27.337790727615356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3201
=== ep: 3202, time 27.37318706512451, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3202
goal_identified
=== ep: 3203, time 27.097105026245117, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3203
goal_identified
goal_identified
goal_identified
=== ep: 3204, time 27.170000553131104, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3204
goal_identified
=== ep: 3205, time 27.332471132278442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3205
=== ep: 3206, time 27.59286069869995, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3206
goal_identified
goal_identified
=== ep: 3207, time 27.29155421257019, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3207
goal_identified
goal_identified
goal_identified
=== ep: 3208, time 27.04989457130432, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3208
goal_identified
=== ep: 3209, time 31.840118408203125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3209
goal_identified
=== ep: 3210, time 27.407609224319458, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3210
goal_identified
goal_identified
=== ep: 3211, time 27.16889190673828, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3211
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3212, time 26.905286073684692, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3212
=== ep: 3213, time 27.087206602096558, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3213
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3214, time 27.310324907302856, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3214
=== ep: 3215, time 27.453479766845703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3215
goal_identified
goal_identified
goal_identified
=== ep: 3216, time 26.96308708190918, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3216
goal_identified
goal_identified
goal_identified
=== ep: 3217, time 27.22807550430298, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3217
goal_identified
goal_identified
goal_identified
=== ep: 3218, time 27.176034927368164, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3218
goal_identified
=== ep: 3219, time 31.615160942077637, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3219
goal_identified
=== ep: 3220, time 27.267940759658813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3220
=== ep: 3221, time 27.17762780189514, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3221
goal_identified
goal_identified
=== ep: 3222, time 26.76680302619934, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3222
goal_identified
goal_identified
goal_identified
=== ep: 3223, time 26.67297124862671, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3223
=== ep: 3224, time 27.25446605682373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3224
goal_identified
goal_identified
goal_identified
=== ep: 3225, time 26.902662992477417, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3226, time 27.281373500823975, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3226
=== ep: 3227, time 27.60193705558777, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3227
goal_identified
goal_identified
goal_identified
=== ep: 3228, time 26.680453777313232, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3228
=== ep: 3229, time 31.83321475982666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3229
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3230, time 27.1202712059021, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3230
=== ep: 3231, time 26.97485375404358, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3231
goal_identified
=== ep: 3232, time 27.146194458007812, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3232
goal_identified
goal_identified
=== ep: 3233, time 27.349318027496338, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3233
goal_identified
goal_identified
goal_identified
=== ep: 3234, time 27.42089080810547, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3234
goal_identified
=== ep: 3235, time 27.29600429534912, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3235
goal_identified
=== ep: 3236, time 27.146257400512695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3236
goal_identified
goal_identified
=== ep: 3237, time 27.354910373687744, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3237
goal_identified
goal_identified
goal_identified
=== ep: 3238, time 27.05010199546814, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3238
goal_identified
goal_identified
=== ep: 3239, time 31.730316162109375, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3239
goal_identified
=== ep: 3240, time 26.805787801742554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3240
goal_identified
goal_identified
goal_identified
=== ep: 3241, time 26.969176530838013, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3241
goal_identified
goal_identified
=== ep: 3242, time 27.225059032440186, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3242
goal_identified
=== ep: 3243, time 27.493800163269043, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3243
goal_identified
=== ep: 3244, time 27.228017807006836, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3244
goal_identified
goal_identified
=== ep: 3245, time 26.908544778823853, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3245
goal_identified
=== ep: 3246, time 26.740891456604004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3246
=== ep: 3247, time 27.359535694122314, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3247
goal_identified
=== ep: 3248, time 27.347286224365234, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3248
goal_identified
=== ep: 3249, time 31.802148580551147, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3249
goal_identified
goal_identified
goal_identified
=== ep: 3250, time 26.972153902053833, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3250
goal_identified
=== ep: 3251, time 26.835383892059326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3251
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3252, time 26.934407472610474, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3252
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3253, time 26.7585768699646, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3253
goal_identified
=== ep: 3254, time 27.164228677749634, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3254
goal_identified
=== ep: 3255, time 27.23320746421814, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3255
goal_identified
=== ep: 3256, time 27.493444442749023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3256
goal_identified
=== ep: 3257, time 27.281665563583374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3257
goal_identified
=== ep: 3258, time 27.133676767349243, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3258
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3259, time 31.77696466445923, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3259
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3260, time 27.356556177139282, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3260
goal_identified
=== ep: 3261, time 27.224866151809692, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3261
goal_identified
goal_identified
=== ep: 3262, time 27.2155704498291, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3262
goal_identified
goal_identified
goal_identified
=== ep: 3263, time 27.0255286693573, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3263
=== ep: 3264, time 27.05417227745056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3264
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3265, time 26.799510717391968, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3265
goal_identified
goal_identified
goal_identified
=== ep: 3266, time 27.18756628036499, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3267, time 26.91098690032959, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3267
goal_identified
goal_identified
=== ep: 3268, time 27.27661657333374, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3268
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3269, time 31.504190921783447, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3269
goal_identified
=== ep: 3270, time 27.2807776927948, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3270
goal_identified
=== ep: 3271, time 26.92535400390625, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3271
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3272, time 27.139591217041016, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3272
goal_identified
goal_identified
goal_identified
=== ep: 3273, time 26.93309783935547, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3273
goal_identified
goal_identified
=== ep: 3274, time 27.17163872718811, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3274
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3275, time 27.148395776748657, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3275
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3276, time 32.7988977432251, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3276
goal_identified
=== ep: 3277, time 27.732040405273438, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3277
goal_identified
goal_identified
=== ep: 3278, time 27.28507971763611, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3278
goal_identified
goal_identified
goal_identified
=== ep: 3279, time 31.731498956680298, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3279
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3280, time 27.32644557952881, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3280
goal_identified
=== ep: 3281, time 27.296049118041992, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3281
goal_identified
=== ep: 3282, time 27.68046498298645, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3282
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3283, time 26.789689302444458, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3283
goal_identified
goal_identified
goal_identified
=== ep: 3284, time 27.099010944366455, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3284
goal_identified
goal_identified
goal_identified
=== ep: 3285, time 26.9447922706604, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3285
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3286, time 27.04921841621399, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3286
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3287, time 26.903690338134766, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3287
goal_identified
goal_identified
goal_identified
=== ep: 3288, time 27.154949188232422, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3288
goal_identified
goal_identified
=== ep: 3289, time 32.08964920043945, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3289
goal_identified
goal_identified
=== ep: 3290, time 28.805150985717773, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3290
goal_identified
goal_identified
goal_identified
=== ep: 3291, time 27.19787621498108, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3291
goal_identified
=== ep: 3292, time 26.883716106414795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3292
goal_identified
goal_identified
goal_identified
=== ep: 3293, time 27.32411551475525, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3293
goal_identified
goal_identified
goal_identified
=== ep: 3294, time 27.106409549713135, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3294
goal_identified
goal_identified
goal_identified
=== ep: 3295, time 26.96894645690918, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3295
=== ep: 3296, time 27.651121139526367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3296
goal_identified
goal_identified
=== ep: 3297, time 27.475841999053955, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3297
goal_identified
goal_identified
=== ep: 3298, time 27.550346612930298, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3298
goal_identified
goal_identified
=== ep: 3299, time 31.631823778152466, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3299
goal_identified
goal_identified
=== ep: 3300, time 27.25495982170105, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3300
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3301, time 27.377042770385742, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3301
goal_identified
goal_identified
=== ep: 3302, time 27.114680528640747, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3302
goal_identified
=== ep: 3303, time 27.03824734687805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3303
goal_identified
goal_identified
=== ep: 3304, time 27.180718183517456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3304
goal_identified
goal_identified
=== ep: 3305, time 26.801432132720947, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3306, time 27.35689067840576, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3306
goal_identified
goal_identified
goal_identified
=== ep: 3307, time 26.733222007751465, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3307
goal_identified
goal_identified
goal_identified
=== ep: 3308, time 26.963109016418457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3308
goal_identified
=== ep: 3309, time 32.07374143600464, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3309
goal_identified
goal_identified
goal_identified
=== ep: 3310, time 27.077205181121826, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3311, time 27.16971182823181, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3311
goal_identified
goal_identified
goal_identified
=== ep: 3312, time 26.968841552734375, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3312
goal_identified
goal_identified
=== ep: 3313, time 27.21629285812378, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3313
goal_identified
=== ep: 3314, time 27.384434461593628, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3314
goal_identified
goal_identified
goal_identified
=== ep: 3315, time 27.139803409576416, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3315
goal_identified
goal_identified
=== ep: 3316, time 27.365678310394287, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3316
goal_identified
=== ep: 3317, time 27.063931226730347, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 27.233682870864868, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3318
goal_identified
goal_identified
=== ep: 3319, time 31.669660329818726, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3319
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3320, time 27.139068365097046, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3320
goal_identified
goal_identified
=== ep: 3321, time 27.65173077583313, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3321
goal_identified
goal_identified
goal_identified
=== ep: 3322, time 27.453885793685913, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3322
goal_identified
goal_identified
=== ep: 3323, time 27.129919290542603, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3323
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3324, time 27.10189914703369, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3324
goal_identified
goal_identified
goal_identified
=== ep: 3325, time 27.381531715393066, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3325
goal_identified
goal_identified
=== ep: 3326, time 26.950823068618774, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3326
goal_identified
goal_identified
goal_identified
=== ep: 3327, time 27.031445026397705, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3327
goal_identified
goal_identified
=== ep: 3328, time 27.116098642349243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3328
goal_identified
goal_identified
=== ep: 3329, time 32.13912343978882, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3329
goal_identified
goal_identified
=== ep: 3330, time 27.11443519592285, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3330
goal_identified
goal_identified
=== ep: 3331, time 27.2053005695343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3331
goal_identified
goal_identified
goal_identified
=== ep: 3332, time 27.09886074066162, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3332
goal_identified
goal_identified
=== ep: 3333, time 27.33556580543518, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3333
goal_identified
goal_identified
goal_identified
=== ep: 3334, time 27.22272229194641, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3334
goal_identified
=== ep: 3335, time 27.183902502059937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3335
goal_identified
goal_identified
=== ep: 3336, time 27.281432151794434, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3336
=== ep: 3337, time 27.209681272506714, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3337
goal_identified
goal_identified
goal_identified
=== ep: 3338, time 27.186704397201538, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3338
goal_identified
goal_identified
goal_identified
=== ep: 3339, time 31.642406463623047, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3339
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3340, time 26.953969478607178, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3340
goal_identified
goal_identified
goal_identified
=== ep: 3341, time 27.187263011932373, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3341
goal_identified
=== ep: 3342, time 27.172767639160156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3342
goal_identified
goal_identified
=== ep: 3343, time 27.240936994552612, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3343
goal_identified
=== ep: 3344, time 27.07659411430359, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3344
goal_identified
goal_identified
=== ep: 3345, time 27.182941913604736, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3345
goal_identified
=== ep: 3346, time 27.13017249107361, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3346
goal_identified
=== ep: 3347, time 27.421974658966064, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3347
goal_identified
goal_identified
=== ep: 3348, time 27.21427297592163, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3348
goal_identified
goal_identified
=== ep: 3349, time 31.8872127532959, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3349
goal_identified
goal_identified
=== ep: 3350, time 26.891210794448853, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3350
=== ep: 3351, time 27.212035179138184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3351
goal_identified
goal_identified
goal_identified
=== ep: 3352, time 27.22121024131775, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3352
goal_identified
goal_identified
=== ep: 3353, time 26.87149429321289, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3353
goal_identified
=== ep: 3354, time 27.15694785118103, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3354
goal_identified
goal_identified
=== ep: 3355, time 27.220528841018677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3355
goal_identified
goal_identified
=== ep: 3356, time 27.37739109992981, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3356
goal_identified
goal_identified
=== ep: 3357, time 26.967884063720703, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3357
goal_identified
=== ep: 3358, time 27.044447660446167, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3358
goal_identified
goal_identified
=== ep: 3359, time 31.456332683563232, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3359
goal_identified
goal_identified
goal_identified
=== ep: 3360, time 26.87616991996765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3360
goal_identified
goal_identified
goal_identified
=== ep: 3361, time 27.24206829071045, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3361
goal_identified
goal_identified
=== ep: 3362, time 27.05692434310913, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3362
=== ep: 3363, time 27.05033254623413, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3363
goal_identified
goal_identified
goal_identified
=== ep: 3364, time 27.42193365097046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3364
=== ep: 3365, time 26.946202039718628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3365
goal_identified
goal_identified
=== ep: 3366, time 27.067272424697876, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3366
goal_identified
=== ep: 3367, time 27.132675647735596, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3367
goal_identified
goal_identified
=== ep: 3368, time 26.88567042350769, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3368
goal_identified
goal_identified
goal_identified
=== ep: 3369, time 31.73958992958069, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3370, time 26.923826217651367, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3370
goal_identified
goal_identified
goal_identified
=== ep: 3371, time 27.124216079711914, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3371
goal_identified
=== ep: 3372, time 27.258805751800537, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3372
goal_identified
goal_identified
goal_identified
=== ep: 3373, time 26.814550399780273, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3373
goal_identified
goal_identified
goal_identified
=== ep: 3374, time 27.26154923439026, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3374
goal_identified
goal_identified
=== ep: 3375, time 27.163543462753296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3375
goal_identified
goal_identified
goal_identified
=== ep: 3376, time 27.08803653717041, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3376
goal_identified
goal_identified
goal_identified
=== ep: 3377, time 27.188730478286743, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3377
goal_identified
goal_identified
=== ep: 3378, time 27.069596529006958, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3378
goal_identified
goal_identified
=== ep: 3379, time 31.68917679786682, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3379
goal_identified
goal_identified
=== ep: 3380, time 26.945584058761597, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3380
=== ep: 3381, time 27.45003604888916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3381
goal_identified
=== ep: 3382, time 27.11726951599121, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3382
=== ep: 3383, time 26.989371061325073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3383
goal_identified
goal_identified
=== ep: 3384, time 27.441903114318848, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3384
goal_identified
goal_identified
=== ep: 3385, time 27.258992671966553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3385
goal_identified
goal_identified
goal_identified
=== ep: 3386, time 27.143374919891357, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3386
goal_identified
goal_identified
goal_identified
=== ep: 3387, time 27.038857221603394, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3387
goal_identified
goal_identified
goal_identified
=== ep: 3388, time 27.174463272094727, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3388
=== ep: 3389, time 32.01925778388977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3389
goal_identified
goal_identified
=== ep: 3390, time 27.11468815803528, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3390
goal_identified
goal_identified
=== ep: 3391, time 26.750380277633667, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3391
goal_identified
goal_identified
=== ep: 3392, time 26.972323417663574, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3392
goal_identified
goal_identified
=== ep: 3393, time 27.321061849594116, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3393
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3394, time 27.22802209854126, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3394
goal_identified
goal_identified
goal_identified
=== ep: 3395, time 27.23226261138916, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3395
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3396, time 26.810019731521606, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3396
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3397, time 27.035686016082764, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3397
goal_identified
goal_identified
goal_identified
=== ep: 3398, time 27.35463285446167, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3398
goal_identified
=== ep: 3399, time 32.0577495098114, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3399
goal_identified
=== ep: 3400, time 27.0135498046875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3400
goal_identified
=== ep: 3401, time 27.1718430519104, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3401
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3402, time 26.89511489868164, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3402
goal_identified
goal_identified
=== ep: 3403, time 27.725318908691406, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3403
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3404, time 26.96939206123352, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2792
goal_identified
goal_identified
goal_identified
=== ep: 3405, time 26.842686653137207, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3405
goal_identified
=== ep: 3406, time 27.172717571258545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3406
goal_identified
goal_identified
=== ep: 3407, time 27.21639132499695, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3407
=== ep: 3408, time 27.623405933380127, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3408
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3409, time 31.41218066215515, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3409
goal_identified
=== ep: 3410, time 27.48847270011902, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3410
goal_identified
goal_identified
=== ep: 3411, time 26.977428674697876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3411
=== ep: 3412, time 27.129924535751343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3412
goal_identified
goal_identified
=== ep: 3413, time 27.202802658081055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3414, time 26.874184131622314, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3414
goal_identified
goal_identified
=== ep: 3415, time 27.012476921081543, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3415
goal_identified
=== ep: 3416, time 27.00561785697937, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3416
goal_identified
goal_identified
=== ep: 3417, time 27.15067219734192, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3417
goal_identified
=== ep: 3418, time 27.375078916549683, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3418
goal_identified
goal_identified
goal_identified
=== ep: 3419, time 31.173190116882324, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3419
goal_identified
goal_identified
goal_identified
=== ep: 3420, time 27.262707948684692, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3420
goal_identified
=== ep: 3421, time 26.582622528076172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3421
goal_identified
goal_identified
goal_identified
=== ep: 3422, time 27.03443717956543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3422
goal_identified
goal_identified
=== ep: 3423, time 26.932263374328613, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3423
goal_identified
goal_identified
goal_identified
=== ep: 3424, time 27.57435417175293, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 135/135)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3424
goal_identified
goal_identified
goal_identified
=== ep: 3425, time 26.715856313705444, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3425
goal_identified
=== ep: 3426, time 27.458078384399414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3426
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3427, time 26.733936309814453, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3427
=== ep: 3428, time 26.96327042579651, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3428
goal_identified
=== ep: 3429, time 31.293692111968994, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3429
goal_identified
=== ep: 3430, time 27.41638493537903, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3430
goal_identified
goal_identified
goal_identified
=== ep: 3431, time 27.13437843322754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3431
goal_identified
goal_identified
=== ep: 3432, time 27.067216396331787, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3432
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3433, time 27.422157287597656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3433
goal_identified
=== ep: 3434, time 26.861793041229248, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3434
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3435, time 27.01242470741272, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3435
=== ep: 3436, time 27.294058322906494, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3436
goal_identified
=== ep: 3437, time 27.247050046920776, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3437
goal_identified
goal_identified
=== ep: 3438, time 27.007500648498535, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3438
goal_identified
=== ep: 3439, time 32.05283880233765, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3439
goal_identified
=== ep: 3440, time 27.245445728302002, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3440
goal_identified
=== ep: 3441, time 27.01807975769043, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3441
goal_identified
=== ep: 3442, time 27.00316858291626, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3442
goal_identified
=== ep: 3443, time 27.06190586090088, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3443
goal_identified
=== ep: 3444, time 27.642772912979126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3444
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3445, time 27.095569133758545, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3445
goal_identified
goal_identified
=== ep: 3446, time 26.77071261405945, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3446
goal_identified
goal_identified
=== ep: 3447, time 27.44104266166687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3447
goal_identified
goal_identified
goal_identified
=== ep: 3448, time 26.901553630828857, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3448
goal_identified
goal_identified
goal_identified
=== ep: 3449, time 31.441604614257812, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3449
goal_identified
goal_identified
=== ep: 3450, time 27.535077810287476, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3450
goal_identified
=== ep: 3451, time 27.04328489303589, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3451
goal_identified
=== ep: 3452, time 27.429898977279663, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3452
goal_identified
goal_identified
=== ep: 3453, time 27.116042613983154, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3453
goal_identified
goal_identified
goal_identified
=== ep: 3454, time 26.957744598388672, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3454
=== ep: 3455, time 27.402989387512207, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3455
goal_identified
=== ep: 3456, time 27.417516708374023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3456
goal_identified
goal_identified
goal_identified
=== ep: 3457, time 26.859078645706177, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3457
goal_identified
goal_identified
=== ep: 3458, time 27.392456769943237, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3458
goal_identified
goal_identified
goal_identified
=== ep: 3459, time 31.656403064727783, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3459
goal_identified
goal_identified
=== ep: 3460, time 26.91085696220398, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3460
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3461, time 27.27540874481201, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3461
goal_identified
goal_identified
=== ep: 3462, time 27.36791682243347, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3463, time 27.409471035003662, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3463
goal_identified
=== ep: 3464, time 27.070183753967285, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3464
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3465, time 27.029850006103516, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3465
goal_identified
goal_identified
goal_identified
=== ep: 3466, time 26.997949600219727, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3466
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3467, time 27.094066858291626, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3467
=== ep: 3468, time 27.08513832092285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3468
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3469, time 31.612964391708374, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3469
goal_identified
goal_identified
=== ep: 3470, time 27.23770761489868, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3470
goal_identified
=== ep: 3471, time 27.171271800994873, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3471
goal_identified
goal_identified
goal_identified
=== ep: 3472, time 27.00193214416504, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3472
goal_identified
=== ep: 3473, time 27.31572675704956, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3473
goal_identified
=== ep: 3474, time 27.092679023742676, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3474
=== ep: 3475, time 27.096346378326416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3475
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3476, time 27.23307991027832, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3476
goal_identified
goal_identified
goal_identified
=== ep: 3477, time 27.377660989761353, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 27.12376832962036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3478
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3479, time 31.488195419311523, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3479
goal_identified
=== ep: 3480, time 27.182888984680176, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3480
goal_identified
=== ep: 3481, time 26.925456762313843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3481
goal_identified
goal_identified
=== ep: 3482, time 27.142526149749756, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3482
goal_identified
goal_identified
=== ep: 3483, time 26.954665184020996, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3483
goal_identified
goal_identified
=== ep: 3484, time 27.399040460586548, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3485, time 27.56357431411743, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3485
goal_identified
goal_identified
=== ep: 3486, time 27.511537075042725, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3486
goal_identified
=== ep: 3487, time 27.4528067111969, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3487
goal_identified
goal_identified
goal_identified
=== ep: 3488, time 27.599100589752197, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3488
goal_identified
goal_identified
goal_identified
=== ep: 3489, time 33.169896602630615, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3489
goal_identified
goal_identified
=== ep: 3490, time 27.16809320449829, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3490
=== ep: 3491, time 27.250121355056763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3491
goal_identified
=== ep: 3492, time 27.271090269088745, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3492
goal_identified
=== ep: 3493, time 27.021820306777954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3493
=== ep: 3494, time 27.253211498260498, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3494
goal_identified
goal_identified
=== ep: 3495, time 26.9329252243042, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3495
goal_identified
=== ep: 3496, time 27.08774733543396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3496
goal_identified
goal_identified
goal_identified
=== ep: 3497, time 27.010837078094482, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3497
goal_identified
goal_identified
goal_identified
=== ep: 3498, time 26.78417205810547, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3498
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3499, time 33.32100772857666, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3499
goal_identified
goal_identified
=== ep: 3500, time 27.366130828857422, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3500
goal_identified
goal_identified
=== ep: 3501, time 27.171088218688965, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3501
goal_identified
=== ep: 3502, time 27.60811686515808, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3502
goal_identified
=== ep: 3503, time 26.935458183288574, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3503
=== ep: 3504, time 27.09668517112732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3504
goal_identified
goal_identified
=== ep: 3505, time 26.793710947036743, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3505
goal_identified
goal_identified
goal_identified
=== ep: 3506, time 27.029777765274048, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3506
goal_identified
=== ep: 3507, time 27.62782907485962, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3507
goal_identified
goal_identified
=== ep: 3508, time 26.931047201156616, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3508
goal_identified
goal_identified
=== ep: 3509, time 31.429620265960693, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3509
goal_identified
=== ep: 3510, time 27.28082251548767, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3510
goal_identified
=== ep: 3511, time 27.253952980041504, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3511
=== ep: 3512, time 26.92706799507141, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3512
goal_identified
goal_identified
goal_identified
=== ep: 3513, time 26.993841648101807, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3513
=== ep: 3514, time 27.184751272201538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3514
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3515, time 26.442331314086914, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3515
goal_identified
goal_identified
=== ep: 3516, time 26.86997652053833, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3516
goal_identified
=== ep: 3517, time 27.069877862930298, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3518, time 27.36839509010315, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3518
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3519, time 31.364526748657227, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3519
goal_identified
=== ep: 3520, time 26.940122604370117, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3520
goal_identified
goal_identified
=== ep: 3521, time 26.89700412750244, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3521
goal_identified
goal_identified
goal_identified
=== ep: 3522, time 26.907692432403564, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3522
=== ep: 3523, time 27.12975835800171, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3523
goal_identified
goal_identified
=== ep: 3524, time 27.047361612319946, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3524
=== ep: 3525, time 27.412219285964966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3525
goal_identified
goal_identified
goal_identified
=== ep: 3526, time 27.23697304725647, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3526
goal_identified
=== ep: 3527, time 27.053998231887817, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3527
goal_identified
goal_identified
goal_identified
=== ep: 3528, time 27.199936866760254, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3528
goal_identified
goal_identified
goal_identified
=== ep: 3529, time 31.686726570129395, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3529
goal_identified
goal_identified
=== ep: 3530, time 27.11016869544983, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3530
goal_identified
=== ep: 3531, time 27.242826461791992, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3531
goal_identified
goal_identified
=== ep: 3532, time 26.85459804534912, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3532
goal_identified
=== ep: 3533, time 26.96265983581543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3533
goal_identified
goal_identified
=== ep: 3534, time 27.38762331008911, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3534
goal_identified
goal_identified
goal_identified
=== ep: 3535, time 27.050299882888794, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3535
goal_identified
=== ep: 3536, time 27.234113216400146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3536
goal_identified
=== ep: 3537, time 27.171075105667114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3537
goal_identified
goal_identified
=== ep: 3538, time 26.657087326049805, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3538
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3539, time 32.09872603416443, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3539
goal_identified
goal_identified
goal_identified
=== ep: 3540, time 27.235977172851562, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3540
goal_identified
goal_identified
=== ep: 3541, time 27.267375469207764, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3541
goal_identified
goal_identified
goal_identified
=== ep: 3542, time 27.024466514587402, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3542
goal_identified
=== ep: 3543, time 27.209675788879395, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3543
goal_identified
goal_identified
goal_identified
=== ep: 3544, time 26.99887776374817, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3544
goal_identified
goal_identified
=== ep: 3545, time 26.93961477279663, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3545
goal_identified
goal_identified
goal_identified
=== ep: 3546, time 26.912290334701538, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3546
goal_identified
=== ep: 3547, time 27.112494230270386, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3547
goal_identified
=== ep: 3548, time 26.98784303665161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3548
=== ep: 3549, time 31.683287858963013, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3549
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3550, time 27.11114740371704, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3550
goal_identified
=== ep: 3551, time 27.180360078811646, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3551
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3552, time 26.842799186706543, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3552
goal_identified
goal_identified
=== ep: 3553, time 27.218917846679688, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3554, time 27.24110436439514, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3006
goal_identified
goal_identified
goal_identified
=== ep: 3555, time 27.28716468811035, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3555
goal_identified
goal_identified
=== ep: 3556, time 27.2438485622406, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3556
goal_identified
goal_identified
=== ep: 3557, time 27.593119859695435, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3557
goal_identified
goal_identified
=== ep: 3558, time 27.036791563034058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3558
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3559, time 31.782654523849487, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3559
=== ep: 3560, time 27.028122901916504, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3560
goal_identified
goal_identified
=== ep: 3561, time 27.319777250289917, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3561
goal_identified
goal_identified
=== ep: 3562, time 27.03932476043701, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3562
goal_identified
goal_identified
=== ep: 3563, time 27.704704523086548, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3564, time 27.32254409790039, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3564
goal_identified
goal_identified
=== ep: 3565, time 27.081557989120483, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3565
goal_identified
goal_identified
=== ep: 3566, time 27.113393783569336, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3566
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3567, time 26.99174189567566, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3567
goal_identified
=== ep: 3568, time 27.43125081062317, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3568
goal_identified
goal_identified
goal_identified
=== ep: 3569, time 31.69120717048645, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3569
goal_identified
goal_identified
=== ep: 3570, time 27.562330961227417, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3570
goal_identified
goal_identified
=== ep: 3571, time 27.10396409034729, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3571
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3572, time 27.113274574279785, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3572
goal_identified
=== ep: 3573, time 26.92076849937439, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3573
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3574, time 27.394504070281982, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3574
goal_identified
goal_identified
=== ep: 3575, time 27.40789031982422, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3575
goal_identified
goal_identified
goal_identified
=== ep: 3576, time 27.139888048171997, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3576
goal_identified
goal_identified
goal_identified
=== ep: 3577, time 27.28692364692688, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3577
goal_identified
goal_identified
goal_identified
=== ep: 3578, time 27.01689052581787, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3579, time 32.12831449508667, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3579
goal_identified
goal_identified
=== ep: 3580, time 27.54276728630066, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3580
goal_identified
=== ep: 3581, time 27.023340702056885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3582, time 27.324693202972412, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3582
goal_identified
goal_identified
goal_identified
=== ep: 3583, time 27.09507465362549, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3583
goal_identified
goal_identified
=== ep: 3584, time 27.116788864135742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3584
goal_identified
goal_identified
=== ep: 3585, time 27.01318860054016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3585
goal_identified
=== ep: 3586, time 27.16597557067871, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3586
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3587, time 27.23842763900757, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 26.78114366531372, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3588
goal_identified
=== ep: 3589, time 32.31719994544983, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3590, time 27.551527500152588, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3590
goal_identified
=== ep: 3591, time 26.81139874458313, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 27.32575273513794, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3592
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3593, time 27.01732110977173, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3593
goal_identified
goal_identified
goal_identified
=== ep: 3594, time 26.94449472427368, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3594
goal_identified
goal_identified
goal_identified
=== ep: 3595, time 26.911418914794922, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3595
goal_identified
goal_identified
goal_identified
=== ep: 3596, time 27.43513774871826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3596
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3597, time 26.68087863922119, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3597
goal_identified
=== ep: 3598, time 27.12749981880188, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3598
goal_identified
goal_identified
=== ep: 3599, time 31.077966451644897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3599
goal_identified
=== ep: 3600, time 27.103365182876587, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3600
goal_identified
=== ep: 3601, time 27.174712896347046, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3601
goal_identified
=== ep: 3602, time 27.1878879070282, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3602
goal_identified
=== ep: 3603, time 27.404690265655518, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3603
goal_identified
goal_identified
goal_identified
=== ep: 3604, time 27.143905639648438, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3604
goal_identified
=== ep: 3605, time 27.379342794418335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3605
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3606, time 27.035752296447754, eps 0.001, sum reward: 4, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3606
goal_identified
=== ep: 3607, time 27.156846046447754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3607
=== ep: 3608, time 27.291420459747314, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3608
goal_identified
goal_identified
=== ep: 3609, time 31.90481734275818, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3609
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3610, time 26.808255195617676, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3610
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3611, time 27.273053646087646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3611
goal_identified
=== ep: 3612, time 27.360769987106323, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3612
goal_identified
=== ep: 3613, time 27.602315425872803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3613
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3614, time 27.066671133041382, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3614
goal_identified
goal_identified
=== ep: 3615, time 27.047123432159424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3615
goal_identified
goal_identified
goal_identified
=== ep: 3616, time 27.20807719230652, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3616
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3617, time 26.996278762817383, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3617
goal_identified
=== ep: 3618, time 27.214494466781616, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3618
goal_identified
goal_identified
=== ep: 3619, time 31.579479694366455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3619
goal_identified
goal_identified
goal_identified
=== ep: 3620, time 27.003730297088623, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3620
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3621, time 27.008825063705444, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3622, time 27.375619888305664, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3622
goal_identified
goal_identified
goal_identified
=== ep: 3623, time 27.02750039100647, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3623
goal_identified
goal_identified
goal_identified
=== ep: 3624, time 27.55366611480713, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3624
goal_identified
goal_identified
=== ep: 3625, time 27.073955297470093, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3625
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3626, time 27.03541898727417, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3626
goal_identified
goal_identified
goal_identified
=== ep: 3627, time 27.220399856567383, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3627
goal_identified
=== ep: 3628, time 27.01637101173401, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3628
goal_identified
=== ep: 3629, time 31.734655618667603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3629
goal_identified
goal_identified
=== ep: 3630, time 27.804951906204224, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3630
goal_identified
=== ep: 3631, time 27.17621159553528, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3631
goal_identified
=== ep: 3632, time 26.88216733932495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3632
goal_identified
=== ep: 3633, time 27.234123706817627, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3633
goal_identified
=== ep: 3634, time 27.425522089004517, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3634
=== ep: 3635, time 27.706337213516235, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3635
goal_identified
=== ep: 3636, time 27.778461694717407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3636
goal_identified
goal_identified
=== ep: 3637, time 27.49258852005005, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3637
goal_identified
=== ep: 3638, time 26.959950923919678, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3638
goal_identified
goal_identified
=== ep: 3639, time 32.2085964679718, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3639
=== ep: 3640, time 27.25915789604187, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3640
goal_identified
goal_identified
=== ep: 3641, time 27.040487051010132, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3641
=== ep: 3642, time 27.08685040473938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3642
=== ep: 3643, time 27.22219204902649, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3643
goal_identified
goal_identified
=== ep: 3644, time 27.226216316223145, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3644
goal_identified
=== ep: 3645, time 27.459998607635498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3645
goal_identified
goal_identified
goal_identified
=== ep: 3646, time 27.086327075958252, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3646
goal_identified
goal_identified
=== ep: 3647, time 27.53341245651245, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3647
goal_identified
goal_identified
goal_identified
=== ep: 3648, time 27.000173807144165, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3648
goal_identified
goal_identified
=== ep: 3649, time 31.60637640953064, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3649
goal_identified
=== ep: 3650, time 27.248430967330933, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3650
goal_identified
goal_identified
goal_identified
=== ep: 3651, time 27.187007904052734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 26.891716241836548, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3652
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3653, time 27.313552379608154, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3653
goal_identified
goal_identified
=== ep: 3654, time 26.709123611450195, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3654
goal_identified
goal_identified
=== ep: 3655, time 27.30439305305481, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3655
goal_identified
=== ep: 3656, time 27.281192779541016, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3656
=== ep: 3657, time 27.437817811965942, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3657
goal_identified
=== ep: 3658, time 26.768112421035767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3658
goal_identified
goal_identified
=== ep: 3659, time 32.010013818740845, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3659
goal_identified
goal_identified
goal_identified
=== ep: 3660, time 26.757261276245117, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3660
goal_identified
goal_identified
=== ep: 3661, time 27.30740261077881, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3661
goal_identified
=== ep: 3662, time 27.275952100753784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3662
goal_identified
goal_identified
=== ep: 3663, time 26.820367336273193, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3663
goal_identified
goal_identified
=== ep: 3664, time 27.280701160430908, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3664
goal_identified
goal_identified
=== ep: 3665, time 27.75029420852661, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3665
=== ep: 3666, time 27.25394630432129, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3666
goal_identified
goal_identified
=== ep: 3667, time 27.11675214767456, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3667
goal_identified
=== ep: 3668, time 27.198010444641113, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3668
goal_identified
goal_identified
=== ep: 3669, time 31.691439628601074, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3669
goal_identified
goal_identified
=== ep: 3670, time 27.053449153900146, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3670
goal_identified
=== ep: 3671, time 27.240368604660034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3671
goal_identified
goal_identified
goal_identified
=== ep: 3672, time 27.471333265304565, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3672
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3673, time 26.86474299430847, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3673
goal_identified
=== ep: 3674, time 27.235182762145996, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3674
goal_identified
goal_identified
=== ep: 3675, time 27.004889488220215, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3675
goal_identified
goal_identified
goal_identified
=== ep: 3676, time 27.080628395080566, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3676
=== ep: 3677, time 27.500746726989746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3677
goal_identified
goal_identified
=== ep: 3678, time 27.281526803970337, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3678
goal_identified
=== ep: 3679, time 31.47194743156433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3679
goal_identified
=== ep: 3680, time 27.186171531677246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3680
goal_identified
goal_identified
goal_identified
=== ep: 3681, time 27.099208116531372, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3681
goal_identified
=== ep: 3682, time 26.97928524017334, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3682
goal_identified
goal_identified
=== ep: 3683, time 27.2377347946167, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3683
goal_identified
goal_identified
=== ep: 3684, time 27.14251208305359, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3684
goal_identified
=== ep: 3685, time 26.887245416641235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3685
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3686, time 26.83280873298645, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3686
goal_identified
=== ep: 3687, time 27.219102144241333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3687
goal_identified
goal_identified
goal_identified
=== ep: 3688, time 27.342310667037964, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3688
goal_identified
=== ep: 3689, time 32.03514575958252, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3689
goal_identified
goal_identified
=== ep: 3690, time 26.647451162338257, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3690
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3691, time 26.941731929779053, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3691
goal_identified
goal_identified
=== ep: 3692, time 27.11750340461731, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3692
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3693, time 27.08187222480774, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3693
=== ep: 3694, time 27.247528076171875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3694
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3695, time 27.41096782684326, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3695
goal_identified
=== ep: 3696, time 27.419302701950073, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3696
goal_identified
=== ep: 3697, time 27.546496391296387, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3697
goal_identified
goal_identified
=== ep: 3698, time 27.275349378585815, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3698
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3699, time 31.658230304718018, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3699
goal_identified
goal_identified
goal_identified
=== ep: 3700, time 27.211411237716675, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3700
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3701, time 27.23057508468628, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3701
goal_identified
=== ep: 3702, time 27.20121669769287, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3702
goal_identified
goal_identified
goal_identified
=== ep: 3703, time 27.11709952354431, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3703
goal_identified
goal_identified
goal_identified
=== ep: 3704, time 27.274450063705444, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3704
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3705, time 27.05618906021118, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3705
goal_identified
goal_identified
=== ep: 3706, time 27.301015853881836, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3706
goal_identified
=== ep: 3707, time 26.71326994895935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3707
goal_identified
=== ep: 3708, time 27.528425455093384, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3708
goal_identified
goal_identified
=== ep: 3709, time 32.00861382484436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3709
goal_identified
goal_identified
=== ep: 3710, time 27.072338819503784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3710
goal_identified
goal_identified
=== ep: 3711, time 27.22046399116516, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3711
goal_identified
goal_identified
=== ep: 3712, time 26.964588165283203, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3712
goal_identified
goal_identified
=== ep: 3713, time 27.132538318634033, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3713
goal_identified
goal_identified
goal_identified
=== ep: 3714, time 27.05050492286682, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3714
goal_identified
goal_identified
goal_identified
=== ep: 3715, time 27.07225513458252, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3715
goal_identified
=== ep: 3716, time 27.5647075176239, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3716
goal_identified
goal_identified
=== ep: 3717, time 27.00519871711731, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3717
goal_identified
goal_identified
goal_identified
=== ep: 3718, time 26.887256622314453, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3718
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3719, time 32.003918170928955, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3719
goal_identified
goal_identified
goal_identified
=== ep: 3720, time 27.39944314956665, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3721, time 27.328813314437866, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3722, time 26.933654069900513, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3024
goal_identified
goal_identified
goal_identified
=== ep: 3723, time 27.27810788154602, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3723
goal_identified
=== ep: 3724, time 27.45577049255371, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3724
goal_identified
=== ep: 3725, time 27.449902057647705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3725
goal_identified
goal_identified
=== ep: 3726, time 27.411856412887573, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3726
goal_identified
goal_identified
goal_identified
=== ep: 3727, time 27.28216314315796, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3727
goal_identified
goal_identified
=== ep: 3728, time 27.078221559524536, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3728
goal_identified
goal_identified
goal_identified
=== ep: 3729, time 32.006340980529785, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3729
goal_identified
goal_identified
goal_identified
=== ep: 3730, time 27.114842653274536, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3730
=== ep: 3731, time 27.621739149093628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3731
goal_identified
goal_identified
=== ep: 3732, time 27.265679836273193, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3732
goal_identified
=== ep: 3733, time 27.325085163116455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3733
goal_identified
goal_identified
goal_identified
=== ep: 3734, time 27.24724793434143, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3734
goal_identified
=== ep: 3735, time 27.249638557434082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3735
goal_identified
goal_identified
=== ep: 3736, time 27.636709690093994, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3736
goal_identified
goal_identified
=== ep: 3737, time 27.058626890182495, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3737
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3738, time 27.13519287109375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3738
goal_identified
goal_identified
=== ep: 3739, time 31.919795274734497, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3739
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3740, time 27.023788452148438, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3740
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3741, time 27.34294605255127, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3741
goal_identified
=== ep: 3742, time 27.319186449050903, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3742
goal_identified
=== ep: 3743, time 27.405047178268433, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3743
goal_identified
goal_identified
=== ep: 3744, time 27.232884883880615, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3744
goal_identified
=== ep: 3745, time 27.447168827056885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3745
goal_identified
goal_identified
=== ep: 3746, time 27.06136703491211, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3746
goal_identified
goal_identified
=== ep: 3747, time 27.081854581832886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 27.221578359603882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3748
goal_identified
=== ep: 3749, time 31.921396493911743, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3749
goal_identified
goal_identified
=== ep: 3750, time 27.079665184020996, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 27.326024293899536, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3751
goal_identified
goal_identified
=== ep: 3752, time 27.35343909263611, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3752
goal_identified
=== ep: 3753, time 27.175649881362915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3753
goal_identified
goal_identified
goal_identified
=== ep: 3754, time 26.93883752822876, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3754
goal_identified
=== ep: 3755, time 27.312147617340088, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3755
goal_identified
=== ep: 3756, time 27.226758241653442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3756
goal_identified
goal_identified
=== ep: 3757, time 26.962161779403687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3757
=== ep: 3758, time 27.046523809432983, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3758
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3759, time 32.13642954826355, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3759
goal_identified
goal_identified
=== ep: 3760, time 26.86721110343933, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3761, time 27.301262140274048, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3761
goal_identified
goal_identified
=== ep: 3762, time 27.137749671936035, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3762
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3763, time 27.13905143737793, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3763
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3764, time 27.170039415359497, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3764
goal_identified
goal_identified
goal_identified
=== ep: 3765, time 27.157642126083374, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3765
goal_identified
goal_identified
goal_identified
=== ep: 3766, time 27.850301265716553, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3766
goal_identified
goal_identified
goal_identified
=== ep: 3767, time 27.116353273391724, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3767
goal_identified
goal_identified
=== ep: 3768, time 27.33092713356018, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3768
goal_identified
goal_identified
=== ep: 3769, time 31.787435293197632, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3769
goal_identified
=== ep: 3770, time 27.141971111297607, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3770
goal_identified
goal_identified
=== ep: 3771, time 27.350074529647827, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3771
=== ep: 3772, time 27.296613693237305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3772
goal_identified
goal_identified
=== ep: 3773, time 26.696732997894287, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3773
goal_identified
=== ep: 3774, time 27.230847120285034, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3774
goal_identified
=== ep: 3775, time 27.34241008758545, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3775
=== ep: 3776, time 27.031023740768433, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3776
goal_identified
=== ep: 3777, time 27.649753093719482, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3777
goal_identified
=== ep: 3778, time 27.263630867004395, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3778
goal_identified
goal_identified
=== ep: 3779, time 32.3392333984375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3779
goal_identified
goal_identified
=== ep: 3780, time 26.78528666496277, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3780
goal_identified
goal_identified
goal_identified
=== ep: 3781, time 27.086253881454468, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3781
=== ep: 3782, time 27.463799238204956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3782
goal_identified
goal_identified
goal_identified
=== ep: 3783, time 27.269429922103882, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3783
goal_identified
goal_identified
=== ep: 3784, time 27.06569766998291, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3784
=== ep: 3785, time 27.20659899711609, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3785
=== ep: 3786, time 27.283528566360474, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3786
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3787, time 27.535653114318848, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3159
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 27.46966028213501, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3788
goal_identified
goal_identified
goal_identified
=== ep: 3789, time 32.252519369125366, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3789
goal_identified
=== ep: 3790, time 27.20301580429077, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3790
goal_identified
goal_identified
goal_identified
=== ep: 3791, time 27.719134092330933, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3791
goal_identified
=== ep: 3792, time 26.930407285690308, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3792
goal_identified
goal_identified
goal_identified
=== ep: 3793, time 27.430422067642212, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3793
goal_identified
goal_identified
=== ep: 3794, time 27.443018913269043, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3794
goal_identified
=== ep: 3795, time 27.159116983413696, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3795
goal_identified
goal_identified
=== ep: 3796, time 26.895671367645264, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3796
goal_identified
goal_identified
=== ep: 3797, time 26.689313650131226, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3797
goal_identified
goal_identified
=== ep: 3798, time 27.29061722755432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3798
goal_identified
goal_identified
=== ep: 3799, time 31.87762188911438, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3799
goal_identified
goal_identified
goal_identified
=== ep: 3800, time 27.5990629196167, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3800
goal_identified
goal_identified
goal_identified
=== ep: 3801, time 27.160865306854248, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3801
goal_identified
=== ep: 3802, time 27.291136741638184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3802
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3803, time 27.3333420753479, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3804, time 27.20726752281189, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3804
goal_identified
goal_identified
=== ep: 3805, time 27.16321635246277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3805
=== ep: 3806, time 27.114617109298706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3806
goal_identified
goal_identified
=== ep: 3807, time 26.968077898025513, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3807
goal_identified
=== ep: 3808, time 26.985170364379883, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3808
goal_identified
goal_identified
=== ep: 3809, time 31.748891592025757, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3809
goal_identified
goal_identified
goal_identified
=== ep: 3810, time 27.22697138786316, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3810
=== ep: 3811, time 26.777363538742065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3811
goal_identified
goal_identified
goal_identified
=== ep: 3812, time 26.92719578742981, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3812
goal_identified
goal_identified
=== ep: 3813, time 27.517176866531372, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3813
goal_identified
goal_identified
=== ep: 3814, time 27.364601373672485, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3814
goal_identified
=== ep: 3815, time 26.958658456802368, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3815
goal_identified
=== ep: 3816, time 26.770705699920654, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3816
=== ep: 3817, time 27.222434043884277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3817
goal_identified
=== ep: 3818, time 27.23368239402771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3818
goal_identified
=== ep: 3819, time 32.36744737625122, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3819
goal_identified
goal_identified
=== ep: 3820, time 27.41274070739746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3820
goal_identified
goal_identified
=== ep: 3821, time 27.05427837371826, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3821
goal_identified
goal_identified
=== ep: 3822, time 27.58525586128235, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3822
goal_identified
goal_identified
goal_identified
=== ep: 3823, time 27.513410329818726, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3823
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3824, time 27.358153104782104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3824
goal_identified
goal_identified
=== ep: 3825, time 27.371280431747437, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3825
=== ep: 3826, time 26.86910629272461, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3826
goal_identified
goal_identified
=== ep: 3827, time 27.254529237747192, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3827
goal_identified
goal_identified
=== ep: 3828, time 27.484697103500366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3828
=== ep: 3829, time 31.600772380828857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3829
goal_identified
goal_identified
goal_identified
=== ep: 3830, time 27.104018688201904, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3830
goal_identified
goal_identified
goal_identified
=== ep: 3831, time 26.804489612579346, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3831
goal_identified
goal_identified
goal_identified
=== ep: 3832, time 27.09527850151062, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3832
goal_identified
=== ep: 3833, time 27.304574251174927, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3833
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3834, time 27.182912349700928, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3834
goal_identified
=== ep: 3835, time 27.06935954093933, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3835
goal_identified
=== ep: 3836, time 27.534460306167603, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3836
goal_identified
goal_identified
=== ep: 3837, time 26.986576318740845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3838, time 27.46592617034912, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3838
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3839, time 31.799850702285767, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3839
goal_identified
goal_identified
goal_identified
=== ep: 3840, time 27.284518003463745, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3840
=== ep: 3841, time 27.443929195404053, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 27.08086895942688, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3842
goal_identified
goal_identified
=== ep: 3843, time 27.13846516609192, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3843
goal_identified
goal_identified
=== ep: 3844, time 27.381072759628296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3844
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3845, time 27.33764386177063, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3845
goal_identified
goal_identified
=== ep: 3846, time 27.17217206954956, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3846
goal_identified
goal_identified
goal_identified
=== ep: 3847, time 27.05521845817566, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3847
goal_identified
goal_identified
goal_identified
=== ep: 3848, time 27.150126218795776, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3848
goal_identified
=== ep: 3849, time 31.753750801086426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3850, time 26.917690992355347, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3850
goal_identified
=== ep: 3851, time 26.62678074836731, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3851
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3852, time 27.00975275039673, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3852
goal_identified
goal_identified
=== ep: 3853, time 27.070907592773438, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3853
=== ep: 3854, time 27.704419136047363, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3854
goal_identified
=== ep: 3855, time 27.245718955993652, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3855
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3856, time 27.13134217262268, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3856
goal_identified
goal_identified
=== ep: 3857, time 27.10173749923706, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3857
goal_identified
goal_identified
=== ep: 3858, time 27.216586112976074, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3858
goal_identified
goal_identified
goal_identified
=== ep: 3859, time 31.900701761245728, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3859
goal_identified
goal_identified
goal_identified
=== ep: 3860, time 27.358275890350342, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3860
goal_identified
=== ep: 3861, time 27.167345762252808, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3861
goal_identified
goal_identified
=== ep: 3862, time 27.104740619659424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3862
goal_identified
goal_identified
=== ep: 3863, time 27.09689426422119, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3863
goal_identified
goal_identified
=== ep: 3864, time 27.169986248016357, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3864
goal_identified
goal_identified
goal_identified
=== ep: 3865, time 27.851088047027588, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3865
goal_identified
goal_identified
goal_identified
=== ep: 3866, time 27.71700096130371, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3866
goal_identified
goal_identified
=== ep: 3867, time 27.113467693328857, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3868, time 27.49126672744751, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3868
goal_identified
goal_identified
=== ep: 3869, time 32.31234264373779, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3869
=== ep: 3870, time 27.513919353485107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3870
goal_identified
=== ep: 3871, time 27.44960355758667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3871
goal_identified
=== ep: 3872, time 27.220964908599854, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3872
goal_identified
=== ep: 3873, time 26.98111629486084, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3873
goal_identified
goal_identified
=== ep: 3874, time 26.910855293273926, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3874
=== ep: 3875, time 26.981706380844116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3875
=== ep: 3876, time 27.095287084579468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3876
goal_identified
=== ep: 3877, time 27.10015892982483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3877
goal_identified
=== ep: 3878, time 27.00809383392334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3879, time 32.14358711242676, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3162
goal_identified
goal_identified
=== ep: 3880, time 27.4040846824646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3880
goal_identified
goal_identified
=== ep: 3881, time 27.157800912857056, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3881
goal_identified
goal_identified
=== ep: 3882, time 26.927873611450195, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3882
goal_identified
=== ep: 3883, time 26.7593834400177, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3883
goal_identified
=== ep: 3884, time 27.255696058273315, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3884
goal_identified
goal_identified
goal_identified
=== ep: 3885, time 27.278026580810547, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3885
goal_identified
goal_identified
=== ep: 3886, time 26.90291452407837, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3886
goal_identified
goal_identified
goal_identified
=== ep: 3887, time 27.27667212486267, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3887
goal_identified
goal_identified
goal_identified
=== ep: 3888, time 27.394145727157593, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3888
goal_identified
=== ep: 3889, time 31.778367519378662, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3889
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3890, time 27.341894388198853, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3890
goal_identified
goal_identified
=== ep: 3891, time 27.40076732635498, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3891
=== ep: 3892, time 27.692591428756714, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3892
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3893, time 27.299293518066406, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3893
goal_identified
=== ep: 3894, time 27.601696014404297, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3894
goal_identified
goal_identified
=== ep: 3895, time 27.28653073310852, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3895
goal_identified
=== ep: 3896, time 27.446980714797974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3896
goal_identified
=== ep: 3897, time 27.338486433029175, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3897
goal_identified
goal_identified
goal_identified
=== ep: 3898, time 27.215660095214844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3898
goal_identified
=== ep: 3899, time 32.219523191452026, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3899
goal_identified
goal_identified
=== ep: 3900, time 27.09900212287903, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3900
goal_identified
goal_identified
=== ep: 3901, time 27.117756366729736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3901
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3902, time 27.424232006072998, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3902
goal_identified
=== ep: 3903, time 27.300528526306152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3904, time 27.008938789367676, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3904
goal_identified
goal_identified
goal_identified
=== ep: 3905, time 27.18105912208557, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3905
goal_identified
goal_identified
goal_identified
=== ep: 3906, time 26.966179132461548, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3906
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3907, time 27.60415029525757, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3907
goal_identified
goal_identified
=== ep: 3908, time 27.31088900566101, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3908
goal_identified
goal_identified
=== ep: 3909, time 32.04849290847778, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3909
goal_identified
goal_identified
=== ep: 3910, time 27.19827675819397, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3910
goal_identified
goal_identified
=== ep: 3911, time 26.93645215034485, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3911
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3912, time 26.993436098098755, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3912
goal_identified
goal_identified
=== ep: 3913, time 27.076470136642456, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3913
goal_identified
goal_identified
=== ep: 3914, time 27.21587324142456, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3914
goal_identified
=== ep: 3915, time 27.241023302078247, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3915
=== ep: 3916, time 27.27201199531555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3916
goal_identified
=== ep: 3917, time 27.059969663619995, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3917
goal_identified
goal_identified
=== ep: 3918, time 27.115355253219604, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3918
goal_identified
=== ep: 3919, time 31.9305739402771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3919
goal_identified
=== ep: 3920, time 27.2755343914032, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3920
=== ep: 3921, time 27.08650493621826, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3921
goal_identified
goal_identified
goal_identified
=== ep: 3922, time 27.12969994544983, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3922
goal_identified
=== ep: 3923, time 26.98591160774231, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3923
=== ep: 3924, time 26.89132595062256, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3924
goal_identified
goal_identified
=== ep: 3925, time 26.66344690322876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3925
goal_identified
=== ep: 3926, time 26.929847478866577, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3926
goal_identified
goal_identified
=== ep: 3927, time 26.8512442111969, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3927
goal_identified
goal_identified
=== ep: 3928, time 27.274138927459717, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3928
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3929, time 32.34991097450256, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3929
goal_identified
goal_identified
goal_identified
=== ep: 3930, time 26.863749980926514, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3930
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3931, time 27.275113821029663, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3931
goal_identified
goal_identified
goal_identified
=== ep: 3932, time 27.49941635131836, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3932
goal_identified
goal_identified
goal_identified
=== ep: 3933, time 27.145524263381958, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3933
goal_identified
=== ep: 3934, time 27.183337926864624, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3934
goal_identified
goal_identified
goal_identified
=== ep: 3935, time 27.048118591308594, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 26.986603021621704, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3936
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3937, time 27.164759874343872, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3937
goal_identified
goal_identified
=== ep: 3938, time 27.178309440612793, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3938
goal_identified
=== ep: 3939, time 32.20851945877075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3939
goal_identified
=== ep: 3940, time 27.34563970565796, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3940
goal_identified
goal_identified
goal_identified
=== ep: 3941, time 26.707427978515625, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3941
goal_identified
=== ep: 3942, time 26.607070207595825, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3942
goal_identified
goal_identified
=== ep: 3943, time 27.178263187408447, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3943
=== ep: 3944, time 27.226837635040283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3944
=== ep: 3945, time 27.187365531921387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3945
goal_identified
=== ep: 3946, time 26.86497402191162, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3946
goal_identified
=== ep: 3947, time 27.075021743774414, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3947
goal_identified
=== ep: 3948, time 27.35360097885132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3948
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3949, time 31.93215036392212, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3949
goal_identified
=== ep: 3950, time 27.458284616470337, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3950
goal_identified
goal_identified
=== ep: 3951, time 26.770840167999268, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3951
goal_identified
goal_identified
goal_identified
=== ep: 3952, time 26.959425687789917, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3952
=== ep: 3953, time 27.086239337921143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3953
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3954, time 27.197734117507935, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3954
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3955, time 26.893616437911987, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3955
goal_identified
=== ep: 3956, time 26.775723934173584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3956
goal_identified
goal_identified
=== ep: 3957, time 27.162453651428223, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3957
goal_identified
goal_identified
=== ep: 3958, time 26.95648741722107, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3958
goal_identified
goal_identified
=== ep: 3959, time 32.17532467842102, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3959
goal_identified
goal_identified
=== ep: 3960, time 27.272847414016724, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3961, time 26.950921058654785, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3404
=== ep: 3962, time 27.323370456695557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3962
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3963, time 27.37144708633423, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3963
goal_identified
goal_identified
goal_identified
=== ep: 3964, time 26.89317011833191, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3964
goal_identified
=== ep: 3965, time 27.140690088272095, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3965
goal_identified
goal_identified
goal_identified
=== ep: 3966, time 27.243523120880127, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3966
goal_identified
goal_identified
goal_identified
=== ep: 3967, time 26.987248182296753, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3967
goal_identified
goal_identified
=== ep: 3968, time 27.287725687026978, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3968
=== ep: 3969, time 33.80096673965454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3969
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3970, time 27.540748834609985, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3970
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3971, time 27.032901763916016, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3971
goal_identified
goal_identified
=== ep: 3972, time 27.430107831954956, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3972
goal_identified
=== ep: 3973, time 27.444741010665894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3973
=== ep: 3974, time 27.47028946876526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3974
goal_identified
goal_identified
=== ep: 3975, time 26.971922874450684, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3975
=== ep: 3976, time 27.190770864486694, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3976
goal_identified
goal_identified
goal_identified
=== ep: 3977, time 27.354658365249634, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3977
goal_identified
goal_identified
=== ep: 3978, time 27.192384719848633, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3978
goal_identified
goal_identified
=== ep: 3979, time 34.291401624679565, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3979
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3980, time 27.303067922592163, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3980
goal_identified
goal_identified
=== ep: 3981, time 26.961878538131714, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3981
goal_identified
goal_identified
=== ep: 3982, time 27.187564611434937, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3982
goal_identified
=== ep: 3983, time 27.184629201889038, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3983
goal_identified
goal_identified
=== ep: 3984, time 27.21614408493042, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3984
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3985, time 27.08045983314514, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3985
goal_identified
=== ep: 3986, time 27.23384141921997, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3986
goal_identified
=== ep: 3987, time 26.977901935577393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3987
goal_identified
=== ep: 3988, time 27.66688346862793, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3988
goal_identified
goal_identified
=== ep: 3989, time 33.67133188247681, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3989
goal_identified
goal_identified
=== ep: 3990, time 27.422083139419556, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3990
goal_identified
goal_identified
goal_identified
=== ep: 3991, time 26.984307765960693, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3992, time 27.34102702140808, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 27.402517795562744, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3554
goal_identified
goal_identified
=== ep: 3994, time 27.24272584915161, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3994
=== ep: 3995, time 27.31861686706543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3995
goal_identified
goal_identified
goal_identified
=== ep: 3996, time 28.507163524627686, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3996
goal_identified
goal_identified
=== ep: 3997, time 27.28750467300415, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3997
goal_identified
goal_identified
goal_identified
=== ep: 3998, time 27.285345792770386, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3999, time 32.5243022441864, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
