==> Playing in 11_vs_11_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['attack', 'defend'])
==>Currently learning win_game to choose from above OTs.
==>using device cuda
==>critic has 2 layers and 3 hidden units.
goal_identified
=== ep: 0, time 28.592163562774658, eps 0.9, right preds for atk and def: 102/204 = 0.5, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 1, time 28.769907474517822, eps 0.8561552526261419, right preds for atk and def: 69/131 = 0.5267175572519084, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 2, time 30.084456205368042, eps 0.8144488388143276, right preds for atk and def: 74/173 = 0.4277456647398844, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 3, time 30.815632581710815, eps 0.774776470806127, right preds for atk and def: 62/127 = 0.4881889763779528, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 4, time 31.769992113113403, eps 0.7370389470171057, right preds for atk and def: 73/163 = 0.44785276073619634, score_diff -1, tot learning steps 10 (total env steps 3001)
=== ep: 5, time 31.77492642402649, eps 0.701141903981193, right preds for atk and def: 64/159 = 0.4025157232704403, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 6, time 31.637655019760132, eps 0.6669955803928644, right preds for atk and def: 84/170 = 0.49411764705882355, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 7, time 32.986483097076416, eps 0.6345145926571234, right preds for atk and def: 53/114 = 0.4649122807017544, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 8, time 35.12790536880493, eps 0.6036177213860398, right preds for atk and def: 59/150 = 0.3933333333333333, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 9, time 36.176092863082886, eps 0.5742277083079742, right preds for atk and def: 81/204 = 0.39705882352941174, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 10, time 34.39995765686035, eps 0.5462710630816575, right preds for atk and def: 66/148 = 0.44594594594594594, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 11, time 34.58574819564819, eps 0.5196778795320575, right preds for atk and def: 63/155 = 0.4064516129032258, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 12, time 39.34940814971924, eps 0.49438166084852986, right preds for atk and def: 93/221 = 0.42081447963800905, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 13, time 45.314770221710205, eps 0.47031915330815344, right preds for atk and def: 70/192 = 0.3645833333333333, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 14, time 40.74847912788391, eps 0.4474301881084772, right preds for atk and def: 77/186 = 0.41397849462365593, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 15, time 45.48749661445618, eps 0.42565753091417224, right preds for atk and def: 82/224 = 0.36607142857142855, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 16, time 46.9670844078064, eps 0.4049467387413822, right preds for atk and def: 87/275 = 0.31636363636363635, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 17, time 36.08105516433716, eps 0.3852460238219053, right preds for atk and def: 58/185 = 0.31351351351351353, score_diff 4, tot learning steps 10 (total env steps 3001)
=== ep: 18, time 44.084486961364746, eps 0.3665061241067986, right preds for atk and def: 71/108 = 0.6574074074074074, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 19, time 44.778964042663574, eps 0.3486801800855966, right preds for atk and def: 58/69 = 0.8405797101449275, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 17
=== ep: 20, time 38.29692983627319, eps 0.3317236176131267, right preds for atk and def: 73/88 = 0.8295454545454546, score_diff 0, tot learning steps 10 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies.py:453: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20 and we are deleting ep 16
=== ep: 21, time 43.966241121292114, eps 0.31559403645092865, right preds for atk and def: 73/84 = 0.8690476190476191, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 13
=== ep: 22, time 50.94766664505005, eps 0.3002511042445735, right preds for atk and def: 67/75 = 0.8933333333333333, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 15
=== ep: 23, time 46.90133285522461, eps 0.2856564556717689, right preds for atk and def: 119/147 = 0.8095238095238095, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 8
=== ep: 24, time 38.363430976867676, eps 0.27177359650906974, right preds for atk and def: 73/81 = 0.9012345679012346, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 9
goal_identified
goal_identified
goal_identified
=== ep: 25, time 42.20218825340271, eps 0.2585678123773109, right preds for atk and def: 88/99 = 0.8888888888888888, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 5
=== ep: 26, time 41.52294564247131, eps 0.24600608193757734, right preds for atk and def: 78/85 = 0.9176470588235294, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 11
=== ep: 27, time 38.937082290649414, eps 0.23405699432065646, right preds for atk and def: 85/99 = 0.8585858585858586, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 14
=== ep: 28, time 43.66758990287781, eps 0.22269067058350425, right preds for atk and def: 77/81 = 0.9506172839506173, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 12
goal_identified
goal_identified
=== ep: 29, time 41.573631048202515, eps 0.2118786889963241, right preds for atk and def: 83/93 = 0.8924731182795699, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 2
=== ep: 30, time 32.11762547492981, eps 0.2015940139734384, right preds for atk and def: 54/61 = 0.8852459016393442, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 10
goal_identified
goal_identified
=== ep: 31, time 45.319854974746704, eps 0.191810928470242, right preds for atk and def: 82/89 = 0.9213483146067416, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 4
goal_identified
goal_identified
goal_identified
=== ep: 32, time 46.94071674346924, eps 0.1825049696771952, right preds for atk and def: 94/101 = 0.9306930693069307, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 7
goal_identified
=== ep: 33, time 38.684900760650635, eps 0.17365286785005798, right preds for atk and def: 76/82 = 0.926829268292683, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 34, time 36.82337141036987, eps 0.16523248812340846, right preds for atk and def: 78/82 = 0.9512195121951219, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 6
=== ep: 35, time 39.18270945549011, eps 0.15722277516195018, right preds for atk and def: 91/101 = 0.900990099009901, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 0
=== ep: 36, time 34.93988108634949, eps 0.1496037005112063, right preds for atk and def: 87/97 = 0.8969072164948454, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 1
=== ep: 37, time 39.68651056289673, eps 0.14235621251595124, right preds for atk and def: 68/71 = 0.9577464788732394, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 18
=== ep: 38, time 36.629987955093384, eps 0.13546218868114893, right preds for atk and def: 98/108 = 0.9074074074074074, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 23
goal_identified
goal_identified
=== ep: 39, time 37.113120794296265, eps 0.1289043903562757, right preds for atk and def: 71/74 = 0.9594594594594594, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 20
=== ep: 40, time 38.10489082336426, eps 0.12266641962971482, right preds for atk and def: 87/90 = 0.9666666666666667, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 19
goal_identified
=== ep: 41, time 41.11668634414673, eps 0.116732678325436, right preds for atk and def: 83/86 = 0.9651162790697675, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 27
goal_identified
=== ep: 42, time 41.405383348464966, eps 0.11108832899943073, right preds for atk and def: 92/95 = 0.968421052631579, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 21
=== ep: 43, time 34.64836287498474, eps 0.10571925783837377, right preds for atk and def: 83/89 = 0.9325842696629213, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 30
goal_identified
goal_identified
=== ep: 44, time 38.64972972869873, eps 0.10061203936773815, right preds for atk and def: 90/95 = 0.9473684210526315, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 25
goal_identified
goal_identified
=== ep: 45, time 35.1261305809021, eps 0.09575390288111604, right preds for atk and def: 95/99 = 0.9595959595959596, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 29
=== ep: 46, time 34.580066204071045, eps 0.09113270050680057, right preds for atk and def: 49/51 = 0.9607843137254902, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 22
=== ep: 47, time 37.88586974143982, eps 0.08673687683177911, right preds for atk and def: 99/103 = 0.9611650485436893, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 36
goal_identified
=== ep: 48, time 36.88447403907776, eps 0.08255544000718185, right preds for atk and def: 81/83 = 0.9759036144578314, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
=== ep: 49, time 34.58243107795715, eps 0.07857793426293408, right preds for atk and def: 94/97 = 0.9690721649484536, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 24
=== ep: 50, time 35.32359957695007, eps 0.07479441376288502, right preds for atk and def: 93/94 = 0.9893617021276596, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 38
goal_identified
=== ep: 51, time 38.477280139923096, eps 0.0711954177350367, right preds for atk and def: 98/104 = 0.9423076923076923, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 26
goal_identified
goal_identified
=== ep: 52, time 39.76829957962036, eps 0.06777194681468615, right preds for atk and def: 57/60 = 0.95, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 31
goal_identified
=== ep: 53, time 33.78960728645325, eps 0.06451544054132621, right preds for atk and def: 81/84 = 0.9642857142857143, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 33
goal_identified
goal_identified
=== ep: 54, time 37.00387716293335, eps 0.06141775595303503, right preds for atk and def: 88/93 = 0.946236559139785, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 32
goal_identified
=== ep: 55, time 38.61956310272217, eps 0.05847114722483011, right preds for atk and def: 98/99 = 0.98989898989899, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 43
goal_identified
=== ep: 56, time 37.570918798446655, eps 0.05566824630007096, right preds for atk and def: 75/77 = 0.974025974025974, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 51
goal_identified
=== ep: 57, time 31.73980450630188, eps 0.05300204446647978, right preds for atk and def: 76/77 = 0.987012987012987, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 54
goal_identified
=== ep: 58, time 36.500468730926514, eps 0.050465874830710106, right preds for atk and def: 73/76 = 0.9605263157894737, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 44
goal_identified
goal_identified
=== ep: 59, time 35.87813949584961, eps 0.04805339564764071, right preds for atk and def: 89/91 = 0.978021978021978, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 52
goal_identified
=== ep: 60, time 33.93199419975281, eps 0.045758574462709686, right preds for atk and def: 92/92 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 28
goal_identified
=== ep: 61, time 35.97263264656067, eps 0.043575673027635695, right preds for atk and def: 80/83 = 0.963855421686747, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 62, time 40.344176054000854, eps 0.04149923295180846, right preds for atk and def: 77/80 = 0.9625, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 37
=== ep: 63, time 40.01940369606018, eps 0.03952406205346913, right preds for atk and def: 75/76 = 0.9868421052631579, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 39
goal_identified
goal_identified
=== ep: 64, time 34.64826035499573, eps 0.03764522137655123, right preds for atk and def: 75/76 = 0.9868421052631579, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 45
goal_identified
=== ep: 65, time 35.28666543960571, eps 0.03585801284071809, right preds for atk and def: 67/70 = 0.9571428571428572, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 65
=== ep: 66, time 37.856308698654175, eps 0.034157967493714775, right preds for atk and def: 82/83 = 0.9879518072289156, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 58
=== ep: 67, time 35.08051681518555, eps 0.03254083433665968, right preds for atk and def: 65/65 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 46
goal_identified
goal_identified
=== ep: 68, time 37.27526807785034, eps 0.031002569694333147, right preds for atk and def: 83/84 = 0.9880952380952381, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 47
goal_identified
=== ep: 69, time 35.88331866264343, eps 0.02953932710388308, right preds for atk and def: 59/60 = 0.9833333333333333, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 62
goal_identified
=== ep: 70, time 34.57955455780029, eps 0.028147447696664333, right preds for atk and def: 75/75 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 61
goal_identified
=== ep: 71, time 37.650972843170166, eps 0.026823451049161253, right preds for atk and def: 102/103 = 0.9902912621359223, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 53
goal_identified
=== ep: 72, time 37.47365188598633, eps 0.025564026480116013, right preds for atk and def: 95/98 = 0.9693877551020408, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 41
=== ep: 73, time 41.448952436447144, eps 0.02436602477210106, right preds for atk and def: 79/80 = 0.9875, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 40
goal_identified
=== ep: 74, time 39.44866585731506, eps 0.02322645029683511, right preds for atk and def: 85/85 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 42
goal_identified
=== ep: 75, time 38.96547722816467, eps 0.02214245352455219, right preds for atk and def: 69/69 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 49
=== ep: 76, time 35.459181785583496, eps 0.02111132389869288, right preds for atk and def: 91/91 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 72
=== ep: 77, time 36.645806312561035, eps 0.020130483058101077, right preds for atk and def: 76/76 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 56
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 78, time 38.56420707702637, eps 0.019197478389778148, right preds for atk and def: 102/104 = 0.9807692307692307, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 48
goal_identified
=== ep: 79, time 37.94857978820801, eps 0.018309976896072843, right preds for atk and def: 75/76 = 0.9868421052631579, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 59
goal_identified
=== ep: 80, time 32.024330854415894, eps 0.017465759360972027, right preds for atk and def: 109/112 = 0.9732142857142857, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 80
goal_identified
goal_identified
=== ep: 81, time 37.22525954246521, eps 0.01666271480090467, right preds for atk and def: 117/118 = 0.9915254237288136, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 78
goal_identified
=== ep: 82, time 41.495627641677856, eps 0.015898835186183367, right preds for atk and def: 55/55 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 69
goal_identified
=== ep: 83, time 42.590845346450806, eps 0.015172210419884185, right preds for atk and def: 82/83 = 0.9879518072289156, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 63
=== ep: 84, time 33.41288208961487, eps 0.014481023561609456, right preds for atk and def: 87/88 = 0.9886363636363636, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 64
=== ep: 85, time 38.257420778274536, eps 0.01382354628419033, right preds for atk and def: 67/67 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 86, time 38.320631980895996, eps 0.013198134551968641, right preds for atk and def: 69/70 = 0.9857142857142858, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 86
=== ep: 87, time 34.66369152069092, eps 0.012603224509851407, right preds for atk and def: 63/64 = 0.984375, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 87
=== ep: 88, time 34.62931966781616, eps 0.012037328572858524, right preds for atk and def: 102/102 = 1.0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 57
=== ep: 89, time 36.867000341415405, eps 0.011499031706385502, right preds for atk and def: 96/96 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 73
goal_identified
=== ep: 90, time 33.64852952957153, eps 0.010986987887879832, right preds for atk and def: 97/98 = 0.9897959183673469, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 66
goal_identified
goal_identified
=== ep: 91, time 37.53851556777954, eps 0.010499916741083536, right preds for atk and def: 79/79 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 83
goal_identified
=== ep: 92, time 38.52644634246826, eps 0.010036600334425595, right preds for atk and def: 97/97 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 68
goal_identified
=== ep: 93, time 42.70681071281433, eps 0.00959588013555861, right preds for atk and def: 79/79 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 84
goal_identified
=== ep: 94, time 37.70817732810974, eps 0.009176654114424539, right preds for atk and def: 77/77 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 50
goal_identified
=== ep: 95, time 38.00749731063843, eps 0.00877787398760545, right preds for atk and def: 107/107 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 90
goal_identified
=== ep: 96, time 35.41089987754822, eps 0.008398542597069007, right preds for atk and def: 117/117 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 55
goal_identified
=== ep: 97, time 34.16318154335022, eps 0.008037711416753971, right preds for atk and def: 76/76 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 71
=== ep: 98, time 36.51835560798645, eps 0.00769447818076098, right preds for atk and def: 77/77 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 81
=== ep: 99, time 36.17038345336914, eps 0.007367984627217855, right preds for atk and def: 83/85 = 0.9764705882352941, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 99
=== ep: 100, time 31.167033433914185, eps 0.007057414352177835, right preds for atk and def: 79/79 = 1.0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 60
=== ep: 101, time 36.7092342376709, eps 0.006761990768184489, right preds for atk and def: 73/74 = 0.9864864864864865, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 101
=== ep: 102, time 41.90145754814148, eps 0.006480975162398559, right preds for atk and def: 93/93 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 67
goal_identified
goal_identified
=== ep: 103, time 41.73935079574585, eps 0.006213664849431085, right preds for atk and def: 85/85 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 70
=== ep: 104, time 32.16259002685547, eps 0.005959391414263934, right preds for atk and def: 90/90 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 74
goal_identified
=== ep: 105, time 36.17234492301941, eps 0.005717519040864065, right preds for atk and def: 73/73 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 75
goal_identified
goal_identified
=== ep: 106, time 36.62470006942749, eps 0.005487442922312285, right preds for atk and def: 76/76 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 76
=== ep: 107, time 34.663074016571045, eps 0.005268587748470919, right preds for atk and def: 89/89 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 77
=== ep: 108, time 36.68926382064819, eps 0.005060406267408787, right preds for atk and def: 77/77 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 82
goal_identified
goal_identified
=== ep: 109, time 34.64321303367615, eps 0.004862377916986354, right preds for atk and def: 82/82 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 85
=== ep: 110, time 36.710301876068115, eps 0.004674007523179196, right preds for atk and def: 91/91 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 88
goal_identified
goal_identified
=== ep: 111, time 40.22916030883789, eps 0.004494824061885041, right preds for atk and def: 91/91 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 89
goal_identified
goal_identified
goal_identified
=== ep: 112, time 43.494863986968994, eps 0.0043243794811181555, right preds for atk and def: 80/80 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 91
=== ep: 113, time 38.68294620513916, eps 0.0041622475806460035, right preds for atk and def: 74/74 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 92
goal_identified
=== ep: 114, time 35.783941984176636, eps 0.0040080229462666735, right preds for atk and def: 96/98 = 0.9795918367346939, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 114
goal_identified
goal_identified
goal_identified
=== ep: 115, time 39.990795373916626, eps 0.0038613199360621906, right preds for atk and def: 81/83 = 0.9759036144578314, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 115
goal_identified
=== ep: 116, time 39.73250389099121, eps 0.003721771716092858, right preds for atk and def: 61/61 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 93
goal_identified
=== ep: 117, time 31.373518705368042, eps 0.0035890293431213305, right preds for atk and def: 87/87 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 94
goal_identified
=== ep: 118, time 36.187183141708374, eps 0.0034627608920727634, right preds for atk and def: 91/91 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 95
goal_identified
goal_identified
goal_identified
=== ep: 119, time 36.658745765686035, eps 0.00334265062604924, right preds for atk and def: 83/83 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 6 layers and 500 hidden units.
=== ep: 0, time 28.089083194732666, eps 0.9, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 1, time 28.44408631324768, eps 0.8561552526261419, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 2, time 28.145138263702393, eps 0.8144488388143276, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
=== ep: 3, time 28.149345874786377, eps 0.774776470806127, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
=== ep: 4, time 28.014003038406372, eps 0.7370389470171057, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 5, time 28.216513633728027, eps 0.701141903981193, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
=== ep: 6, time 28.371545553207397, eps 0.6669955803928644, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 7, time 28.442800521850586, eps 0.6345145926571234, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
=== ep: 8, time 28.420073986053467, eps 0.6036177213860398, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 9, time 31.456373691558838, eps 0.5742277083079742, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
goal_identified
=== ep: 10, time 28.140961408615112, eps 0.5462710630816575, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
goal_identified
=== ep: 11, time 28.455804347991943, eps 0.5196778795320575, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
=== ep: 12, time 27.872395753860474, eps 0.49438166084852986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
=== ep: 13, time 27.977967977523804, eps 0.47031915330815344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
=== ep: 14, time 27.83609127998352, eps 0.4474301881084772, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
=== ep: 15, time 28.02914834022522, eps 0.42565753091417224, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
=== ep: 16, time 28.525455474853516, eps 0.4049467387413822, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 17, time 29.159032106399536, eps 0.3852460238219053, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 18, time 28.37492060661316, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
=== ep: 19, time 34.06891059875488, eps 0.3486801800855966, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 20, time 28.317952156066895, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
goal_identified
=== ep: 21, time 28.484509468078613, eps 0.31559403645092865, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
goal_identified
goal_identified
=== ep: 22, time 28.66830348968506, eps 0.3002511042445735, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
goal_identified
=== ep: 23, time 28.133805990219116, eps 0.2856564556717689, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
=== ep: 24, time 28.168349742889404, eps 0.27177359650906974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
=== ep: 25, time 28.46367573738098, eps 0.2585678123773109, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
goal_identified
goal_identified
=== ep: 26, time 28.74083375930786, eps 0.24600608193757734, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
=== ep: 27, time 28.24273943901062, eps 0.23405699432065646, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 28, time 28.465466260910034, eps 0.22269067058350425, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
goal_identified
=== ep: 29, time 36.55951428413391, eps 0.2118786889963241, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 30, time 28.82516646385193, eps 0.2015940139734384, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
=== ep: 31, time 28.256913423538208, eps 0.191810928470242, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
=== ep: 32, time 28.03626775741577, eps 0.1825049696771952, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
=== ep: 33, time 28.080129861831665, eps 0.17365286785005798, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 34, time 28.13879895210266, eps 0.16523248812340846, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
=== ep: 35, time 28.556154012680054, eps 0.15722277516195018, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
=== ep: 36, time 28.406876802444458, eps 0.1496037005112063, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 37, time 28.289865732192993, eps 0.14235621251595124, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
goal_identified
=== ep: 38, time 28.431232452392578, eps 0.13546218868114893, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
goal_identified
=== ep: 39, time 33.91656684875488, eps 0.1289043903562757, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
=== ep: 40, time 28.663362503051758, eps 0.12266641962971482, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 41, time 28.694355487823486, eps 0.116732678325436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
=== ep: 42, time 28.68929123878479, eps 0.11108832899943073, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 43, time 28.790213584899902, eps 0.10571925783837377, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
=== ep: 44, time 28.293760538101196, eps 0.10061203936773815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 45, time 28.992764234542847, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
goal_identified
goal_identified
=== ep: 46, time 28.63795804977417, eps 0.09113270050680057, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 47, time 28.381434679031372, eps 0.08673687683177911, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 48, time 28.601998329162598, eps 0.08255544000718185, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
goal_identified
=== ep: 49, time 35.937705755233765, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 50, time 28.778629541397095, eps 0.07479441376288502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
=== ep: 51, time 28.703975439071655, eps 0.0711954177350367, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
=== ep: 52, time 28.404399633407593, eps 0.06777194681468615, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
=== ep: 53, time 28.565191745758057, eps 0.06451544054132621, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
goal_identified
=== ep: 54, time 28.13304305076599, eps 0.06141775595303503, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
=== ep: 55, time 28.901081800460815, eps 0.05847114722483011, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 28.88951539993286, eps 0.05566824630007096, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 28.32734203338623, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 28.609190225601196, eps 0.050465874830710106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
=== ep: 59, time 37.73752498626709, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
goal_identified
=== ep: 60, time 28.584410667419434, eps 0.045758574462709686, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
goal_identified
=== ep: 61, time 28.947117805480957, eps 0.043575673027635695, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
goal_identified
=== ep: 62, time 28.776346683502197, eps 0.04149923295180846, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
=== ep: 63, time 28.652226448059082, eps 0.03952406205346913, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
=== ep: 64, time 28.79839849472046, eps 0.03764522137655123, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 28.66706919670105, eps 0.03585801284071809, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 66, time 29.023330450057983, eps 0.034157967493714775, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
goal_identified
=== ep: 67, time 29.077170610427856, eps 0.03254083433665968, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 68, time 29.287108421325684, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
=== ep: 69, time 40.496782064437866, eps 0.02953932710388308, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
goal_identified
goal_identified
goal_identified
=== ep: 70, time 28.586491584777832, eps 0.028147447696664333, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 71, time 28.703913688659668, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 29.36377501487732, eps 0.025564026480116013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 73, time 28.896922826766968, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
=== ep: 74, time 28.657580852508545, eps 0.02322645029683511, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
goal_identified
=== ep: 75, time 29.211299896240234, eps 0.02214245352455219, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 28.534159660339355, eps 0.02111132389869288, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 28.83781337738037, eps 0.020130483058101077, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 29.102387189865112, eps 0.019197478389778148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
=== ep: 79, time 42.881767988204956, eps 0.018309976896072843, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
=== ep: 80, time 28.755091190338135, eps 0.017465759360972027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
=== ep: 81, time 28.52215027809143, eps 0.01666271480090467, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 82, time 29.219919204711914, eps 0.015898835186183367, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
goal_identified
=== ep: 83, time 29.016730785369873, eps 0.015172210419884185, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
=== ep: 84, time 29.013220071792603, eps 0.014481023561609456, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 85, time 28.45709800720215, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 86, time 28.40065050125122, eps 0.013198134551968641, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 87, time 28.90269660949707, eps 0.012603224509851407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
=== ep: 88, time 28.89185857772827, eps 0.012037328572858524, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
=== ep: 89, time 41.786274671554565, eps 0.011499031706385502, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
=== ep: 90, time 28.784031629562378, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
=== ep: 91, time 29.71134877204895, eps 0.010499916741083536, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
=== ep: 92, time 28.72808861732483, eps 0.010036600334425595, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
=== ep: 93, time 28.979889631271362, eps 0.00959588013555861, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
=== ep: 94, time 28.904518127441406, eps 0.009176654114424539, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
goal_identified
=== ep: 95, time 29.017589330673218, eps 0.00877787398760545, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 29.323121309280396, eps 0.008398542597069007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
goal_identified
=== ep: 97, time 28.901429653167725, eps 0.008037711416753971, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
=== ep: 98, time 28.855683088302612, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
goal_identified
=== ep: 99, time 38.38296031951904, eps 0.007367984627217855, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 29.094685554504395, eps 0.007057414352177835, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 101, time 28.79267907142639, eps 0.006761990768184489, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
=== ep: 102, time 29.759576320648193, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 103, time 29.31879448890686, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
=== ep: 104, time 28.716240644454956, eps 0.005959391414263934, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
=== ep: 105, time 29.266801357269287, eps 0.005717519040864065, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
=== ep: 106, time 29.341792345046997, eps 0.005487442922312285, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
goal_identified
goal_identified
=== ep: 107, time 29.144785165786743, eps 0.005268587748470919, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
goal_identified
=== ep: 108, time 28.4513156414032, eps 0.005060406267408787, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
=== ep: 109, time 42.498287200927734, eps 0.004862377916986354, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 110, time 29.021483182907104, eps 0.004674007523179196, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
goal_identified
=== ep: 111, time 29.39113688468933, eps 0.004494824061885041, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
goal_identified
=== ep: 112, time 29.083781719207764, eps 0.0043243794811181555, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 28.49372696876526, eps 0.0041622475806460035, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 29.273358821868896, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 29.844720363616943, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
=== ep: 116, time 29.498716115951538, eps 0.003721771716092858, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 29.06982398033142, eps 0.0035890293431213305, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
=== ep: 118, time 29.049152851104736, eps 0.0034627608920727634, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
goal_identified
=== ep: 119, time 40.49691200256348, eps 0.00334265062604924, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 29.59695553779602, eps 0.0032283982068230565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 28.99290943145752, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
=== ep: 122, time 28.970752954483032, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 123, time 28.834545612335205, eps 0.0029180001112638996, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 28.81426215171814, eps 0.002824458142029865, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
=== ep: 125, time 29.042508125305176, eps 0.0027354782684687108, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
=== ep: 126, time 29.05515694618225, eps 0.0026508379945489875, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
=== ep: 127, time 29.623027563095093, eps 0.0025703256754987464, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
=== ep: 128, time 29.09619951248169, eps 0.0024937399885833667, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
goal_identified
=== ep: 129, time 39.43789339065552, eps 0.0024208894296938593, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
=== ep: 130, time 29.052531719207764, eps 0.0023515918344868374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
=== ep: 131, time 28.81079363822937, eps 0.002285673922878779, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 132, time 29.142279863357544, eps 0.0022229708657555565, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 29.64473867416382, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 29.393964529037476, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 28.935956716537476, eps 0.002052620779091266, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
goal_identified
=== ep: 136, time 29.131629705429077, eps 0.0020012838579124784, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
=== ep: 137, time 29.01068925857544, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
=== ep: 138, time 29.530466079711914, eps 0.001905999100714611, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 41.6628839969635, eps 0.001861813003170924, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
goal_identified
=== ep: 140, time 29.912830114364624, eps 0.0018197818870335101, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
=== ep: 141, time 27.59613871574402, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 27.788033962249756, eps 0.0017417693260160481, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 27.818408250808716, eps 0.0017055928090985275, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 27.958658933639526, eps 0.0016711806417306348, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 27.74027395248413, eps 0.0016384467755694515, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 27.381115674972534, eps 0.0016073093588992661, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
goal_identified
=== ep: 147, time 27.52150845527649, eps 0.0015776905319596466, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
=== ep: 148, time 27.709112644195557, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 40.22585725784302, eps 0.0015227160093621863, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
goal_identified
goal_identified
=== ep: 150, time 27.501358032226562, eps 0.0014972228487629025, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
=== ep: 151, time 28.064339637756348, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 28.065953016281128, eps 0.001449905838663109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
=== ep: 153, time 27.86623239517212, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
=== ep: 154, time 27.855719327926636, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
=== ep: 155, time 27.798925638198853, eps 0.001387237543977543, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 27.5973699092865, eps 0.0013683517461028282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 157, time 27.602596044540405, eps 0.0013503870194592265, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 27.64639139175415, eps 0.0013332984428727204, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
goal_identified
=== ep: 159, time 37.64477324485779, eps 0.001317043286000802, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 160, time 27.50164556503296, eps 0.0013015809024843582, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 27.926259994506836, eps 0.0012868726283106018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 27.50054693222046, eps 0.0012728816851329014, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
goal_identified
=== ep: 163, time 27.196927309036255, eps 0.0012595730883057546, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
=== ep: 164, time 27.843128442764282, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
=== ep: 165, time 28.02841019630432, eps 0.0012348714430141991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
goal_identified
=== ep: 166, time 28.36142349243164, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 167, time 28.2105495929718, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
goal_identified
=== ep: 168, time 27.50029706954956, eps 0.0012021557244367845, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
=== ep: 169, time 39.63194465637207, eps 0.0011922964734155277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
goal_identified
=== ep: 170, time 27.422820568084717, eps 0.001182918063740569, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
goal_identified
=== ep: 171, time 27.15639638900757, eps 0.0011739970445027263, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
=== ep: 172, time 27.9033043384552, eps 0.0011655111085071537, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 27.6826593875885, eps 0.001157439036493735, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
goal_identified
=== ep: 174, time 27.481788396835327, eps 0.0011497606440778825, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
goal_identified
goal_identified
goal_identified
=== ep: 175, time 27.840506553649902, eps 0.0011424567312790603, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
goal_identified
=== ep: 176, time 27.34512209892273, eps 0.0011355090345108335, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
goal_identified
goal_identified
=== ep: 177, time 28.23544716835022, eps 0.0011289001809123877, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 178, time 27.847612142562866, eps 0.0011226136449073282, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
goal_identified
=== ep: 179, time 40.835511445999146, eps 0.001116633706881133, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 180, time 28.030277729034424, eps 0.001110945413873925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 27.58232045173645, eps 0.001105534542190287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
goal_identified
=== ep: 182, time 27.587789297103882, eps 0.0011003875618326132, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
goal_identified
=== ep: 183, time 27.794116973876953, eps 0.0010954916026690664, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
goal_identified
goal_identified
=== ep: 184, time 27.608093976974487, eps 0.001090834422251547, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
goal_identified
goal_identified
=== ep: 185, time 27.593838453292847, eps 0.0010864043752031938, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 186, time 28.001835346221924, eps 0.0010821903840988777, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 187, time 27.564926385879517, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
goal_identified
goal_identified
=== ep: 188, time 27.835745573043823, eps 0.0010743689349354123, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
goal_identified
=== ep: 189, time 39.048927307128906, eps 0.0010707419191793434, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
=== ep: 190, time 28.272961616516113, eps 0.0010672917950690429, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
goal_identified
=== ep: 191, time 28.011967658996582, eps 0.0010640099354971456, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 28.011614561080933, eps 0.0010608881341052777, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
=== ep: 193, time 28.318243741989136, eps 0.0010579185847638855, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 194, time 27.667870044708252, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
=== ep: 195, time 28.37014651298523, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 28.25005841255188, eps 0.001049850987889527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 27.748164176940918, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 28.029362440109253, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
=== ep: 199, time 39.35592436790466, eps 0.001042907142909185, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 28.075207471847534, eps 0.001040814536856474, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 28.33180594444275, eps 0.0010388239884052469, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
goal_identified
=== ep: 202, time 27.899973154067993, eps 0.0010369305201475454, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
goal_identified
goal_identified
=== ep: 203, time 29.05063796043396, eps 0.0010351293974264616, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 204, time 28.55734419822693, eps 0.00103341611649703, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 28.93996572494507, eps 0.0010317863932645186, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 27.84878945350647, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 28.12869930267334, eps 0.0010287615180101426, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 28.04156231880188, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
=== ep: 209, time 50.47147488594055, eps 0.0010260244976950921, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 210, time 28.09386897087097, eps 0.0010247552679654227, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
=== ep: 211, time 28.972182989120483, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 212, time 28.004145622253418, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
goal_identified
=== ep: 213, time 27.736469745635986, eps 0.001021307056596379, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 214, time 28.486178874969482, eps 0.0010202678991839778, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
=== ep: 215, time 28.917646646499634, eps 0.0010192794220766138, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
goal_identified
=== ep: 216, time 28.270249843597412, eps 0.0010183391535666436, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
=== ep: 217, time 28.91872239112854, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 218, time 28.73449397087097, eps 0.0010165939523622068, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
goal_identified
=== ep: 219, time 47.182008266448975, eps 0.0010157846557556941, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 220, time 28.375473260879517, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
goal_identified
=== ep: 221, time 28.99396586418152, eps 0.0010142825471585687, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
goal_identified
=== ep: 222, time 28.162416219711304, eps 0.0010135859791140496, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
goal_identified
=== ep: 223, time 28.63885998725891, eps 0.0010129233830939361, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
goal_identified
=== ep: 224, time 28.76617407798767, eps 0.0010122931022630473, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
=== ep: 225, time 27.984341144561768, eps 0.001011693560591007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 27.459952116012573, eps 0.0010111232589113477, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
=== ep: 227, time 28.185391902923584, eps 0.0010105807711728136, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
=== ep: 228, time 28.376474857330322, eps 0.0010100647408734893, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 56.73367953300476, eps 0.001009573877668838, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
=== ep: 230, time 27.958240509033203, eps 0.001009106954145169, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 28.20620894432068, eps 0.0010086628027504636, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 28.418701887130737, eps 0.0010082403128748867, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 28.132250547409058, eps 0.0010078384280736842, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
goal_identified
=== ep: 234, time 28.600157260894775, eps 0.001007456143425521, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 235, time 28.80909013748169, eps 0.001007092503019653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
goal_identified
goal_identified
=== ep: 236, time 28.13962697982788, eps 0.001006746597565654, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 237, time 28.936272859573364, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
=== ep: 238, time 28.5053608417511, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 56.87038469314575, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
goal_identified
=== ep: 240, time 28.70885682106018, eps 0.001005523646905642, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
goal_identified
=== ep: 241, time 28.019997596740723, eps 0.001005254255467199, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 28.621642112731934, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 28.74739718437195, eps 0.0010047542469506416, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
=== ep: 244, time 27.977497100830078, eps 0.0010045223795907931, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
=== ep: 245, time 28.26156497001648, eps 0.001004301820535524, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 28.120008945465088, eps 0.0010040920182723119, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
goal_identified
=== ep: 247, time 28.154378414154053, eps 0.0010038924481862177, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
goal_identified
=== ep: 248, time 28.259002447128296, eps 0.0010037026112480747, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
=== ep: 249, time 48.963449478149414, eps 0.0010035220327666559, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
=== ep: 250, time 27.781229734420776, eps 0.0010033502612016988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
goal_identified
goal_identified
=== ep: 251, time 27.57935357093811, eps 0.001003186867034819, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 252, time 28.097450256347656, eps 0.001003031441695491, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
=== ep: 253, time 27.792734622955322, eps 0.0010028835965394094, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
=== ep: 254, time 27.9818115234375, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
goal_identified
goal_identified
=== ep: 255, time 28.97611665725708, eps 0.0010026091860473767, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
goal_identified
=== ep: 256, time 28.390917539596558, eps 0.0010024819345422614, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
goal_identified
=== ep: 257, time 28.505682468414307, eps 0.0010023608891662839, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
goal_identified
=== ep: 258, time 27.91374707221985, eps 0.001002245747242954, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 45.22370457649231, eps 0.0010021362208574892, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 28.098201751708984, eps 0.001002032036136876, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
=== ep: 261, time 28.13499665260315, eps 0.0010019329325650452, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 28.021052360534668, eps 0.0010018386623314465, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 27.904125690460205, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
goal_identified
goal_identified
=== ep: 264, time 28.253167867660522, eps 0.0010016636904766263, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
goal_identified
=== ep: 265, time 28.08960199356079, eps 0.0010015825513346283, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
goal_identified
goal_identified
=== ep: 266, time 27.948589086532593, eps 0.0010015053693952815, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 28.41083335876465, eps 0.0010014319516635345, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 268, time 28.23147940635681, eps 0.0010013621145568167, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 53.7982497215271, eps 0.0010012956834459848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 28.410803079605103, eps 0.0010012324922186594, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
=== ep: 271, time 28.056148052215576, eps 0.001001172382863857, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 29.038466453552246, eps 0.0010011152050768812, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 28.645360469818115, eps 0.0010010608158834819, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
goal_identified
=== ep: 274, time 28.494764804840088, eps 0.0010010090792823456, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
=== ep: 275, time 28.980297565460205, eps 0.0010009598659050213, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 276, time 28.27147674560547, eps 0.0010009130526924313, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 28.173633813858032, eps 0.0010008685225871602, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 28.74731731414795, eps 0.0010008261642407504, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
goal_identified
goal_identified
=== ep: 279, time 47.561744928359985, eps 0.001000785871735272, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
goal_identified
=== ep: 280, time 27.612592220306396, eps 0.0010007475443184742, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
goal_identified
=== ep: 281, time 28.47546672821045, eps 0.001000711086151851, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
goal_identified
goal_identified
=== ep: 282, time 28.650253295898438, eps 0.0010006764060709957, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
goal_identified
=== ep: 283, time 27.742095232009888, eps 0.001000643417357642, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 284, time 27.695234775543213, eps 0.0010006120375228235, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
=== ep: 285, time 27.375027894973755, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
goal_identified
goal_identified
=== ep: 286, time 27.518516302108765, eps 0.0010005537944518927, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
goal_identified
=== ep: 287, time 27.915441751480103, eps 0.0010005267855777657, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 27.28968834877014, eps 0.0010005010939419733, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
goal_identified
goal_identified
=== ep: 289, time 52.79319429397583, eps 0.001000476655302044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 290, time 28.025068044662476, eps 0.0010004534085486486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 27.97092294692993, eps 0.0010004312955527947, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 28.59470510482788, eps 0.0010004102610204745, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
=== ep: 293, time 27.377493143081665, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
=== ep: 294, time 27.84458899497986, eps 0.0010003712195224871, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
goal_identified
=== ep: 295, time 29.224538803100586, eps 0.0010003531149327387, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 296, time 28.359158515930176, eps 0.0010003358933142518, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
goal_identified
=== ep: 297, time 28.34744358062744, eps 0.0010003195116040093, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 28.244499921798706, eps 0.0010003039288392032, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 47.819988489151, eps 0.0010002891060548044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
=== ep: 300, time 27.92034363746643, eps 0.0010002750061861312, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
goal_identified
=== ep: 301, time 27.674248695373535, eps 0.0010002615939761676, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
=== ep: 302, time 27.750877141952515, eps 0.001000248835887403, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
goal_identified
goal_identified
=== ep: 303, time 27.906264066696167, eps 0.0010002367000179694, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
=== ep: 304, time 27.09311008453369, eps 0.0010002251560218723, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
=== ep: 305, time 27.565724849700928, eps 0.0010002141750331084, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
goal_identified
=== ep: 306, time 28.80733585357666, eps 0.0010002037295934862, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
=== ep: 307, time 28.150664567947388, eps 0.0010001937935839656, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
goal_identified
=== ep: 308, time 27.787043571472168, eps 0.0010001843421593476, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
goal_identified
=== ep: 309, time 47.54065155982971, eps 0.0010001753516861473, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
goal_identified
=== ep: 310, time 28.885743379592896, eps 0.0010001667996834991, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 28.271381616592407, eps 0.001000158664766942, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 312, time 28.08932065963745, eps 0.0010001509265949466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
=== ep: 313, time 28.135221242904663, eps 0.001000143565818053, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
=== ep: 314, time 27.863740921020508, eps 0.0010001365640304844, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 27.6915922164917, eps 0.0010001299037241253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
goal_identified
=== ep: 316, time 27.93653130531311, eps 0.0010001235682447402, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
=== ep: 317, time 28.162188053131104, eps 0.0010001175417503308, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
=== ep: 318, time 27.942328691482544, eps 0.0010001118091715218, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 49.437941551208496, eps 0.0010001063561738807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
goal_identified
=== ep: 320, time 27.23768162727356, eps 0.0010001011691220727, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
=== ep: 321, time 27.30240488052368, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
goal_identified
=== ep: 322, time 27.56331706047058, eps 0.0010000915416072012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
goal_identified
=== ep: 323, time 27.20662546157837, eps 0.0010000870770703358, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 27.98671293258667, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
goal_identified
=== ep: 325, time 27.68069362640381, eps 0.0010000787905914928, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 27.758742332458496, eps 0.0010000749479290019, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
=== ep: 327, time 27.269885301589966, eps 0.001000071292675372, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
goal_identified
=== ep: 328, time 27.416931629180908, eps 0.001000067815690565, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
=== ep: 329, time 46.21638631820679, eps 0.0010000645082803084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
goal_identified
goal_identified
=== ep: 330, time 27.38632869720459, eps 0.0010000613621743532, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
=== ep: 331, time 27.35315990447998, eps 0.0010000583695057963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
goal_identified
=== ep: 332, time 27.875041723251343, eps 0.0010000555227914069, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
goal_identified
=== ep: 333, time 27.59217643737793, eps 0.0010000528149129166, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 27.515815019607544, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 335, time 27.532816648483276, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
goal_identified
goal_identified
=== ep: 336, time 28.408430814743042, eps 0.0010000454582168217, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
=== ep: 337, time 27.733508586883545, eps 0.001000043241193426, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 27.35305905342102, eps 0.0010000411322955373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 339, time 45.38861060142517, eps 0.0010000391262498123, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
=== ep: 340, time 28.27008557319641, eps 0.001000037218040092, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 27.716064929962158, eps 0.0010000354028948577, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
goal_identified
=== ep: 342, time 28.3762526512146, eps 0.0010000336762753012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
goal_identified
goal_identified
=== ep: 343, time 27.69485831260681, eps 0.001000032033863974, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 344, time 28.427273511886597, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
=== ep: 345, time 27.71450972557068, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 27.74303913116455, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 27.985036849975586, eps 0.0010000262271095755, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
goal_identified
=== ep: 348, time 27.783572912216187, eps 0.0010000249479983478, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
goal_identified
=== ep: 349, time 53.09425115585327, eps 0.0010000237312701107, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 28.974749088287354, eps 0.00100002257388241, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 29.109138250350952, eps 0.0010000214729411737, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
goal_identified
=== ep: 352, time 28.164673805236816, eps 0.0010000204256934752, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
=== ep: 353, time 28.63088083267212, eps 0.0010000194295206493, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
=== ep: 354, time 28.110365867614746, eps 0.0010000184819317455, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
=== ep: 355, time 27.883549690246582, eps 0.001000017580557298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
goal_identified
=== ep: 356, time 28.108912706375122, eps 0.001000016723143401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
=== ep: 357, time 27.941038846969604, eps 0.0010000159075460732, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 27.989269495010376, eps 0.0010000151317258964, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 48.405925273895264, eps 0.0010000143937429161, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
=== ep: 360, time 27.617141246795654, eps 0.0010000136917517905, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
=== ep: 361, time 27.559514045715332, eps 0.001000013023997176, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 362, time 28.33845281600952, eps 0.0010000123888093385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 27.885117292404175, eps 0.0010000117845999773, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 28.52962636947632, eps 0.0010000112098582543, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 27.78530263900757, eps 0.001000010663147016, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 28.126300573349, eps 0.0010000101430991996, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
=== ep: 367, time 27.459182024002075, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 27.304036617279053, eps 0.0010000091778556905, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
goal_identified
=== ep: 369, time 46.099547147750854, eps 0.0010000087302463867, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
goal_identified
=== ep: 370, time 28.22315216064453, eps 0.001000008304467246, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 371, time 27.515308141708374, eps 0.0010000078994535993, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 27.77783727645874, eps 0.0010000075141927012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
goal_identified
=== ep: 373, time 27.65405011177063, eps 0.0010000071477211988, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
goal_identified
=== ep: 374, time 27.625529289245605, eps 0.0010000067991227223, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
goal_identified
=== ep: 375, time 28.225659608840942, eps 0.0010000064675255943, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
goal_identified
=== ep: 376, time 27.75841188430786, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 27.873080492019653, eps 0.0010000058520591598, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 27.988874197006226, eps 0.0010000055666508666, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 379, time 46.956873178482056, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
=== ep: 380, time 27.407075881958008, eps 0.0010000050369139975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
=== ep: 381, time 28.22512984275818, eps 0.001000004791260803, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
=== ep: 382, time 28.438624620437622, eps 0.0010000045575882562, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
=== ep: 383, time 27.84616732597351, eps 0.001000004335312054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
goal_identified
=== ep: 384, time 29.006166696548462, eps 0.0010000041238763903, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 28.406104803085327, eps 0.0010000039227525655, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 27.781086206436157, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
=== ep: 387, time 27.997389554977417, eps 0.001000003549453303, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
=== ep: 388, time 28.25926446914673, eps 0.0010000033763444226, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 47.57036757469177, eps 0.001000003211678162, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
goal_identified
=== ep: 390, time 29.043216228485107, eps 0.0010000030550427698, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
=== ep: 391, time 28.822274208068848, eps 0.0010000029060465757, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
goal_identified
=== ep: 392, time 28.32952642440796, eps 0.0010000027643170119, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
goal_identified
=== ep: 393, time 28.14501690864563, eps 0.0010000026294996803, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
=== ep: 394, time 27.638936042785645, eps 0.0010000025012574677, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
=== ep: 395, time 27.673537969589233, eps 0.0010000023792697014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
=== ep: 396, time 28.812851905822754, eps 0.0010000022632313489, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 28.401617288589478, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 27.86618399620056, eps 0.00100000204785641, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 44.518699169158936, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 400, time 27.747594833374023, eps 0.0010000018529771066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
=== ep: 401, time 27.612884044647217, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
goal_identified
=== ep: 402, time 28.272174835205078, eps 0.0010000016766430208, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
goal_identified
=== ep: 403, time 28.03381037712097, eps 0.0010000015948721758, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 27.919792652130127, eps 0.001000001517089342, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
=== ep: 405, time 27.937514305114746, eps 0.0010000014431000217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 27.614019870758057, eps 0.001000001372719203, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
=== ep: 407, time 27.559255599975586, eps 0.0010000013057708975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 27.936405658721924, eps 0.0010000012420876994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
=== ep: 409, time 46.69794940948486, eps 0.0010000011815103674, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
goal_identified
goal_identified
goal_identified
=== ep: 410, time 28.03593420982361, eps 0.001000001123887427, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
=== ep: 411, time 27.96130871772766, eps 0.0010000010690747903, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 27.988662004470825, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
goal_identified
=== ep: 413, time 28.379321575164795, eps 0.0010000009673388729, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
=== ep: 414, time 28.13665246963501, eps 0.0010000009201611994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
=== ep: 415, time 28.043092250823975, eps 0.0010000008752844081, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
goal_identified
=== ep: 416, time 27.935054540634155, eps 0.0010000008325962838, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
=== ep: 417, time 27.98860216140747, eps 0.001000000791990084, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
=== ep: 418, time 28.287256479263306, eps 0.0010000007533642718, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
=== ep: 419, time 50.015925884246826, eps 0.0010000007166222626, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 28.342376708984375, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 27.952266216278076, eps 0.001000000648426638, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 28.137873888015747, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 28.329957008361816, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 424, time 27.841464519500732, eps 0.0010000005581059794, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
goal_identified
=== ep: 425, time 27.74369716644287, eps 0.0010000005308868295, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
=== ep: 426, time 28.405821561813354, eps 0.0010000005049951733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 28.33065104484558, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 27.70339798927307, eps 0.0010000004569385287, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 50.34753656387329, eps 0.0010000004346533736, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
=== ep: 430, time 27.981013298034668, eps 0.0010000004134550786, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 27.77446484565735, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
goal_identified
=== ep: 432, time 27.99472403526306, eps 0.0010000003741096257, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
goal_identified
=== ep: 433, time 28.6341290473938, eps 0.001000000355864084, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 27.925352811813354, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
=== ep: 435, time 28.53113627433777, eps 0.001000000321999139, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
=== ep: 436, time 27.617916345596313, eps 0.0010000003062950555, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
goal_identified
=== ep: 437, time 27.790208339691162, eps 0.0010000002913568694, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 28.16165280342102, eps 0.0010000002771472273, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
goal_identified
=== ep: 439, time 48.3441116809845, eps 0.0010000002636305976, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
=== ep: 440, time 27.708305597305298, eps 0.0010000002507731815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
goal_identified
goal_identified
=== ep: 441, time 27.83029794692993, eps 0.0010000002385428292, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
=== ep: 442, time 27.73390507698059, eps 0.0010000002269089582, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
goal_identified
goal_identified
=== ep: 443, time 28.028753757476807, eps 0.0010000002158424776, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
goal_identified
goal_identified
=== ep: 444, time 28.207287788391113, eps 0.0010000002053157158, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
goal_identified
=== ep: 445, time 27.886908292770386, eps 0.0010000001953023503, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
goal_identified
goal_identified
goal_identified
=== ep: 446, time 27.82386541366577, eps 0.001000000185777342, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 447, time 27.553605318069458, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
=== ep: 448, time 27.83072566986084, eps 0.0010000001680982905, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
=== ep: 449, time 43.07663011550903, eps 0.0010000001599000403, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 28.78454613685608, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
=== ep: 451, time 28.00952649116516, eps 0.0010000001446835395, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 28.04180598258972, eps 0.0010000001376272401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 27.71197247505188, eps 0.0010000001309150804, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
goal_identified
=== ep: 454, time 28.39155912399292, eps 0.0010000001245302765, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
=== ep: 455, time 27.89494276046753, eps 0.0010000001184568633, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
=== ep: 456, time 28.091936349868774, eps 0.0010000001126796538, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
goal_identified
=== ep: 457, time 28.287126302719116, eps 0.0010000001071842023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 28.4707350730896, eps 0.001000000101956767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 47.398247480392456, eps 0.001000000096984277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
goal_identified
=== ep: 460, time 28.089829921722412, eps 0.001000000092254298, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 461, time 28.810279607772827, eps 0.0010000000877550027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 29.220009326934814, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 27.95017647743225, eps 0.00100000007940401, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
goal_identified
goal_identified
=== ep: 464, time 28.485939025878906, eps 0.0010000000755314307, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 465, time 28.61851739883423, eps 0.0010000000718477194, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 28.361695766448975, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 28.527925729751587, eps 0.001000000065010505, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
goal_identified
goal_identified
=== ep: 468, time 29.088040828704834, eps 0.0010000000618399052, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
=== ep: 469, time 47.80363440513611, eps 0.0010000000588239375, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
goal_identified
=== ep: 470, time 28.1072518825531, eps 0.0010000000559550603, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
goal_identified
=== ep: 471, time 29.327333688735962, eps 0.0010000000532260998, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
goal_identified
=== ep: 472, time 28.607253313064575, eps 0.0010000000506302322, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
goal_identified
goal_identified
=== ep: 473, time 28.650863885879517, eps 0.0010000000481609666, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
=== ep: 474, time 27.95435643196106, eps 0.0010000000458121286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 28.42108130455017, eps 0.0010000000435778447, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
goal_identified
goal_identified
=== ep: 476, time 27.613369941711426, eps 0.001000000041452528, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 477, time 28.06866765022278, eps 0.0010000000394308644, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
goal_identified
=== ep: 478, time 28.76414442062378, eps 0.0010000000375077985, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
=== ep: 479, time 45.87981176376343, eps 0.0010000000356785216, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 27.92917227745056, eps 0.0010000000339384595, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 28.221720695495605, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
goal_identified
=== ep: 482, time 27.79764151573181, eps 0.0010000000307087882, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
goal_identified
goal_identified
=== ep: 483, time 27.981377124786377, eps 0.001000000029211103, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 27.947934865951538, eps 0.0010000000277864607, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
goal_identified
goal_identified
=== ep: 485, time 28.741889238357544, eps 0.0010000000264312988, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
goal_identified
=== ep: 486, time 27.871288776397705, eps 0.0010000000251422292, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
goal_identified
=== ep: 487, time 28.09607458114624, eps 0.0010000000239160282, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 488, time 28.35236620903015, eps 0.00100000002274963, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
goal_identified
goal_identified
goal_identified
=== ep: 489, time 41.54232382774353, eps 0.0010000000216401172, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
=== ep: 490, time 28.19580316543579, eps 0.0010000000205847162, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
goal_identified
=== ep: 491, time 28.64802622795105, eps 0.0010000000195807877, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
goal_identified
goal_identified
=== ep: 492, time 28.695061922073364, eps 0.0010000000186258216, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 28.07950711250305, eps 0.0010000000177174295, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 29.131932497024536, eps 0.0010000000168533404, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
goal_identified
=== ep: 495, time 29.493794441223145, eps 0.0010000000160313932, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 27.970620155334473, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 29.77286124229431, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 28.419641971588135, eps 0.001000000013798348, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 499, time 45.12908434867859, eps 0.0010000000131253947, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
goal_identified
=== ep: 500, time 27.925365209579468, eps 0.0010000000124852615, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 28.153217554092407, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
goal_identified
=== ep: 502, time 28.28416681289673, eps 0.0010000000112971319, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 27.416337728500366, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 27.730543613433838, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
goal_identified
goal_identified
goal_identified
=== ep: 505, time 28.108785390853882, eps 0.0010000000097235315, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
goal_identified
goal_identified
=== ep: 506, time 27.968100547790527, eps 0.0010000000092493092, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 28.262341499328613, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
goal_identified
=== ep: 508, time 27.820585012435913, eps 0.0010000000083691212, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 39.8555543422699, eps 0.0010000000079609542, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
=== ep: 510, time 28.304763317108154, eps 0.001000000007572694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
goal_identified
=== ep: 511, time 28.24391794204712, eps 0.0010000000072033692, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 29.715171337127686, eps 0.001000000006852057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
=== ep: 513, time 29.352024793624878, eps 0.001000000006517878, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
=== ep: 514, time 28.67136263847351, eps 0.0010000000061999974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 28.547455549240112, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
=== ep: 516, time 27.607228994369507, eps 0.0010000000056099897, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
=== ep: 517, time 28.26177978515625, eps 0.0010000000053363872, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
=== ep: 518, time 28.47167420387268, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
=== ep: 519, time 47.75620198249817, eps 0.001000000004828563, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 29.050665855407715, eps 0.001000000004593071, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
goal_identified
=== ep: 521, time 28.507792949676514, eps 0.0010000000043690644, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
goal_identified
=== ep: 522, time 27.944822311401367, eps 0.0010000000041559827, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
=== ep: 523, time 28.533878564834595, eps 0.0010000000039532928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
=== ep: 524, time 28.872479915618896, eps 0.0010000000037604885, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
=== ep: 525, time 28.885756969451904, eps 0.0010000000035770874, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 29.305410385131836, eps 0.0010000000034026306, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
goal_identified
=== ep: 527, time 30.165241718292236, eps 0.0010000000032366824, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 28.154643297195435, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
=== ep: 529, time 45.216715812683105, eps 0.0010000000029286714, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
goal_identified
=== ep: 530, time 28.052482843399048, eps 0.0010000000027858384, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 28.224531888961792, eps 0.0010000000026499714, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 532, time 27.593976497650146, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
=== ep: 533, time 28.094021558761597, eps 0.0010000000023977934, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
goal_identified
goal_identified
=== ep: 534, time 28.254400491714478, eps 0.0010000000022808515, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
goal_identified
=== ep: 535, time 27.870621919631958, eps 0.0010000000021696133, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
goal_identified
goal_identified
=== ep: 536, time 28.49194574356079, eps 0.0010000000020637999, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 27.943443775177002, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
=== ep: 538, time 28.90277123451233, eps 0.0010000000018674034, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 539, time 50.536550998687744, eps 0.001000000001776329, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
=== ep: 540, time 28.2533118724823, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
goal_identified
=== ep: 541, time 27.699012517929077, eps 0.001000000001607289, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
goal_identified
goal_identified
=== ep: 542, time 27.75655460357666, eps 0.0010000000015289005, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
=== ep: 543, time 27.938255071640015, eps 0.0010000000014543352, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
goal_identified
goal_identified
=== ep: 544, time 28.07863187789917, eps 0.0010000000013834064, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
goal_identified
goal_identified
goal_identified
=== ep: 545, time 27.631073713302612, eps 0.001000000001315937, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
=== ep: 546, time 27.282480478286743, eps 0.0010000000012517578, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 27.497936964035034, eps 0.001000000001190709, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
goal_identified
goal_identified
=== ep: 548, time 27.62532114982605, eps 0.0010000000011326374, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
goal_identified
goal_identified
=== ep: 549, time 41.942235469818115, eps 0.001000000001077398, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
goal_identified
=== ep: 550, time 27.46548318862915, eps 0.0010000000010248527, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
goal_identified
=== ep: 551, time 27.765105485916138, eps 0.00100000000097487, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 552, time 27.722907304763794, eps 0.001000000000927325, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
goal_identified
=== ep: 553, time 27.883791208267212, eps 0.0010000000008820989, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
goal_identified
goal_identified
=== ep: 554, time 28.016696214675903, eps 0.0010000000008390784, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
goal_identified
goal_identified
=== ep: 555, time 27.770471572875977, eps 0.001000000000798156, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
goal_identified
goal_identified
goal_identified
=== ep: 556, time 28.513378858566284, eps 0.0010000000007592295, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
=== ep: 557, time 28.2118501663208, eps 0.0010000000007222014, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
goal_identified
goal_identified
goal_identified
=== ep: 558, time 27.977881908416748, eps 0.0010000000006869794, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
goal_identified
goal_identified
goal_identified
=== ep: 559, time 39.09486126899719, eps 0.001000000000653475, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
goal_identified
goal_identified
goal_identified
=== ep: 560, time 27.907006978988647, eps 0.0010000000006216046, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
goal_identified
=== ep: 561, time 27.901928663253784, eps 0.0010000000005912885, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
goal_identified
goal_identified
=== ep: 562, time 28.301708936691284, eps 0.0010000000005624511, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
goal_identified
=== ep: 563, time 28.249860286712646, eps 0.00100000000053502, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 27.767334938049316, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 565, time 28.32910990715027, eps 0.001000000000484106, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 566, time 28.028571128845215, eps 0.001000000000460496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
goal_identified
goal_identified
goal_identified
=== ep: 567, time 28.46220588684082, eps 0.0010000000004380374, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
goal_identified
goal_identified
=== ep: 568, time 28.129000425338745, eps 0.001000000000416674, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
goal_identified
=== ep: 569, time 39.7495756149292, eps 0.0010000000003963527, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
goal_identified
=== ep: 570, time 27.72591257095337, eps 0.0010000000003770222, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
goal_identified
=== ep: 571, time 28.329763889312744, eps 0.0010000000003586346, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
goal_identified
goal_identified
=== ep: 572, time 27.744860410690308, eps 0.0010000000003411438, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
=== ep: 573, time 27.554651260375977, eps 0.001000000000324506, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
goal_identified
goal_identified
=== ep: 574, time 27.756720066070557, eps 0.0010000000003086798, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 28.25930094718933, eps 0.0010000000002936252, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 576, time 27.785539388656616, eps 0.001000000000279305, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 577, time 27.950716972351074, eps 0.0010000000002656831, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 28.143739938735962, eps 0.0010000000002527256, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
goal_identified
goal_identified
=== ep: 579, time 40.21099305152893, eps 0.0010000000002404, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 28.21049952507019, eps 0.0010000000002286756, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
goal_identified
goal_identified
goal_identified
=== ep: 581, time 28.026822805404663, eps 0.0010000000002175229, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
goal_identified
goal_identified
goal_identified
=== ep: 582, time 27.719704389572144, eps 0.0010000000002069142, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
goal_identified
goal_identified
goal_identified
=== ep: 583, time 28.170876026153564, eps 0.0010000000001968228, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
goal_identified
goal_identified
=== ep: 584, time 27.830755949020386, eps 0.0010000000001872237, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
goal_identified
=== ep: 585, time 27.888604164123535, eps 0.0010000000001780928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
goal_identified
=== ep: 586, time 28.016409397125244, eps 0.001000000000169407, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 27.975377082824707, eps 0.001000000000161145, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
=== ep: 588, time 27.585696935653687, eps 0.0010000000001532858, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
goal_identified
=== ep: 589, time 38.232255697250366, eps 0.00100000000014581, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
goal_identified
goal_identified
=== ep: 590, time 28.00717782974243, eps 0.0010000000001386988, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
goal_identified
goal_identified
goal_identified
=== ep: 591, time 27.903210163116455, eps 0.0010000000001319344, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 26.39081645011902, eps 0.0010000000001255, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
goal_identified
=== ep: 593, time 27.63897442817688, eps 0.0010000000001193791, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
goal_identified
goal_identified
goal_identified
=== ep: 594, time 27.959718704223633, eps 0.001000000000113557, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
goal_identified
goal_identified
=== ep: 595, time 27.930502891540527, eps 0.0010000000001080186, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 596, time 27.85517430305481, eps 0.0010000000001027505, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 597, time 27.486771821975708, eps 0.0010000000000977393, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 27.903112649917603, eps 0.0010000000000929725, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
goal_identified
goal_identified
goal_identified
=== ep: 599, time 38.45147109031677, eps 0.0010000000000884382, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
=== ep: 600, time 28.03153657913208, eps 0.001000000000084125, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 601, time 28.351016521453857, eps 0.0010000000000800222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
goal_identified
=== ep: 602, time 27.936264038085938, eps 0.0010000000000761195, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
goal_identified
=== ep: 603, time 27.74946618080139, eps 0.0010000000000724072, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
goal_identified
=== ep: 604, time 28.081727743148804, eps 0.0010000000000688757, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 28.570523500442505, eps 0.0010000000000655166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
goal_identified
=== ep: 606, time 28.062962770462036, eps 0.0010000000000623215, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 28.254475831985474, eps 0.001000000000059282, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 608, time 28.060692071914673, eps 0.0010000000000563907, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
=== ep: 609, time 36.06346821784973, eps 0.0010000000000536405, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
goal_identified
=== ep: 610, time 27.789387464523315, eps 0.0010000000000510245, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
goal_identified
goal_identified
=== ep: 611, time 27.93284320831299, eps 0.0010000000000485358, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
goal_identified
goal_identified
=== ep: 612, time 28.437406539916992, eps 0.0010000000000461688, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 27.938497304916382, eps 0.0010000000000439171, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
goal_identified
goal_identified
goal_identified
=== ep: 614, time 28.009945392608643, eps 0.0010000000000417752, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
goal_identified
goal_identified
=== ep: 615, time 28.070297479629517, eps 0.0010000000000397378, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
=== ep: 616, time 27.74204730987549, eps 0.0010000000000377999, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 27.496121883392334, eps 0.0010000000000359563, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
=== ep: 618, time 27.93042516708374, eps 0.0010000000000342027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
goal_identified
goal_identified
=== ep: 619, time 38.05023241043091, eps 0.0010000000000325345, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
goal_identified
goal_identified
=== ep: 620, time 28.168031215667725, eps 0.001000000000030948, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
goal_identified
=== ep: 621, time 27.94777250289917, eps 0.0010000000000294385, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
goal_identified
=== ep: 622, time 28.13966655731201, eps 0.0010000000000280028, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 623, time 27.859641551971436, eps 0.0010000000000266371, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
goal_identified
=== ep: 624, time 27.9511878490448, eps 0.001000000000025338, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
goal_identified
=== ep: 625, time 27.519083499908447, eps 0.0010000000000241023, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
=== ep: 626, time 27.79670548439026, eps 0.0010000000000229268, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
goal_identified
=== ep: 627, time 28.122220993041992, eps 0.0010000000000218085, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
goal_identified
goal_identified
=== ep: 628, time 27.918121099472046, eps 0.001000000000020745, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
goal_identified
goal_identified
=== ep: 629, time 35.841034173965454, eps 0.0010000000000197332, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
goal_identified
goal_identified
=== ep: 630, time 28.332510948181152, eps 0.0010000000000187708, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
=== ep: 631, time 28.05033802986145, eps 0.0010000000000178553, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 632, time 27.81877589225769, eps 0.0010000000000169845, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 633, time 27.879669427871704, eps 0.0010000000000161562, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
=== ep: 634, time 27.959370374679565, eps 0.0010000000000153684, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
goal_identified
=== ep: 635, time 28.030920267105103, eps 0.0010000000000146188, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 636, time 28.524928092956543, eps 0.0010000000000139058, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 637, time 27.99462652206421, eps 0.0010000000000132275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
goal_identified
=== ep: 638, time 28.01104712486267, eps 0.0010000000000125824, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 39.594494581222534, eps 0.0010000000000119687, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 27.81806707382202, eps 0.001000000000011385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
=== ep: 641, time 27.847017526626587, eps 0.00100000000001083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
