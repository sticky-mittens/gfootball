==> Playing in 11_vs_11_hard_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['attack', 'defend'])
==>Currently learning win_game to choose from above OTs.
==>using device cuda
==>critic has 2 layers and 3 hidden units.
goal_identified
=== ep: 0, time 28.191993951797485, eps 0.9, right preds for atk and def: 72/141 = 0.5106382978723404, score_diff -1, tot learning steps 10 (total env steps 3001)
=== ep: 1, time 27.537007808685303, eps 0.8561552526261419, right preds for atk and def: 40/85 = 0.47058823529411764, score_diff -2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 2, time 29.552342891693115, eps 0.8144488388143276, right preds for atk and def: 65/165 = 0.3939393939393939, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 3, time 29.593323469161987, eps 0.774776470806127, right preds for atk and def: 68/134 = 0.5074626865671642, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 4, time 30.39773678779602, eps 0.7370389470171057, right preds for atk and def: 74/158 = 0.46835443037974683, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 5, time 31.855557680130005, eps 0.701141903981193, right preds for atk and def: 78/188 = 0.4148936170212766, score_diff -3, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 6, time 30.788270235061646, eps 0.6669955803928644, right preds for atk and def: 67/150 = 0.44666666666666666, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 7, time 31.14293074607849, eps 0.6345145926571234, right preds for atk and def: 78/184 = 0.42391304347826086, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 8, time 33.605656147003174, eps 0.6036177213860398, right preds for atk and def: 59/143 = 0.4125874125874126, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 9, time 35.93617010116577, eps 0.5742277083079742, right preds for atk and def: 72/182 = 0.3956043956043956, score_diff -1, tot learning steps 10 (total env steps 3001)
=== ep: 10, time 34.64769458770752, eps 0.5462710630816575, right preds for atk and def: 93/234 = 0.3974358974358974, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 11, time 34.30769181251526, eps 0.5196778795320575, right preds for atk and def: 68/153 = 0.4444444444444444, score_diff 2, tot learning steps 10 (total env steps 3001)
=== ep: 12, time 40.21977949142456, eps 0.49438166084852986, right preds for atk and def: 77/200 = 0.385, score_diff -2, tot learning steps 10 (total env steps 3001)
=== ep: 13, time 46.659544944763184, eps 0.47031915330815344, right preds for atk and def: 74/180 = 0.4111111111111111, score_diff -2, tot learning steps 10 (total env steps 3001)
=== ep: 14, time 38.77163481712341, eps 0.4474301881084772, right preds for atk and def: 75/213 = 0.352112676056338, score_diff -5, tot learning steps 10 (total env steps 3001)
=== ep: 15, time 41.12898635864258, eps 0.42565753091417224, right preds for atk and def: 54/157 = 0.34394904458598724, score_diff -2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 16, time 43.91272687911987, eps 0.4049467387413822, right preds for atk and def: 73/241 = 0.3029045643153527, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 17, time 37.272473096847534, eps 0.3852460238219053, right preds for atk and def: 61/216 = 0.2824074074074074, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 18, time 38.809935331344604, eps 0.3665061241067986, right preds for atk and def: 60/79 = 0.759493670886076, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 19, time 46.942671060562134, eps 0.3486801800855966, right preds for atk and def: 97/121 = 0.8016528925619835, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 17
goal_identified
=== ep: 20, time 42.319339752197266, eps 0.3317236176131267, right preds for atk and def: 84/100 = 0.84, score_diff 0, tot learning steps 10 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies.py:453: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20 and we are deleting ep 16
=== ep: 21, time 37.36763596534729, eps 0.31559403645092865, right preds for atk and def: 91/100 = 0.91, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 15
goal_identified
=== ep: 22, time 43.60501980781555, eps 0.3002511042445735, right preds for atk and def: 78/94 = 0.8297872340425532, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 14
=== ep: 23, time 48.0241494178772, eps 0.2856564556717689, right preds for atk and def: 89/105 = 0.8476190476190476, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 12
=== ep: 24, time 36.82433009147644, eps 0.27177359650906974, right preds for atk and def: 68/79 = 0.8607594936708861, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 2
=== ep: 25, time 39.24963736534119, eps 0.2585678123773109, right preds for atk and def: 70/82 = 0.8536585365853658, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 9
goal_identified
=== ep: 26, time 42.3437602519989, eps 0.24600608193757734, right preds for atk and def: 70/79 = 0.8860759493670886, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 10
=== ep: 27, time 39.21610140800476, eps 0.23405699432065646, right preds for atk and def: 70/79 = 0.8860759493670886, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 13
goal_identified
=== ep: 28, time 38.99864649772644, eps 0.22269067058350425, right preds for atk and def: 98/113 = 0.8672566371681416, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 8
=== ep: 29, time 40.532806396484375, eps 0.2118786889963241, right preds for atk and def: 66/73 = 0.9041095890410958, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 5
=== ep: 30, time 40.26555562019348, eps 0.2015940139734384, right preds for atk and def: 94/108 = 0.8703703703703703, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 7
goal_identified
goal_identified
=== ep: 31, time 31.497111797332764, eps 0.191810928470242, right preds for atk and def: 71/76 = 0.9342105263157895, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 11
goal_identified
=== ep: 32, time 45.44766879081726, eps 0.1825049696771952, right preds for atk and def: 87/97 = 0.8969072164948454, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 6
=== ep: 33, time 44.764546394348145, eps 0.17365286785005798, right preds for atk and def: 59/64 = 0.921875, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 4
goal_identified
=== ep: 34, time 37.072280168533325, eps 0.16523248812340846, right preds for atk and def: 76/82 = 0.926829268292683, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 1
=== ep: 35, time 35.65324902534485, eps 0.15722277516195018, right preds for atk and def: 77/88 = 0.875, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 3
=== ep: 36, time 38.335602045059204, eps 0.1496037005112063, right preds for atk and def: 91/99 = 0.9191919191919192, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 37, time 34.58780026435852, eps 0.14235621251595124, right preds for atk and def: 84/87 = 0.9655172413793104, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 18
=== ep: 38, time 38.942020654678345, eps 0.13546218868114893, right preds for atk and def: 78/84 = 0.9285714285714286, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 19
goal_identified
goal_identified
=== ep: 39, time 34.64260935783386, eps 0.1289043903562757, right preds for atk and def: 57/63 = 0.9047619047619048, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 22
=== ep: 40, time 38.04448175430298, eps 0.12266641962971482, right preds for atk and def: 73/73 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 20
goal_identified
=== ep: 41, time 39.04240131378174, eps 0.116732678325436, right preds for atk and def: 95/103 = 0.9223300970873787, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 23
=== ep: 42, time 40.9583055973053, eps 0.11108832899943073, right preds for atk and def: 68/74 = 0.918918918918919, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 25
=== ep: 43, time 40.52123427391052, eps 0.10571925783837377, right preds for atk and def: 110/115 = 0.9565217391304348, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 24
=== ep: 44, time 34.01925253868103, eps 0.10061203936773815, right preds for atk and def: 85/90 = 0.9444444444444444, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 28
=== ep: 45, time 37.60014796257019, eps 0.09575390288111604, right preds for atk and def: 82/90 = 0.9111111111111111, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 30
goal_identified
=== ep: 46, time 32.835691690444946, eps 0.09113270050680057, right preds for atk and def: 92/93 = 0.989247311827957, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 35
goal_identified
=== ep: 47, time 35.50553488731384, eps 0.08673687683177911, right preds for atk and def: 80/86 = 0.9302325581395349, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 26
goal_identified
=== ep: 48, time 37.368011713027954, eps 0.08255544000718185, right preds for atk and def: 66/71 = 0.9295774647887324, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 27
=== ep: 49, time 37.67582106590271, eps 0.07857793426293408, right preds for atk and def: 82/83 = 0.9879518072289156, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 32
=== ep: 50, time 33.475645542144775, eps 0.07479441376288502, right preds for atk and def: 45/48 = 0.9375, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 29
=== ep: 51, time 32.90591263771057, eps 0.0711954177350367, right preds for atk and def: 110/116 = 0.9482758620689655, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 39
goal_identified
=== ep: 52, time 35.760526180267334, eps 0.06777194681468615, right preds for atk and def: 75/81 = 0.9259259259259259, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 21
goal_identified
=== ep: 53, time 39.846829414367676, eps 0.06451544054132621, right preds for atk and def: 81/84 = 0.9642857142857143, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 45
goal_identified
=== ep: 54, time 36.533427715301514, eps 0.06141775595303503, right preds for atk and def: 75/77 = 0.974025974025974, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 42
=== ep: 55, time 34.54369282722473, eps 0.05847114722483011, right preds for atk and def: 96/98 = 0.9795918367346939, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 36
=== ep: 56, time 36.1919002532959, eps 0.05566824630007096, right preds for atk and def: 76/76 = 1.0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 33
goal_identified
=== ep: 57, time 35.66778540611267, eps 0.05300204446647978, right preds for atk and def: 73/74 = 0.9864864864864865, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 41
=== ep: 58, time 30.44517469406128, eps 0.050465874830710106, right preds for atk and def: 85/88 = 0.9659090909090909, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 59, time 35.41665554046631, eps 0.04805339564764071, right preds for atk and def: 79/79 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 34
=== ep: 60, time 36.060686111450195, eps 0.045758574462709686, right preds for atk and def: 78/82 = 0.9512195121951219, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 38
=== ep: 61, time 36.49100112915039, eps 0.043575673027635695, right preds for atk and def: 63/63 = 1.0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 48
=== ep: 62, time 32.514625787734985, eps 0.04149923295180846, right preds for atk and def: 87/89 = 0.9775280898876404, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 47
goal_identified
=== ep: 63, time 39.229434967041016, eps 0.03952406205346913, right preds for atk and def: 89/93 = 0.956989247311828, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 31
goal_identified
=== ep: 64, time 40.02443790435791, eps 0.03764522137655123, right preds for atk and def: 83/83 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 50
=== ep: 65, time 37.510080099105835, eps 0.03585801284071809, right preds for atk and def: 83/86 = 0.9651162790697675, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 44
goal_identified
=== ep: 66, time 32.910584688186646, eps 0.034157967493714775, right preds for atk and def: 82/83 = 0.9879518072289156, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 51
=== ep: 67, time 36.024181604385376, eps 0.03254083433665968, right preds for atk and def: 65/68 = 0.9558823529411765, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 60
=== ep: 68, time 35.901305198669434, eps 0.031002569694333147, right preds for atk and def: 70/71 = 0.9859154929577465, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 67
goal_identified
=== ep: 69, time 34.065232276916504, eps 0.02953932710388308, right preds for atk and def: 71/72 = 0.9861111111111112, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 43
=== ep: 70, time 32.83423471450806, eps 0.028147447696664333, right preds for atk and def: 69/71 = 0.971830985915493, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 63
goal_identified
=== ep: 71, time 35.18796920776367, eps 0.026823451049161253, right preds for atk and def: 57/58 = 0.9827586206896551, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 53
=== ep: 72, time 32.8267707824707, eps 0.025564026480116013, right preds for atk and def: 104/104 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 65
goal_identified
=== ep: 73, time 36.60765814781189, eps 0.02436602477210106, right preds for atk and def: 89/91 = 0.978021978021978, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 37
=== ep: 74, time 36.46985483169556, eps 0.02322645029683511, right preds for atk and def: 79/79 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 58
=== ep: 75, time 40.89388298988342, eps 0.02214245352455219, right preds for atk and def: 106/107 = 0.9906542056074766, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 70
goal_identified
goal_identified
=== ep: 76, time 38.4874472618103, eps 0.02111132389869288, right preds for atk and def: 77/78 = 0.9871794871794872, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 54
=== ep: 77, time 39.458853244781494, eps 0.020130483058101077, right preds for atk and def: 77/77 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 62
goal_identified
=== ep: 78, time 34.59316873550415, eps 0.019197478389778148, right preds for atk and def: 78/78 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 73
=== ep: 79, time 36.3133008480072, eps 0.018309976896072843, right preds for atk and def: 93/93 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 55
goal_identified
goal_identified
=== ep: 80, time 37.47521114349365, eps 0.017465759360972027, right preds for atk and def: 89/90 = 0.9888888888888889, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 71
goal_identified
=== ep: 81, time 37.17412281036377, eps 0.01666271480090467, right preds for atk and def: 59/60 = 0.9833333333333333, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 81
goal_identified
goal_identified
=== ep: 82, time 31.419517993927002, eps 0.015898835186183367, right preds for atk and def: 73/73 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 68
goal_identified
=== ep: 83, time 36.82164168357849, eps 0.015172210419884185, right preds for atk and def: 67/67 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 69
=== ep: 84, time 40.93828535079956, eps 0.014481023561609456, right preds for atk and def: 63/64 = 0.984375, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 84
goal_identified
=== ep: 85, time 42.27899384498596, eps 0.01382354628419033, right preds for atk and def: 105/105 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 57
goal_identified
=== ep: 86, time 30.70778775215149, eps 0.013198134551968641, right preds for atk and def: 90/92 = 0.9782608695652174, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 86
=== ep: 87, time 38.451494455337524, eps 0.012603224509851407, right preds for atk and def: 106/106 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 76
=== ep: 88, time 39.5454785823822, eps 0.012037328572858524, right preds for atk and def: 109/110 = 0.990909090909091, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 49
goal_identified
goal_identified
=== ep: 89, time 34.33142423629761, eps 0.011499031706385502, right preds for atk and def: 102/106 = 0.9622641509433962, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 89
=== ep: 90, time 33.6174750328064, eps 0.010986987887879832, right preds for atk and def: 89/90 = 0.9888888888888889, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 66
=== ep: 91, time 36.88008737564087, eps 0.010499916741083536, right preds for atk and def: 85/85 = 1.0, score_diff -6, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 80
goal_identified
goal_identified
=== ep: 92, time 32.975750207901, eps 0.010036600334425595, right preds for atk and def: 62/62 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 90
=== ep: 93, time 36.03461170196533, eps 0.00959588013555861, right preds for atk and def: 99/99 = 1.0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 46
goal_identified
=== ep: 94, time 36.272584199905396, eps 0.009176654114424539, right preds for atk and def: 107/107 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 75
=== ep: 95, time 40.34066700935364, eps 0.00877787398760545, right preds for atk and def: 85/85 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 88
=== ep: 96, time 32.975167989730835, eps 0.008398542597069007, right preds for atk and def: 83/83 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 40
=== ep: 97, time 36.28952074050903, eps 0.008037711416753971, right preds for atk and def: 73/73 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 56
goal_identified
=== ep: 98, time 33.46898102760315, eps 0.00769447818076098, right preds for atk and def: 80/80 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 59
goal_identified
=== ep: 99, time 34.24062538146973, eps 0.007367984627217855, right preds for atk and def: 85/85 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 61
goal_identified
=== ep: 100, time 38.11157584190369, eps 0.007057414352177835, right preds for atk and def: 107/108 = 0.9907407407407407, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 100
=== ep: 101, time 38.415050983428955, eps 0.006761990768184489, right preds for atk and def: 91/92 = 0.9891304347826086, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 101
goal_identified
=== ep: 102, time 32.84353566169739, eps 0.006480975162398559, right preds for atk and def: 77/77 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 64
=== ep: 103, time 33.11972236633301, eps 0.006213664849431085, right preds for atk and def: 79/79 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 72
goal_identified
=== ep: 104, time 36.23949122428894, eps 0.005959391414263934, right preds for atk and def: 90/90 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 74
goal_identified
=== ep: 105, time 38.820489168167114, eps 0.005717519040864065, right preds for atk and def: 53/53 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 77
=== ep: 106, time 40.06735420227051, eps 0.005487442922312285, right preds for atk and def: 79/79 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 78
=== ep: 107, time 30.695723295211792, eps 0.005268587748470919, right preds for atk and def: 67/67 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 79
goal_identified
=== ep: 108, time 35.89877533912659, eps 0.005060406267408787, right preds for atk and def: 77/77 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 82
=== ep: 109, time 35.881277561187744, eps 0.004862377916986354, right preds for atk and def: 61/61 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 83
goal_identified
=== ep: 110, time 33.536646366119385, eps 0.004674007523179196, right preds for atk and def: 85/86 = 0.9883720930232558, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 110
goal_identified
=== ep: 111, time 32.826935052871704, eps 0.004494824061885041, right preds for atk and def: 95/95 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 85
goal_identified
=== ep: 112, time 34.092034578323364, eps 0.0043243794811181555, right preds for atk and def: 99/99 = 1.0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 87
goal_identified
=== ep: 113, time 34.3051974773407, eps 0.0041622475806460035, right preds for atk and def: 79/79 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 91
=== ep: 114, time 38.3129141330719, eps 0.0040080229462666735, right preds for atk and def: 69/69 = 1.0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 92
goal_identified
=== ep: 115, time 42.416120767593384, eps 0.0038613199360621906, right preds for atk and def: 91/91 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 93
=== ep: 116, time 38.471189975738525, eps 0.003721771716092858, right preds for atk and def: 75/75 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 94
=== ep: 117, time 34.83763861656189, eps 0.0035890293431213305, right preds for atk and def: 77/77 = 1.0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 95
goal_identified
=== ep: 118, time 38.24947142601013, eps 0.0034627608920727634, right preds for atk and def: 73/73 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 96
=== ep: 119, time 39.426904916763306, eps 0.00334265062604924, right preds for atk and def: 107/107 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 7 layers and 500 hidden units.
=== ep: 0, time 27.613673210144043, eps 0.9, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
=== ep: 1, time 28.039174556732178, eps 0.8561552526261419, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 2, time 27.73584008216858, eps 0.8144488388143276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
=== ep: 3, time 27.600491762161255, eps 0.774776470806127, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 4, time 27.46262812614441, eps 0.7370389470171057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 5, time 27.6180157661438, eps 0.701141903981193, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
=== ep: 6, time 27.62585735321045, eps 0.6669955803928644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 7, time 27.519379377365112, eps 0.6345145926571234, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 8, time 27.94405245780945, eps 0.6036177213860398, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 9, time 31.289959192276, eps 0.5742277083079742, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
=== ep: 10, time 27.83901047706604, eps 0.5462710630816575, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
=== ep: 11, time 27.897295236587524, eps 0.5196778795320575, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
=== ep: 12, time 27.685840129852295, eps 0.49438166084852986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
=== ep: 13, time 27.673278093338013, eps 0.47031915330815344, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
goal_identified
=== ep: 14, time 27.85184121131897, eps 0.4474301881084772, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
goal_identified
=== ep: 15, time 27.788216829299927, eps 0.42565753091417224, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
=== ep: 16, time 28.015466451644897, eps 0.4049467387413822, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
=== ep: 17, time 27.462077379226685, eps 0.3852460238219053, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 18, time 28.026363849639893, eps 0.3665061241067986, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
=== ep: 19, time 32.09186267852783, eps 0.3486801800855966, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 20, time 27.691190242767334, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
goal_identified
=== ep: 21, time 27.76605796813965, eps 0.31559403645092865, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
goal_identified
=== ep: 22, time 27.978073120117188, eps 0.3002511042445735, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
=== ep: 23, time 27.704711198806763, eps 0.2856564556717689, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
=== ep: 24, time 28.168962240219116, eps 0.27177359650906974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
=== ep: 25, time 27.886852264404297, eps 0.2585678123773109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
=== ep: 26, time 27.810465574264526, eps 0.24600608193757734, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 27, time 27.88860845565796, eps 0.23405699432065646, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
=== ep: 28, time 27.756649017333984, eps 0.22269067058350425, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 29, time 32.33512353897095, eps 0.2118786889963241, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
=== ep: 30, time 27.814882516860962, eps 0.2015940139734384, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
=== ep: 31, time 28.353129625320435, eps 0.191810928470242, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
=== ep: 32, time 27.906702995300293, eps 0.1825049696771952, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 33, time 27.986750841140747, eps 0.17365286785005798, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 34, time 28.089350938796997, eps 0.16523248812340846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 35, time 28.343570232391357, eps 0.15722277516195018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
=== ep: 36, time 27.972809314727783, eps 0.1496037005112063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
=== ep: 37, time 27.956233024597168, eps 0.14235621251595124, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
=== ep: 38, time 27.857720851898193, eps 0.13546218868114893, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 39, time 30.617055654525757, eps 0.1289043903562757, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
=== ep: 40, time 28.036290168762207, eps 0.12266641962971482, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 41, time 28.045761346817017, eps 0.116732678325436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 42, time 27.929169416427612, eps 0.11108832899943073, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 43, time 27.98963952064514, eps 0.10571925783837377, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
=== ep: 44, time 27.92876672744751, eps 0.10061203936773815, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 45, time 27.84034752845764, eps 0.09575390288111604, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
=== ep: 46, time 28.038944244384766, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 47, time 27.84628701210022, eps 0.08673687683177911, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
goal_identified
=== ep: 48, time 27.857237100601196, eps 0.08255544000718185, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
goal_identified
=== ep: 49, time 31.25662922859192, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
=== ep: 50, time 27.626774549484253, eps 0.07479441376288502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 51, time 28.12970542907715, eps 0.0711954177350367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
=== ep: 52, time 27.631476879119873, eps 0.06777194681468615, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
goal_identified
=== ep: 53, time 28.242252111434937, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 54, time 27.95893359184265, eps 0.06141775595303503, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 27.94208335876465, eps 0.05847114722483011, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
=== ep: 56, time 28.71087098121643, eps 0.05566824630007096, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 27.897032737731934, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 27.989908456802368, eps 0.050465874830710106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
=== ep: 59, time 31.7070152759552, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
=== ep: 60, time 27.916263818740845, eps 0.045758574462709686, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 27.763718128204346, eps 0.043575673027635695, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
=== ep: 62, time 28.08567523956299, eps 0.04149923295180846, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
goal_identified
=== ep: 63, time 28.107534885406494, eps 0.03952406205346913, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
=== ep: 64, time 27.72322130203247, eps 0.03764522137655123, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
=== ep: 65, time 27.977994441986084, eps 0.03585801284071809, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 66, time 28.347368955612183, eps 0.034157967493714775, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 67, time 27.976873874664307, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
=== ep: 68, time 27.354833126068115, eps 0.031002569694333147, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 69, time 31.057145595550537, eps 0.02953932710388308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
goal_identified
=== ep: 70, time 27.82978844642639, eps 0.028147447696664333, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
=== ep: 71, time 28.063511610031128, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 28.287800312042236, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
=== ep: 73, time 27.540024042129517, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
=== ep: 74, time 28.028762578964233, eps 0.02322645029683511, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 75, time 28.299675703048706, eps 0.02214245352455219, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 25.545711517333984, eps 0.02111132389869288, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 28.233216047286987, eps 0.020130483058101077, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
goal_identified
=== ep: 78, time 28.05342698097229, eps 0.019197478389778148, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
=== ep: 79, time 31.713040351867676, eps 0.018309976896072843, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
goal_identified
=== ep: 80, time 27.82111930847168, eps 0.017465759360972027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
=== ep: 81, time 28.010247945785522, eps 0.01666271480090467, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 82, time 27.95288634300232, eps 0.015898835186183367, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
goal_identified
=== ep: 83, time 28.35961103439331, eps 0.015172210419884185, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
=== ep: 84, time 28.288422346115112, eps 0.014481023561609456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 85, time 28.16474747657776, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
=== ep: 86, time 27.959988355636597, eps 0.013198134551968641, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 87, time 28.07159972190857, eps 0.012603224509851407, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
=== ep: 88, time 27.950408220291138, eps 0.012037328572858524, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
=== ep: 89, time 31.50846028327942, eps 0.011499031706385502, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
=== ep: 90, time 28.36872887611389, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
=== ep: 91, time 28.268587112426758, eps 0.010499916741083536, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
=== ep: 92, time 27.933716297149658, eps 0.010036600334425595, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 27.870272159576416, eps 0.00959588013555861, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
goal_identified
=== ep: 94, time 27.81546926498413, eps 0.009176654114424539, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
=== ep: 95, time 28.161476373672485, eps 0.00877787398760545, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 28.111586332321167, eps 0.008398542597069007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
=== ep: 97, time 26.914047956466675, eps 0.008037711416753971, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
=== ep: 98, time 26.603660106658936, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
=== ep: 99, time 29.359806776046753, eps 0.007367984627217855, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 26.7401123046875, eps 0.007057414352177835, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 101, time 26.971906185150146, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
=== ep: 102, time 26.361608028411865, eps 0.006480975162398559, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
=== ep: 103, time 26.503777503967285, eps 0.006213664849431085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
=== ep: 104, time 26.911983013153076, eps 0.005959391414263934, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
=== ep: 105, time 26.772265911102295, eps 0.005717519040864065, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
=== ep: 106, time 26.71763801574707, eps 0.005487442922312285, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
=== ep: 107, time 27.06897521018982, eps 0.005268587748470919, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
=== ep: 108, time 26.47653889656067, eps 0.005060406267408787, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
=== ep: 109, time 30.295210123062134, eps 0.004862377916986354, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 110, time 26.972936153411865, eps 0.004674007523179196, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 26.559460639953613, eps 0.004494824061885041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 26.836347579956055, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 25.15960168838501, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 26.829277515411377, eps 0.0040080229462666735, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 26.202263116836548, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
goal_identified
=== ep: 116, time 26.927083730697632, eps 0.003721771716092858, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 27.156646728515625, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
=== ep: 118, time 26.705198526382446, eps 0.0034627608920727634, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 119, time 30.375123739242554, eps 0.00334265062604924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 26.89167881011963, eps 0.0032283982068230565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 26.611536741256714, eps 0.0031197179438347193, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
=== ep: 122, time 26.95347285270691, eps 0.0030163380798177374, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
=== ep: 123, time 27.05427098274231, eps 0.0029180001112638996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 26.868885040283203, eps 0.002824458142029865, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
=== ep: 125, time 27.09028387069702, eps 0.0027354782684687108, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
=== ep: 126, time 27.01414179801941, eps 0.0026508379945489875, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
=== ep: 127, time 26.778253078460693, eps 0.0025703256754987464, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
goal_identified
=== ep: 128, time 26.67655873298645, eps 0.0024937399885833667, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 129, time 29.2298002243042, eps 0.0024208894296938593, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
goal_identified
=== ep: 130, time 27.095274686813354, eps 0.0023515918344868374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
=== ep: 131, time 26.938931465148926, eps 0.002285673922878779, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 132, time 26.963621854782104, eps 0.0022229708657555565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 26.567508935928345, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 26.654916524887085, eps 0.0021065898005034594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
goal_identified
=== ep: 135, time 26.923248052597046, eps 0.002052620779091266, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 27.061559200286865, eps 0.0020012838579124784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
=== ep: 137, time 27.08229660987854, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
goal_identified
=== ep: 138, time 26.548397064208984, eps 0.001905999100714611, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 30.971126794815063, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 26.827387809753418, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 26.85263419151306, eps 0.0017798006526189953, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 26.692936897277832, eps 0.0017417693260160481, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 26.962408542633057, eps 0.0017055928090985275, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 26.927972316741943, eps 0.0016711806417306348, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 26.68661093711853, eps 0.0016384467755694515, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 26.43673062324524, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 26.937588453292847, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
goal_identified
goal_identified
=== ep: 148, time 26.692699193954468, eps 0.0015495162322554856, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
goal_identified
=== ep: 149, time 29.8207106590271, eps 0.0015227160093621863, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
goal_identified
goal_identified
=== ep: 150, time 27.23315405845642, eps 0.0014972228487629025, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
goal_identified
=== ep: 151, time 26.65790629386902, eps 0.0014729730042773413, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 27.534913301467896, eps 0.001449905838663109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
=== ep: 153, time 26.95571994781494, eps 0.00142796367199102, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 154, time 27.276405334472656, eps 0.0014070916374152305, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 155, time 26.759013175964355, eps 0.001387237543977543, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
=== ep: 156, time 26.875115156173706, eps 0.0013683517461028282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
=== ep: 157, time 27.230849742889404, eps 0.0013503870194592265, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
=== ep: 158, time 27.553914308547974, eps 0.0013332984428727204, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
=== ep: 159, time 29.64289140701294, eps 0.001317043286000802, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 160, time 26.90910530090332, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 27.178519010543823, eps 0.0012868726283106018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 26.310583114624023, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 163, time 27.15793204307556, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
=== ep: 164, time 26.884747982025146, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
goal_identified
goal_identified
=== ep: 165, time 26.42707109451294, eps 0.0012348714430141991, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
=== ep: 166, time 26.96571969985962, eps 0.0012234166275700486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
=== ep: 167, time 26.80417537689209, eps 0.001212520470067348, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
=== ep: 168, time 27.07820224761963, eps 0.0012021557244367845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
=== ep: 169, time 30.57958436012268, eps 0.0011922964734155277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
=== ep: 170, time 27.166770696640015, eps 0.001182918063740569, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 27.152663946151733, eps 0.0011739970445027263, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 172, time 27.24126648902893, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 27.30086851119995, eps 0.001157439036493735, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
=== ep: 174, time 26.98789620399475, eps 0.0011497606440778825, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 175, time 27.156657695770264, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
goal_identified
=== ep: 176, time 27.24923610687256, eps 0.0011355090345108335, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 27.072684049606323, eps 0.0011289001809123877, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 178, time 26.765577793121338, eps 0.0011226136449073282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
=== ep: 179, time 31.993059873580933, eps 0.001116633706881133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 26.96431064605713, eps 0.001110945413873925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 27.096400022506714, eps 0.001105534542190287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
=== ep: 182, time 26.92297339439392, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 27.077178955078125, eps 0.0010954916026690664, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
goal_identified
=== ep: 184, time 26.983506202697754, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
=== ep: 185, time 27.323585510253906, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
=== ep: 186, time 27.22256851196289, eps 0.0010821903840988777, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 187, time 27.166383743286133, eps 0.0010781819117658682, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
=== ep: 188, time 27.272407054901123, eps 0.0010743689349354123, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
=== ep: 189, time 30.119298458099365, eps 0.0010707419191793434, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
goal_identified
=== ep: 190, time 26.847583532333374, eps 0.0010672917950690429, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
=== ep: 191, time 27.10954713821411, eps 0.0010640099354971456, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
=== ep: 192, time 27.028560638427734, eps 0.0010608881341052777, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
=== ep: 193, time 27.049384593963623, eps 0.0010579185847638855, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 194, time 27.096161603927612, eps 0.0010550938620528466, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
=== ep: 195, time 27.293972730636597, eps 0.001052406902694051, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 27.138017654418945, eps 0.001049850987889527, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 27.327921390533447, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
goal_identified
=== ep: 198, time 27.122947454452515, eps 0.0010451070391685015, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
=== ep: 199, time 32.21937918663025, eps 0.001042907142909185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 26.90942692756653, eps 0.001040814536856474, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 27.167497634887695, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 27.21215510368347, eps 0.0010369305201475454, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
goal_identified
goal_identified
=== ep: 203, time 27.340949773788452, eps 0.0010351293974264616, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
=== ep: 204, time 27.21562886238098, eps 0.00103341611649703, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
=== ep: 205, time 27.130226135253906, eps 0.0010317863932645186, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 27.39127230644226, eps 0.0010302361525719613, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 27.541933059692383, eps 0.0010287615180101426, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 27.20788264274597, eps 0.001027358802224555, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
=== ep: 209, time 31.034940481185913, eps 0.0010260244976950921, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 210, time 27.090383052825928, eps 0.0010247552679654227, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
=== ep: 211, time 27.22260093688965, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 212, time 27.601842880249023, eps 0.0010223994927486214, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
=== ep: 213, time 26.994805812835693, eps 0.001021307056596379, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
=== ep: 214, time 27.6964373588562, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 215, time 27.865251064300537, eps 0.0010192794220766138, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 216, time 27.731700897216797, eps 0.0010183391535666436, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
goal_identified
goal_identified
=== ep: 217, time 27.14236807823181, eps 0.0010174447424930286, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
goal_identified
=== ep: 218, time 27.27806305885315, eps 0.0010165939523622068, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
goal_identified
=== ep: 219, time 32.573105335235596, eps 0.0010157846557556941, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 220, time 27.243509531021118, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 27.526469945907593, eps 0.0010142825471585687, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
=== ep: 222, time 27.2375910282135, eps 0.0010135859791140496, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 223, time 27.474336862564087, eps 0.0010129233830939361, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
=== ep: 224, time 27.679428339004517, eps 0.0010122931022630473, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
=== ep: 225, time 27.619850635528564, eps 0.001011693560591007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 27.478725910186768, eps 0.0010111232589113477, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
goal_identified
=== ep: 227, time 27.53028964996338, eps 0.0010105807711728136, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
=== ep: 228, time 27.430938959121704, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 31.177839517593384, eps 0.001009573877668838, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
=== ep: 230, time 27.525241374969482, eps 0.001009106954145169, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 27.34303903579712, eps 0.0010086628027504636, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 27.372501373291016, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 27.046265125274658, eps 0.0010078384280736842, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 234, time 27.730612993240356, eps 0.001007456143425521, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
=== ep: 235, time 27.285477876663208, eps 0.001007092503019653, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
=== ep: 236, time 27.174893856048584, eps 0.001006746597565654, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 237, time 27.055022478103638, eps 0.001006417562119715, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
=== ep: 238, time 27.528750896453857, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 33.07052493095398, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
=== ep: 240, time 27.029820203781128, eps 0.001005523646905642, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
=== ep: 241, time 28.030588150024414, eps 0.001005254255467199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
=== ep: 242, time 27.253472328186035, eps 0.0010049980024042435, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 27.459312438964844, eps 0.0010047542469506416, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
=== ep: 244, time 27.155555963516235, eps 0.0010045223795907931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
=== ep: 245, time 27.403669834136963, eps 0.001004301820535524, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
=== ep: 246, time 27.272908687591553, eps 0.0010040920182723119, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
=== ep: 247, time 27.370462656021118, eps 0.0010038924481862177, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
=== ep: 248, time 27.652700185775757, eps 0.0010037026112480747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
=== ep: 249, time 31.68050241470337, eps 0.0010035220327666559, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
=== ep: 250, time 27.496474266052246, eps 0.0010033502612016988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
goal_identified
=== ep: 251, time 28.132970094680786, eps 0.001003186867034819, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 252, time 26.900369882583618, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
=== ep: 253, time 27.27159070968628, eps 0.0010028835965394094, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
=== ep: 254, time 27.50849437713623, eps 0.0010027429618766747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
=== ep: 255, time 27.676430463790894, eps 0.0010026091860473767, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
goal_identified
=== ep: 256, time 27.2819242477417, eps 0.0010024819345422614, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
=== ep: 257, time 27.566408395767212, eps 0.0010023608891662839, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 27.277674198150635, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 33.349053621292114, eps 0.0010021362208574892, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
=== ep: 260, time 27.601358890533447, eps 0.001002032036136876, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
=== ep: 261, time 27.638906955718994, eps 0.0010019329325650452, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
=== ep: 262, time 27.801796436309814, eps 0.0010018386623314465, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
goal_identified
goal_identified
=== ep: 263, time 27.617124557495117, eps 0.0010017489897113931, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
goal_identified
=== ep: 264, time 27.683679342269897, eps 0.0010016636904766263, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
=== ep: 265, time 27.80844497680664, eps 0.0010015825513346283, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 27.640709400177002, eps 0.0010015053693952815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
=== ep: 267, time 27.85981583595276, eps 0.0010014319516635345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 27.805302381515503, eps 0.0010013621145568167, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 31.160239934921265, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 27.581798791885376, eps 0.0010012324922186594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
=== ep: 271, time 27.599600076675415, eps 0.001001172382863857, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
=== ep: 272, time 28.055744409561157, eps 0.0010011152050768812, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 27.267725944519043, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 27.40058207511902, eps 0.0010010090792823456, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
=== ep: 275, time 27.800382137298584, eps 0.0010009598659050213, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 276, time 27.775139808654785, eps 0.0010009130526924313, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 27.413228273391724, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 27.72965669631958, eps 0.0010008261642407504, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 30.524938106536865, eps 0.001000785871735272, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
=== ep: 280, time 27.253612279891968, eps 0.0010007475443184742, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 28.05310297012329, eps 0.001000711086151851, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 27.850542306900024, eps 0.0010006764060709957, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 27.386377334594727, eps 0.001000643417357642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
=== ep: 284, time 27.621023893356323, eps 0.0010006120375228235, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
=== ep: 285, time 27.768980979919434, eps 0.0010005821881006083, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 286, time 27.36800456047058, eps 0.0010005537944518927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
=== ep: 287, time 27.559139728546143, eps 0.0010005267855777657, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
=== ep: 288, time 27.690281867980957, eps 0.0010005010939419733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
=== ep: 289, time 31.735828638076782, eps 0.001000476655302044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
=== ep: 290, time 27.590340614318848, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 27.87181282043457, eps 0.0010004312955527947, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 27.399658679962158, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
=== ep: 293, time 27.67497754096985, eps 0.0010003902523544011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
=== ep: 294, time 27.573832988739014, eps 0.0010003712195224871, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
goal_identified
=== ep: 295, time 27.45678734779358, eps 0.0010003531149327387, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
=== ep: 296, time 27.978446006774902, eps 0.0010003358933142518, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
=== ep: 297, time 27.71140766143799, eps 0.0010003195116040093, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 27.395832061767578, eps 0.0010003039288392032, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 31.206220865249634, eps 0.0010002891060548044, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
=== ep: 300, time 27.72258687019348, eps 0.0010002750061861312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
=== ep: 301, time 27.658827781677246, eps 0.0010002615939761676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
=== ep: 302, time 27.53368353843689, eps 0.001000248835887403, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 27.894516706466675, eps 0.0010002367000179694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
=== ep: 304, time 27.45321536064148, eps 0.0010002251560218723, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
goal_identified
=== ep: 305, time 27.451940774917603, eps 0.0010002141750331084, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
=== ep: 306, time 27.417379140853882, eps 0.0010002037295934862, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
=== ep: 307, time 27.606287002563477, eps 0.0010001937935839656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 27.502260446548462, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
goal_identified
=== ep: 309, time 31.31709885597229, eps 0.0010001753516861473, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
=== ep: 310, time 28.056304931640625, eps 0.0010001667996834991, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 27.48566460609436, eps 0.001000158664766942, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 312, time 27.289810180664062, eps 0.0010001509265949466, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
=== ep: 313, time 28.005101919174194, eps 0.001000143565818053, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
=== ep: 314, time 27.354628562927246, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 27.67122173309326, eps 0.0010001299037241253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
=== ep: 316, time 27.80018401145935, eps 0.0010001235682447402, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
=== ep: 317, time 28.108837366104126, eps 0.0010001175417503308, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
=== ep: 318, time 27.447291612625122, eps 0.0010001118091715218, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 32.249372243881226, eps 0.0010001063561738807, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
=== ep: 320, time 27.672107219696045, eps 0.0010001011691220727, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 321, time 27.399577379226685, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
=== ep: 322, time 28.228075981140137, eps 0.0010000915416072012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
=== ep: 323, time 27.861845016479492, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 27.675976991653442, eps 0.0010000828302715028, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 27.48900008201599, eps 0.0010000787905914928, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
=== ep: 326, time 27.335522651672363, eps 0.0010000749479290019, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
=== ep: 327, time 27.51048755645752, eps 0.001000071292675372, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 27.738842964172363, eps 0.001000067815690565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
goal_identified
=== ep: 329, time 30.142126083374023, eps 0.0010000645082803084, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
=== ep: 330, time 27.497341871261597, eps 0.0010000613621743532, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 27.282790899276733, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 27.673147916793823, eps 0.0010000555227914069, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 27.52753782272339, eps 0.0010000528149129166, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
=== ep: 334, time 27.36663508415222, eps 0.0010000502390992187, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 335, time 27.989598035812378, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
=== ep: 336, time 28.20602536201477, eps 0.0010000454582168217, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 27.557719230651855, eps 0.001000043241193426, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 27.65902590751648, eps 0.0010000411322955373, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
=== ep: 339, time 31.706156969070435, eps 0.0010000391262498123, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 28.022405862808228, eps 0.001000037218040092, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 28.225759744644165, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 27.915738821029663, eps 0.0010000336762753012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 27.986762046813965, eps 0.001000032033863974, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
goal_identified
=== ep: 344, time 27.682754039764404, eps 0.0010000304715539925, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
=== ep: 345, time 27.650973558425903, eps 0.001000028985438768, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 27.586363554000854, eps 0.001000027571802238, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
=== ep: 347, time 27.562591552734375, eps 0.0010000262271095755, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
goal_identified
=== ep: 348, time 27.12452006340027, eps 0.0010000249479983478, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
=== ep: 349, time 32.19743299484253, eps 0.0010000237312701107, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 350, time 27.837626695632935, eps 0.00100002257388241, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 27.72083616256714, eps 0.0010000214729411737, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 27.538385152816772, eps 0.0010000204256934752, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 27.951144218444824, eps 0.0010000194295206493, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
=== ep: 354, time 27.874443531036377, eps 0.0010000184819317455, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
=== ep: 355, time 27.17792844772339, eps 0.001000017580557298, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
=== ep: 356, time 27.436137914657593, eps 0.001000016723143401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
goal_identified
=== ep: 357, time 27.757999420166016, eps 0.0010000159075460732, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 27.57252860069275, eps 0.0010000151317258964, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 32.00783395767212, eps 0.0010000143937429161, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 27.907929182052612, eps 0.0010000136917517905, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
=== ep: 361, time 27.801498651504517, eps 0.001000013023997176, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 362, time 28.004201650619507, eps 0.0010000123888093385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
goal_identified
=== ep: 363, time 28.110164165496826, eps 0.0010000117845999773, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 364, time 27.989431381225586, eps 0.0010000112098582543, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
goal_identified
=== ep: 365, time 27.90025496482849, eps 0.001000010663147016, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 28.038849115371704, eps 0.0010000101430991996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
=== ep: 367, time 27.836312770843506, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
=== ep: 368, time 27.89790654182434, eps 0.0010000091778556905, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
=== ep: 369, time 30.804782390594482, eps 0.0010000087302463867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
goal_identified
=== ep: 370, time 27.427794694900513, eps 0.001000008304467246, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 371, time 28.043205976486206, eps 0.0010000078994535993, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
=== ep: 372, time 27.997981071472168, eps 0.0010000075141927012, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
goal_identified
=== ep: 373, time 27.450965642929077, eps 0.0010000071477211988, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 374, time 27.516059398651123, eps 0.0010000067991227223, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
=== ep: 375, time 27.710556745529175, eps 0.0010000064675255943, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 376, time 27.784271240234375, eps 0.001000006152100649, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 27.845267295837402, eps 0.0010000058520591598, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 28.07195782661438, eps 0.0010000055666508666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 379, time 31.51542067527771, eps 0.0010000052951621003, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
=== ep: 380, time 27.486302137374878, eps 0.0010000050369139975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
goal_identified
=== ep: 381, time 27.467164039611816, eps 0.001000004791260803, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
goal_identified
=== ep: 382, time 27.384713649749756, eps 0.0010000045575882562, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 383, time 28.018368244171143, eps 0.001000004335312054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
=== ep: 384, time 27.44208264350891, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 27.856306314468384, eps 0.0010000039227525655, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 27.993724584579468, eps 0.0010000037314376652, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
=== ep: 387, time 27.82957363128662, eps 0.001000003549453303, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
=== ep: 388, time 27.780988693237305, eps 0.0010000033763444226, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 389, time 31.545109748840332, eps 0.001000003211678162, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 27.41990089416504, eps 0.0010000030550427698, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
=== ep: 391, time 27.40030789375305, eps 0.0010000029060465757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
goal_identified
=== ep: 392, time 28.034247398376465, eps 0.0010000027643170119, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
=== ep: 393, time 27.463886737823486, eps 0.0010000026294996803, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
=== ep: 394, time 27.88745927810669, eps 0.0010000025012574677, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
goal_identified
=== ep: 395, time 27.855242252349854, eps 0.0010000023792697014, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
=== ep: 396, time 32.8319787979126, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
goal_identified
goal_identified
=== ep: 397, time 27.23256754875183, eps 0.0010000021528522535, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 27.65178942680359, eps 0.00100000204785641, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 31.757648944854736, eps 0.0010000019479812744, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 400, time 27.469273805618286, eps 0.0010000018529771066, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
goal_identified
=== ep: 401, time 27.792600870132446, eps 0.0010000017626063467, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
=== ep: 402, time 27.92505955696106, eps 0.0010000016766430208, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 27.789809465408325, eps 0.0010000015948721758, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
=== ep: 404, time 27.77336072921753, eps 0.001000001517089342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
=== ep: 405, time 28.197670936584473, eps 0.0010000014431000217, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 28.13286805152893, eps 0.001000001372719203, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
=== ep: 407, time 28.204293727874756, eps 0.0010000013057708975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 27.933552980422974, eps 0.0010000012420876994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
=== ep: 409, time 34.96654915809631, eps 0.0010000011815103674, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 410, time 27.738305807113647, eps 0.001000001123887427, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 411, time 27.832600831985474, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 28.019157886505127, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 27.442885875701904, eps 0.0010000009673388729, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
=== ep: 414, time 27.728150606155396, eps 0.0010000009201611994, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
goal_identified
=== ep: 415, time 27.49566912651062, eps 0.0010000008752844081, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 27.863118886947632, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
=== ep: 417, time 27.393266916275024, eps 0.001000000791990084, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
=== ep: 418, time 27.96852421760559, eps 0.0010000007533642718, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
goal_identified
=== ep: 419, time 31.000820636749268, eps 0.0010000007166222626, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 27.7910897731781, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 28.397742986679077, eps 0.001000000648426638, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 27.542340993881226, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 27.693451642990112, eps 0.0010000005867206849, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
=== ep: 424, time 27.8411922454834, eps 0.0010000005581059794, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 425, time 27.62484836578369, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
goal_identified
=== ep: 426, time 27.52252507209778, eps 0.0010000005049951733, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 427, time 28.304341316223145, eps 0.001000000480366268, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 28.342928171157837, eps 0.0010000004569385287, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 31.158190727233887, eps 0.0010000004346533736, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
=== ep: 430, time 28.33074450492859, eps 0.0010000004134550786, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
=== ep: 431, time 27.250784397125244, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
=== ep: 432, time 27.533770322799683, eps 0.0010000003741096257, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 27.96762228012085, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 27.72437286376953, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
=== ep: 435, time 28.18540906906128, eps 0.001000000321999139, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
=== ep: 436, time 27.92485785484314, eps 0.0010000003062950555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 27.742717266082764, eps 0.0010000002913568694, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
=== ep: 438, time 27.794670343399048, eps 0.0010000002771472273, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 439, time 33.24374270439148, eps 0.0010000002636305976, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
=== ep: 440, time 27.95161008834839, eps 0.0010000002507731815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 27.998989582061768, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
=== ep: 442, time 27.842316150665283, eps 0.0010000002269089582, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
=== ep: 443, time 28.096225023269653, eps 0.0010000002158424776, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
=== ep: 444, time 28.047513484954834, eps 0.0010000002053157158, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
goal_identified
=== ep: 445, time 27.895849227905273, eps 0.0010000001953023503, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
=== ep: 446, time 28.119919538497925, eps 0.001000000185777342, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 447, time 27.982457399368286, eps 0.0010000001767168742, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
=== ep: 448, time 28.12885046005249, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
=== ep: 449, time 33.13777685165405, eps 0.0010000001599000403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
=== ep: 450, time 28.263397932052612, eps 0.0010000001521016232, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
=== ep: 451, time 27.857852935791016, eps 0.0010000001446835395, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 27.778451681137085, eps 0.0010000001376272401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 27.77324151992798, eps 0.0010000001309150804, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
goal_identified
=== ep: 454, time 27.83192539215088, eps 0.0010000001245302765, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
=== ep: 455, time 27.668849229812622, eps 0.0010000001184568633, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
goal_identified
=== ep: 456, time 28.51491379737854, eps 0.0010000001126796538, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 457, time 27.601980447769165, eps 0.0010000001071842023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 27.683561325073242, eps 0.001000000101956767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 31.53414034843445, eps 0.001000000096984277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
=== ep: 460, time 28.076922178268433, eps 0.001000000092254298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
=== ep: 461, time 27.546299934387207, eps 0.0010000000877550027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 27.932839155197144, eps 0.0010000000834751407, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 28.169671058654785, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 28.173799991607666, eps 0.0010000000755314307, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
goal_identified
=== ep: 465, time 28.15955090522766, eps 0.0010000000718477194, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 28.02650260925293, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 27.799116373062134, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 27.57284951210022, eps 0.0010000000618399052, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 30.843239784240723, eps 0.0010000000588239375, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
=== ep: 470, time 27.850298643112183, eps 0.0010000000559550603, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
=== ep: 471, time 28.08356761932373, eps 0.0010000000532260998, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 472, time 27.87425708770752, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
=== ep: 473, time 28.152981996536255, eps 0.0010000000481609666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
=== ep: 474, time 27.84221386909485, eps 0.0010000000458121286, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 28.06581997871399, eps 0.0010000000435778447, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 476, time 28.093170642852783, eps 0.001000000041452528, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
=== ep: 477, time 27.883633852005005, eps 0.0010000000394308644, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 27.89910316467285, eps 0.0010000000375077985, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
=== ep: 479, time 34.523767948150635, eps 0.0010000000356785216, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 27.836540460586548, eps 0.0010000000339384595, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
=== ep: 481, time 24.049940586090088, eps 0.0010000000322832614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
goal_identified
=== ep: 482, time 27.6966392993927, eps 0.0010000000307087882, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
=== ep: 483, time 27.816453218460083, eps 0.001000000029211103, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 28.08033061027527, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
=== ep: 485, time 27.674316883087158, eps 0.0010000000264312988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
=== ep: 486, time 27.709386110305786, eps 0.0010000000251422292, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 27.597984790802002, eps 0.0010000000239160282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
=== ep: 488, time 27.796584129333496, eps 0.00100000002274963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
=== ep: 489, time 32.05029368400574, eps 0.0010000000216401172, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
=== ep: 490, time 27.854148864746094, eps 0.0010000000205847162, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 27.769832372665405, eps 0.0010000000195807877, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 26.425438165664673, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
goal_identified
=== ep: 493, time 27.721424341201782, eps 0.0010000000177174295, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
goal_identified
=== ep: 494, time 28.42892098426819, eps 0.0010000000168533404, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
=== ep: 495, time 28.46458601951599, eps 0.0010000000160313932, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 28.23252010345459, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 27.88057017326355, eps 0.0010000000145058043, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 27.897506713867188, eps 0.001000000013798348, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
goal_identified
=== ep: 499, time 31.02413582801819, eps 0.0010000000131253947, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 500, time 28.01850914955139, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 28.051750421524048, eps 0.0010000000118763482, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 28.32316780090332, eps 0.0010000000112971319, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 27.45186734199524, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 27.67092251777649, eps 0.0010000000102220676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 27.38741946220398, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
goal_identified
=== ep: 506, time 28.208727836608887, eps 0.0010000000092493092, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
goal_identified
=== ep: 507, time 28.021904706954956, eps 0.0010000000087982152, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 27.9218533039093, eps 0.0010000000083691212, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 32.250442028045654, eps 0.0010000000079609542, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
=== ep: 510, time 27.837400913238525, eps 0.001000000007572694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
=== ep: 511, time 28.143574237823486, eps 0.0010000000072033692, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 27.80981707572937, eps 0.001000000006852057, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
=== ep: 513, time 27.586556673049927, eps 0.001000000006517878, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
=== ep: 514, time 27.922231674194336, eps 0.0010000000061999974, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 27.92491126060486, eps 0.0010000000058976199, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
=== ep: 516, time 28.06020736694336, eps 0.0010000000056099897, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
=== ep: 517, time 27.798178672790527, eps 0.0010000000053363872, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
=== ep: 518, time 27.988545179367065, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
=== ep: 519, time 31.07384753227234, eps 0.001000000004828563, sum reward: 1, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
=== ep: 520, time 28.25845170021057, eps 0.001000000004593071, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
goal_identified
=== ep: 521, time 28.1101713180542, eps 0.0010000000043690644, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 28.0525119304657, eps 0.0010000000041559827, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
=== ep: 523, time 28.600333213806152, eps 0.0010000000039532928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
=== ep: 524, time 27.738430738449097, eps 0.0010000000037604885, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
=== ep: 525, time 27.953758716583252, eps 0.0010000000035770874, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
=== ep: 526, time 28.026474714279175, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
goal_identified
=== ep: 527, time 27.74961805343628, eps 0.0010000000032366824, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
=== ep: 528, time 28.036713123321533, eps 0.0010000000030788276, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
=== ep: 529, time 30.568180084228516, eps 0.0010000000029286714, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
=== ep: 530, time 28.21818447113037, eps 0.0010000000027858384, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
goal_identified
=== ep: 531, time 27.796568393707275, eps 0.0010000000026499714, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 532, time 28.087973594665527, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
=== ep: 533, time 27.76910948753357, eps 0.0010000000023977934, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
=== ep: 534, time 28.528801441192627, eps 0.0010000000022808515, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
=== ep: 535, time 27.973368167877197, eps 0.0010000000021696133, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
=== ep: 536, time 27.971633672714233, eps 0.0010000000020637999, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 27.981825828552246, eps 0.0010000000019631471, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
=== ep: 538, time 28.291227340698242, eps 0.0010000000018674034, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 539, time 32.21553158760071, eps 0.001000000001776329, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
=== ep: 540, time 27.51432514190674, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
=== ep: 541, time 27.772581577301025, eps 0.001000000001607289, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 542, time 27.734825372695923, eps 0.0010000000015289005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
=== ep: 543, time 33.777422189712524, eps 0.0010000000014543352, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
=== ep: 544, time 28.11704707145691, eps 0.0010000000013834064, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 27.54009246826172, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 546, time 27.898958683013916, eps 0.0010000000012517578, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 547, time 27.792731523513794, eps 0.001000000001190709, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
=== ep: 548, time 28.355926036834717, eps 0.0010000000011326374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 33.42742109298706, eps 0.001000000001077398, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 28.15171980857849, eps 0.0010000000010248527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
=== ep: 551, time 27.821338415145874, eps 0.00100000000097487, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 27.997719764709473, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
goal_identified
=== ep: 553, time 28.069597244262695, eps 0.0010000000008820989, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
=== ep: 554, time 27.673938751220703, eps 0.0010000000008390784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 27.618125915527344, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
goal_identified
=== ep: 556, time 27.696241855621338, eps 0.0010000000007592295, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 27.634373664855957, eps 0.0010000000007222014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 27.813349723815918, eps 0.0010000000006869794, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
goal_identified
=== ep: 559, time 31.39753746986389, eps 0.001000000000653475, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 28.008756399154663, eps 0.0010000000006216046, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 27.99153208732605, eps 0.0010000000005912885, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
goal_identified
=== ep: 562, time 27.853559732437134, eps 0.0010000000005624511, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
goal_identified
=== ep: 563, time 27.85132908821106, eps 0.00100000000053502, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
goal_identified
goal_identified
=== ep: 564, time 27.63857102394104, eps 0.001000000000508927, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 565, time 27.827214241027832, eps 0.001000000000484106, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
=== ep: 566, time 28.391891956329346, eps 0.001000000000460496, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 27.75699734687805, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 28.40018939971924, eps 0.001000000000416674, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 31.83670163154602, eps 0.0010000000003963527, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 27.944613218307495, eps 0.0010000000003770222, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
=== ep: 571, time 27.7509503364563, eps 0.0010000000003586346, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 27.354695081710815, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
=== ep: 573, time 28.105838298797607, eps 0.001000000000324506, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
=== ep: 574, time 28.449261903762817, eps 0.0010000000003086798, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 28.13168478012085, eps 0.0010000000002936252, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
=== ep: 576, time 28.045968055725098, eps 0.001000000000279305, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
=== ep: 577, time 28.21897554397583, eps 0.0010000000002656831, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
=== ep: 578, time 27.41788148880005, eps 0.0010000000002527256, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
=== ep: 579, time 32.96413779258728, eps 0.0010000000002404, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
goal_identified
goal_identified
=== ep: 580, time 27.950857400894165, eps 0.0010000000002286756, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 581, time 27.661846160888672, eps 0.0010000000002175229, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
=== ep: 582, time 28.236275911331177, eps 0.0010000000002069142, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
=== ep: 583, time 28.027156114578247, eps 0.0010000000001968228, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 584, time 28.077353954315186, eps 0.0010000000001872237, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 585, time 28.1832013130188, eps 0.0010000000001780928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
=== ep: 586, time 28.11079978942871, eps 0.001000000000169407, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
=== ep: 587, time 27.942026376724243, eps 0.001000000000161145, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
=== ep: 588, time 27.94949245452881, eps 0.0010000000001532858, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
=== ep: 589, time 30.32554841041565, eps 0.00100000000014581, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
=== ep: 590, time 27.602887630462646, eps 0.0010000000001386988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
goal_identified
=== ep: 591, time 27.76965856552124, eps 0.0010000000001319344, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 28.016892910003662, eps 0.0010000000001255, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
=== ep: 593, time 28.141984701156616, eps 0.0010000000001193791, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 27.85938048362732, eps 0.001000000000113557, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
=== ep: 595, time 28.00945234298706, eps 0.0010000000001080186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
=== ep: 596, time 27.810002088546753, eps 0.0010000000001027505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
=== ep: 597, time 28.04461145401001, eps 0.0010000000000977393, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 28.04689121246338, eps 0.0010000000000929725, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
=== ep: 599, time 31.244407415390015, eps 0.0010000000000884382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
=== ep: 600, time 27.53541588783264, eps 0.001000000000084125, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
=== ep: 601, time 27.586594104766846, eps 0.0010000000000800222, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
=== ep: 602, time 27.644473552703857, eps 0.0010000000000761195, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 603, time 27.63429594039917, eps 0.0010000000000724072, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
=== ep: 604, time 27.881950616836548, eps 0.0010000000000688757, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 28.10185146331787, eps 0.0010000000000655166, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
goal_identified
=== ep: 606, time 27.82061266899109, eps 0.0010000000000623215, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
=== ep: 607, time 28.27318000793457, eps 0.001000000000059282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
goal_identified
=== ep: 608, time 27.45407772064209, eps 0.0010000000000563907, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
=== ep: 609, time 32.30291247367859, eps 0.0010000000000536405, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 28.270313024520874, eps 0.0010000000000510245, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
=== ep: 611, time 27.59503149986267, eps 0.0010000000000485358, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
=== ep: 612, time 28.343016147613525, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
goal_identified
goal_identified
=== ep: 613, time 27.69334888458252, eps 0.0010000000000439171, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
goal_identified
=== ep: 614, time 27.663352966308594, eps 0.0010000000000417752, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 615, time 27.787919282913208, eps 0.0010000000000397378, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
=== ep: 616, time 27.870190858840942, eps 0.0010000000000377999, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 28.09086585044861, eps 0.0010000000000359563, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 618, time 26.83683943748474, eps 0.0010000000000342027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 31.73861265182495, eps 0.0010000000000325345, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
=== ep: 620, time 27.81009602546692, eps 0.001000000000030948, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
=== ep: 621, time 28.375894784927368, eps 0.0010000000000294385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
=== ep: 622, time 28.004173040390015, eps 0.0010000000000280028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
=== ep: 623, time 27.871787309646606, eps 0.0010000000000266371, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
=== ep: 624, time 27.595384120941162, eps 0.001000000000025338, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 28.21838879585266, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
=== ep: 626, time 27.352179527282715, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 627, time 28.327415704727173, eps 0.0010000000000218085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
=== ep: 628, time 27.636186361312866, eps 0.001000000000020745, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
=== ep: 629, time 31.546884536743164, eps 0.0010000000000197332, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
=== ep: 630, time 28.033568382263184, eps 0.0010000000000187708, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
=== ep: 631, time 28.037473917007446, eps 0.0010000000000178553, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 632, time 28.014468669891357, eps 0.0010000000000169845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
=== ep: 633, time 27.65776824951172, eps 0.0010000000000161562, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
=== ep: 634, time 28.001766681671143, eps 0.0010000000000153684, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 28.066588401794434, eps 0.0010000000000146188, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
=== ep: 636, time 27.790238857269287, eps 0.0010000000000139058, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
=== ep: 637, time 28.005829334259033, eps 0.0010000000000132275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 28.054094791412354, eps 0.0010000000000125824, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 31.5762197971344, eps 0.0010000000000119687, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 28.397794485092163, eps 0.001000000000011385, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
=== ep: 641, time 28.044981241226196, eps 0.00100000000001083, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
=== ep: 642, time 27.928467273712158, eps 0.0010000000000103017, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
=== ep: 643, time 27.65337562561035, eps 0.0010000000000097993, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 27.842981100082397, eps 0.0010000000000093213, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
=== ep: 645, time 27.886773347854614, eps 0.0010000000000088666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
=== ep: 646, time 27.46493649482727, eps 0.0010000000000084342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 27.793065786361694, eps 0.001000000000008023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
goal_identified
=== ep: 648, time 28.29152536392212, eps 0.0010000000000076317, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 32.35237789154053, eps 0.0010000000000072594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 27.91528034210205, eps 0.0010000000000069055, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 27.760414123535156, eps 0.0010000000000065686, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 27.87496256828308, eps 0.0010000000000062483, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 27.77573800086975, eps 0.0010000000000059436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
=== ep: 654, time 28.075478076934814, eps 0.0010000000000056537, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
=== ep: 655, time 27.922138452529907, eps 0.0010000000000053779, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 27.912196397781372, eps 0.0010000000000051157, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
=== ep: 657, time 27.811550617218018, eps 0.0010000000000048661, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
=== ep: 658, time 27.94718337059021, eps 0.001000000000004629, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
=== ep: 659, time 30.9807186126709, eps 0.0010000000000044032, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
goal_identified
=== ep: 660, time 27.622596502304077, eps 0.0010000000000041883, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 27.724528074264526, eps 0.001000000000003984, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
=== ep: 662, time 28.016675233840942, eps 0.0010000000000037897, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 27.85616183280945, eps 0.001000000000003605, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 27.79215693473816, eps 0.0010000000000034291, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
=== ep: 665, time 28.02353811264038, eps 0.001000000000003262, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
=== ep: 666, time 27.88215398788452, eps 0.0010000000000031028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
=== ep: 667, time 27.84268307685852, eps 0.0010000000000029514, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
=== ep: 668, time 27.905658960342407, eps 0.0010000000000028075, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 31.94707989692688, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
goal_identified
=== ep: 670, time 28.40470838546753, eps 0.0010000000000025403, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 671, time 27.90915322303772, eps 0.0010000000000024165, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
=== ep: 672, time 27.79905366897583, eps 0.0010000000000022985, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
=== ep: 673, time 27.992315530776978, eps 0.0010000000000021864, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 27.790846586227417, eps 0.00100000000000208, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 28.16875386238098, eps 0.0010000000000019785, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
goal_identified
=== ep: 676, time 27.841458559036255, eps 0.001000000000001882, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
=== ep: 677, time 27.99093747138977, eps 0.0010000000000017903, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
goal_identified
=== ep: 678, time 27.94615888595581, eps 0.0010000000000017029, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
=== ep: 679, time 31.44182515144348, eps 0.0010000000000016198, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 28.409223318099976, eps 0.0010000000000015409, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 27.766087770462036, eps 0.0010000000000014656, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 28.219701766967773, eps 0.0010000000000013943, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
=== ep: 683, time 28.000758409500122, eps 0.0010000000000013262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
goal_identified
=== ep: 684, time 28.06855297088623, eps 0.0010000000000012616, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
