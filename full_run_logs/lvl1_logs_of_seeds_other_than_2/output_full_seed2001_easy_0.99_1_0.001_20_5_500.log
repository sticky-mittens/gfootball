==> Playing in 11_vs_11_easy_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['attack', 'defend'])
==>Currently learning win_game to choose from above OTs.
==>using device cuda
==>critic has 2 layers and 3 hidden units.
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 0, time 25.962172985076904, eps 0.9, right preds for atk and def: 53/106 = 0.5, score_diff 5, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 1, time 25.929976224899292, eps 0.8561552526261419, right preds for atk and def: 52/108 = 0.48148148148148145, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 2, time 26.11041259765625, eps 0.8144488388143276, right preds for atk and def: 44/105 = 0.41904761904761906, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 3, time 28.24248194694519, eps 0.774776470806127, right preds for atk and def: 60/121 = 0.49586776859504134, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 4, time 28.14067769050598, eps 0.7370389470171057, right preds for atk and def: 56/124 = 0.45161290322580644, score_diff 2, tot learning steps 10 (total env steps 3001)
=== ep: 5, time 25.82301950454712, eps 0.701141903981193, right preds for atk and def: 21/42 = 0.5, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 6, time 28.87961745262146, eps 0.6669955803928644, right preds for atk and def: 56/125 = 0.448, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 7, time 29.2965087890625, eps 0.6345145926571234, right preds for atk and def: 90/202 = 0.44554455445544555, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 8, time 29.693325996398926, eps 0.6036177213860398, right preds for atk and def: 54/116 = 0.46551724137931033, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 9, time 29.563275575637817, eps 0.5742277083079742, right preds for atk and def: 33/66 = 0.5, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 10, time 33.453633308410645, eps 0.5462710630816575, right preds for atk and def: 69/166 = 0.41566265060240964, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 11, time 31.95107936859131, eps 0.5196778795320575, right preds for atk and def: 57/145 = 0.3931034482758621, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 12, time 32.43570160865784, eps 0.49438166084852986, right preds for atk and def: 63/168 = 0.375, score_diff 2, tot learning steps 10 (total env steps 3001)
=== ep: 13, time 36.283814430236816, eps 0.47031915330815344, right preds for atk and def: 59/151 = 0.39072847682119205, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 14, time 40.05005431175232, eps 0.4474301881084772, right preds for atk and def: 51/145 = 0.35172413793103446, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 15, time 34.264222383499146, eps 0.42565753091417224, right preds for atk and def: 35/91 = 0.38461538461538464, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 16, time 32.04033708572388, eps 0.4049467387413822, right preds for atk and def: 51/131 = 0.3893129770992366, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 17, time 40.051624059677124, eps 0.3852460238219053, right preds for atk and def: 51/182 = 0.2802197802197802, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 18, time 41.09825801849365, eps 0.3665061241067986, right preds for atk and def: 68/79 = 0.8607594936708861, score_diff 4, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 19, time 33.52030324935913, eps 0.3486801800855966, right preds for atk and def: 59/70 = 0.8428571428571429, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 17
goal_identified
goal_identified
=== ep: 20, time 36.41610288619995, eps 0.3317236176131267, right preds for atk and def: 65/75 = 0.8666666666666667, score_diff 2, tot learning steps 10 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies.py:453: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20 and we are deleting ep 14
goal_identified
goal_identified
goal_identified
=== ep: 21, time 39.42617416381836, eps 0.31559403645092865, right preds for atk and def: 52/59 = 0.8813559322033898, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 12
goal_identified
=== ep: 22, time 38.335323095321655, eps 0.3002511042445735, right preds for atk and def: 54/56 = 0.9642857142857143, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 15
goal_identified
goal_identified
=== ep: 23, time 31.75163221359253, eps 0.2856564556717689, right preds for atk and def: 70/82 = 0.8536585365853658, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 16
goal_identified
goal_identified
=== ep: 24, time 37.5813045501709, eps 0.27177359650906974, right preds for atk and def: 54/60 = 0.9, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 13
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 25, time 43.37128806114197, eps 0.2585678123773109, right preds for atk and def: 71/86 = 0.8255813953488372, score_diff 6, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 26, time 40.52563834190369, eps 0.24600608193757734, right preds for atk and def: 70/81 = 0.8641975308641975, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 10
goal_identified
goal_identified
goal_identified
=== ep: 27, time 31.444246768951416, eps 0.23405699432065646, right preds for atk and def: 68/74 = 0.918918918918919, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 2
goal_identified
goal_identified
goal_identified
=== ep: 28, time 34.84232687950134, eps 0.22269067058350425, right preds for atk and def: 69/74 = 0.9324324324324325, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 7
goal_identified
=== ep: 29, time 36.06029415130615, eps 0.2118786889963241, right preds for atk and def: 78/88 = 0.8863636363636364, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 6
goal_identified
goal_identified
=== ep: 30, time 34.622315406799316, eps 0.2015940139734384, right preds for atk and def: 72/76 = 0.9473684210526315, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 4
goal_identified
goal_identified
=== ep: 31, time 33.5311644077301, eps 0.191810928470242, right preds for atk and def: 59/67 = 0.8805970149253731, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 8
goal_identified
goal_identified
=== ep: 32, time 35.35668635368347, eps 0.1825049696771952, right preds for atk and def: 74/83 = 0.891566265060241, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 1
goal_identified
goal_identified
=== ep: 33, time 33.63171577453613, eps 0.17365286785005798, right preds for atk and def: 64/67 = 0.9552238805970149, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 3
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 34, time 31.643064975738525, eps 0.16523248812340846, right preds for atk and def: 66/67 = 0.9850746268656716, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 35, time 30.3920156955719, eps 0.15722277516195018, right preds for atk and def: 63/69 = 0.9130434782608695, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 36, time 37.99541974067688, eps 0.1496037005112063, right preds for atk and def: 71/81 = 0.8765432098765432, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 9
goal_identified
=== ep: 37, time 39.752737045288086, eps 0.14235621251595124, right preds for atk and def: 52/55 = 0.9454545454545454, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 25
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 38, time 34.435052156448364, eps 0.13546218868114893, right preds for atk and def: 47/48 = 0.9791666666666666, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 19
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 39, time 30.512518167495728, eps 0.1289043903562757, right preds for atk and def: 62/67 = 0.9253731343283582, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 23
goal_identified
=== ep: 40, time 27.735018253326416, eps 0.12266641962971482, right preds for atk and def: 32/32 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 18
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 41, time 32.53678917884827, eps 0.116732678325436, right preds for atk and def: 64/68 = 0.9411764705882353, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 26
goal_identified
goal_identified
goal_identified
=== ep: 42, time 30.43295669555664, eps 0.11108832899943073, right preds for atk and def: 73/77 = 0.948051948051948, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 20
goal_identified
=== ep: 43, time 34.798253297805786, eps 0.10571925783837377, right preds for atk and def: 70/75 = 0.9333333333333333, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 36
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 44, time 33.16174674034119, eps 0.10061203936773815, right preds for atk and def: 76/79 = 0.9620253164556962, score_diff 6, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 31
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 45, time 34.8739800453186, eps 0.09575390288111604, right preds for atk and def: 69/72 = 0.9583333333333334, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 21
goal_identified
=== ep: 46, time 34.695502519607544, eps 0.09113270050680057, right preds for atk and def: 68/71 = 0.9577464788732394, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 29
goal_identified
goal_identified
goal_identified
=== ep: 47, time 34.12870168685913, eps 0.08673687683177911, right preds for atk and def: 54/56 = 0.9642857142857143, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 48, time 33.30615425109863, eps 0.08255544000718185, right preds for atk and def: 61/62 = 0.9838709677419355, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 24
goal_identified
goal_identified
goal_identified
=== ep: 49, time 34.79339098930359, eps 0.07857793426293408, right preds for atk and def: 59/62 = 0.9516129032258065, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
=== ep: 50, time 32.16460299491882, eps 0.07479441376288502, right preds for atk and def: 75/79 = 0.9493670886075949, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 27
goal_identified
goal_identified
=== ep: 51, time 33.29962468147278, eps 0.0711954177350367, right preds for atk and def: 79/79 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 39
=== ep: 52, time 31.62391948699951, eps 0.06777194681468615, right preds for atk and def: 84/91 = 0.9230769230769231, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 52
=== ep: 53, time 31.151779890060425, eps 0.06451544054132621, right preds for atk and def: 62/62 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 28
=== ep: 54, time 35.05371642112732, eps 0.06141775595303503, right preds for atk and def: 72/74 = 0.972972972972973, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 43
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 55, time 34.56934189796448, eps 0.05847114722483011, right preds for atk and def: 71/76 = 0.9342105263157895, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 55
goal_identified
goal_identified
goal_identified
=== ep: 56, time 31.20222568511963, eps 0.05566824630007096, right preds for atk and def: 73/75 = 0.9733333333333334, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
=== ep: 57, time 30.801084995269775, eps 0.05300204446647978, right preds for atk and def: 82/82 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 37
goal_identified
goal_identified
goal_identified
=== ep: 58, time 32.52999782562256, eps 0.050465874830710106, right preds for atk and def: 77/78 = 0.9871794871794872, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 30
goal_identified
goal_identified
goal_identified
=== ep: 59, time 35.90791606903076, eps 0.04805339564764071, right preds for atk and def: 93/96 = 0.96875, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 42
goal_identified
goal_identified
=== ep: 60, time 35.81356859207153, eps 0.045758574462709686, right preds for atk and def: 93/95 = 0.9789473684210527, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 50
goal_identified
=== ep: 61, time 29.544398546218872, eps 0.043575673027635695, right preds for atk and def: 77/78 = 0.9871794871794872, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 49
goal_identified
goal_identified
goal_identified
=== ep: 62, time 33.304505825042725, eps 0.04149923295180846, right preds for atk and def: 52/53 = 0.9811320754716981, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 63, time 34.84736919403076, eps 0.03952406205346913, right preds for atk and def: 55/55 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 46
goal_identified
goal_identified
goal_identified
=== ep: 64, time 33.84335899353027, eps 0.03764522137655123, right preds for atk and def: 61/64 = 0.953125, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 64
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 65, time 28.74907898902893, eps 0.03585801284071809, right preds for atk and def: 89/90 = 0.9888888888888889, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 45
goal_identified
=== ep: 66, time 33.07522916793823, eps 0.034157967493714775, right preds for atk and def: 68/69 = 0.9855072463768116, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 44
goal_identified
goal_identified
goal_identified
=== ep: 67, time 33.59134793281555, eps 0.03254083433665968, right preds for atk and def: 59/59 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 22
goal_identified
goal_identified
goal_identified
=== ep: 68, time 33.544068574905396, eps 0.031002569694333147, right preds for atk and def: 64/65 = 0.9846153846153847, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 47
goal_identified
goal_identified
=== ep: 69, time 30.512632846832275, eps 0.02953932710388308, right preds for atk and def: 56/60 = 0.9333333333333333, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 70, time 36.29297113418579, eps 0.028147447696664333, right preds for atk and def: 80/82 = 0.975609756097561, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 59
goal_identified
goal_identified
=== ep: 71, time 37.53507351875305, eps 0.026823451049161253, right preds for atk and def: 91/91 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 54
goal_identified
goal_identified
=== ep: 72, time 36.100836992263794, eps 0.025564026480116013, right preds for atk and def: 55/56 = 0.9821428571428571, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 56
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 73, time 31.329421281814575, eps 0.02436602477210106, right preds for atk and def: 97/98 = 0.9897959183673469, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 70
goal_identified
goal_identified
=== ep: 74, time 33.36131286621094, eps 0.02322645029683511, right preds for atk and def: 58/60 = 0.9666666666666667, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 74
goal_identified
goal_identified
goal_identified
=== ep: 75, time 34.23448371887207, eps 0.02214245352455219, right preds for atk and def: 58/58 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 60
goal_identified
goal_identified
goal_identified
=== ep: 76, time 31.182847261428833, eps 0.02111132389869288, right preds for atk and def: 48/50 = 0.96, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 76
goal_identified
goal_identified
goal_identified
=== ep: 77, time 33.28968930244446, eps 0.020130483058101077, right preds for atk and def: 68/68 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 38
goal_identified
goal_identified
=== ep: 78, time 30.887934684753418, eps 0.019197478389778148, right preds for atk and def: 90/91 = 0.989010989010989, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 62
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 79, time 32.59241199493408, eps 0.018309976896072843, right preds for atk and def: 88/89 = 0.9887640449438202, score_diff 6, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 72
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 80, time 32.669087171554565, eps 0.017465759360972027, right preds for atk and def: 62/62 = 1.0, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 48
goal_identified
goal_identified
=== ep: 81, time 33.240870237350464, eps 0.01666271480090467, right preds for atk and def: 72/73 = 0.9863013698630136, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 68
goal_identified
goal_identified
goal_identified
=== ep: 82, time 33.62542366981506, eps 0.015898835186183367, right preds for atk and def: 90/90 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 34
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 83, time 35.90253758430481, eps 0.015172210419884185, right preds for atk and def: 76/77 = 0.987012987012987, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 66
goal_identified
goal_identified
=== ep: 84, time 36.30229043960571, eps 0.014481023561609456, right preds for atk and def: 61/61 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 81
goal_identified
goal_identified
=== ep: 85, time 36.10186529159546, eps 0.01382354628419033, right preds for atk and def: 78/78 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 83
goal_identified
goal_identified
goal_identified
=== ep: 86, time 32.331265687942505, eps 0.013198134551968641, right preds for atk and def: 86/86 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 58
goal_identified
goal_identified
=== ep: 87, time 31.20470643043518, eps 0.012603224509851407, right preds for atk and def: 76/77 = 0.987012987012987, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 87
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 88, time 33.838722467422485, eps 0.012037328572858524, right preds for atk and def: 69/69 = 1.0, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 89, time 33.97710394859314, eps 0.011499031706385502, right preds for atk and def: 64/64 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 79
goal_identified
goal_identified
goal_identified
=== ep: 90, time 32.34984111785889, eps 0.010986987887879832, right preds for atk and def: 72/73 = 0.9863013698630136, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 90
goal_identified
goal_identified
goal_identified
=== ep: 91, time 32.49138164520264, eps 0.010499916741083536, right preds for atk and def: 98/98 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 65
goal_identified
goal_identified
=== ep: 92, time 34.38686537742615, eps 0.010036600334425595, right preds for atk and def: 54/54 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 78
goal_identified
goal_identified
=== ep: 93, time 36.624839544296265, eps 0.00959588013555861, right preds for atk and def: 68/68 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 73
goal_identified
goal_identified
goal_identified
=== ep: 94, time 36.80380916595459, eps 0.009176654114424539, right preds for atk and def: 64/64 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 40
=== ep: 95, time 30.032492637634277, eps 0.00877787398760545, right preds for atk and def: 64/64 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 51
goal_identified
=== ep: 96, time 35.88156247138977, eps 0.008398542597069007, right preds for atk and def: 69/69 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 53
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 97, time 36.23536205291748, eps 0.008037711416753971, right preds for atk and def: 62/63 = 0.9841269841269841, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 97
goal_identified
goal_identified
goal_identified
=== ep: 98, time 32.014052867889404, eps 0.00769447818076098, right preds for atk and def: 74/75 = 0.9866666666666667, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 98
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 99, time 30.24511218070984, eps 0.007367984627217855, right preds for atk and def: 66/66 = 1.0, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 57
goal_identified
goal_identified
=== ep: 100, time 33.10073685646057, eps 0.007057414352177835, right preds for atk and def: 88/89 = 0.9887640449438202, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 100
goal_identified
goal_identified
goal_identified
=== ep: 101, time 33.40507364273071, eps 0.006761990768184489, right preds for atk and def: 60/60 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 63
goal_identified
=== ep: 102, time 30.720084190368652, eps 0.006480975162398559, right preds for atk and def: 69/69 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 67
goal_identified
=== ep: 103, time 33.799299240112305, eps 0.006213664849431085, right preds for atk and def: 58/58 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 71
goal_identified
=== ep: 104, time 35.68100118637085, eps 0.005959391414263934, right preds for atk and def: 61/61 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 75
=== ep: 105, time 37.075583934783936, eps 0.005717519040864065, right preds for atk and def: 68/68 = 1.0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 77
goal_identified
=== ep: 106, time 34.068625688552856, eps 0.005487442922312285, right preds for atk and def: 79/79 = 1.0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 80
goal_identified
goal_identified
goal_identified
=== ep: 107, time 33.08765149116516, eps 0.005268587748470919, right preds for atk and def: 70/70 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 82
goal_identified
=== ep: 108, time 30.474326372146606, eps 0.005060406267408787, right preds for atk and def: 70/70 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 84
goal_identified
=== ep: 109, time 30.031609296798706, eps 0.004862377916986354, right preds for atk and def: 60/61 = 0.9836065573770492, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 110, time 35.217283487319946, eps 0.004674007523179196, right preds for atk and def: 72/72 = 1.0, score_diff 5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 111, time 33.719475507736206, eps 0.004494824061885041, right preds for atk and def: 56/56 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 86
goal_identified
=== ep: 112, time 30.602762937545776, eps 0.0043243794811181555, right preds for atk and def: 71/71 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 113, time 30.80590295791626, eps 0.0041622475806460035, right preds for atk and def: 70/70 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 89
goal_identified
=== ep: 114, time 32.81372380256653, eps 0.0040080229462666735, right preds for atk and def: 65/65 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 91
goal_identified
goal_identified
goal_identified
=== ep: 115, time 37.43317174911499, eps 0.0038613199360621906, right preds for atk and def: 55/55 = 1.0, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 92
goal_identified
goal_identified
=== ep: 116, time 37.87344765663147, eps 0.003721771716092858, right preds for atk and def: 59/59 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 117, time 28.330503940582275, eps 0.0035890293431213305, right preds for atk and def: 60/60 = 1.0, score_diff 6, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 94
goal_identified
goal_identified
goal_identified
=== ep: 118, time 30.96110987663269, eps 0.0034627608920727634, right preds for atk and def: 84/84 = 1.0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 95
goal_identified
=== ep: 119, time 33.48806309700012, eps 0.00334265062604924, right preds for atk and def: 78/78 = 1.0, score_diff 1, tot learning steps 10 (total env steps 3001)
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
goal_identified
=== ep: 0, time 27.02523636817932, eps 0.9, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 1, time 27.20379877090454, eps 0.8561552526261419, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 2, time 27.939536809921265, eps 0.8144488388143276, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 3, time 27.920401096343994, eps 0.774776470806127, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 4, time 28.441657543182373, eps 0.7370389470171057, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 5, time 26.471596002578735, eps 0.701141903981193, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 6, time 30.97617530822754, eps 0.6669955803928644, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 7, time 31.717621564865112, eps 0.6345145926571234, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 8, time 31.397040128707886, eps 0.6036177213860398, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 9, time 31.408022165298462, eps 0.5742277083079742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 10, time 31.5949866771698, eps 0.5462710630816575, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 11, time 32.71692967414856, eps 0.5196778795320575, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 12, time 35.25613307952881, eps 0.49438166084852986, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 13, time 27.823697566986084, eps 0.47031915330815344, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 14, time 33.42081618309021, eps 0.4474301881084772, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 15, time 33.82745981216431, eps 0.42565753091417224, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
=== ep: 16, time 34.57114005088806, eps 0.4049467387413822, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 17, time 31.649893283843994, eps 0.3852460238219053, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 18, time 35.478577613830566, eps 0.3665061241067986, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 19, time 41.35823464393616, eps 0.3486801800855966, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
=== ep: 20, time 36.67137551307678, eps 0.3317236176131267, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
=== ep: 21, time 37.381062269210815, eps 0.31559403645092865, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
=== ep: 22, time 35.43138122558594, eps 0.3002511042445735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
goal_identified
=== ep: 23, time 35.32756590843201, eps 0.2856564556717689, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
=== ep: 24, time 37.559330701828, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
goal_identified
=== ep: 25, time 38.06187605857849, eps 0.2585678123773109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
=== ep: 26, time 33.72488975524902, eps 0.24600608193757734, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
=== ep: 27, time 37.04074549674988, eps 0.23405699432065646, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 28, time 39.60559964179993, eps 0.22269067058350425, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
goal_identified
goal_identified
goal_identified
=== ep: 29, time 42.499844551086426, eps 0.2118786889963241, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
goal_identified
=== ep: 30, time 36.24478316307068, eps 0.2015940139734384, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
=== ep: 31, time 41.29465675354004, eps 0.191810928470242, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
=== ep: 32, time 41.920793294906616, eps 0.1825049696771952, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
=== ep: 33, time 34.4901762008667, eps 0.17365286785005798, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
goal_identified
goal_identified
=== ep: 34, time 43.12198781967163, eps 0.16523248812340846, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
=== ep: 35, time 44.1785786151886, eps 0.15722277516195018, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
=== ep: 36, time 41.625696420669556, eps 0.1496037005112063, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
goal_identified
goal_identified
=== ep: 37, time 42.606297969818115, eps 0.14235621251595124, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
goal_identified
=== ep: 38, time 50.16132640838623, eps 0.13546218868114893, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
goal_identified
goal_identified
=== ep: 39, time 41.595317125320435, eps 0.1289043903562757, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
=== ep: 40, time 41.11895537376404, eps 0.12266641962971482, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
goal_identified
goal_identified
=== ep: 41, time 43.85561227798462, eps 0.116732678325436, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 42, time 42.806072473526, eps 0.11108832899943073, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
=== ep: 43, time 45.403276443481445, eps 0.10571925783837377, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
=== ep: 44, time 41.372472524642944, eps 0.10061203936773815, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
=== ep: 45, time 43.207255363464355, eps 0.09575390288111604, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
goal_identified
=== ep: 46, time 50.290183305740356, eps 0.09113270050680057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
=== ep: 47, time 47.17590689659119, eps 0.08673687683177911, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
goal_identified
=== ep: 48, time 45.04016160964966, eps 0.08255544000718185, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
=== ep: 49, time 46.39877128601074, eps 0.07857793426293408, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
=== ep: 50, time 46.6746609210968, eps 0.07479441376288502, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
goal_identified
goal_identified
=== ep: 51, time 45.00436186790466, eps 0.0711954177350367, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
=== ep: 52, time 46.74573993682861, eps 0.06777194681468615, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
goal_identified
goal_identified
goal_identified
=== ep: 53, time 46.26656484603882, eps 0.06451544054132621, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
goal_identified
=== ep: 54, time 50.09445381164551, eps 0.06141775595303503, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
=== ep: 55, time 50.84076237678528, eps 0.05847114722483011, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 56, time 49.26267147064209, eps 0.05566824630007096, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 57, time 46.55538630485535, eps 0.05300204446647978, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
=== ep: 58, time 53.75224542617798, eps 0.050465874830710106, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
=== ep: 59, time 48.92154383659363, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
=== ep: 60, time 44.884958028793335, eps 0.045758574462709686, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
=== ep: 61, time 56.507686138153076, eps 0.043575673027635695, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
=== ep: 62, time 57.21036767959595, eps 0.04149923295180846, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
goal_identified
=== ep: 63, time 56.44498634338379, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
goal_identified
goal_identified
=== ep: 64, time 48.971837520599365, eps 0.03764522137655123, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 57.11390829086304, eps 0.03585801284071809, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
=== ep: 66, time 49.828742265701294, eps 0.034157967493714775, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
goal_identified
goal_identified
=== ep: 67, time 52.28732776641846, eps 0.03254083433665968, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
goal_identified
goal_identified
=== ep: 68, time 61.10934400558472, eps 0.031002569694333147, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 69, time 51.20453071594238, eps 0.02953932710388308, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 70, time 53.95076608657837, eps 0.028147447696664333, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
=== ep: 71, time 55.849852323532104, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 47.14030575752258, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
=== ep: 73, time 53.8822762966156, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
=== ep: 74, time 56.96692132949829, eps 0.02322645029683511, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 75, time 59.1734185218811, eps 0.02214245352455219, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
=== ep: 76, time 54.28341245651245, eps 0.02111132389869288, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
=== ep: 77, time 49.7132408618927, eps 0.020130483058101077, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
goal_identified
=== ep: 78, time 53.12267255783081, eps 0.019197478389778148, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
=== ep: 79, time 56.65379571914673, eps 0.018309976896072843, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 80, time 55.03846287727356, eps 0.017465759360972027, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
goal_identified
=== ep: 81, time 58.66416025161743, eps 0.01666271480090467, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
goal_identified
goal_identified
goal_identified
=== ep: 82, time 55.348819971084595, eps 0.015898835186183367, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
=== ep: 83, time 58.523736238479614, eps 0.015172210419884185, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
=== ep: 84, time 50.7277889251709, eps 0.014481023561609456, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
goal_identified
=== ep: 85, time 60.17438292503357, eps 0.01382354628419033, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
goal_identified
goal_identified
=== ep: 86, time 57.27311563491821, eps 0.013198134551968641, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
=== ep: 87, time 45.58214020729065, eps 0.012603224509851407, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
goal_identified
goal_identified
=== ep: 88, time 67.27942323684692, eps 0.012037328572858524, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
=== ep: 89, time 58.30933713912964, eps 0.011499031706385502, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
goal_identified
=== ep: 90, time 45.50660800933838, eps 0.010986987887879832, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
goal_identified
goal_identified
=== ep: 91, time 62.210222244262695, eps 0.010499916741083536, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
goal_identified
=== ep: 92, time 59.36865162849426, eps 0.010036600334425595, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
=== ep: 93, time 54.29830813407898, eps 0.00959588013555861, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 94, time 62.70558762550354, eps 0.009176654114424539, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 95, time 66.75176668167114, eps 0.00877787398760545, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
goal_identified
=== ep: 96, time 58.35214424133301, eps 0.008398542597069007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
goal_identified
=== ep: 97, time 58.2624831199646, eps 0.008037711416753971, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
goal_identified
goal_identified
=== ep: 98, time 60.131569385528564, eps 0.00769447818076098, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
=== ep: 99, time 55.682252168655396, eps 0.007367984627217855, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
goal_identified
goal_identified
goal_identified
=== ep: 100, time 62.33624768257141, eps 0.007057414352177835, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
goal_identified
=== ep: 101, time 73.09917616844177, eps 0.006761990768184489, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
goal_identified
=== ep: 102, time 63.88313388824463, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
=== ep: 103, time 64.41888165473938, eps 0.006213664849431085, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
=== ep: 104, time 51.80693483352661, eps 0.005959391414263934, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
goal_identified
=== ep: 105, time 63.35410785675049, eps 0.005717519040864065, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
goal_identified
goal_identified
=== ep: 106, time 58.236074924468994, eps 0.005487442922312285, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
=== ep: 107, time 69.86113667488098, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
goal_identified
goal_identified
=== ep: 108, time 64.95398020744324, eps 0.005060406267408787, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 109, time 59.077576637268066, eps 0.004862377916986354, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 110, time 68.03276085853577, eps 0.004674007523179196, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
=== ep: 111, time 62.027055501937866, eps 0.004494824061885041, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
=== ep: 112, time 63.68031287193298, eps 0.0043243794811181555, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
=== ep: 113, time 70.35867834091187, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 66.89475202560425, eps 0.0040080229462666735, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
goal_identified
=== ep: 115, time 55.97630000114441, eps 0.0038613199360621906, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
=== ep: 116, time 63.861204385757446, eps 0.003721771716092858, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
=== ep: 117, time 58.660950899124146, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
goal_identified
goal_identified
=== ep: 118, time 69.96273612976074, eps 0.0034627608920727634, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
=== ep: 119, time 75.31593251228333, eps 0.00334265062604924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
goal_identified
=== ep: 120, time 52.42806792259216, eps 0.0032283982068230565, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 71.26747584342957, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 61.911479234695435, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
=== ep: 123, time 63.716637134552, eps 0.0029180001112638996, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
=== ep: 124, time 69.57393050193787, eps 0.002824458142029865, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 70.44652032852173, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 126, time 65.8092987537384, eps 0.0026508379945489875, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
=== ep: 127, time 68.75301146507263, eps 0.0025703256754987464, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
goal_identified
=== ep: 128, time 67.07499361038208, eps 0.0024937399885833667, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
=== ep: 129, time 67.42051291465759, eps 0.0024208894296938593, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
=== ep: 130, time 77.32633900642395, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
goal_identified
goal_identified
=== ep: 131, time 65.61166262626648, eps 0.002285673922878779, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
goal_identified
goal_identified
=== ep: 132, time 75.20906233787537, eps 0.0022229708657555565, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
goal_identified
=== ep: 133, time 64.9260790348053, eps 0.0021633258728137976, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
=== ep: 134, time 71.48420739173889, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 135, time 87.02836227416992, eps 0.002052620779091266, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 136, time 63.43958520889282, eps 0.0020012838579124784, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 78.6142110824585, eps 0.0019524506679239415, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
=== ep: 138, time 74.13164186477661, eps 0.001905999100714611, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 139, time 74.60321927070618, eps 0.001861813003170924, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
goal_identified
goal_identified
goal_identified
=== ep: 140, time 98.04632377624512, eps 0.0018197818870335101, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
goal_identified
=== ep: 141, time 77.52163314819336, eps 0.0017798006526189953, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
goal_identified
=== ep: 142, time 83.45244932174683, eps 0.0017417693260160481, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
=== ep: 143, time 84.58934211730957, eps 0.0017055928090985275, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
=== ep: 144, time 90.3397388458252, eps 0.0016711806417306348, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
goal_identified
=== ep: 145, time 98.10178351402283, eps 0.0016384467755694515, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 73.30414319038391, eps 0.0016073093588992661, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
goal_identified
goal_identified
=== ep: 147, time 79.97237610816956, eps 0.0015776905319596466, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
goal_identified
goal_identified
=== ep: 148, time 80.4893319606781, eps 0.0015495162322554856, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
goal_identified
=== ep: 149, time 76.90609908103943, eps 0.0015227160093621863, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
goal_identified
=== ep: 150, time 95.55723214149475, eps 0.0014972228487629025, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 70.5571072101593, eps 0.0014729730042773413, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
=== ep: 152, time 83.74180912971497, eps 0.001449905838663109, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 77.93654990196228, eps 0.00142796367199102, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 154, time 91.85756516456604, eps 0.0014070916374152305, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
=== ep: 155, time 90.6133816242218, eps 0.001387237543977543, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
=== ep: 156, time 76.9702935218811, eps 0.0013683517461028282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 82.4868221282959, eps 0.0013503870194592265, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
=== ep: 158, time 82.7764036655426, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
=== ep: 159, time 89.00408720970154, eps 0.001317043286000802, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
goal_identified
=== ep: 160, time 87.82894158363342, eps 0.0013015809024843582, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 74.14002180099487, eps 0.0012868726283106018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
goal_identified
goal_identified
=== ep: 162, time 85.26754140853882, eps 0.0012728816851329014, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
goal_identified
=== ep: 163, time 81.69087409973145, eps 0.0012595730883057546, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
=== ep: 164, time 84.52755308151245, eps 0.001246913559404956, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
goal_identified
goal_identified
=== ep: 165, time 87.83271741867065, eps 0.0012348714430141991, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
=== ep: 166, time 70.1661958694458, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
goal_identified
=== ep: 167, time 82.64891457557678, eps 0.001212520470067348, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
=== ep: 168, time 72.36439633369446, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
goal_identified
=== ep: 169, time 86.7095718383789, eps 0.0011922964734155277, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
goal_identified
goal_identified
goal_identified
=== ep: 170, time 91.0429630279541, eps 0.001182918063740569, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 171, time 76.21562123298645, eps 0.0011739970445027263, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
goal_identified
=== ep: 172, time 79.4988853931427, eps 0.0011655111085071537, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
=== ep: 173, time 88.8990523815155, eps 0.001157439036493735, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
goal_identified
=== ep: 174, time 73.52967500686646, eps 0.0011497606440778825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
=== ep: 175, time 93.0407395362854, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
goal_identified
=== ep: 176, time 78.3095350265503, eps 0.0011355090345108335, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
=== ep: 177, time 91.14045190811157, eps 0.0011289001809123877, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
goal_identified
goal_identified
=== ep: 178, time 81.47378516197205, eps 0.0011226136449073282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
goal_identified
goal_identified
goal_identified
=== ep: 179, time 101.03017163276672, eps 0.001116633706881133, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
=== ep: 180, time 86.24045276641846, eps 0.001110945413873925, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
goal_identified
=== ep: 181, time 81.10643887519836, eps 0.001105534542190287, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
goal_identified
goal_identified
=== ep: 182, time 78.7210841178894, eps 0.0011003875618326132, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
goal_identified
goal_identified
=== ep: 183, time 76.55023193359375, eps 0.0010954916026690664, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
goal_identified
=== ep: 184, time 103.25907707214355, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
=== ep: 185, time 86.38053131103516, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
goal_identified
goal_identified
=== ep: 186, time 82.7916054725647, eps 0.0010821903840988777, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
goal_identified
goal_identified
goal_identified
=== ep: 187, time 88.31964159011841, eps 0.0010781819117658682, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
goal_identified
goal_identified
=== ep: 188, time 71.34655475616455, eps 0.0010743689349354123, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
goal_identified
goal_identified
=== ep: 189, time 105.1187527179718, eps 0.0010707419191793434, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
goal_identified
goal_identified
goal_identified
=== ep: 190, time 69.20326375961304, eps 0.0010672917950690429, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
=== ep: 191, time 93.2811496257782, eps 0.0010640099354971456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 73.40032577514648, eps 0.0010608881341052777, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 88.1063551902771, eps 0.0010579185847638855, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
goal_identified
=== ep: 194, time 97.14007782936096, eps 0.0010550938620528466, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
goal_identified
goal_identified
=== ep: 195, time 82.61282730102539, eps 0.001052406902694051, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
goal_identified
goal_identified
=== ep: 196, time 90.53928279876709, eps 0.001049850987889527, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
goal_identified
=== ep: 197, time 81.59481501579285, eps 0.0010474197265209469, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
=== ep: 198, time 105.40196347236633, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
goal_identified
goal_identified
=== ep: 199, time 76.07029867172241, eps 0.001042907142909185, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 200, time 91.70481300354004, eps 0.001040814536856474, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
goal_identified
=== ep: 201, time 83.59320044517517, eps 0.0010388239884052469, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
goal_identified
goal_identified
goal_identified
=== ep: 202, time 92.60104036331177, eps 0.0010369305201475454, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
=== ep: 203, time 87.57198286056519, eps 0.0010351293974264616, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
goal_identified
=== ep: 204, time 91.36276769638062, eps 0.00103341611649703, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 75.44951224327087, eps 0.0010317863932645186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
goal_identified
goal_identified
=== ep: 206, time 92.9065933227539, eps 0.0010302361525719613, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
=== ep: 207, time 97.65988612174988, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
goal_identified
goal_identified
=== ep: 208, time 99.32015061378479, eps 0.001027358802224555, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
=== ep: 209, time 81.24549674987793, eps 0.0010260244976950921, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
goal_identified
=== ep: 210, time 94.15071177482605, eps 0.0010247552679654227, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
goal_identified
=== ep: 211, time 99.98214221000671, eps 0.00102354793930011, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
goal_identified
=== ep: 212, time 101.13975238800049, eps 0.0010223994927486214, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
goal_identified
=== ep: 213, time 89.7933578491211, eps 0.001021307056596379, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
goal_identified
=== ep: 214, time 95.68175077438354, eps 0.0010202678991839778, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 215, time 90.73224020004272, eps 0.0010192794220766138, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
goal_identified
goal_identified
=== ep: 216, time 102.13669157028198, eps 0.0010183391535666436, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
=== ep: 217, time 87.97283864021301, eps 0.0010174447424930286, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
goal_identified
=== ep: 218, time 94.57406830787659, eps 0.0010165939523622068, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
goal_identified
goal_identified
=== ep: 219, time 94.0333354473114, eps 0.0010157846557556941, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 104.0545437335968, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
goal_identified
=== ep: 221, time 100.3098452091217, eps 0.0010142825471585687, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
goal_identified
goal_identified
=== ep: 222, time 85.7414493560791, eps 0.0010135859791140496, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
=== ep: 223, time 95.03319191932678, eps 0.0010129233830939361, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
goal_identified
goal_identified
=== ep: 224, time 117.56433844566345, eps 0.0010122931022630473, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
goal_identified
=== ep: 225, time 104.80653119087219, eps 0.001011693560591007, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
goal_identified
=== ep: 226, time 97.94995546340942, eps 0.0010111232589113477, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
goal_identified
goal_identified
=== ep: 227, time 99.03774046897888, eps 0.0010105807711728136, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
=== ep: 228, time 113.9799735546112, eps 0.0010100647408734893, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 90.97327280044556, eps 0.001009573877668838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 99.21353077888489, eps 0.001009106954145169, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
=== ep: 231, time 97.83201694488525, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
goal_identified
=== ep: 232, time 114.66509056091309, eps 0.0010082403128748867, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
goal_identified
=== ep: 233, time 102.15364742279053, eps 0.0010078384280736842, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
goal_identified
=== ep: 234, time 93.96007680892944, eps 0.001007456143425521, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
goal_identified
goal_identified
=== ep: 235, time 88.75072121620178, eps 0.001007092503019653, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
=== ep: 236, time 111.69961357116699, eps 0.001006746597565654, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
goal_identified
=== ep: 237, time 95.29757857322693, eps 0.001006417562119715, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
goal_identified
goal_identified
=== ep: 238, time 94.28651261329651, eps 0.0010061045739218342, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
=== ep: 239, time 82.57011413574219, eps 0.0010058068503384884, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
goal_identified
=== ep: 240, time 98.25010085105896, eps 0.001005523646905642, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
=== ep: 241, time 99.07252502441406, eps 0.001005254255467199, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
=== ep: 242, time 105.13155436515808, eps 0.0010049980024042435, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
=== ep: 243, time 85.15293073654175, eps 0.0010047542469506416, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
goal_identified
goal_identified
=== ep: 244, time 99.40320634841919, eps 0.0010045223795907931, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
goal_identified
=== ep: 245, time 110.62397265434265, eps 0.001004301820535524, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 89.77916669845581, eps 0.0010040920182723119, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
=== ep: 247, time 90.24438071250916, eps 0.0010038924481862177, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
=== ep: 248, time 95.42994260787964, eps 0.0010037026112480747, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
=== ep: 249, time 107.40057063102722, eps 0.0010035220327666559, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
goal_identified
=== ep: 250, time 91.9670557975769, eps 0.0010033502612016988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
=== ep: 251, time 92.63499212265015, eps 0.001003186867034819, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
=== ep: 252, time 96.88541579246521, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
goal_identified
goal_identified
=== ep: 253, time 107.62180423736572, eps 0.0010028835965394094, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
goal_identified
=== ep: 254, time 99.00014400482178, eps 0.0010027429618766747, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
goal_identified
goal_identified
=== ep: 255, time 79.11252331733704, eps 0.0010026091860473767, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
goal_identified
goal_identified
=== ep: 256, time 98.7391631603241, eps 0.0010024819345422614, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
=== ep: 257, time 100.25672149658203, eps 0.0010023608891662839, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
goal_identified
=== ep: 258, time 96.71720099449158, eps 0.001002245747242954, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
goal_identified
=== ep: 259, time 78.40178513526917, eps 0.0010021362208574892, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
=== ep: 260, time 101.15108728408813, eps 0.001002032036136876, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
=== ep: 261, time 95.08514952659607, eps 0.0010019329325650452, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
goal_identified
=== ep: 262, time 94.8339295387268, eps 0.0010018386623314465, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
goal_identified
=== ep: 263, time 92.02357935905457, eps 0.0010017489897113931, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
=== ep: 264, time 88.92895936965942, eps 0.0010016636904766263, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
goal_identified
goal_identified
=== ep: 265, time 100.24167537689209, eps 0.0010015825513346283, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
goal_identified
=== ep: 266, time 98.17935109138489, eps 0.0010015053693952815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 89.65132474899292, eps 0.0010014319516635345, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
=== ep: 268, time 88.48338508605957, eps 0.0010013621145568167, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
goal_identified
goal_identified
=== ep: 269, time 92.60948753356934, eps 0.0010012956834459848, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
=== ep: 270, time 107.65728402137756, eps 0.0010012324922186594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
=== ep: 271, time 78.63285684585571, eps 0.001001172382863857, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 97.82516598701477, eps 0.0010011152050768812, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
goal_identified
goal_identified
=== ep: 273, time 84.18943643569946, eps 0.0010010608158834819, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
goal_identified
goal_identified
=== ep: 274, time 111.91690325737, eps 0.0010010090792823456, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
=== ep: 275, time 92.3511815071106, eps 0.0010009598659050213, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
=== ep: 276, time 96.285804271698, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
goal_identified
goal_identified
