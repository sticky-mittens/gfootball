==> Playing in 11_vs_11_easy_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 28.361366033554077, eps 0.9, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
goal_identified
goal_identified
goal_identified
=== ep: 1, time 28.298506021499634, eps 0.8561552526261419, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
goal_identified
goal_identified
=== ep: 2, time 29.03222966194153, eps 0.8144488388143276, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
=== ep: 3, time 36.053138256073, eps 0.774776470806127, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
goal_identified
goal_identified
goal_identified
=== ep: 4, time 32.055720806121826, eps 0.7370389470171057, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 5, time 30.58277201652527, eps 0.701141903981193, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
goal_identified
goal_identified
=== ep: 6, time 32.03013372421265, eps 0.6669955803928644, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 7, time 32.332172870635986, eps 0.6345145926571234, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
goal_identified
=== ep: 8, time 33.03088021278381, eps 0.6036177213860398, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
goal_identified
goal_identified
=== ep: 9, time 33.3697292804718, eps 0.5742277083079742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
=== ep: 10, time 34.457777976989746, eps 0.5462710630816575, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
goal_identified
goal_identified
=== ep: 11, time 34.58069443702698, eps 0.5196778795320575, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 12, time 35.19323205947876, eps 0.49438166084852986, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
=== ep: 13, time 35.31488013267517, eps 0.47031915330815344, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
goal_identified
goal_identified
=== ep: 14, time 35.58226656913757, eps 0.4474301881084772, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 15, time 35.82183027267456, eps 0.42565753091417224, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
goal_identified
=== ep: 16, time 37.50255870819092, eps 0.4049467387413822, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
=== ep: 17, time 44.44362258911133, eps 0.3852460238219053, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 18, time 37.557149171829224, eps 0.3665061241067986, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
goal_identified
=== ep: 19, time 37.9971604347229, eps 0.3486801800855966, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
=== ep: 20, time 37.92401933670044, eps 0.3317236176131267, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 21, time 37.495912075042725, eps 0.31559403645092865, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 22, time 38.58908748626709, eps 0.3002511042445735, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
=== ep: 23, time 37.631529569625854, eps 0.2856564556717689, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
=== ep: 24, time 38.970083713531494, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
goal_identified
goal_identified
=== ep: 25, time 39.2602744102478, eps 0.2585678123773109, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 26, time 39.90900015830994, eps 0.24600608193757734, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
goal_identified
goal_identified
goal_identified
=== ep: 27, time 40.12959671020508, eps 0.23405699432065646, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
goal_identified
=== ep: 28, time 40.85132074356079, eps 0.22269067058350425, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
=== ep: 29, time 40.29054236412048, eps 0.2118786889963241, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
=== ep: 30, time 39.55074954032898, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
goal_identified
goal_identified
goal_identified
=== ep: 31, time 39.66460680961609, eps 0.191810928470242, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
goal_identified
=== ep: 32, time 40.43713712692261, eps 0.1825049696771952, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
=== ep: 33, time 40.332906007766724, eps 0.17365286785005798, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
=== ep: 34, time 40.24350595474243, eps 0.16523248812340846, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
goal_identified
=== ep: 35, time 40.242077350616455, eps 0.15722277516195018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
goal_identified
goal_identified
=== ep: 36, time 41.18339920043945, eps 0.1496037005112063, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
goal_identified
goal_identified
goal_identified
=== ep: 37, time 41.95903730392456, eps 0.14235621251595124, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 38, time 42.56301712989807, eps 0.13546218868114893, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
=== ep: 39, time 43.535210371017456, eps 0.1289043903562757, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
=== ep: 40, time 42.39420795440674, eps 0.12266641962971482, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
goal_identified
goal_identified
=== ep: 41, time 42.85491633415222, eps 0.116732678325436, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
goal_identified
goal_identified
goal_identified
=== ep: 42, time 41.39959096908569, eps 0.11108832899943073, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 43, time 39.30718779563904, eps 0.10571925783837377, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 44, time 37.690929889678955, eps 0.10061203936773815, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
goal_identified
goal_identified
=== ep: 45, time 37.56017518043518, eps 0.09575390288111604, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 46, time 37.034255504608154, eps 0.09113270050680057, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
goal_identified
goal_identified
goal_identified
=== ep: 47, time 36.87402033805847, eps 0.08673687683177911, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 48, time 36.94951844215393, eps 0.08255544000718185, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 49, time 37.28808236122131, eps 0.07857793426293408, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 50, time 37.18293833732605, eps 0.07479441376288502, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 51, time 37.403870820999146, eps 0.0711954177350367, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 52, time 37.640806674957275, eps 0.06777194681468615, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
goal_identified
=== ep: 53, time 37.04035043716431, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 54, time 37.57965326309204, eps 0.06141775595303503, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
=== ep: 55, time 36.950873136520386, eps 0.05847114722483011, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 56, time 37.54594659805298, eps 0.05566824630007096, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
goal_identified
goal_identified
=== ep: 57, time 37.33155870437622, eps 0.05300204446647978, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
goal_identified
goal_identified
goal_identified
=== ep: 58, time 37.31983017921448, eps 0.050465874830710106, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
=== ep: 59, time 38.08962059020996, eps 0.04805339564764071, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 60, time 38.04399800300598, eps 0.045758574462709686, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
=== ep: 61, time 39.20123362541199, eps 0.043575673027635695, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
goal_identified
=== ep: 62, time 37.64814805984497, eps 0.04149923295180846, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
goal_identified
goal_identified
goal_identified
=== ep: 63, time 37.69096398353577, eps 0.03952406205346913, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
goal_identified
goal_identified
goal_identified
=== ep: 64, time 37.82929229736328, eps 0.03764522137655123, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
=== ep: 65, time 40.25316882133484, eps 0.03585801284071809, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 66, time 37.430118560791016, eps 0.034157967493714775, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 67, time 37.001869916915894, eps 0.03254083433665968, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
=== ep: 68, time 38.19586992263794, eps 0.031002569694333147, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 69, time 37.097079277038574, eps 0.02953932710388308, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
goal_identified
goal_identified
=== ep: 70, time 37.96054458618164, eps 0.028147447696664333, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 39.28764367103577, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
goal_identified
=== ep: 72, time 37.57627463340759, eps 0.025564026480116013, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 73, time 37.26920461654663, eps 0.02436602477210106, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
goal_identified
goal_identified
=== ep: 74, time 38.13844347000122, eps 0.02322645029683511, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
goal_identified
goal_identified
=== ep: 75, time 37.54176330566406, eps 0.02214245352455219, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 76, time 38.11833167076111, eps 0.02111132389869288, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 77, time 38.80985736846924, eps 0.020130483058101077, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
=== ep: 78, time 38.119877099990845, eps 0.019197478389778148, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
goal_identified
goal_identified
goal_identified
=== ep: 79, time 38.57205295562744, eps 0.018309976896072843, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
=== ep: 80, time 38.23802900314331, eps 0.017465759360972027, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
goal_identified
=== ep: 81, time 38.650575399398804, eps 0.01666271480090467, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
goal_identified
goal_identified
=== ep: 82, time 38.72920846939087, eps 0.015898835186183367, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 83, time 39.166786670684814, eps 0.015172210419884185, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 84, time 38.97211980819702, eps 0.014481023561609456, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
goal_identified
goal_identified
=== ep: 85, time 39.19799304008484, eps 0.01382354628419033, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 86, time 39.437989234924316, eps 0.013198134551968641, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 87, time 38.66228270530701, eps 0.012603224509851407, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 88, time 39.44899606704712, eps 0.012037328572858524, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
goal_identified
=== ep: 89, time 39.722888231277466, eps 0.011499031706385502, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 90, time 39.02741622924805, eps 0.010986987887879832, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 91, time 39.28560447692871, eps 0.010499916741083536, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 92, time 39.5726752281189, eps 0.010036600334425595, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
goal_identified
=== ep: 93, time 39.30351376533508, eps 0.00959588013555861, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 94, time 39.59209394454956, eps 0.009176654114424539, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
goal_identified
goal_identified
=== ep: 95, time 39.574585914611816, eps 0.00877787398760545, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
goal_identified
goal_identified
goal_identified
=== ep: 96, time 39.3886034488678, eps 0.008398542597069007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
=== ep: 97, time 39.608694076538086, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
=== ep: 98, time 40.20886826515198, eps 0.00769447818076098, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 99, time 39.59328269958496, eps 0.007367984627217855, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
goal_identified
goal_identified
=== ep: 100, time 39.803916215896606, eps 0.007057414352177835, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 101, time 39.624751329422, eps 0.006761990768184489, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
goal_identified
goal_identified
=== ep: 102, time 39.44489550590515, eps 0.006480975162398559, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
=== ep: 103, time 40.04025936126709, eps 0.006213664849431085, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 104, time 39.685887575149536, eps 0.005959391414263934, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 105, time 39.599273443222046, eps 0.005717519040864065, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 106, time 39.65377616882324, eps 0.005487442922312285, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 107, time 39.7182035446167, eps 0.005268587748470919, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 108, time 39.636862993240356, eps 0.005060406267408787, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 109, time 40.003251791000366, eps 0.004862377916986354, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
goal_identified
goal_identified
=== ep: 110, time 39.67120337486267, eps 0.004674007523179196, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 111, time 40.00704646110535, eps 0.004494824061885041, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 112, time 40.179216384887695, eps 0.0043243794811181555, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
goal_identified
goal_identified
=== ep: 113, time 39.47390079498291, eps 0.0041622475806460035, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
goal_identified
goal_identified
=== ep: 114, time 39.54917597770691, eps 0.0040080229462666735, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
goal_identified
=== ep: 115, time 38.94991755485535, eps 0.0038613199360621906, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 116, time 39.554892778396606, eps 0.003721771716092858, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 117, time 39.50344371795654, eps 0.0035890293431213305, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 118, time 39.77034282684326, eps 0.0034627608920727634, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 119, time 39.39768743515015, eps 0.00334265062604924, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 120, time 39.413167238235474, eps 0.0032283982068230565, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 121, time 39.38661050796509, eps 0.0031197179438347193, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 122, time 39.60466408729553, eps 0.0030163380798177374, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
goal_identified
goal_identified
=== ep: 123, time 39.09855031967163, eps 0.0029180001112638996, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 124, time 38.10056805610657, eps 0.002824458142029865, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
goal_identified
=== ep: 125, time 38.285250186920166, eps 0.0027354782684687108, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 126, time 37.92153024673462, eps 0.0026508379945489875, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 127, time 38.120277404785156, eps 0.0025703256754987464, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
=== ep: 128, time 37.70516037940979, eps 0.0024937399885833667, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
=== ep: 129, time 38.869014263153076, eps 0.0024208894296938593, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 130, time 38.01181173324585, eps 0.0023515918344868374, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 131, time 37.705024003982544, eps 0.002285673922878779, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 132, time 37.772327184677124, eps 0.0022229708657555565, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
goal_identified
goal_identified
goal_identified
=== ep: 133, time 38.0335328578949, eps 0.0021633258728137976, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 134, time 38.3349552154541, eps 0.0021065898005034594, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 135, time 37.9540114402771, eps 0.002052620779091266, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 136, time 38.88495326042175, eps 0.0020012838579124784, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 137, time 38.50964665412903, eps 0.0019524506679239415, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 138, time 39.671377658843994, eps 0.001905999100714611, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 139, time 45.60732436180115, eps 0.001861813003170924, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 140, time 37.93494176864624, eps 0.0018197818870335101, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
goal_identified
goal_identified
goal_identified
=== ep: 141, time 38.114789962768555, eps 0.0017798006526189953, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
goal_identified
goal_identified
=== ep: 142, time 38.08973002433777, eps 0.0017417693260160481, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
=== ep: 143, time 38.250694274902344, eps 0.0017055928090985275, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 144, time 38.206058502197266, eps 0.0016711806417306348, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 145, time 37.803313970565796, eps 0.0016384467755694515, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 146, time 38.02956461906433, eps 0.0016073093588992661, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
goal_identified
=== ep: 147, time 38.70011115074158, eps 0.0015776905319596466, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 148, time 38.270819902420044, eps 0.0015495162322554856, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 149, time 37.98417377471924, eps 0.0015227160093621863, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
goal_identified
goal_identified
=== ep: 150, time 38.85120630264282, eps 0.0014972228487629025, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
goal_identified
=== ep: 151, time 38.52996492385864, eps 0.0014729730042773413, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
goal_identified
goal_identified
=== ep: 152, time 37.890143156051636, eps 0.001449905838663109, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
goal_identified
goal_identified
goal_identified
=== ep: 153, time 38.44600796699524, eps 0.00142796367199102, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
goal_identified
goal_identified
=== ep: 154, time 37.614800453186035, eps 0.0014070916374152305, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 155, time 39.46211886405945, eps 0.001387237543977543, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
goal_identified
=== ep: 156, time 38.36273550987244, eps 0.0013683517461028282, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
goal_identified
goal_identified
=== ep: 157, time 38.48226547241211, eps 0.0013503870194592265, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 158, time 38.23723912239075, eps 0.0013332984428727204, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 159, time 38.75718379020691, eps 0.001317043286000802, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 160, time 38.47917652130127, eps 0.0013015809024843582, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 161, time 38.435142993927, eps 0.0012868726283106018, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 162, time 38.60016918182373, eps 0.0012728816851329014, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 163, time 39.08313322067261, eps 0.0012595730883057546, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 164, time 38.17460250854492, eps 0.001246913559404956, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 165, time 38.68855118751526, eps 0.0012348714430141991, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 166, time 38.34529519081116, eps 0.0012234166275700486, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 167, time 38.17653822898865, eps 0.001212520470067348, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 168, time 38.267979860305786, eps 0.0012021557244367845, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
goal_identified
=== ep: 169, time 37.89087200164795, eps 0.0011922964734155277, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
goal_identified
goal_identified
goal_identified
=== ep: 170, time 38.08869194984436, eps 0.001182918063740569, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
goal_identified
goal_identified
goal_identified
=== ep: 171, time 38.65653991699219, eps 0.0011739970445027263, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 172, time 38.313695669174194, eps 0.0011655111085071537, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
goal_identified
goal_identified
=== ep: 173, time 38.8180046081543, eps 0.001157439036493735, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 174, time 38.508333921432495, eps 0.0011497606440778825, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 175, time 38.66026520729065, eps 0.0011424567312790603, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
goal_identified
goal_identified
=== ep: 176, time 38.5294976234436, eps 0.0011355090345108335, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
goal_identified
goal_identified
=== ep: 177, time 38.421916007995605, eps 0.0011289001809123877, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 178, time 38.741689920425415, eps 0.0011226136449073282, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
goal_identified
goal_identified
=== ep: 179, time 38.19290614128113, eps 0.001116633706881133, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 180, time 38.26832294464111, eps 0.001110945413873925, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
goal_identified
=== ep: 181, time 37.7622766494751, eps 0.001105534542190287, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
goal_identified
goal_identified
=== ep: 182, time 37.58665204048157, eps 0.0011003875618326132, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 183, time 38.3349244594574, eps 0.0010954916026690664, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
goal_identified
goal_identified
goal_identified
=== ep: 184, time 38.01652812957764, eps 0.001090834422251547, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 185, time 38.81981897354126, eps 0.0010864043752031938, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 186, time 37.54242467880249, eps 0.0010821903840988777, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 187, time 37.86862850189209, eps 0.0010781819117658682, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 188, time 38.435890913009644, eps 0.0010743689349354123, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 189, time 38.42468190193176, eps 0.0010707419191793434, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 190, time 37.87954640388489, eps 0.0010672917950690429, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
goal_identified
goal_identified
=== ep: 191, time 37.806748390197754, eps 0.0010640099354971456, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 192, time 37.81291460990906, eps 0.0010608881341052777, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
goal_identified
goal_identified
goal_identified
=== ep: 193, time 38.48337268829346, eps 0.0010579185847638855, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 194, time 38.445706844329834, eps 0.0010550938620528466, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
goal_identified
goal_identified
=== ep: 195, time 38.08181667327881, eps 0.001052406902694051, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 196, time 37.82440447807312, eps 0.001049850987889527, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 197, time 38.34808301925659, eps 0.0010474197265209469, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 198, time 38.43914723396301, eps 0.0010451070391685015, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 199, time 38.743383169174194, eps 0.001042907142909185, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 200, time 38.4106924533844, eps 0.001040814536856474, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 201, time 38.389625787734985, eps 0.0010388239884052469, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
goal_identified
goal_identified
goal_identified
=== ep: 202, time 38.220081090927124, eps 0.0010369305201475454, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
goal_identified
goal_identified
goal_identified
=== ep: 203, time 38.079227924346924, eps 0.0010351293974264616, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 204, time 39.208401918411255, eps 0.00103341611649703, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
goal_identified
goal_identified
=== ep: 205, time 38.48430585861206, eps 0.0010317863932645186, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 206, time 37.93228530883789, eps 0.0010302361525719613, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
goal_identified
goal_identified
goal_identified
=== ep: 207, time 37.87010145187378, eps 0.0010287615180101426, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 208, time 38.65936994552612, eps 0.001027358802224555, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 209, time 38.766597509384155, eps 0.0010260244976950921, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
goal_identified
=== ep: 210, time 38.629921197891235, eps 0.0010247552679654227, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 211, time 38.882925033569336, eps 0.00102354793930011, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 212, time 39.21556878089905, eps 0.0010223994927486214, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 213, time 39.05652356147766, eps 0.001021307056596379, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 214, time 37.654202699661255, eps 0.0010202678991839778, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
goal_identified
goal_identified
goal_identified
=== ep: 215, time 38.36394929885864, eps 0.0010192794220766138, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 216, time 38.53762769699097, eps 0.0010183391535666436, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 217, time 38.29801058769226, eps 0.0010174447424930286, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
goal_identified
goal_identified
goal_identified
=== ep: 218, time 38.931880950927734, eps 0.0010165939523622068, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
goal_identified
goal_identified
goal_identified
=== ep: 219, time 38.959999561309814, eps 0.0010157846557556941, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 220, time 38.66192102432251, eps 0.001015014829010431, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 221, time 38.80179524421692, eps 0.0010142825471585687, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 222, time 38.23823165893555, eps 0.0010135859791140496, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
goal_identified
goal_identified
goal_identified
=== ep: 223, time 38.18730068206787, eps 0.0010129233830939361, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 224, time 38.29280686378479, eps 0.0010122931022630473, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 225, time 38.75490188598633, eps 0.001011693560591007, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 226, time 38.49974298477173, eps 0.0010111232589113477, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 227, time 38.75001764297485, eps 0.0010105807711728136, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 228, time 38.54845714569092, eps 0.0010100647408734893, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
goal_identified
goal_identified
goal_identified
=== ep: 229, time 38.85718393325806, eps 0.001009573877668838, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 230, time 38.122048139572144, eps 0.001009106954145169, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 231, time 37.93812084197998, eps 0.0010086628027504636, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 232, time 37.939494132995605, eps 0.0010082403128748867, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
goal_identified
goal_identified
goal_identified
=== ep: 233, time 37.95244908332825, eps 0.0010078384280736842, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 234, time 38.33551621437073, eps 0.001007456143425521, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
goal_identified
goal_identified
=== ep: 235, time 37.94310784339905, eps 0.001007092503019653, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
goal_identified
goal_identified
goal_identified
=== ep: 236, time 37.79601764678955, eps 0.001006746597565654, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
goal_identified
goal_identified
goal_identified
=== ep: 237, time 37.818514585494995, eps 0.001006417562119715, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 238, time 37.504586935043335, eps 0.0010061045739218342, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 239, time 38.44398880004883, eps 0.0010058068503384884, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
goal_identified
goal_identified
=== ep: 240, time 37.98072290420532, eps 0.001005523646905642, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 241, time 37.807740688323975, eps 0.001005254255467199, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 38.249157190322876, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 243, time 37.979881048202515, eps 0.0010047542469506416, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 244, time 38.8018913269043, eps 0.0010045223795907931, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 245, time 38.37320423126221, eps 0.001004301820535524, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 246, time 38.46010708808899, eps 0.0010040920182723119, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 247, time 38.40214014053345, eps 0.0010038924481862177, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 248, time 38.25895929336548, eps 0.0010037026112480747, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
goal_identified
goal_identified
=== ep: 249, time 37.86327576637268, eps 0.0010035220327666559, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
goal_identified
=== ep: 250, time 38.153966665267944, eps 0.0010033502612016988, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
goal_identified
=== ep: 251, time 35.594337701797485, eps 0.001003186867034819, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
goal_identified
goal_identified
goal_identified
=== ep: 252, time 37.726961612701416, eps 0.001003031441695491, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
goal_identified
goal_identified
goal_identified
=== ep: 253, time 38.40830850601196, eps 0.0010028835965394094, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 254, time 37.96348261833191, eps 0.0010027429618766747, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 255, time 38.326441287994385, eps 0.0010026091860473767, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 256, time 37.88860297203064, eps 0.0010024819345422614, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 257, time 38.82426476478577, eps 0.0010023608891662839, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
=== ep: 258, time 37.73233723640442, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 259, time 38.24651336669922, eps 0.0010021362208574892, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 260, time 37.76689434051514, eps 0.001002032036136876, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 261, time 37.565253257751465, eps 0.0010019329325650452, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
goal_identified
goal_identified
=== ep: 262, time 38.53555154800415, eps 0.0010018386623314465, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
=== ep: 263, time 37.70559477806091, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 264, time 37.93679738044739, eps 0.0010016636904766263, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
=== ep: 265, time 38.26508903503418, eps 0.0010015825513346283, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
goal_identified
=== ep: 266, time 38.6127245426178, eps 0.0010015053693952815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
goal_identified
goal_identified
=== ep: 267, time 38.19484853744507, eps 0.0010014319516635345, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 268, time 38.331082582473755, eps 0.0010013621145568167, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 269, time 39.36903119087219, eps 0.0010012956834459848, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 270, time 38.60858869552612, eps 0.0010012324922186594, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 38.656360387802124, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 272, time 39.072028398513794, eps 0.0010011152050768812, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
goal_identified
goal_identified
=== ep: 273, time 39.69816279411316, eps 0.0010010608158834819, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 274, time 38.75445055961609, eps 0.0010010090792823456, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
goal_identified
goal_identified
=== ep: 275, time 39.392040491104126, eps 0.0010009598659050213, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
goal_identified
=== ep: 276, time 38.38473987579346, eps 0.0010009130526924313, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 277, time 39.02635622024536, eps 0.0010008685225871602, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 39.142091274261475, eps 0.0010008261642407504, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 279, time 39.14344668388367, eps 0.001000785871735272, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
=== ep: 280, time 38.96045732498169, eps 0.0010007475443184742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 281, time 39.24487328529358, eps 0.001000711086151851, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 282, time 39.351662397384644, eps 0.0010006764060709957, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
goal_identified
goal_identified
goal_identified
=== ep: 283, time 38.628150939941406, eps 0.001000643417357642, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
goal_identified
=== ep: 284, time 39.16992115974426, eps 0.0010006120375228235, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 285, time 38.81261420249939, eps 0.0010005821881006083, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
goal_identified
goal_identified
goal_identified
=== ep: 286, time 39.39769625663757, eps 0.0010005537944518927, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 287, time 39.57854747772217, eps 0.0010005267855777657, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
goal_identified
goal_identified
goal_identified
=== ep: 288, time 38.88647818565369, eps 0.0010005010939419733, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 289, time 38.51300549507141, eps 0.001000476655302044, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 290, time 39.131887435913086, eps 0.0010004534085486486, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 291, time 38.61405611038208, eps 0.0010004312955527947, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
goal_identified
goal_identified
=== ep: 292, time 38.76879858970642, eps 0.0010004102610204745, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 293, time 39.17665386199951, eps 0.0010003902523544011, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 294, time 39.37195038795471, eps 0.0010003712195224871, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
goal_identified
goal_identified
goal_identified
=== ep: 295, time 38.57795166969299, eps 0.0010003531149327387, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
goal_identified
goal_identified
goal_identified
=== ep: 296, time 38.8602020740509, eps 0.0010003358933142518, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
=== ep: 297, time 38.36573123931885, eps 0.0010003195116040093, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 298, time 39.463473320007324, eps 0.0010003039288392032, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 299, time 38.59800624847412, eps 0.0010002891060548044, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 300, time 39.16510009765625, eps 0.0010002750061861312, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
goal_identified
=== ep: 301, time 38.39807724952698, eps 0.0010002615939761676, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 302, time 38.298712968826294, eps 0.001000248835887403, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
goal_identified
goal_identified
goal_identified
=== ep: 303, time 39.17004466056824, eps 0.0010002367000179694, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 304, time 38.531774282455444, eps 0.0010002251560218723, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 305, time 38.4927818775177, eps 0.0010002141750331084, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 306, time 38.339080572128296, eps 0.0010002037295934862, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 307, time 38.8505117893219, eps 0.0010001937935839656, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
goal_identified
goal_identified
=== ep: 308, time 38.158472537994385, eps 0.0010001843421593476, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 309, time 38.00791692733765, eps 0.0010001753516861473, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
goal_identified
goal_identified
=== ep: 310, time 37.83898949623108, eps 0.0010001667996834991, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 311, time 38.71724271774292, eps 0.001000158664766942, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
goal_identified
goal_identified
=== ep: 312, time 38.65486812591553, eps 0.0010001509265949466, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
goal_identified
goal_identified
goal_identified
=== ep: 313, time 38.32383394241333, eps 0.001000143565818053, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 314, time 38.27809453010559, eps 0.0010001365640304844, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
goal_identified
goal_identified
=== ep: 315, time 38.168004751205444, eps 0.0010001299037241253, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
goal_identified
goal_identified
goal_identified
=== ep: 316, time 38.35734152793884, eps 0.0010001235682447402, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
goal_identified
goal_identified
=== ep: 317, time 37.86833143234253, eps 0.0010001175417503308, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 318, time 38.55284237861633, eps 0.0010001118091715218, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 134/134)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
goal_identified
goal_identified
goal_identified
=== ep: 319, time 37.975449085235596, eps 0.0010001063561738807, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 320, time 38.42392158508301, eps 0.0010001011691220727, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
goal_identified
goal_identified
goal_identified
=== ep: 321, time 38.51331043243408, eps 0.0010000962350457665, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 322, time 38.51394557952881, eps 0.0010000915416072012, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
goal_identified
goal_identified
=== ep: 323, time 38.143855571746826, eps 0.0010000870770703358, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 324, time 38.1511127948761, eps 0.0010000828302715028, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
goal_identified
=== ep: 325, time 38.18248105049133, eps 0.0010000787905914928, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
goal_identified
goal_identified
=== ep: 326, time 38.54046845436096, eps 0.0010000749479290019, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 327, time 38.17282962799072, eps 0.001000071292675372, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
goal_identified
goal_identified
goal_identified
=== ep: 328, time 38.3851854801178, eps 0.001000067815690565, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
goal_identified
goal_identified
=== ep: 329, time 38.85504603385925, eps 0.0010000645082803084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 330, time 38.74033832550049, eps 0.0010000613621743532, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 331, time 38.37895059585571, eps 0.0010000583695057963, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 332, time 38.6200213432312, eps 0.0010000555227914069, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 333, time 38.643916606903076, eps 0.0010000528149129166, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
goal_identified
=== ep: 334, time 38.52398729324341, eps 0.0010000502390992187, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 335, time 38.689008474349976, eps 0.0010000477889094373, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
goal_identified
=== ep: 336, time 39.19062161445618, eps 0.0010000454582168217, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 337, time 38.90390586853027, eps 0.001000043241193426, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 338, time 38.70115661621094, eps 0.0010000411322955373, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 339, time 38.43967294692993, eps 0.0010000391262498123, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
goal_identified
goal_identified
=== ep: 340, time 38.540098905563354, eps 0.001000037218040092, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 341, time 38.64177894592285, eps 0.0010000354028948577, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
goal_identified
goal_identified
goal_identified
=== ep: 342, time 39.62340807914734, eps 0.0010000336762753012, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 343, time 38.86648201942444, eps 0.001000032033863974, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 344, time 38.987834453582764, eps 0.0010000304715539925, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 345, time 38.66634392738342, eps 0.001000028985438768, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 346, time 38.845664978027344, eps 0.001000027571802238, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 347, time 38.43263483047485, eps 0.0010000262271095755, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 348, time 38.83245325088501, eps 0.0010000249479983478, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 349, time 38.7887704372406, eps 0.0010000237312701107, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 39.05243468284607, eps 0.00100002257388241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 351, time 38.690873861312866, eps 0.0010000214729411737, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 352, time 39.19782829284668, eps 0.0010000204256934752, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
goal_identified
goal_identified
=== ep: 353, time 38.70119071006775, eps 0.0010000194295206493, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 354, time 39.061304569244385, eps 0.0010000184819317455, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
goal_identified
=== ep: 355, time 38.83028602600098, eps 0.001000017580557298, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 356, time 39.20914959907532, eps 0.001000016723143401, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 357, time 40.155876874923706, eps 0.0010000159075460732, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 358, time 38.99608373641968, eps 0.0010000151317258964, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
goal_identified
=== ep: 359, time 39.39918088912964, eps 0.0010000143937429161, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 360, time 38.9387309551239, eps 0.0010000136917517905, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
goal_identified
=== ep: 361, time 38.6723096370697, eps 0.001000013023997176, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 39.15422058105469, eps 0.0010000123888093385, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 363, time 38.915507078170776, eps 0.0010000117845999773, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
goal_identified
goal_identified
=== ep: 364, time 38.980050802230835, eps 0.0010000112098582543, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 365, time 39.40017342567444, eps 0.001000010663147016, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 366, time 38.93110275268555, eps 0.0010000101430991996, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 367, time 38.49126100540161, eps 0.0010000096484144142, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
goal_identified
goal_identified
=== ep: 368, time 38.30533981323242, eps 0.0010000091778556905, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 369, time 38.79168772697449, eps 0.0010000087302463867, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 370, time 38.88534474372864, eps 0.001000008304467246, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
goal_identified
goal_identified
goal_identified
=== ep: 371, time 38.81063270568848, eps 0.0010000078994535993, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 372, time 39.585196018218994, eps 0.0010000075141927012, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
goal_identified
goal_identified
=== ep: 373, time 38.69073510169983, eps 0.0010000071477211988, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 374, time 38.96615266799927, eps 0.0010000067991227223, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
=== ep: 375, time 38.68405723571777, eps 0.0010000064675255943, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
goal_identified
goal_identified
goal_identified
=== ep: 376, time 38.49536609649658, eps 0.001000006152100649, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
goal_identified
goal_identified
=== ep: 377, time 38.641600131988525, eps 0.0010000058520591598, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 378, time 39.07490253448486, eps 0.0010000055666508666, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 379, time 38.95985388755798, eps 0.0010000052951621003, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
goal_identified
goal_identified
=== ep: 380, time 39.78056859970093, eps 0.0010000050369139975, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 381, time 38.80807614326477, eps 0.001000004791260803, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 382, time 38.509414196014404, eps 0.0010000045575882562, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
goal_identified
=== ep: 383, time 38.6569390296936, eps 0.001000004335312054, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
goal_identified
goal_identified
goal_identified
=== ep: 384, time 38.37349605560303, eps 0.0010000041238763903, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
goal_identified
goal_identified
=== ep: 385, time 38.81949257850647, eps 0.0010000039227525655, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 386, time 38.92500686645508, eps 0.0010000037314376652, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 387, time 38.96490287780762, eps 0.001000003549453303, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 388, time 39.38095235824585, eps 0.0010000033763444226, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
goal_identified
goal_identified
goal_identified
=== ep: 389, time 38.944557666778564, eps 0.001000003211678162, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 390, time 39.09930229187012, eps 0.0010000030550427698, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
goal_identified
goal_identified
goal_identified
=== ep: 391, time 38.961801290512085, eps 0.0010000029060465757, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 392, time 39.45198702812195, eps 0.0010000027643170119, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
goal_identified
goal_identified
=== ep: 393, time 39.4087188243866, eps 0.0010000026294996803, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 394, time 39.48877215385437, eps 0.0010000025012574677, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 395, time 39.43452787399292, eps 0.0010000023792697014, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 396, time 39.34988236427307, eps 0.0010000022632313489, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 397, time 39.66167235374451, eps 0.0010000021528522535, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 39.07827425003052, eps 0.00100000204785641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 399, time 39.364070892333984, eps 0.0010000019479812744, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 400, time 39.520708084106445, eps 0.0010000018529771066, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
=== ep: 401, time 39.35505390167236, eps 0.0010000017626063467, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 402, time 39.4691264629364, eps 0.0010000016766430208, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 403, time 39.98290014266968, eps 0.0010000015948721758, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 404, time 40.324310302734375, eps 0.001000001517089342, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 405, time 39.55673694610596, eps 0.0010000014431000217, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
goal_identified
goal_identified
goal_identified
=== ep: 406, time 39.355151414871216, eps 0.001000001372719203, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 407, time 39.19160723686218, eps 0.0010000013057708975, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 408, time 39.38339447975159, eps 0.0010000012420876994, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 409, time 39.38036799430847, eps 0.0010000011815103674, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
goal_identified
goal_identified
=== ep: 410, time 39.44345664978027, eps 0.001000001123887427, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 411, time 39.08343267440796, eps 0.0010000010690747903, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
goal_identified
=== ep: 412, time 40.013113260269165, eps 0.0010000010169353975, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 413, time 39.837785482406616, eps 0.0010000009673388729, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 414, time 39.31340765953064, eps 0.0010000009201611994, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 415, time 39.571242570877075, eps 0.0010000008752844081, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 416, time 39.550450563430786, eps 0.0010000008325962838, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
goal_identified
goal_identified
goal_identified
=== ep: 417, time 39.38820838928223, eps 0.001000000791990084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
goal_identified
goal_identified
=== ep: 418, time 38.92187428474426, eps 0.0010000007533642718, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 419, time 39.575223207473755, eps 0.0010000007166222626, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 420, time 39.459545850753784, eps 0.0010000006816721825, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
goal_identified
goal_identified
goal_identified
=== ep: 421, time 40.04549026489258, eps 0.001000000648426638, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 422, time 39.32971429824829, eps 0.0010000006168024976, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 423, time 39.67216420173645, eps 0.0010000005867206849, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
goal_identified
goal_identified
goal_identified
=== ep: 424, time 39.64931607246399, eps 0.0010000005581059794, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 425, time 39.581849813461304, eps 0.0010000005308868295, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 426, time 39.18445634841919, eps 0.0010000005049951733, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 427, time 39.40016460418701, eps 0.001000000480366268, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
goal_identified
=== ep: 428, time 39.71538043022156, eps 0.0010000004569385287, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 429, time 39.38177680969238, eps 0.0010000004346533736, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 430, time 40.562586069107056, eps 0.0010000004134550786, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 431, time 39.90289783477783, eps 0.0010000003932906364, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 432, time 39.60410213470459, eps 0.0010000003741096257, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 433, time 39.38648867607117, eps 0.001000000355864084, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 434, time 39.80630159378052, eps 0.0010000003385083878, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 435, time 39.31044101715088, eps 0.001000000321999139, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 436, time 39.799946784973145, eps 0.0010000003062950555, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 437, time 39.71414661407471, eps 0.0010000002913568694, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 438, time 39.67835235595703, eps 0.0010000002771472273, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 439, time 39.76708674430847, eps 0.0010000002636305976, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 440, time 40.31947112083435, eps 0.0010000002507731815, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
goal_identified
goal_identified
=== ep: 441, time 39.36128091812134, eps 0.0010000002385428292, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
goal_identified
goal_identified
goal_identified
=== ep: 442, time 39.67567849159241, eps 0.0010000002269089582, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 443, time 39.63562202453613, eps 0.0010000002158424776, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 444, time 39.652597188949585, eps 0.0010000002053157158, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
goal_identified
=== ep: 445, time 39.18767786026001, eps 0.0010000001953023503, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 446, time 39.77467107772827, eps 0.001000000185777342, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
goal_identified
goal_identified
=== ep: 447, time 39.614829301834106, eps 0.0010000001767168742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 448, time 39.71608018875122, eps 0.0010000001680982905, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 449, time 40.788522243499756, eps 0.0010000001599000403, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
goal_identified
goal_identified
=== ep: 450, time 39.47225379943848, eps 0.0010000001521016232, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 451, time 40.19496560096741, eps 0.0010000001446835395, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 452, time 40.02436685562134, eps 0.0010000001376272401, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 453, time 40.327369689941406, eps 0.0010000001309150804, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
goal_identified
=== ep: 454, time 40.57265496253967, eps 0.0010000001245302765, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
goal_identified
=== ep: 455, time 40.18280625343323, eps 0.0010000001184568633, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
goal_identified
goal_identified
goal_identified
=== ep: 456, time 40.27739214897156, eps 0.0010000001126796538, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
goal_identified
goal_identified
goal_identified
=== ep: 457, time 40.86299729347229, eps 0.0010000001071842023, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 458, time 40.168053150177, eps 0.001000000101956767, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 459, time 39.58901047706604, eps 0.001000000096984277, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 460, time 40.275054931640625, eps 0.001000000092254298, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 461, time 39.593475580215454, eps 0.0010000000877550027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
goal_identified
=== ep: 462, time 39.448333978652954, eps 0.0010000000834751407, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
goal_identified
goal_identified
=== ep: 463, time 40.06369471549988, eps 0.00100000007940401, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 464, time 39.90368914604187, eps 0.0010000000755314307, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 465, time 39.65700316429138, eps 0.0010000000718477194, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 466, time 39.41997528076172, eps 0.0010000000683436647, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 467, time 40.233601093292236, eps 0.001000000065010505, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
goal_identified
goal_identified
=== ep: 468, time 40.006240129470825, eps 0.0010000000618399052, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 469, time 40.737494230270386, eps 0.0010000000588239375, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 470, time 40.195982694625854, eps 0.0010000000559550603, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 471, time 39.99077343940735, eps 0.0010000000532260998, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
goal_identified
goal_identified
goal_identified
=== ep: 472, time 40.16876840591431, eps 0.0010000000506302322, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
goal_identified
goal_identified
goal_identified
=== ep: 473, time 39.99149227142334, eps 0.0010000000481609666, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 474, time 40.029423236846924, eps 0.0010000000458121286, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 475, time 39.93940448760986, eps 0.0010000000435778447, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
goal_identified
goal_identified
=== ep: 476, time 40.7331120967865, eps 0.001000000041452528, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 477, time 39.536360025405884, eps 0.0010000000394308644, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 478, time 40.13059377670288, eps 0.0010000000375077985, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 479, time 39.69641137123108, eps 0.0010000000356785216, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
goal_identified
=== ep: 480, time 39.964367151260376, eps 0.0010000000339384595, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
goal_identified
=== ep: 481, time 40.034080028533936, eps 0.0010000000322832614, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 482, time 39.58766293525696, eps 0.0010000000307087882, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
goal_identified
=== ep: 483, time 40.05891418457031, eps 0.001000000029211103, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 484, time 40.66010355949402, eps 0.0010000000277864607, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
goal_identified
goal_identified
=== ep: 485, time 39.802608251571655, eps 0.0010000000264312988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 486, time 40.24246120452881, eps 0.0010000000251422292, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 487, time 40.03903841972351, eps 0.0010000000239160282, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
=== ep: 488, time 40.11318802833557, eps 0.00100000002274963, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 489, time 41.489192724227905, eps 0.0010000000216401172, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
goal_identified
goal_identified
goal_identified
=== ep: 490, time 39.74533414840698, eps 0.0010000000205847162, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 491, time 39.83554720878601, eps 0.0010000000195807877, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 492, time 39.903088331222534, eps 0.0010000000186258216, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 493, time 40.13042855262756, eps 0.0010000000177174295, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 494, time 40.024712562561035, eps 0.0010000000168533404, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 495, time 39.75105905532837, eps 0.0010000000160313932, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
goal_identified
goal_identified
goal_identified
=== ep: 496, time 40.039689779281616, eps 0.001000000015249533, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 497, time 40.01299977302551, eps 0.0010000000145058043, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 498, time 39.84575271606445, eps 0.001000000013798348, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 499, time 40.12851619720459, eps 0.0010000000131253947, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 500, time 41.00351071357727, eps 0.0010000000124852615, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
goal_identified
goal_identified
=== ep: 501, time 39.879374980926514, eps 0.0010000000118763482, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 502, time 40.253750801086426, eps 0.0010000000112971319, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 503, time 40.193108797073364, eps 0.0010000000107461642, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 504, time 40.23663830757141, eps 0.0010000000102220676, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
goal_identified
goal_identified
goal_identified
=== ep: 505, time 40.46468663215637, eps 0.0010000000097235315, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
goal_identified
goal_identified
goal_identified
=== ep: 506, time 39.821528911590576, eps 0.0010000000092493092, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
goal_identified
goal_identified
goal_identified
=== ep: 507, time 40.09183883666992, eps 0.0010000000087982152, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 508, time 40.058579444885254, eps 0.0010000000083691212, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
goal_identified
goal_identified
=== ep: 509, time 40.90176248550415, eps 0.0010000000079609542, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 510, time 40.17799973487854, eps 0.001000000007572694, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
goal_identified
goal_identified
=== ep: 511, time 40.43521022796631, eps 0.0010000000072033692, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
goal_identified
=== ep: 512, time 40.309070348739624, eps 0.001000000006852057, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 513, time 40.08342218399048, eps 0.001000000006517878, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 514, time 40.099735260009766, eps 0.0010000000061999974, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
goal_identified
goal_identified
goal_identified
=== ep: 515, time 39.101441621780396, eps 0.0010000000058976199, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 516, time 39.96719193458557, eps 0.0010000000056099897, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
goal_identified
goal_identified
=== ep: 517, time 40.29103374481201, eps 0.0010000000053363872, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 518, time 40.86623740196228, eps 0.0010000000050761286, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 519, time 40.034364461898804, eps 0.001000000004828563, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 520, time 40.02865481376648, eps 0.001000000004593071, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 521, time 39.99190545082092, eps 0.0010000000043690644, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 522, time 40.866597175598145, eps 0.0010000000041559827, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 523, time 39.96868872642517, eps 0.0010000000039532928, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 524, time 40.14704966545105, eps 0.0010000000037604885, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
goal_identified
=== ep: 525, time 40.08775329589844, eps 0.0010000000035770874, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 526, time 39.87551546096802, eps 0.0010000000034026306, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
goal_identified
goal_identified
=== ep: 527, time 41.00632047653198, eps 0.0010000000032366824, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 528, time 40.37460112571716, eps 0.0010000000030788276, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 529, time 40.11344289779663, eps 0.0010000000029286714, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
goal_identified
goal_identified
goal_identified
=== ep: 530, time 39.780943155288696, eps 0.0010000000027858384, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
=== ep: 531, time 40.091537952423096, eps 0.0010000000026499714, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 532, time 40.48468375205994, eps 0.0010000000025207308, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 533, time 39.977272510528564, eps 0.0010000000023977934, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 534, time 40.033488750457764, eps 0.0010000000022808515, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
goal_identified
goal_identified
goal_identified
=== ep: 535, time 39.304659605026245, eps 0.0010000000021696133, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 536, time 40.34254312515259, eps 0.0010000000020637999, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 537, time 40.81089401245117, eps 0.0010000000019631471, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 538, time 40.019126415252686, eps 0.0010000000018674034, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
goal_identified
goal_identified
goal_identified
=== ep: 539, time 40.224926471710205, eps 0.001000000001776329, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 540, time 40.08082294464111, eps 0.0010000000016896964, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
goal_identified
goal_identified
=== ep: 541, time 39.88982582092285, eps 0.001000000001607289, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 542, time 39.79361701011658, eps 0.0010000000015289005, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
goal_identified
goal_identified
=== ep: 543, time 40.48073363304138, eps 0.0010000000014543352, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 544, time 39.4904899597168, eps 0.0010000000013834064, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
goal_identified
goal_identified
=== ep: 545, time 40.465237855911255, eps 0.001000000001315937, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 546, time 40.369805097579956, eps 0.0010000000012517578, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 547, time 39.28577017784119, eps 0.001000000001190709, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
goal_identified
=== ep: 548, time 40.87695240974426, eps 0.0010000000011326374, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 549, time 40.752625942230225, eps 0.001000000001077398, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
goal_identified
goal_identified
goal_identified
=== ep: 550, time 40.263556241989136, eps 0.0010000000010248527, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 551, time 40.11681151390076, eps 0.00100000000097487, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 552, time 39.81783366203308, eps 0.001000000000927325, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
goal_identified
goal_identified
=== ep: 553, time 39.83491349220276, eps 0.0010000000008820989, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
goal_identified
=== ep: 554, time 39.828542947769165, eps 0.0010000000008390784, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
goal_identified
=== ep: 555, time 39.8380229473114, eps 0.001000000000798156, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
goal_identified
goal_identified
=== ep: 556, time 39.47771883010864, eps 0.0010000000007592295, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 557, time 40.00697565078735, eps 0.0010000000007222014, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 558, time 43.60131525993347, eps 0.0010000000006869794, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 559, time 43.0827431678772, eps 0.001000000000653475, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 560, time 42.938124895095825, eps 0.0010000000006216046, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 561, time 43.11407136917114, eps 0.0010000000005912885, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 562, time 43.40416073799133, eps 0.0010000000005624511, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
goal_identified
=== ep: 563, time 43.378360986709595, eps 0.00100000000053502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 564, time 43.12252235412598, eps 0.001000000000508927, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 565, time 42.94466996192932, eps 0.001000000000484106, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 566, time 43.056880712509155, eps 0.001000000000460496, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 567, time 42.5443377494812, eps 0.0010000000004380374, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 568, time 43.087984800338745, eps 0.001000000000416674, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
=== ep: 569, time 42.98239731788635, eps 0.0010000000003963527, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 570, time 44.56162643432617, eps 0.0010000000003770222, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
goal_identified
goal_identified
=== ep: 571, time 42.824241161346436, eps 0.0010000000003586346, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 572, time 43.44055485725403, eps 0.0010000000003411438, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 573, time 42.855671644210815, eps 0.001000000000324506, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
goal_identified
=== ep: 574, time 42.75273823738098, eps 0.0010000000003086798, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
goal_identified
goal_identified
=== ep: 575, time 43.09092950820923, eps 0.0010000000002936252, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
goal_identified
goal_identified
goal_identified
=== ep: 576, time 42.56281495094299, eps 0.001000000000279305, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
goal_identified
goal_identified
=== ep: 577, time 42.782567262649536, eps 0.0010000000002656831, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 578, time 42.885743141174316, eps 0.0010000000002527256, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 579, time 42.41419768333435, eps 0.0010000000002404, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
goal_identified
goal_identified
goal_identified
=== ep: 580, time 41.57745361328125, eps 0.0010000000002286756, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 581, time 43.65205788612366, eps 0.0010000000002175229, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 582, time 42.97498536109924, eps 0.0010000000002069142, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
goal_identified
goal_identified
goal_identified
=== ep: 583, time 42.72131538391113, eps 0.0010000000001968228, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
goal_identified
goal_identified
goal_identified
=== ep: 584, time 42.80419182777405, eps 0.0010000000001872237, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
goal_identified
goal_identified
=== ep: 585, time 42.40776205062866, eps 0.0010000000001780928, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 586, time 42.9656183719635, eps 0.001000000000169407, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 42.78884434700012, eps 0.001000000000161145, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 588, time 42.6606879234314, eps 0.0010000000001532858, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 589, time 42.62546253204346, eps 0.00100000000014581, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 590, time 42.568419456481934, eps 0.0010000000001386988, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
goal_identified
=== ep: 591, time 42.46829390525818, eps 0.0010000000001319344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 592, time 42.91409468650818, eps 0.0010000000001255, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 593, time 43.564359188079834, eps 0.0010000000001193791, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 594, time 43.17816972732544, eps 0.001000000000113557, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 595, time 43.27219867706299, eps 0.0010000000001080186, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
goal_identified
goal_identified
goal_identified
=== ep: 596, time 43.44871711730957, eps 0.0010000000001027505, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 597, time 43.48482704162598, eps 0.0010000000000977393, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 598, time 43.28208255767822, eps 0.0010000000000929725, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
goal_identified
goal_identified
goal_identified
=== ep: 599, time 43.60614609718323, eps 0.0010000000000884382, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 600, time 43.433417320251465, eps 0.001000000000084125, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 601, time 43.17992830276489, eps 0.0010000000000800222, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 602, time 43.89035773277283, eps 0.0010000000000761195, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 603, time 44.3699517250061, eps 0.0010000000000724072, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
goal_identified
goal_identified
goal_identified
=== ep: 604, time 43.73160719871521, eps 0.0010000000000688757, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
goal_identified
goal_identified
goal_identified
=== ep: 605, time 43.87980532646179, eps 0.0010000000000655166, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 606, time 43.197746992111206, eps 0.0010000000000623215, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 607, time 43.1313955783844, eps 0.001000000000059282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 608, time 43.078861474990845, eps 0.0010000000000563907, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 609, time 42.26371908187866, eps 0.0010000000000536405, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 610, time 41.679110050201416, eps 0.0010000000000510245, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 611, time 41.493186235427856, eps 0.0010000000000485358, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 612, time 40.02925634384155, eps 0.0010000000000461688, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 613, time 39.88188099861145, eps 0.0010000000000439171, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
goal_identified
goal_identified
goal_identified
=== ep: 614, time 39.713897705078125, eps 0.0010000000000417752, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
goal_identified
goal_identified
goal_identified
=== ep: 615, time 39.84497952461243, eps 0.0010000000000397378, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 615
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 616, time 40.50757932662964, eps 0.0010000000000377999, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 617, time 39.56666612625122, eps 0.0010000000000359563, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 618, time 39.804359912872314, eps 0.0010000000000342027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
goal_identified
goal_identified
=== ep: 619, time 39.50873112678528, eps 0.0010000000000325345, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 620, time 39.23664331436157, eps 0.001000000000030948, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
goal_identified
=== ep: 621, time 39.94889783859253, eps 0.0010000000000294385, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
=== ep: 622, time 39.81144452095032, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
goal_identified
goal_identified
=== ep: 623, time 39.764654874801636, eps 0.0010000000000266371, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 624, time 40.36501955986023, eps 0.001000000000025338, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 625, time 43.13005471229553, eps 0.0010000000000241023, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 626, time 43.17982816696167, eps 0.0010000000000229268, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 627, time 43.01202344894409, eps 0.0010000000000218085, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 628, time 43.85015916824341, eps 0.001000000000020745, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
goal_identified
=== ep: 629, time 43.005024433135986, eps 0.0010000000000197332, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
goal_identified
goal_identified
goal_identified
=== ep: 630, time 42.87512183189392, eps 0.0010000000000187708, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
goal_identified
goal_identified
=== ep: 631, time 42.55233883857727, eps 0.0010000000000178553, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
goal_identified
goal_identified
=== ep: 632, time 43.2612886428833, eps 0.0010000000000169845, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 633, time 43.325790882110596, eps 0.0010000000000161562, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 634, time 43.54343628883362, eps 0.0010000000000153684, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 635, time 43.265108585357666, eps 0.0010000000000146188, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 636, time 43.165072202682495, eps 0.0010000000000139058, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 637, time 42.59173512458801, eps 0.0010000000000132275, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 638, time 43.05909490585327, eps 0.0010000000000125824, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
goal_identified
goal_identified
goal_identified
=== ep: 639, time 42.74862313270569, eps 0.0010000000000119687, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 640, time 44.04789996147156, eps 0.001000000000011385, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 641, time 43.45794224739075, eps 0.00100000000001083, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 642, time 43.266746044158936, eps 0.0010000000000103017, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
goal_identified
goal_identified
goal_identified
=== ep: 643, time 43.171635150909424, eps 0.0010000000000097993, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
goal_identified
goal_identified
=== ep: 644, time 45.28422522544861, eps 0.0010000000000093213, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 645, time 44.960211753845215, eps 0.0010000000000088666, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 646, time 45.12577772140503, eps 0.0010000000000084342, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 647, time 45.606913805007935, eps 0.001000000000008023, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
goal_identified
=== ep: 648, time 45.36353921890259, eps 0.0010000000000076317, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
goal_identified
goal_identified
goal_identified
=== ep: 649, time 45.01733326911926, eps 0.0010000000000072594, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 650, time 45.30895709991455, eps 0.0010000000000069055, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 651, time 46.489869356155396, eps 0.0010000000000065686, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 652, time 45.833019733428955, eps 0.0010000000000062483, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
goal_identified
goal_identified
goal_identified
=== ep: 653, time 45.68912363052368, eps 0.0010000000000059436, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
goal_identified
goal_identified
=== ep: 654, time 45.830873250961304, eps 0.0010000000000056537, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 655, time 47.69353771209717, eps 0.0010000000000053779, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
goal_identified
goal_identified
goal_identified
=== ep: 656, time 48.555235147476196, eps 0.0010000000000051157, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
goal_identified
goal_identified
goal_identified
=== ep: 657, time 49.6437406539917, eps 0.0010000000000048661, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 658, time 42.17591857910156, eps 0.001000000000004629, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 659, time 41.90279269218445, eps 0.0010000000000044032, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 660, time 42.071510314941406, eps 0.0010000000000041883, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 661, time 41.703579902648926, eps 0.001000000000003984, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 662, time 42.122150182724, eps 0.0010000000000037897, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 663, time 44.95874214172363, eps 0.001000000000003605, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 664, time 42.50144076347351, eps 0.0010000000000034291, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
goal_identified
goal_identified
=== ep: 665, time 42.07209348678589, eps 0.001000000000003262, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 666, time 47.397714614868164, eps 0.0010000000000031028, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 47.5860869884491, eps 0.0010000000000029514, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 668, time 48.05031752586365, eps 0.0010000000000028075, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 669, time 48.34644293785095, eps 0.0010000000000026706, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 670, time 47.86202645301819, eps 0.0010000000000025403, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 671, time 48.638805866241455, eps 0.0010000000000024165, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 672, time 49.393961906433105, eps 0.0010000000000022985, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 673, time 48.553853273391724, eps 0.0010000000000021864, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 674, time 48.203866481781006, eps 0.00100000000000208, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 675, time 48.124393701553345, eps 0.0010000000000019785, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
goal_identified
goal_identified
goal_identified
=== ep: 676, time 48.55396556854248, eps 0.001000000000001882, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 677, time 48.11192607879639, eps 0.0010000000000017903, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 678, time 47.925588846206665, eps 0.0010000000000017029, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 679, time 47.083667516708374, eps 0.0010000000000016198, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 680, time 50.06310987472534, eps 0.0010000000000015409, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 681, time 49.909560680389404, eps 0.0010000000000014656, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 682, time 50.89163374900818, eps 0.0010000000000013943, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 683, time 50.13313841819763, eps 0.0010000000000013262, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 684, time 49.63605570793152, eps 0.0010000000000012616, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 685, time 50.28559708595276, eps 0.0010000000000012, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 686, time 50.23645091056824, eps 0.0010000000000011415, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
goal_identified
goal_identified
goal_identified
=== ep: 687, time 49.531888246536255, eps 0.0010000000000010857, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 688, time 49.53967022895813, eps 0.0010000000000010328, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
goal_identified
goal_identified
goal_identified
=== ep: 689, time 51.682607650756836, eps 0.0010000000000009825, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 690, time 53.23457932472229, eps 0.0010000000000009346, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 691, time 52.57394075393677, eps 0.001000000000000889, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
goal_identified
goal_identified
=== ep: 692, time 52.548839807510376, eps 0.0010000000000008457, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 693, time 52.641295194625854, eps 0.0010000000000008045, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 694, time 54.36834764480591, eps 0.0010000000000007653, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
goal_identified
goal_identified
=== ep: 695, time 53.33619284629822, eps 0.0010000000000007277, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
goal_identified
goal_identified
goal_identified
=== ep: 696, time 54.53674030303955, eps 0.0010000000000006924, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
goal_identified
goal_identified
goal_identified
=== ep: 697, time 56.40752029418945, eps 0.0010000000000006586, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 698, time 55.30301022529602, eps 0.0010000000000006265, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 699, time 55.06488347053528, eps 0.001000000000000596, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 700, time 54.547064781188965, eps 0.0010000000000005668, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 701, time 55.82835245132446, eps 0.0010000000000005393, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 702, time 55.43950390815735, eps 0.0010000000000005128, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 703, time 54.32157611846924, eps 0.001000000000000488, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 704, time 56.766013383865356, eps 0.001000000000000464, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
goal_identified
goal_identified
=== ep: 705, time 55.833266496658325, eps 0.0010000000000004415, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 706, time 57.918248891830444, eps 0.00100000000000042, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 707, time 55.8307740688324, eps 0.0010000000000003994, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
goal_identified
=== ep: 708, time 56.97929811477661, eps 0.00100000000000038, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 709, time 56.103862285614014, eps 0.0010000000000003615, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
goal_identified
goal_identified
goal_identified
=== ep: 710, time 56.50879669189453, eps 0.0010000000000003437, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 711, time 56.12386465072632, eps 0.001000000000000327, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 712, time 56.75793695449829, eps 0.0010000000000003112, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
goal_identified
goal_identified
=== ep: 713, time 55.83145523071289, eps 0.001000000000000296, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
goal_identified
goal_identified
goal_identified
=== ep: 714, time 49.9900176525116, eps 0.0010000000000002815, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
goal_identified
=== ep: 715, time 47.40382361412048, eps 0.0010000000000002678, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 716, time 47.75342917442322, eps 0.0010000000000002548, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
goal_identified
goal_identified
=== ep: 717, time 49.14493012428284, eps 0.0010000000000002422, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
goal_identified
goal_identified
goal_identified
=== ep: 718, time 48.48741292953491, eps 0.0010000000000002305, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 719, time 47.711971044540405, eps 0.0010000000000002192, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 720, time 47.93665051460266, eps 0.0010000000000002086, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
goal_identified
goal_identified
goal_identified
=== ep: 721, time 47.559433698654175, eps 0.0010000000000001984, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 722, time 47.41473126411438, eps 0.0010000000000001887, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 723, time 47.56763958930969, eps 0.0010000000000001796, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
goal_identified
goal_identified
goal_identified
=== ep: 724, time 47.654200077056885, eps 0.0010000000000001707, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 725, time 49.21587061882019, eps 0.0010000000000001624, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 726, time 48.875930309295654, eps 0.0010000000000001544, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 727, time 47.89657473564148, eps 0.001000000000000147, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 728, time 47.21842050552368, eps 0.0010000000000001399, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
goal_identified
goal_identified
=== ep: 729, time 47.954731464385986, eps 0.001000000000000133, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 730, time 46.35308575630188, eps 0.0010000000000001264, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
goal_identified
goal_identified
goal_identified
=== ep: 731, time 46.59397077560425, eps 0.0010000000000001204, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 732, time 46.46779155731201, eps 0.0010000000000001145, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 733, time 45.98401379585266, eps 0.0010000000000001089, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 734, time 46.484187841415405, eps 0.0010000000000001037, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
goal_identified
=== ep: 735, time 46.38092565536499, eps 0.0010000000000000985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 736, time 45.998234272003174, eps 0.0010000000000000937, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 737, time 45.94428896903992, eps 0.0010000000000000891, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
goal_identified
goal_identified
goal_identified
=== ep: 738, time 44.164992809295654, eps 0.0010000000000000848, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
goal_identified
goal_identified
=== ep: 739, time 43.26098561286926, eps 0.0010000000000000807, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 740, time 40.07200026512146, eps 0.0010000000000000768, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 741, time 40.05012273788452, eps 0.001000000000000073, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 742, time 40.98761510848999, eps 0.0010000000000000694, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
goal_identified
=== ep: 743, time 39.36940288543701, eps 0.001000000000000066, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
goal_identified
=== ep: 744, time 39.38062238693237, eps 0.001000000000000063, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 745, time 40.90806770324707, eps 0.0010000000000000599, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 746, time 40.45507216453552, eps 0.0010000000000000568, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 747, time 41.07572388648987, eps 0.001000000000000054, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
goal_identified
goal_identified
=== ep: 748, time 40.64782452583313, eps 0.0010000000000000514, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
goal_identified
goal_identified
=== ep: 749, time 41.22843813896179, eps 0.001000000000000049, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 750, time 40.28320670127869, eps 0.0010000000000000466, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 751, time 41.353055477142334, eps 0.0010000000000000443, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 752, time 41.41630530357361, eps 0.001000000000000042, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 753, time 40.431156158447266, eps 0.0010000000000000401, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 754, time 42.14335370063782, eps 0.0010000000000000382, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 755, time 41.483028411865234, eps 0.0010000000000000362, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 756, time 40.540624380111694, eps 0.0010000000000000345, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 757, time 40.2232608795166, eps 0.0010000000000000328, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 758, time 41.21886229515076, eps 0.0010000000000000312, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
goal_identified
goal_identified
=== ep: 759, time 42.162275552749634, eps 0.0010000000000000297, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 760, time 43.10357642173767, eps 0.0010000000000000282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 761, time 42.83171796798706, eps 0.001000000000000027, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 762, time 41.74197030067444, eps 0.0010000000000000256, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
goal_identified
goal_identified
=== ep: 763, time 42.01344347000122, eps 0.0010000000000000243, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 764, time 41.318896532058716, eps 0.0010000000000000232, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 765, time 40.54888439178467, eps 0.001000000000000022, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
goal_identified
goal_identified
=== ep: 766, time 40.55744695663452, eps 0.0010000000000000208, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
=== ep: 767, time 40.33064079284668, eps 0.00100000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 768, time 43.11786961555481, eps 0.0010000000000000189, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
goal_identified
=== ep: 769, time 42.286686420440674, eps 0.001000000000000018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 770, time 40.534931898117065, eps 0.0010000000000000172, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 771, time 39.80032253265381, eps 0.0010000000000000163, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 772, time 42.60687327384949, eps 0.0010000000000000154, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 773, time 43.33593940734863, eps 0.0010000000000000148, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 774, time 42.22570252418518, eps 0.0010000000000000141, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 775, time 42.4781334400177, eps 0.0010000000000000132, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 776, time 43.00320339202881, eps 0.0010000000000000126, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
=== ep: 777, time 42.47549366950989, eps 0.0010000000000000122, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 778, time 42.67504525184631, eps 0.0010000000000000115, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 779, time 42.95150279998779, eps 0.0010000000000000109, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 780, time 43.82797455787659, eps 0.0010000000000000104, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 781, time 42.39396786689758, eps 0.00100000000000001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 782, time 41.874900341033936, eps 0.0010000000000000093, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 783, time 42.7004873752594, eps 0.001000000000000009, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 784, time 42.11787009239197, eps 0.0010000000000000085, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 785, time 42.50817251205444, eps 0.001000000000000008, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 786, time 42.74489712715149, eps 0.0010000000000000076, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
goal_identified
goal_identified
goal_identified
=== ep: 787, time 41.909841775894165, eps 0.0010000000000000074, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
goal_identified
goal_identified
=== ep: 788, time 42.489734172821045, eps 0.001000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 789, time 42.223384380340576, eps 0.0010000000000000067, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 790, time 42.677480697631836, eps 0.0010000000000000063, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 791, time 41.87632989883423, eps 0.001000000000000006, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 792, time 43.68317484855652, eps 0.0010000000000000057, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 793, time 42.86331915855408, eps 0.0010000000000000054, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 794, time 42.68203020095825, eps 0.0010000000000000052, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
goal_identified
goal_identified
=== ep: 795, time 42.48131084442139, eps 0.001000000000000005, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 796, time 42.279244899749756, eps 0.0010000000000000048, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
goal_identified
goal_identified
goal_identified
=== ep: 797, time 42.31425452232361, eps 0.0010000000000000044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 798, time 42.66865873336792, eps 0.0010000000000000041, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 799, time 42.4698588848114, eps 0.0010000000000000041, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
goal_identified
goal_identified
=== ep: 800, time 42.9339165687561, eps 0.001000000000000004, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 801, time 42.3225622177124, eps 0.0010000000000000037, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
goal_identified
goal_identified
=== ep: 802, time 42.45196294784546, eps 0.0010000000000000035, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
goal_identified
goal_identified
goal_identified
=== ep: 803, time 42.57010197639465, eps 0.0010000000000000033, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
goal_identified
goal_identified
goal_identified
=== ep: 804, time 43.55123829841614, eps 0.001000000000000003, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
goal_identified
goal_identified
goal_identified
=== ep: 805, time 41.99232625961304, eps 0.001000000000000003, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
goal_identified
goal_identified
goal_identified
=== ep: 806, time 43.77666115760803, eps 0.0010000000000000028, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 807, time 42.6214714050293, eps 0.0010000000000000026, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 808, time 42.67376947402954, eps 0.0010000000000000026, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 809, time 42.39140200614929, eps 0.0010000000000000024, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
goal_identified
goal_identified
=== ep: 810, time 42.536689043045044, eps 0.0010000000000000024, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 811, time 42.82970213890076, eps 0.0010000000000000022, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 812, time 41.96353220939636, eps 0.0010000000000000022, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 813, time 42.352599143981934, eps 0.001000000000000002, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 814, time 42.492838621139526, eps 0.001000000000000002, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
goal_identified
goal_identified
=== ep: 815, time 42.36157441139221, eps 0.0010000000000000018, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
goal_identified
goal_identified
goal_identified
=== ep: 816, time 43.12061333656311, eps 0.0010000000000000018, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
goal_identified
goal_identified
goal_identified
=== ep: 817, time 43.12848091125488, eps 0.0010000000000000018, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
goal_identified
goal_identified
goal_identified
=== ep: 818, time 42.76988697052002, eps 0.0010000000000000015, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 819, time 44.12268018722534, eps 0.0010000000000000015, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 820, time 42.654223680496216, eps 0.0010000000000000013, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 821, time 42.58626675605774, eps 0.0010000000000000013, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 42.422849893569946, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 823, time 43.3327910900116, eps 0.0010000000000000013, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 824, time 42.72621560096741, eps 0.001000000000000001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
goal_identified
goal_identified
=== ep: 825, time 42.720538854599, eps 0.001000000000000001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 826, time 42.64479422569275, eps 0.001000000000000001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
goal_identified
goal_identified
goal_identified
=== ep: 827, time 42.542951822280884, eps 0.001000000000000001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 828, time 42.52984809875488, eps 0.0010000000000000009, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 829, time 42.23177719116211, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 830, time 42.90371108055115, eps 0.0010000000000000009, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
goal_identified
goal_identified
goal_identified
=== ep: 831, time 42.507139444351196, eps 0.0010000000000000009, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
goal_identified
goal_identified
goal_identified
=== ep: 832, time 43.04031944274902, eps 0.0010000000000000009, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 833, time 43.95956301689148, eps 0.0010000000000000007, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 834, time 43.589253664016724, eps 0.0010000000000000007, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 835, time 44.76147484779358, eps 0.0010000000000000007, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 836, time 43.47054624557495, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 837, time 43.235942363739014, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 838, time 43.34395480155945, eps 0.0010000000000000007, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 839, time 43.262819051742554, eps 0.0010000000000000007, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
goal_identified
goal_identified
=== ep: 840, time 43.561572551727295, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 841, time 42.439719676971436, eps 0.0010000000000000005, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 842, time 42.99708008766174, eps 0.0010000000000000005, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 843, time 42.25638794898987, eps 0.0010000000000000005, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 844, time 42.94736909866333, eps 0.0010000000000000005, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
goal_identified
goal_identified
goal_identified
=== ep: 845, time 42.855979681015015, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
goal_identified
goal_identified
goal_identified
=== ep: 846, time 43.01436400413513, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 847, time 44.148563385009766, eps 0.0010000000000000005, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 848, time 43.11984181404114, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 849, time 43.142991065979004, eps 0.0010000000000000005, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 850, time 42.90838885307312, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
goal_identified
goal_identified
goal_identified
=== ep: 851, time 42.80299639701843, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
goal_identified
=== ep: 852, time 43.902183055877686, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 853, time 43.23256206512451, eps 0.0010000000000000002, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 854, time 42.855926752090454, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
goal_identified
goal_identified
goal_identified
=== ep: 855, time 43.039117097854614, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
goal_identified
goal_identified
goal_identified
=== ep: 856, time 42.28331780433655, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 857, time 43.648752212524414, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 858, time 42.78749752044678, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
goal_identified
=== ep: 859, time 45.273674726486206, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
goal_identified
goal_identified
goal_identified
=== ep: 860, time 43.127845287323, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 861, time 44.08228087425232, eps 0.0010000000000000002, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 862, time 44.16868329048157, eps 0.0010000000000000002, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
goal_identified
goal_identified
goal_identified
=== ep: 863, time 44.76114368438721, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
goal_identified
goal_identified
=== ep: 864, time 44.17778730392456, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 865, time 43.191041231155396, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 866, time 45.17644429206848, eps 0.0010000000000000002, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 867, time 43.95528244972229, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 137/137)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 868, time 42.240543365478516, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 869, time 43.1978645324707, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 870, time 44.605061292648315, eps 0.0010000000000000002, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 871, time 46.326473236083984, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
goal_identified
=== ep: 872, time 43.4582896232605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 873, time 43.15217089653015, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
goal_identified
goal_identified
goal_identified
=== ep: 874, time 44.57573175430298, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 875, time 44.71032929420471, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 876, time 45.92965865135193, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 877, time 42.98431181907654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 878, time 42.61851382255554, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 879, time 45.177799701690674, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 880, time 45.019043922424316, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
goal_identified
goal_identified
=== ep: 881, time 46.7817120552063, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 882, time 43.73461556434631, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 883, time 42.87540817260742, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
goal_identified
goal_identified
goal_identified
=== ep: 884, time 42.954060792922974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 885, time 42.79201030731201, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 886, time 45.74897122383118, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
goal_identified
goal_identified
=== ep: 887, time 45.719488859176636, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 888, time 45.26715683937073, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
goal_identified
goal_identified
=== ep: 889, time 45.85462689399719, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
goal_identified
goal_identified
=== ep: 890, time 46.506856203079224, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 891, time 46.44573163986206, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 892, time 43.69522833824158, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
goal_identified
goal_identified
goal_identified
=== ep: 893, time 42.58239221572876, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
goal_identified
=== ep: 894, time 43.17103958129883, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
goal_identified
goal_identified
=== ep: 895, time 42.493876218795776, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 896, time 42.97942662239075, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 897, time 42.83586645126343, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 898, time 45.61176800727844, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 898
goal_identified
goal_identified
goal_identified
=== ep: 899, time 44.336347818374634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 900, time 44.98836064338684, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 901, time 45.122713565826416, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 902, time 45.4682502746582, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 903, time 47.91844630241394, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
goal_identified
goal_identified
=== ep: 904, time 45.61797332763672, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
goal_identified
=== ep: 905, time 47.33072280883789, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 906, time 46.20006251335144, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 907, time 46.29466223716736, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
goal_identified
goal_identified
goal_identified
=== ep: 908, time 47.06044340133667, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 909, time 45.81183409690857, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
goal_identified
goal_identified
goal_identified
=== ep: 910, time 47.94250440597534, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
goal_identified
goal_identified
goal_identified
=== ep: 911, time 46.50168442726135, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
goal_identified
goal_identified
=== ep: 912, time 46.069846630096436, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 913, time 46.09275269508362, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 914, time 46.414597511291504, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 915, time 48.86051678657532, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 916, time 46.56050395965576, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 917, time 46.63771939277649, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 918, time 46.94612383842468, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 919, time 45.796034812927246, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 920, time 50.201106786727905, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 921, time 47.07897448539734, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 922, time 46.56681251525879, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 923, time 45.99002695083618, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 924, time 46.6020393371582, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 925, time 48.282713174819946, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 926, time 46.362754821777344, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 927, time 49.073322772979736, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 928, time 45.30057501792908, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 929, time 47.32871770858765, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
goal_identified
goal_identified
goal_identified
=== ep: 930, time 45.120213747024536, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 931, time 45.2031466960907, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 932, time 47.22693467140198, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 933, time 44.91298961639404, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 934, time 45.11749267578125, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 935, time 44.84598135948181, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
goal_identified
goal_identified
=== ep: 936, time 44.15836954116821, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 937, time 46.02106022834778, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 938, time 44.02568960189819, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 939, time 44.94564151763916, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
goal_identified
goal_identified
goal_identified
=== ep: 940, time 44.40084671974182, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 941, time 44.0851993560791, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
goal_identified
goal_identified
goal_identified
=== ep: 942, time 46.360377073287964, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 943, time 44.1923770904541, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 944, time 44.55610990524292, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 945, time 46.421364545822144, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 946, time 44.199482917785645, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 947, time 46.73202204704285, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 948, time 44.96744966506958, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
goal_identified
goal_identified
goal_identified
=== ep: 949, time 44.63471460342407, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
goal_identified
=== ep: 950, time 45.13353419303894, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 951, time 46.81807470321655, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
goal_identified
=== ep: 952, time 46.237903356552124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 953, time 44.95538806915283, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
goal_identified
goal_identified
goal_identified
=== ep: 954, time 45.05321669578552, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 955, time 44.57415556907654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 956, time 44.51611685752869, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 957, time 47.32945799827576, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
=== ep: 958, time 45.51273727416992, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
goal_identified
=== ep: 959, time 44.30776023864746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
goal_identified
goal_identified
goal_identified
=== ep: 960, time 44.58356857299805, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
goal_identified
goal_identified
goal_identified
=== ep: 961, time 45.5113890171051, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
goal_identified
goal_identified
goal_identified
=== ep: 962, time 46.131454944610596, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
goal_identified
goal_identified
=== ep: 963, time 44.854915380477905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 964, time 45.46917510032654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
goal_identified
=== ep: 965, time 47.59023904800415, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 966, time 46.518290758132935, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
goal_identified
goal_identified
goal_identified
=== ep: 967, time 47.44543385505676, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 968, time 46.09999752044678, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
goal_identified
goal_identified
goal_identified
=== ep: 969, time 47.213500022888184, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 970, time 50.55889081954956, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 971, time 49.320979833602905, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 972, time 51.00496029853821, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 973, time 49.01079750061035, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 974, time 49.71155905723572, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
goal_identified
goal_identified
=== ep: 975, time 48.74639654159546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 976, time 47.65865397453308, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 977, time 48.132912397384644, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 978, time 46.26969599723816, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 979, time 45.80805587768555, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 980, time 45.748708724975586, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 981, time 44.713717460632324, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
goal_identified
goal_identified
goal_identified
=== ep: 982, time 45.40162658691406, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 983, time 44.82260084152222, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 984, time 46.57125735282898, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 985, time 46.8753936290741, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 986, time 45.079272985458374, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 987, time 44.77651333808899, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 988, time 45.076128244400024, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 989, time 44.820191621780396, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
goal_identified
goal_identified
goal_identified
=== ep: 990, time 47.720335245132446, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 991, time 45.78245544433594, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 992, time 46.78313589096069, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 993, time 44.77746367454529, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 994, time 46.05232501029968, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
goal_identified
goal_identified
=== ep: 995, time 49.16950535774231, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 996, time 45.214842796325684, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 997, time 46.5230507850647, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 998, time 45.270756483078, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 999, time 45.695324182510376, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1000, time 52.2919442653656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1001, time 45.33611297607422, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1002, time 45.46870970726013, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1003, time 44.80788826942444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
goal_identified
goal_identified
goal_identified
=== ep: 1004, time 45.31277871131897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
goal_identified
goal_identified
goal_identified
=== ep: 1005, time 48.94462013244629, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1006, time 45.41479802131653, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1007, time 45.6821403503418, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
goal_identified
goal_identified
=== ep: 1008, time 52.85318350791931, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
goal_identified
goal_identified
goal_identified
=== ep: 1009, time 44.71069073677063, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1010, time 45.226908445358276, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
goal_identified
goal_identified
goal_identified
=== ep: 1011, time 45.6702983379364, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1012, time 45.436039686203, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
goal_identified
goal_identified
=== ep: 1013, time 50.29412341117859, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
goal_identified
=== ep: 1014, time 45.38238167762756, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
=== ep: 1015, time 45.039544105529785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1016, time 48.814436197280884, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
goal_identified
goal_identified
=== ep: 1017, time 46.94255566596985, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1018, time 48.32569193840027, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
goal_identified
goal_identified
=== ep: 1019, time 46.1544291973114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
goal_identified
goal_identified
goal_identified
=== ep: 1020, time 45.11237692832947, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1021, time 50.03249931335449, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
goal_identified
goal_identified
goal_identified
=== ep: 1022, time 44.63140797615051, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1023, time 47.71670150756836, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1024, time 45.93346643447876, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1025, time 45.467339277267456, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1026, time 47.97285723686218, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1027, time 45.30690622329712, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1028, time 47.18486428260803, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1029, time 47.8342866897583, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1030, time 45.72027516365051, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
goal_identified
=== ep: 1031, time 45.98806142807007, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1032, time 45.53297305107117, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1033, time 46.910207748413086, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
goal_identified
goal_identified
goal_identified
=== ep: 1034, time 50.56460523605347, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1035, time 45.615410804748535, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1036, time 45.24985599517822, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1037, time 46.30850267410278, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1038, time 48.43421697616577, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
goal_identified
goal_identified
=== ep: 1039, time 47.567890882492065, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1040, time 45.48842453956604, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
goal_identified
goal_identified
=== ep: 1041, time 45.80802607536316, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
goal_identified
goal_identified
=== ep: 1042, time 47.811400175094604, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1043, time 45.29286289215088, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1044, time 45.28084063529968, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1045, time 45.590648889541626, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1046, time 48.406569957733154, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1047, time 49.64626598358154, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1048, time 45.46866822242737, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
goal_identified
goal_identified
goal_identified
=== ep: 1049, time 44.525840759277344, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1050, time 45.564696311950684, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1051, time 49.83381223678589, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1052, time 48.088242292404175, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
=== ep: 1053, time 44.88357996940613, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
goal_identified
goal_identified
goal_identified
=== ep: 1054, time 46.55908417701721, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1055, time 48.87344932556152, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1056, time 45.84127402305603, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1057, time 45.49417519569397, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1058, time 45.616644620895386, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
goal_identified
goal_identified
goal_identified
=== ep: 1059, time 47.86081337928772, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1060, time 49.24027705192566, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1061, time 45.97978377342224, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
goal_identified
=== ep: 1062, time 45.20126461982727, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
goal_identified
goal_identified
=== ep: 1063, time 45.61677384376526, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
goal_identified
goal_identified
=== ep: 1064, time 47.06821417808533, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1065, time 48.710265159606934, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
goal_identified
goal_identified
=== ep: 1066, time 45.492873668670654, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1067, time 46.79433560371399, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1068, time 46.43551850318909, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
goal_identified
goal_identified
=== ep: 1069, time 47.959585428237915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1070, time 48.131282329559326, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1071, time 46.0219042301178, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
goal_identified
goal_identified
=== ep: 1072, time 46.88907742500305, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1072
goal_identified
goal_identified
goal_identified
=== ep: 1073, time 47.600932598114014, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1074, time 46.35937428474426, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
goal_identified
goal_identified
=== ep: 1075, time 45.443270444869995, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1076, time 45.67340707778931, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1077, time 46.694782972335815, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1078, time 48.940531492233276, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1079, time 45.873918294906616, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1080, time 45.77296710014343, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1081, time 46.727091550827026, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1082, time 48.9281005859375, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1083, time 47.06523418426514, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1084, time 46.04761219024658, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1085, time 45.506792068481445, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1086, time 48.705873012542725, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1087, time 50.331687688827515, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1088, time 45.40942406654358, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1089, time 45.780524253845215, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
goal_identified
goal_identified
=== ep: 1090, time 45.91880202293396, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
goal_identified
goal_identified
=== ep: 1091, time 46.97774529457092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
goal_identified
goal_identified
=== ep: 1092, time 45.98518252372742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
goal_identified
=== ep: 1093, time 45.50473737716675, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
goal_identified
goal_identified
goal_identified
=== ep: 1094, time 48.53309917449951, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1095, time 48.44956874847412, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1096, time 45.52617049217224, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
goal_identified
goal_identified
=== ep: 1097, time 45.30264639854431, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1098, time 46.09055805206299, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1099, time 49.52569055557251, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1100, time 47.32282328605652, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
goal_identified
=== ep: 1101, time 45.548807859420776, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1102, time 46.57473063468933, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
goal_identified
goal_identified
goal_identified
=== ep: 1103, time 47.158562898635864, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
goal_identified
goal_identified
=== ep: 1104, time 48.71532654762268, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1105, time 47.6456081867218, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
=== ep: 1106, time 56.55087089538574, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
goal_identified
goal_identified
goal_identified
=== ep: 1107, time 45.70155096054077, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
goal_identified
goal_identified
=== ep: 1108, time 46.91449236869812, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
goal_identified
goal_identified
goal_identified
=== ep: 1109, time 48.54638957977295, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
goal_identified
goal_identified
=== ep: 1110, time 47.531656980514526, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1111, time 45.5650417804718, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
goal_identified
goal_identified
goal_identified
=== ep: 1112, time 49.88413667678833, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
goal_identified
goal_identified
goal_identified
=== ep: 1113, time 46.38395833969116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1114, time 46.38851857185364, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
goal_identified
goal_identified
goal_identified
=== ep: 1115, time 45.862980365753174, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1116, time 45.91861701011658, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
goal_identified
goal_identified
goal_identified
=== ep: 1117, time 48.87598013877869, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
goal_identified
goal_identified
=== ep: 1118, time 48.26414966583252, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1119, time 45.92300844192505, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1120, time 46.71177911758423, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1121, time 46.048293113708496, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1122, time 47.086852073669434, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
goal_identified
goal_identified
=== ep: 1123, time 45.75412201881409, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
goal_identified
goal_identified
=== ep: 1124, time 47.439366579055786, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1125, time 49.983717918395996, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
goal_identified
=== ep: 1126, time 47.008198261260986, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1127, time 45.92712473869324, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
goal_identified
goal_identified
goal_identified
=== ep: 1128, time 48.03632831573486, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
goal_identified
goal_identified
goal_identified
=== ep: 1129, time 46.42280864715576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1130, time 46.866910457611084, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1131, time 48.869348764419556, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1132, time 46.00314974784851, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1133, time 49.77163028717041, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
goal_identified
goal_identified
goal_identified
=== ep: 1134, time 45.84059548377991, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
goal_identified
goal_identified
goal_identified
=== ep: 1135, time 45.82520270347595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1136, time 49.89054775238037, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1137, time 46.282447814941406, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1138, time 48.594970703125, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
goal_identified
goal_identified
goal_identified
=== ep: 1139, time 47.31680202484131, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
goal_identified
goal_identified
=== ep: 1140, time 45.78576159477234, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1141, time 50.101951599121094, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
goal_identified
goal_identified
goal_identified
=== ep: 1142, time 48.06338953971863, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
goal_identified
goal_identified
goal_identified
=== ep: 1143, time 46.16551089286804, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
=== ep: 1144, time 48.13021802902222, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1145, time 45.97887825965881, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
goal_identified
goal_identified
=== ep: 1146, time 47.68787455558777, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
goal_identified
goal_identified
=== ep: 1147, time 46.517643213272095, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1148, time 47.03310704231262, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1149, time 49.728363275527954, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1150, time 46.36237859725952, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1151, time 46.19563817977905, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1152, time 49.879268407821655, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
goal_identified
goal_identified
=== ep: 1153, time 44.91535925865173, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
goal_identified
goal_identified
=== ep: 1154, time 48.311338663101196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1155, time 46.36130380630493, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1156, time 46.629019021987915, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
goal_identified
goal_identified
=== ep: 1157, time 53.36105251312256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
goal_identified
goal_identified
=== ep: 1158, time 45.53817319869995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1159, time 48.01766920089722, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1160, time 48.16890096664429, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
goal_identified
goal_identified
goal_identified
=== ep: 1161, time 47.721336364746094, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
goal_identified
goal_identified
=== ep: 1162, time 50.953258752822876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
goal_identified
goal_identified
=== ep: 1163, time 46.575639963150024, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
goal_identified
goal_identified
=== ep: 1164, time 45.54719519615173, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1165, time 54.04212045669556, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1166, time 46.03096055984497, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1167, time 46.24899888038635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1168, time 48.989887952804565, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1169, time 46.047101736068726, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
goal_identified
goal_identified
goal_identified
=== ep: 1170, time 48.33036708831787, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
goal_identified
goal_identified
goal_identified
=== ep: 1171, time 46.93000364303589, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
goal_identified
goal_identified
goal_identified
=== ep: 1172, time 45.869507789611816, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1173, time 55.71094489097595, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
goal_identified
goal_identified
=== ep: 1174, time 46.17514395713806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
goal_identified
goal_identified
=== ep: 1175, time 45.744489431381226, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1176, time 49.29650020599365, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 702
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1177, time 45.64581799507141, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1178, time 46.56983184814453, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1179, time 46.42046070098877, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1180, time 48.14366841316223, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
goal_identified
goal_identified
=== ep: 1181, time 52.18149495124817, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1182, time 45.54765295982361, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1183, time 45.89089822769165, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
goal_identified
goal_identified
goal_identified
=== ep: 1184, time 51.050309896469116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1185, time 45.862135887145996, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
goal_identified
goal_identified
=== ep: 1186, time 46.091896772384644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1187, time 46.20015501976013, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1188, time 45.54759454727173, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
goal_identified
goal_identified
goal_identified
=== ep: 1189, time 49.22255539894104, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1190, time 45.96901488304138, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1191, time 45.56545424461365, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1192, time 52.36956858634949, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1193, time 45.895158767700195, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1194, time 45.46654963493347, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1195, time 51.17213416099548, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
goal_identified
goal_identified
goal_identified
=== ep: 1196, time 46.02037334442139, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
goal_identified
=== ep: 1197, time 45.948994398117065, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
goal_identified
goal_identified
=== ep: 1198, time 49.036235094070435, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
goal_identified
goal_identified
goal_identified
=== ep: 1199, time 45.86322999000549, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1200, time 47.48049473762512, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
goal_identified
goal_identified
=== ep: 1201, time 45.73141360282898, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1202, time 45.38242530822754, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
goal_identified
goal_identified
goal_identified
=== ep: 1203, time 52.104655027389526, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1204, time 47.27001142501831, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1205, time 45.96990609169006, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
goal_identified
goal_identified
=== ep: 1206, time 47.85759925842285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
goal_identified
goal_identified
goal_identified
=== ep: 1207, time 45.62770485877991, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
goal_identified
goal_identified
goal_identified
=== ep: 1208, time 49.57247233390808, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1209, time 46.080944776535034, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
goal_identified
goal_identified
=== ep: 1210, time 46.077481508255005, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
goal_identified
goal_identified
goal_identified
=== ep: 1211, time 54.094141483306885, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1212, time 46.13102674484253, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
goal_identified
goal_identified
=== ep: 1213, time 46.02116775512695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1214, time 50.016440868377686, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
goal_identified
goal_identified
goal_identified
=== ep: 1215, time 45.50040030479431, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1216, time 45.91767334938049, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
goal_identified
goal_identified
goal_identified
=== ep: 1217, time 45.98259997367859, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
goal_identified
goal_identified
goal_identified
=== ep: 1218, time 48.11193895339966, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1219, time 51.805405139923096, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
goal_identified
goal_identified
goal_identified
=== ep: 1220, time 45.7302725315094, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
goal_identified
goal_identified
goal_identified
=== ep: 1221, time 45.44882369041443, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
goal_identified
goal_identified
=== ep: 1222, time 53.48241662979126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1223, time 45.94938564300537, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1224, time 46.28850769996643, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1225, time 48.6214759349823, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1226, time 46.41805863380432, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
goal_identified
goal_identified
goal_identified
=== ep: 1227, time 46.74542832374573, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1228, time 47.23050594329834, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1229, time 45.98466658592224, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1230, time 48.838669538497925, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
goal_identified
goal_identified
goal_identified
=== ep: 1231, time 45.93971633911133, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1232, time 46.118170738220215, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1233, time 49.23046255111694, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
goal_identified
goal_identified
goal_identified
=== ep: 1234, time 46.06373119354248, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1235, time 46.41867446899414, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
goal_identified
goal_identified
goal_identified
=== ep: 1236, time 49.004454374313354, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1236
goal_identified
goal_identified
=== ep: 1237, time 46.62121939659119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
goal_identified
goal_identified
goal_identified
=== ep: 1238, time 49.01593470573425, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1239, time 46.30198311805725, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
goal_identified
goal_identified
=== ep: 1240, time 46.635730028152466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
goal_identified
goal_identified
=== ep: 1241, time 51.201231241226196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1242, time 46.134888648986816, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1243, time 46.41923666000366, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1244, time 51.274078607559204, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1245, time 46.79621076583862, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
goal_identified
=== ep: 1246, time 47.3553102016449, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
goal_identified
goal_identified
goal_identified
=== ep: 1247, time 45.96587896347046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
goal_identified
goal_identified
goal_identified
=== ep: 1248, time 46.387471199035645, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1249, time 52.81689643859863, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1250, time 46.2402126789093, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1251, time 47.073694467544556, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
goal_identified
=== ep: 1252, time 53.20724940299988, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1253, time 46.573330879211426, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1254, time 46.21863007545471, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
goal_identified
=== ep: 1255, time 47.04945945739746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
goal_identified
goal_identified
goal_identified
=== ep: 1256, time 46.4746778011322, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
goal_identified
goal_identified
=== ep: 1257, time 47.64121961593628, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1258, time 46.845332860946655, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1259, time 48.24065852165222, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1260, time 51.276779651641846, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1261, time 46.408684968948364, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
goal_identified
goal_identified
=== ep: 1262, time 46.23800230026245, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
goal_identified
=== ep: 1263, time 50.61816167831421, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
goal_identified
goal_identified
=== ep: 1264, time 46.421003103256226, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
goal_identified
goal_identified
goal_identified
=== ep: 1265, time 47.45707559585571, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
goal_identified
goal_identified
goal_identified
=== ep: 1266, time 46.43778371810913, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
goal_identified
goal_identified
=== ep: 1267, time 46.283217430114746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
goal_identified
goal_identified
goal_identified
=== ep: 1268, time 53.22980713844299, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1269, time 46.68449139595032, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1270, time 46.30656290054321, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1271, time 50.252577781677246, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1272, time 46.31958746910095, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
goal_identified
goal_identified
goal_identified
=== ep: 1273, time 47.64523005485535, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 46.830031633377075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1275, time 46.03317594528198, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1276, time 52.50342774391174, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1277, time 46.630268812179565, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1278, time 46.44158124923706, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
goal_identified
goal_identified
=== ep: 1279, time 48.69766283035278, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1280, time 49.06732153892517, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1281, time 50.48072576522827, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1282, time 46.71501970291138, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1283, time 46.41420531272888, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1284, time 52.597673416137695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
goal_identified
goal_identified
goal_identified
=== ep: 1285, time 45.468069553375244, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
goal_identified
goal_identified
goal_identified
=== ep: 1286, time 46.716874837875366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1287, time 47.50168490409851, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1288, time 46.57657432556152, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1289, time 49.74352025985718, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1290, time 46.625951290130615, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
goal_identified
goal_identified
=== ep: 1291, time 46.93433880805969, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1292, time 52.77970027923584, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1293, time 46.60134243965149, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1293
goal_identified
goal_identified
goal_identified
=== ep: 1294, time 46.264058113098145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
goal_identified
goal_identified
=== ep: 1295, time 49.3588490486145, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1296, time 46.24141478538513, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
goal_identified
goal_identified
goal_identified
=== ep: 1297, time 51.42005014419556, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1298, time 50.29842758178711, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1299, time 46.73889231681824, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1300, time 53.1506986618042, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1301, time 49.17812132835388, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
goal_identified
goal_identified
goal_identified
=== ep: 1302, time 47.215084075927734, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1303, time 46.19675421714783, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
goal_identified
goal_identified
=== ep: 1304, time 47.02006769180298, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1305, time 53.763370513916016, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
goal_identified
goal_identified
goal_identified
=== ep: 1306, time 46.378607511520386, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1307, time 46.42260718345642, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1308, time 51.891112089157104, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1309, time 46.43497276306152, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1310, time 47.09778642654419, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
=== ep: 1311, time 46.125688791275024, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
goal_identified
goal_identified
goal_identified
=== ep: 1312, time 46.38793182373047, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1313, time 50.15482950210571, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1314, time 46.9095516204834, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1315, time 46.252793073654175, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
goal_identified
goal_identified
=== ep: 1316, time 51.242074728012085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1317, time 46.20631718635559, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 141/141)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1318, time 45.96695899963379, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
goal_identified
goal_identified
goal_identified
=== ep: 1319, time 49.5250449180603, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
goal_identified
goal_identified
goal_identified
=== ep: 1320, time 46.31427550315857, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1321, time 49.62338209152222, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
goal_identified
goal_identified
goal_identified
=== ep: 1322, time 48.62804126739502, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
goal_identified
goal_identified
goal_identified
=== ep: 1323, time 46.528199911117554, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
goal_identified
goal_identified
=== ep: 1324, time 52.39979004859924, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
goal_identified
goal_identified
=== ep: 1325, time 46.86195707321167, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
goal_identified
goal_identified
=== ep: 1326, time 46.66668915748596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1327, time 50.20552968978882, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1328, time 45.90963315963745, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1329, time 49.367077589035034, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
goal_identified
goal_identified
=== ep: 1330, time 45.8124725818634, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1331, time 45.960981369018555, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1332, time 50.99246525764465, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
goal_identified
goal_identified
=== ep: 1333, time 46.20398259162903, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1334, time 46.961299657821655, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1335, time 48.57996916770935, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
goal_identified
goal_identified
goal_identified
=== ep: 1336, time 45.45689415931702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1337, time 48.27229690551758, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1338, time 47.36952781677246, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1339, time 46.965965032577515, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1340, time 50.70329165458679, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
goal_identified
=== ep: 1341, time 47.68284296989441, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1342, time 46.27439832687378, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
goal_identified
goal_identified
=== ep: 1343, time 49.209813594818115, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1344, time 49.26191210746765, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1345, time 52.65126371383667, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1346, time 45.64038300514221, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1347, time 46.800163984298706, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1348, time 51.49011850357056, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
goal_identified
goal_identified
=== ep: 1349, time 46.09925556182861, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
goal_identified
goal_identified
=== ep: 1350, time 46.494582176208496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1351, time 48.611053705215454, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1352, time 45.827702045440674, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1353, time 47.68394422531128, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1354, time 49.534175634384155, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1355, time 46.55794858932495, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1356, time 50.39646100997925, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1357, time 48.277743339538574, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
goal_identified
goal_identified
goal_identified
=== ep: 1358, time 46.153972864151, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
goal_identified
goal_identified
=== ep: 1359, time 46.61503577232361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
goal_identified
=== ep: 1360, time 50.05669116973877, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1361, time 49.58530831336975, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
goal_identified
goal_identified
goal_identified
=== ep: 1362, time 46.53483724594116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1363, time 48.82101917266846, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
goal_identified
goal_identified
goal_identified
=== ep: 1364, time 49.45231628417969, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1365, time 47.252371311187744, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
goal_identified
goal_identified
goal_identified
=== ep: 1366, time 46.14140820503235, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1367, time 49.86001658439636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
goal_identified
goal_identified
=== ep: 1368, time 46.39293122291565, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1369, time 50.20072317123413, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
goal_identified
goal_identified
goal_identified
=== ep: 1370, time 46.824429512023926, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
=== ep: 1371, time 50.12174463272095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1372, time 50.699716091156006, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1373, time 46.51333832740784, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1374, time 47.691956996917725, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
goal_identified
=== ep: 1375, time 46.780051946640015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1376, time 47.05214238166809, eps 0.001, sum reward: 9, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
=== ep: 1377, time 46.16493105888367, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1378, time 47.23224925994873, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1379, time 47.91107368469238, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
goal_identified
goal_identified
goal_identified
=== ep: 1380, time 48.18124198913574, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1381, time 46.06143522262573, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
goal_identified
=== ep: 1382, time 49.75574779510498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1383, time 51.06287145614624, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1384, time 46.92038607597351, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1385, time 48.75372552871704, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
goal_identified
goal_identified
goal_identified
=== ep: 1386, time 49.474650382995605, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
goal_identified
goal_identified
=== ep: 1387, time 46.99265813827515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
goal_identified
goal_identified
=== ep: 1388, time 46.94609045982361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1389, time 47.88468861579895, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1390, time 47.00459694862366, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
goal_identified
goal_identified
goal_identified
=== ep: 1391, time 52.124990701675415, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1392, time 46.72506022453308, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
goal_identified
goal_identified
goal_identified
=== ep: 1393, time 53.845826625823975, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1394, time 46.84893751144409, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1395, time 46.542927265167236, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1396, time 51.30769419670105, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
goal_identified
goal_identified
goal_identified
=== ep: 1397, time 46.719852924346924, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1398, time 47.26524996757507, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1399, time 50.586973667144775, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
goal_identified
goal_identified
goal_identified
=== ep: 1400, time 46.591028451919556, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
goal_identified
goal_identified
goal_identified
=== ep: 1401, time 48.1174898147583, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1402, time 47.23090887069702, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1403, time 46.42220067977905, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
goal_identified
goal_identified
=== ep: 1404, time 48.81697726249695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
goal_identified
goal_identified
=== ep: 1405, time 46.595165967941284, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1406, time 46.665239095687866, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1407, time 51.07781410217285, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
goal_identified
goal_identified
=== ep: 1408, time 46.81483268737793, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1409, time 47.166210889816284, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1410, time 52.295340061187744, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1411, time 47.22804117202759, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
goal_identified
goal_identified
goal_identified
=== ep: 1412, time 46.93060350418091, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
goal_identified
goal_identified
=== ep: 1413, time 49.258771896362305, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1414, time 49.56350302696228, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1415, time 51.97546744346619, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1416, time 46.82790780067444, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1417, time 50.21691393852234, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
goal_identified
goal_identified
=== ep: 1418, time 52.65831255912781, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
goal_identified
goal_identified
goal_identified
=== ep: 1419, time 46.983524322509766, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1420, time 47.08144664764404, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 49.81232786178589, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1422, time 46.818278312683105, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
goal_identified
=== ep: 1423, time 49.772372007369995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1424, time 47.19229316711426, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
goal_identified
goal_identified
goal_identified
=== ep: 1425, time 47.79460954666138, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1426, time 48.195947885513306, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1427, time 46.72446084022522, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
goal_identified
goal_identified
goal_identified
=== ep: 1428, time 46.9219605922699, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
goal_identified
goal_identified
=== ep: 1429, time 47.11770820617676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
goal_identified
goal_identified
goal_identified
=== ep: 1430, time 46.5926308631897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1431, time 47.69123959541321, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1432, time 48.75791883468628, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
goal_identified
goal_identified
=== ep: 1433, time 46.647799491882324, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1434, time 51.673672676086426, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
goal_identified
=== ep: 1435, time 49.67546343803406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1436, time 47.21908497810364, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1437, time 48.161051750183105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1438, time 49.475852489471436, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1439, time 49.643595695495605, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1440, time 48.24814057350159, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1441, time 52.12301850318909, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1442, time 47.72688961029053, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
goal_identified
goal_identified
=== ep: 1443, time 49.070289850234985, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
goal_identified
goal_identified
goal_identified
=== ep: 1444, time 47.420337200164795, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1445, time 47.13164019584656, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
goal_identified
goal_identified
goal_identified
=== ep: 1446, time 50.02058410644531, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
goal_identified
goal_identified
goal_identified
=== ep: 1447, time 51.34673309326172, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1448, time 47.1743586063385, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1449, time 50.07618832588196, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1450, time 50.09008574485779, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1451, time 47.3477725982666, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1452, time 46.32169723510742, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1453, time 49.164305448532104, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1454, time 47.432809352874756, eps 0.001, sum reward: 9, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
goal_identified
goal_identified
goal_identified
=== ep: 1455, time 50.67316818237305, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1456, time 47.69495105743408, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
goal_identified
goal_identified
goal_identified
=== ep: 1457, time 51.86448907852173, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
goal_identified
goal_identified
=== ep: 1458, time 50.24593424797058, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
goal_identified
goal_identified
=== ep: 1459, time 47.62784171104431, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1460, time 49.512146949768066, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1461, time 48.56677794456482, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1462, time 50.470683574676514, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1463, time 51.152185916900635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1464, time 48.44075560569763, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
goal_identified
goal_identified
=== ep: 1465, time 50.24652814865112, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1466, time 49.84764528274536, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1467, time 47.74375581741333, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1468, time 51.106606006622314, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
goal_identified
goal_identified
=== ep: 1469, time 47.952773571014404, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1470, time 48.14365220069885, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
goal_identified
=== ep: 1471, time 51.69268035888672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
goal_identified
goal_identified
goal_identified
=== ep: 1472, time 47.857731103897095, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
goal_identified
goal_identified
goal_identified
=== ep: 1473, time 47.7873649597168, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
goal_identified
goal_identified
=== ep: 1474, time 54.77871251106262, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1475, time 48.046135902404785, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1476, time 47.716596841812134, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1477, time 48.501035928726196, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1478, time 47.65084481239319, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1479, time 47.75493788719177, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1480, time 47.8003351688385, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1481, time 47.22575521469116, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1482, time 51.81024122238159, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1483, time 48.107938051223755, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
goal_identified
goal_identified
=== ep: 1484, time 48.215384006500244, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1485, time 56.077874183654785, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1486, time 49.92270827293396, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
goal_identified
goal_identified
goal_identified
=== ep: 1487, time 47.91581130027771, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1488, time 54.12153363227844, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1489, time 48.581125020980835, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
=== ep: 1490, time 50.53830552101135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1491, time 47.768683671951294, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
goal_identified
goal_identified
goal_identified
=== ep: 1492, time 48.1628053188324, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1493, time 52.6212739944458, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1494, time 47.80938649177551, eps 0.001, sum reward: 5, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
goal_identified
goal_identified
goal_identified
=== ep: 1495, time 47.50705695152283, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
goal_identified
goal_identified
goal_identified
=== ep: 1496, time 51.25806260108948, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
goal_identified
goal_identified
goal_identified
=== ep: 1497, time 48.12782025337219, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
goal_identified
goal_identified
goal_identified
=== ep: 1498, time 50.29527282714844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
goal_identified
goal_identified
goal_identified
=== ep: 1499, time 47.52930808067322, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1500, time 47.60062766075134, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1501, time 52.22214984893799, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1502, time 47.90632939338684, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 47.08190870285034, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
goal_identified
goal_identified
goal_identified
=== ep: 1504, time 51.63930010795593, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
goal_identified
goal_identified
goal_identified
=== ep: 1505, time 47.74576735496521, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
goal_identified
=== ep: 1506, time 47.95397973060608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1507, time 49.57060766220093, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
goal_identified
=== ep: 1508, time 47.18341302871704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
goal_identified
goal_identified
goal_identified
=== ep: 1509, time 48.46707272529602, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
goal_identified
goal_identified
=== ep: 1510, time 53.05212140083313, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1511, time 48.28157997131348, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
goal_identified
=== ep: 1512, time 55.34626340866089, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1513, time 48.48648118972778, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1514, time 47.86112380027771, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
goal_identified
goal_identified
goal_identified
=== ep: 1515, time 47.579245805740356, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1516, time 47.34497356414795, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1517, time 50.141982078552246, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1518, time 50.17030882835388, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
goal_identified
=== ep: 1519, time 47.545273780822754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1520, time 51.454023599624634, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1521, time 52.328386068344116, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
goal_identified
goal_identified
goal_identified
=== ep: 1522, time 48.309417724609375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
goal_identified
goal_identified
=== ep: 1523, time 48.37236428260803, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1524, time 50.87161993980408, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1525, time 48.259674310684204, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1526, time 47.652156591415405, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1527, time 48.97202944755554, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
goal_identified
goal_identified
goal_identified
=== ep: 1528, time 51.216280698776245, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1529, time 48.08340501785278, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1530, time 48.539329051971436, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
goal_identified
goal_identified
goal_identified
=== ep: 1531, time 51.63684153556824, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1532, time 50.229047775268555, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
goal_identified
goal_identified
=== ep: 1533, time 47.91272473335266, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
goal_identified
=== ep: 1534, time 48.15217065811157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
goal_identified
=== ep: 1535, time 53.76975083351135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1536, time 54.895604848861694, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
goal_identified
goal_identified
goal_identified
=== ep: 1537, time 47.88057851791382, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1538, time 50.14216232299805, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1539, time 52.10158061981201, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1540, time 48.253538846969604, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
goal_identified
goal_identified
=== ep: 1541, time 47.88893699645996, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
goal_identified
=== ep: 1542, time 48.02860403060913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1543, time 49.25358486175537, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1544, time 52.4620144367218, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
goal_identified
goal_identified
goal_identified
=== ep: 1545, time 47.8190758228302, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1546, time 51.79314970970154, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
goal_identified
goal_identified
goal_identified
=== ep: 1547, time 49.434136152267456, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1548, time 47.871694564819336, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1549, time 52.81911063194275, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1550, time 48.181052923202515, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1551, time 47.79308342933655, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
goal_identified
goal_identified
=== ep: 1552, time 53.262147665023804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
goal_identified
goal_identified
goal_identified
=== ep: 1553, time 48.15222239494324, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
goal_identified
goal_identified
goal_identified
=== ep: 1554, time 48.61754822731018, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1555, time 50.293299198150635, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1556, time 47.766769886016846, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
goal_identified
=== ep: 1557, time 51.620986461639404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1558, time 51.483219385147095, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1559, time 48.48355221748352, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
goal_identified
goal_identified
goal_identified
=== ep: 1560, time 54.85889649391174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
goal_identified
goal_identified
=== ep: 1561, time 47.594106912612915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1562, time 49.7935905456543, eps 0.001, sum reward: 4, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
goal_identified
goal_identified
goal_identified
=== ep: 1563, time 51.71507000923157, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1564, time 48.672362089157104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
=== ep: 1565, time 50.39303922653198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1566, time 50.2702751159668, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
goal_identified
goal_identified
goal_identified
=== ep: 1567, time 48.17127442359924, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1568, time 47.56187701225281, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1569, time 48.92960238456726, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1570, time 49.9779691696167, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
goal_identified
goal_identified
=== ep: 1571, time 49.949764251708984, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1572, time 47.99129104614258, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
goal_identified
goal_identified
goal_identified
=== ep: 1573, time 51.76700210571289, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1574, time 52.46170520782471, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
goal_identified
goal_identified
goal_identified
=== ep: 1575, time 47.93221592903137, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1576, time 47.68579053878784, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1577, time 48.52821350097656, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1578, time 48.023499488830566, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1579, time 47.56154751777649, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1580, time 48.474759340286255, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
goal_identified
goal_identified
=== ep: 1581, time 50.678229093551636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1582, time 51.716182470321655, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1583, time 49.553964376449585, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
goal_identified
goal_identified
goal_identified
=== ep: 1584, time 52.494410276412964, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1585, time 48.48631405830383, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1586, time 48.27541399002075, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1587, time 51.24112319946289, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
goal_identified
=== ep: 1588, time 48.03561520576477, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
=== ep: 1589, time 52.1606183052063, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1590, time 50.41307854652405, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1591, time 47.84456992149353, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1592, time 51.59057331085205, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1593, time 48.37352657318115, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1594, time 48.309504985809326, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1595, time 51.085243463516235, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1596, time 47.63787627220154, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1597, time 47.94825077056885, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1598, time 51.926719188690186, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
goal_identified
goal_identified
=== ep: 1599, time 48.15011692047119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1600, time 49.939064025878906, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1600
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1601, time 48.81277561187744, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
goal_identified
goal_identified
=== ep: 1602, time 47.73404121398926, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
goal_identified
goal_identified
goal_identified
=== ep: 1603, time 54.07916879653931, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1604, time 48.39262294769287, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1605, time 48.292563676834106, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1606, time 49.88030028343201, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1607, time 53.044026136398315, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1608, time 50.275131940841675, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
=== ep: 1609, time 50.42729330062866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1610, time 48.25162887573242, eps 0.001, sum reward: 9, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
goal_identified
goal_identified
=== ep: 1611, time 52.22369384765625, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
goal_identified
goal_identified
goal_identified
=== ep: 1612, time 48.61319351196289, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1613, time 48.87740206718445, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1614, time 48.05076622962952, eps 0.001, sum reward: 8, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
goal_identified
goal_identified
goal_identified
=== ep: 1615, time 48.132890939712524, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1615
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1616, time 51.15822434425354, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1617, time 51.59578061103821, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1618, time 48.37214016914368, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
goal_identified
goal_identified
goal_identified
=== ep: 1619, time 51.20202827453613, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1620, time 50.113121509552, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1621, time 48.24931597709656, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1622, time 47.86597466468811, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
goal_identified
goal_identified
=== ep: 1623, time 48.73278307914734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1624, time 51.7084481716156, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1625, time 50.409752368927, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
goal_identified
goal_identified
goal_identified
=== ep: 1626, time 47.92707681655884, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1627, time 49.34955382347107, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1627
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1628, time 52.07218050956726, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 49.42014408111572, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1630, time 51.7209529876709, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1631, time 49.233707427978516, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
goal_identified
=== ep: 1632, time 50.386701345443726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1633, time 48.998035192489624, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1634, time 47.81387209892273, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1634
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1635, time 48.136759757995605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1636, time 52.29510450363159, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1637, time 53.974976539611816, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1638, time 48.19864225387573, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
goal_identified
goal_identified
goal_identified
=== ep: 1639, time 49.57617163658142, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1640, time 48.1913001537323, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1641, time 48.40271496772766, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1641
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1642, time 49.6856529712677, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1643, time 47.5623197555542, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1644, time 52.60087561607361, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
goal_identified
goal_identified
goal_identified
=== ep: 1645, time 52.80985450744629, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
goal_identified
=== ep: 1646, time 48.091317892074585, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1647, time 51.852355003356934, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
goal_identified
goal_identified
=== ep: 1648, time 48.79291582107544, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 48.20260691642761, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1650, time 49.963942766189575, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1651, time 47.94472932815552, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 48.34865069389343, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1653, time 54.99845743179321, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
goal_identified
goal_identified
goal_identified
=== ep: 1654, time 48.31713843345642, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1655, time 50.22457456588745, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1656, time 51.152533292770386, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
goal_identified
goal_identified
=== ep: 1657, time 47.920331716537476, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1658, time 52.25086688995361, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1659, time 47.396244525909424, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1660, time 47.44208288192749, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1661, time 56.119139432907104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1662, time 48.87890338897705, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1663, time 47.95102405548096, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1664, time 51.481544971466064, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1665, time 47.96005845069885, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1666, time 51.24252510070801, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1667, time 47.872565031051636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
goal_identified
=== ep: 1668, time 48.16002535820007, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1669, time 54.66533875465393, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
goal_identified
=== ep: 1670, time 48.425163984298706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
goal_identified
=== ep: 1671, time 48.85279893875122, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1672, time 54.36657404899597, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1673, time 48.42084574699402, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1674, time 49.33587455749512, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
goal_identified
goal_identified
=== ep: 1675, time 50.45589780807495, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
=== ep: 1676, time 52.52625012397766, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
goal_identified
goal_identified
=== ep: 1677, time 49.98516845703125, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1678, time 48.39741563796997, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1679, time 48.1556396484375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
goal_identified
=== ep: 1680, time 52.14505863189697, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1681, time 47.96134853363037, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
goal_identified
goal_identified
goal_identified
=== ep: 1682, time 52.29377102851868, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1683, time 50.48998260498047, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1684, time 48.415085315704346, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
goal_identified
goal_identified
goal_identified
=== ep: 1685, time 50.798640727996826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1686, time 48.27973413467407, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
goal_identified
goal_identified
goal_identified
=== ep: 1687, time 49.075502157211304, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
goal_identified
goal_identified
=== ep: 1688, time 52.2703173160553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1689, time 48.10488414764404, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
=== ep: 1690, time 53.9319794178009, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1691, time 50.555442810058594, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1692, time 48.306702852249146, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
goal_identified
goal_identified
=== ep: 1693, time 48.31958842277527, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
goal_identified
goal_identified
=== ep: 1694, time 48.02935552597046, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
goal_identified
goal_identified
goal_identified
=== ep: 1695, time 50.43954062461853, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1696, time 49.776097536087036, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1697, time 48.48062705993652, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1698, time 50.71235227584839, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
goal_identified
goal_identified
=== ep: 1699, time 57.81649684906006, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1700, time 48.85526204109192, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1701, time 47.8791823387146, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1702, time 49.870840311050415, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
goal_identified
goal_identified
goal_identified
=== ep: 1703, time 51.17164468765259, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1704, time 48.24770975112915, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
goal_identified
goal_identified
goal_identified
=== ep: 1705, time 48.0912721157074, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1706, time 48.75402808189392, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1707, time 50.17733073234558, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1708, time 48.5999858379364, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
goal_identified
goal_identified
goal_identified
=== ep: 1709, time 47.761935234069824, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1710, time 51.21678876876831, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
goal_identified
goal_identified
=== ep: 1711, time 50.51241993904114, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
goal_identified
goal_identified
=== ep: 1712, time 48.13440227508545, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
goal_identified
goal_identified
=== ep: 1713, time 51.05557155609131, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1714, time 49.620771408081055, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
goal_identified
goal_identified
=== ep: 1715, time 47.78526282310486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
goal_identified
goal_identified
goal_identified
=== ep: 1716, time 48.972553968429565, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
goal_identified
goal_identified
goal_identified
=== ep: 1717, time 47.954583406448364, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1718, time 50.832300424575806, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1719, time 50.99775242805481, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
goal_identified
=== ep: 1720, time 47.85374450683594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
goal_identified
=== ep: 1721, time 51.35389852523804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
goal_identified
goal_identified
goal_identified
=== ep: 1722, time 55.42427349090576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1723, time 48.095985651016235, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
goal_identified
goal_identified
=== ep: 1724, time 48.25882005691528, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
goal_identified
goal_identified
goal_identified
=== ep: 1725, time 48.8352792263031, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
goal_identified
goal_identified
goal_identified
=== ep: 1726, time 48.70112204551697, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1727, time 48.50163769721985, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
goal_identified
goal_identified
goal_identified
=== ep: 1728, time 48.310386657714844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
goal_identified
goal_identified
goal_identified
=== ep: 1729, time 52.356390953063965, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
goal_identified
=== ep: 1730, time 51.203904151916504, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1731, time 48.16607856750488, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1732, time 50.85707640647888, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1733, time 50.23833417892456, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1734, time 47.54236817359924, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
goal_identified
goal_identified
goal_identified
=== ep: 1735, time 47.69624996185303, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1736, time 48.24005103111267, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
goal_identified
goal_identified
=== ep: 1737, time 48.96379470825195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
goal_identified
goal_identified
=== ep: 1738, time 49.90737295150757, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1739, time 47.93330264091492, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
=== ep: 1740, time 52.501564502716064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
goal_identified
goal_identified
goal_identified
=== ep: 1741, time 50.92805051803589, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1742, time 48.09532928466797, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1743, time 51.13849329948425, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
goal_identified
goal_identified
goal_identified
=== ep: 1744, time 48.1514835357666, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1745, time 49.027360916137695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
goal_identified
goal_identified
goal_identified
=== ep: 1746, time 54.36250114440918, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
goal_identified
goal_identified
=== ep: 1747, time 48.38148236274719, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1748, time 48.72970771789551, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
goal_identified
goal_identified
goal_identified
=== ep: 1749, time 48.92600607872009, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
goal_identified
goal_identified
goal_identified
=== ep: 1750, time 48.10047101974487, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1751, time 51.1048309803009, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1752, time 47.90961313247681, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
goal_identified
goal_identified
=== ep: 1753, time 48.4723687171936, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1754, time 54.358118295669556, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
goal_identified
goal_identified
=== ep: 1755, time 48.04194211959839, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
goal_identified
goal_identified
=== ep: 1756, time 47.88509941101074, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
goal_identified
goal_identified
=== ep: 1757, time 56.7171630859375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1758, time 48.14257454872131, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
=== ep: 1759, time 48.43401551246643, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1760, time 50.14208483695984, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1761, time 47.77553677558899, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
goal_identified
goal_identified
=== ep: 1762, time 52.00948691368103, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1763, time 48.68255043029785, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1764, time 48.207173585891724, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
goal_identified
=== ep: 1765, time 54.702585220336914, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
goal_identified
goal_identified
goal_identified
=== ep: 1766, time 48.06746292114258, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1767, time 47.91977047920227, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1768, time 62.195685625076294, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1769, time 47.989548683166504, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1770, time 48.29598116874695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1771, time 52.51363205909729, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
goal_identified
=== ep: 1772, time 48.092172622680664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1773, time 50.05802774429321, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
=== ep: 1774, time 50.98766231536865, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1775, time 48.72929763793945, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1776, time 55.54924535751343, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
goal_identified
goal_identified
goal_identified
=== ep: 1777, time 48.57655167579651, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
goal_identified
goal_identified
=== ep: 1778, time 48.02324390411377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1779, time 55.34044694900513, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
goal_identified
goal_identified
goal_identified
=== ep: 1780, time 48.19784355163574, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1781, time 48.97512435913086, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
goal_identified
goal_identified
goal_identified
=== ep: 1782, time 49.60872530937195, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1783, time 48.2655029296875, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1784, time 48.70488405227661, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1785, time 47.62921404838562, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1786, time 48.65115165710449, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
goal_identified
goal_identified
=== ep: 1787, time 54.67568874359131, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1788, time 48.49780082702637, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 48.41345405578613, eps 0.001, sum reward: 5, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1790, time 58.32512855529785, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1791, time 47.987605810165405, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1792, time 48.42830729484558, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1793, time 51.339842319488525, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1794, time 47.641226053237915, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1795, time 49.492682218551636, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1796, time 50.240771532058716, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1797, time 48.9761323928833, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
goal_identified
=== ep: 1798, time 51.32501006126404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1799, time 49.60035681724548, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
goal_identified
goal_identified
=== ep: 1800, time 49.39174199104309, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 56.703771114349365, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1802, time 48.9939968585968, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
goal_identified
goal_identified
=== ep: 1803, time 49.97394132614136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1804, time 59.39136719703674, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1805, time 49.36630415916443, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1806, time 49.37087035179138, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1807, time 51.66462421417236, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
goal_identified
goal_identified
goal_identified
=== ep: 1808, time 49.55416679382324, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
goal_identified
goal_identified
=== ep: 1809, time 51.99513125419617, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
goal_identified
goal_identified
goal_identified
=== ep: 1810, time 49.93445062637329, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1811, time 49.52376914024353, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1812, time 57.4620726108551, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
goal_identified
goal_identified
=== ep: 1813, time 54.37689995765686, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1814, time 49.43000602722168, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1815, time 50.58781671524048, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1816, time 49.27893543243408, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
goal_identified
goal_identified
=== ep: 1817, time 51.910420179367065, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
goal_identified
=== ep: 1818, time 50.21414232254028, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1819, time 49.59155869483948, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1820, time 52.569300413131714, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1821, time 50.077187299728394, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1822, time 51.28102159500122, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1823, time 51.87376046180725, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1824, time 50.18317914009094, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1825, time 52.518091678619385, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1826, time 52.33114266395569, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1827, time 49.217427253723145, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1828, time 54.37356448173523, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1829, time 49.80238318443298, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
goal_identified
goal_identified
=== ep: 1830, time 50.044453620910645, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1831, time 54.10488843917847, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1832, time 49.88384747505188, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
goal_identified
goal_identified
goal_identified
=== ep: 1833, time 55.55561900138855, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
goal_identified
goal_identified
=== ep: 1834, time 51.27815389633179, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
goal_identified
goal_identified
=== ep: 1835, time 54.146381855010986, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1836, time 52.22111654281616, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
goal_identified
goal_identified
goal_identified
=== ep: 1837, time 49.506208658218384, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1838, time 49.68698763847351, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1839, time 50.46726870536804, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
goal_identified
goal_identified
goal_identified
=== ep: 1840, time 49.77201271057129, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
goal_identified
goal_identified
=== ep: 1841, time 51.791754484176636, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1842, time 53.12189984321594, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1843, time 49.660845041275024, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1844, time 51.84905672073364, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
goal_identified
goal_identified
goal_identified
=== ep: 1845, time 50.72714114189148, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
goal_identified
goal_identified
goal_identified
=== ep: 1846, time 49.27299666404724, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1847, time 49.16674470901489, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1848, time 49.81877064704895, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1849, time 53.0436589717865, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
goal_identified
goal_identified
=== ep: 1850, time 49.495250940322876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1851, time 49.413262605667114, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
goal_identified
goal_identified
=== ep: 1852, time 52.84378695487976, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1853, time 52.647273778915405, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1854, time 49.391308546066284, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1855, time 49.26614212989807, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1856, time 52.25268244743347, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1857, time 56.53069186210632, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1858, time 49.68831467628479, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
goal_identified
goal_identified
goal_identified
=== ep: 1859, time 48.96393799781799, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1860, time 53.672202348709106, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1861, time 53.19473934173584, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1861
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 49.144142150878906, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1863, time 49.449503660202026, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1864, time 53.30522060394287, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1865, time 51.93001437187195, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
goal_identified
goal_identified
goal_identified
=== ep: 1866, time 52.489033460617065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1867, time 49.367554664611816, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1868, time 50.60870909690857, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1869, time 40.872920751571655, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1870, time 40.74614882469177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1871, time 41.03975319862366, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1872, time 41.16967487335205, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
goal_identified
goal_identified
goal_identified
=== ep: 1873, time 40.88360238075256, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1874, time 41.732983112335205, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1875, time 41.13441181182861, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1876, time 41.174147605895996, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1877, time 41.21602177619934, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1878, time 41.47560214996338, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1879, time 41.99419116973877, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1880, time 43.53204560279846, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
goal_identified
goal_identified
=== ep: 1881, time 43.44755172729492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
goal_identified
=== ep: 1882, time 42.617223262786865, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1883, time 43.71190285682678, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
goal_identified
=== ep: 1884, time 45.19777750968933, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
goal_identified
goal_identified
goal_identified
=== ep: 1885, time 45.301387786865234, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1886, time 45.112513065338135, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1887, time 44.57088589668274, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1888, time 44.72065019607544, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
goal_identified
goal_identified
goal_identified
=== ep: 1889, time 45.01199173927307, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 45.47503614425659, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
goal_identified
goal_identified
goal_identified
=== ep: 1891, time 44.41061186790466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1892, time 45.244221687316895, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1893, time 45.87430453300476, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1894, time 45.65901064872742, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
goal_identified
goal_identified
=== ep: 1895, time 48.730881690979004, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1896, time 45.04650044441223, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1897, time 45.21881031990051, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
goal_identified
goal_identified
goal_identified
=== ep: 1898, time 44.60809326171875, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1899, time 44.91216969490051, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1899
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1900, time 44.86802697181702, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1900
goal_identified
goal_identified
=== ep: 1901, time 44.97612380981445, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1901
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1902, time 45.53270649909973, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1902
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1903, time 45.08389234542847, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1904, time 45.93853139877319, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1904
goal_identified
goal_identified
goal_identified
=== ep: 1905, time 45.184579849243164, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1905
goal_identified
goal_identified
goal_identified
=== ep: 1906, time 44.564034938812256, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1906
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1907, time 45.98717498779297, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1907
goal_identified
goal_identified
goal_identified
=== ep: 1908, time 44.908905029296875, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1908
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1909, time 45.489455461502075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1909
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1910, time 46.12273669242859, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1911, time 45.12488293647766, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1911
goal_identified
goal_identified
goal_identified
=== ep: 1912, time 44.736125230789185, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1912
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1913, time 44.95948672294617, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1913
goal_identified
goal_identified
goal_identified
=== ep: 1914, time 45.18846893310547, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1914
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1915, time 45.644553899765015, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1915
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1916, time 46.48726677894592, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1916
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1917, time 44.374600887298584, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1917
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1918, time 45.626599073410034, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1918
goal_identified
goal_identified
=== ep: 1919, time 46.060094356536865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1919
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1920, time 45.57854199409485, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1920
goal_identified
goal_identified
goal_identified
=== ep: 1921, time 44.65252447128296, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1921
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1922, time 45.821102142333984, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1922
goal_identified
goal_identified
goal_identified
=== ep: 1923, time 44.96106958389282, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1923
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1924, time 45.177348375320435, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1924
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1925, time 45.44140672683716, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1925
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1926, time 45.8461229801178, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1926
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1927, time 49.36477971076965, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1928, time 45.37043857574463, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1929, time 46.40274739265442, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1929
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1930, time 44.98639750480652, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1930
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1931, time 45.37668418884277, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1931
goal_identified
=== ep: 1932, time 46.83042001724243, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1932
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1933, time 45.998122215270996, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1933
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1934, time 45.59693145751953, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1934
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1935, time 45.86001491546631, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1935
goal_identified
goal_identified
goal_identified
=== ep: 1936, time 47.50565314292908, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1936
goal_identified
goal_identified
goal_identified
=== ep: 1937, time 45.202465534210205, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1937
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1938, time 45.32331991195679, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1938
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1939, time 46.829344511032104, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1939
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1940, time 45.79875111579895, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1940
goal_identified
goal_identified
goal_identified
=== ep: 1941, time 44.916696071624756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1941
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1942, time 45.26953077316284, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1942
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1943, time 45.232914686203, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1943
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1944, time 45.25196957588196, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1944
goal_identified
=== ep: 1945, time 45.27476906776428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1945
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1946, time 46.838308811187744, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1946
goal_identified
goal_identified
goal_identified
=== ep: 1947, time 45.54579401016235, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1948, time 44.884257555007935, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
goal_identified
goal_identified
goal_identified
=== ep: 1949, time 47.43168878555298, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1949
goal_identified
goal_identified
goal_identified
=== ep: 1950, time 45.734657764434814, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1950
goal_identified
goal_identified
goal_identified
=== ep: 1951, time 45.92687702178955, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1951
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1952, time 46.48547911643982, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1952
goal_identified
goal_identified
=== ep: 1953, time 47.011404275894165, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1953
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1954, time 45.52991342544556, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1954
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1955, time 48.44954442977905, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1956, time 47.60978698730469, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1957, time 45.710769176483154, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1957
goal_identified
goal_identified
=== ep: 1958, time 45.36009740829468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1958
goal_identified
goal_identified
goal_identified
=== ep: 1959, time 46.66139364242554, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1959
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1960, time 47.13778805732727, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1961, time 45.5759072303772, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1961
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1962, time 46.60407257080078, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1962
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1963, time 47.66297793388367, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1963
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1964, time 45.7500159740448, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1964
goal_identified
goal_identified
goal_identified
=== ep: 1965, time 46.566041231155396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1965
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1966, time 47.55722117424011, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1966
goal_identified
=== ep: 1967, time 45.5217866897583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1967
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1968, time 46.23201251029968, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1968
goal_identified
=== ep: 1969, time 46.53472828865051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1969
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1970, time 45.46258044242859, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1970
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1971, time 45.99197697639465, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1971
goal_identified
goal_identified
goal_identified
=== ep: 1972, time 45.945871114730835, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1972
goal_identified
goal_identified
goal_identified
=== ep: 1973, time 45.173192262649536, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1974, time 45.65726137161255, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1974
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1975, time 45.64601159095764, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 135/135)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1975
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1976, time 45.68993401527405, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1976
goal_identified
goal_identified
=== ep: 1977, time 45.741082429885864, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1977
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1978, time 45.66377067565918, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1978
goal_identified
goal_identified
=== ep: 1979, time 45.38098692893982, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1979
goal_identified
goal_identified
goal_identified
=== ep: 1980, time 44.99488973617554, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1980
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1981, time 45.475239515304565, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1981
goal_identified
=== ep: 1982, time 46.54404807090759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1982
goal_identified
=== ep: 1983, time 45.247254610061646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1983
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1984, time 45.94262742996216, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1984
goal_identified
goal_identified
=== ep: 1985, time 50.39014768600464, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1985
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1986, time 46.99803042411804, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1987, time 45.52847075462341, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1987
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1988, time 46.397072076797485, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1988
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1989, time 48.2702739238739, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1989
goal_identified
goal_identified
goal_identified
=== ep: 1990, time 44.85015821456909, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1990
goal_identified
goal_identified
=== ep: 1991, time 46.32197666168213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1991
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1992, time 47.98904323577881, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1993, time 45.5036940574646, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1993
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1994, time 45.540719747543335, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1994
goal_identified
goal_identified
goal_identified
=== ep: 1995, time 47.4520218372345, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1995
goal_identified
goal_identified
=== ep: 1996, time 45.577986001968384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1996
goal_identified
goal_identified
=== ep: 1997, time 44.994930028915405, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1997
goal_identified
goal_identified
goal_identified
=== ep: 1998, time 47.56639313697815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1999, time 45.65683603286743, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1999
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2000, time 45.36358046531677, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2000
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2001, time 48.15132451057434, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2001
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2002, time 45.61960315704346, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2002
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2003, time 45.48537850379944, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2004, time 48.52229070663452, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2005, time 47.40655565261841, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2005
goal_identified
goal_identified
=== ep: 2006, time 45.94351124763489, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2006
goal_identified
goal_identified
goal_identified
=== ep: 2007, time 48.955607891082764, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2007
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2008, time 48.35371017456055, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2008
goal_identified
goal_identified
goal_identified
=== ep: 2009, time 46.28412961959839, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2009
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2010, time 47.67498826980591, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2010
goal_identified
goal_identified
=== ep: 2011, time 48.650455951690674, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2011
goal_identified
goal_identified
goal_identified
=== ep: 2012, time 46.51487684249878, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2012
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2013, time 46.063196420669556, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2013
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2014, time 47.76879811286926, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2014
goal_identified
goal_identified
goal_identified
=== ep: 2015, time 45.79357981681824, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2015
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2016, time 49.591963052749634, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2017, time 48.09184765815735, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2017
goal_identified
goal_identified
goal_identified
=== ep: 2018, time 46.26754832267761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2018
goal_identified
goal_identified
=== ep: 2019, time 46.68383193016052, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2019
goal_identified
=== ep: 2020, time 47.03615760803223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2020
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2021, time 46.796255588531494, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2021
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2022, time 46.339909076690674, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2022
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2023, time 46.70683932304382, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2024, time 45.66372346878052, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2024
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2025, time 46.36873269081116, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2025
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2026, time 46.66976261138916, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2026
=== ep: 2027, time 45.747602224349976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2028, time 45.78082275390625, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2028
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2029, time 47.20653319358826, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2029
goal_identified
goal_identified
=== ep: 2030, time 46.24183678627014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2030
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2031, time 46.04102873802185, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2031
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2032, time 47.19243144989014, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2032
goal_identified
goal_identified
=== ep: 2033, time 47.11863470077515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2033
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2034, time 45.990994691848755, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2034
goal_identified
goal_identified
=== ep: 2035, time 46.768367767333984, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2035
goal_identified
goal_identified
=== ep: 2036, time 47.34739851951599, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2036
goal_identified
=== ep: 2037, time 45.87768054008484, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2038, time 47.04355025291443, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2038
goal_identified
=== ep: 2039, time 46.63200664520264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2039
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2040, time 46.099605560302734, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2040
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2041, time 46.529908657073975, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2041
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2042, time 46.0542778968811, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2042
goal_identified
goal_identified
goal_identified
=== ep: 2043, time 46.08905577659607, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2043
goal_identified
goal_identified
=== ep: 2044, time 48.77485275268555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2044
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2045, time 48.47021746635437, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2045
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2046, time 45.705238342285156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2047, time 48.795238971710205, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2047
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2048, time 48.88013815879822, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2048
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2049, time 49.803720235824585, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2050, time 47.42882513999939, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2050
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2051, time 49.10041570663452, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2051
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2052, time 46.25715088844299, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2052
goal_identified
goal_identified
=== ep: 2053, time 47.47203612327576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2053
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2054, time 48.47393465042114, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2054
goal_identified
goal_identified
goal_identified
=== ep: 2055, time 46.376976013183594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2055
=== ep: 2056, time 46.53175950050354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2056
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2057, time 48.722416639328, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2057
goal_identified
goal_identified
goal_identified
=== ep: 2058, time 46.17071509361267, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2058
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2059, time 47.20358347892761, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2059
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2060, time 47.09144687652588, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2060
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2061, time 47.548989057540894, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2061
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2062, time 47.88522958755493, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2062
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2063, time 46.75720167160034, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2063
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2064, time 46.47515940666199, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2064
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2065, time 47.72468614578247, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2065
goal_identified
goal_identified
goal_identified
=== ep: 2066, time 46.84114956855774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2066
goal_identified
goal_identified
goal_identified
=== ep: 2067, time 46.0819571018219, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2067
goal_identified
goal_identified
goal_identified
=== ep: 2068, time 47.484464168548584, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2068
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2069, time 47.604856967926025, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2069
goal_identified
=== ep: 2070, time 46.18124198913574, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2070
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2071, time 46.28187942504883, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2071
goal_identified
goal_identified
=== ep: 2072, time 46.499231815338135, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2072
goal_identified
goal_identified
goal_identified
=== ep: 2073, time 46.63107681274414, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2073
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2074, time 47.18146991729736, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2074
goal_identified
goal_identified
goal_identified
=== ep: 2075, time 46.963926553726196, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2075
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2076, time 46.25803184509277, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2076
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2077, time 47.62629699707031, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2077
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2078, time 51.17768335342407, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2079, time 46.37998151779175, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2079
=== ep: 2080, time 46.549726724624634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2080
goal_identified
goal_identified
goal_identified
=== ep: 2081, time 46.729957818984985, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 146/146)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2082, time 46.031129598617554, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2082
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2083, time 46.04269218444824, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2083
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2084, time 47.45207238197327, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2084
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2085, time 45.09323787689209, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2085
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2086, time 46.2608916759491, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
goal_identified
goal_identified
=== ep: 2087, time 49.73264789581299, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2087
goal_identified
goal_identified
goal_identified
=== ep: 2088, time 46.8623263835907, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2088
goal_identified
goal_identified
=== ep: 2089, time 46.50274682044983, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2089
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2090, time 49.27731895446777, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2090
goal_identified
goal_identified
=== ep: 2091, time 46.224226236343384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2091
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2092, time 45.848785400390625, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2092
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2093, time 48.71360731124878, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2093
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2094, time 46.49902892112732, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2094
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2095, time 46.59143781661987, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2095
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2096, time 47.70736360549927, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2096
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2097, time 45.910672664642334, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2097
goal_identified
goal_identified
goal_identified
=== ep: 2098, time 46.4579017162323, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2098
goal_identified
goal_identified
=== ep: 2099, time 46.832382917404175, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2099
goal_identified
=== ep: 2100, time 45.892425537109375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2100
goal_identified
=== ep: 2101, time 46.45618653297424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2101
goal_identified
goal_identified
goal_identified
=== ep: 2102, time 46.0166072845459, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2102
goal_identified
goal_identified
=== ep: 2103, time 46.469401597976685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2103
goal_identified
goal_identified
=== ep: 2104, time 45.87448811531067, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2104
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2105, time 45.982895612716675, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2105
goal_identified
goal_identified
goal_identified
=== ep: 2106, time 46.65727210044861, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2106
goal_identified
goal_identified
=== ep: 2107, time 46.21887016296387, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2107
goal_identified
goal_identified
=== ep: 2108, time 45.563467502593994, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2109, time 47.32069945335388, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2109
goal_identified
goal_identified
goal_identified
=== ep: 2110, time 50.30368399620056, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2110
goal_identified
goal_identified
=== ep: 2111, time 46.00138449668884, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2111
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2112, time 46.21970462799072, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2112
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2113, time 46.50484275817871, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2113
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2114, time 46.78782415390015, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2114
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2115, time 46.68544936180115, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2116, time 46.622485399246216, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2116
goal_identified
=== ep: 2117, time 45.36664342880249, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2117
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2118, time 47.24950051307678, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2118
goal_identified
goal_identified
=== ep: 2119, time 45.96597242355347, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2119
goal_identified
goal_identified
goal_identified
=== ep: 2120, time 46.06669354438782, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2120
goal_identified
=== ep: 2121, time 48.00544023513794, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2121
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2122, time 46.44094204902649, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2122
goal_identified
goal_identified
=== ep: 2123, time 46.31798458099365, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2123
goal_identified
goal_identified
=== ep: 2124, time 48.804604291915894, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2124
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2125, time 46.49061608314514, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2126, time 46.1107964515686, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2126
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2127, time 49.0848970413208, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2127
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2128, time 46.037150382995605, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2129, time 46.2657687664032, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2129
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2130, time 48.31767249107361, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2130
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2131, time 45.47038221359253, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2132, time 46.334333419799805, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2004
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2133, time 47.8132598400116, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2133
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2134, time 46.87781262397766, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2134
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2135, time 46.48873710632324, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2135
goal_identified
=== ep: 2136, time 47.721673250198364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2136
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2137, time 46.30573272705078, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2137
goal_identified
=== ep: 2138, time 46.99345874786377, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2139, time 48.54762864112854, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2139
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2140, time 46.350255250930786, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2140
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2141, time 46.93638324737549, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2023
goal_identified
goal_identified
goal_identified
=== ep: 2142, time 54.57357120513916, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2142
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2143, time 47.251041889190674, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2143
goal_identified
goal_identified
goal_identified
=== ep: 2144, time 47.11210513114929, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2144
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2145, time 50.07919239997864, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2145
goal_identified
goal_identified
goal_identified
=== ep: 2146, time 46.28817391395569, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2146
goal_identified
goal_identified
goal_identified
=== ep: 2147, time 47.53787398338318, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2148, time 50.220635652542114, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2086
goal_identified
goal_identified
goal_identified
=== ep: 2149, time 46.737542152404785, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2149
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2150, time 46.35917949676514, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2150
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2151, time 50.254777669906616, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2151
goal_identified
goal_identified
goal_identified
=== ep: 2152, time 46.6315541267395, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2152
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2153, time 47.281662464141846, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2154, time 49.78111004829407, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2154
goal_identified
goal_identified
goal_identified
=== ep: 2155, time 46.57265067100525, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2156, time 46.61100363731384, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2156
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2157, time 50.123828172683716, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2157
goal_identified
goal_identified
goal_identified
=== ep: 2158, time 45.48973250389099, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2158
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2159, time 47.37214255332947, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2159
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2160, time 50.14632487297058, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2160
goal_identified
goal_identified
=== ep: 2161, time 46.71644139289856, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2162, time 46.93566870689392, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2132
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2163, time 50.214648723602295, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2163
goal_identified
goal_identified
=== ep: 2164, time 46.65326642990112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2164
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2165, time 46.76306223869324, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2165
goal_identified
goal_identified
goal_identified
=== ep: 2166, time 50.40259671211243, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2166
goal_identified
goal_identified
goal_identified
=== ep: 2167, time 46.3604691028595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2167
goal_identified
goal_identified
goal_identified
=== ep: 2168, time 47.35988688468933, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2168
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2169, time 49.952128171920776, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2169
goal_identified
goal_identified
goal_identified
=== ep: 2170, time 46.3756058216095, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2170
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2171, time 46.49928641319275, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2171
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2172, time 49.12624669075012, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2173, time 50.056761026382446, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2173
goal_identified
goal_identified
goal_identified
=== ep: 2174, time 50.11658334732056, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2174
goal_identified
goal_identified
goal_identified
=== ep: 2175, time 47.38024425506592, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2175
goal_identified
goal_identified
goal_identified
=== ep: 2176, time 47.446038246154785, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2176
goal_identified
goal_identified
goal_identified
=== ep: 2177, time 49.78133225440979, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2178, time 46.91816329956055, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2178
goal_identified
goal_identified
goal_identified
=== ep: 2179, time 46.65474271774292, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2180, time 48.71173453330994, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2180
goal_identified
goal_identified
goal_identified
=== ep: 2181, time 47.03559422492981, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2181
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2182, time 46.427507638931274, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2182
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2183, time 48.68077778816223, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2183
goal_identified
goal_identified
goal_identified
=== ep: 2184, time 46.62093710899353, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2184
goal_identified
goal_identified
goal_identified
=== ep: 2185, time 46.42889952659607, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2185
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2186, time 46.71925663948059, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2186
goal_identified
goal_identified
goal_identified
=== ep: 2187, time 47.3478639125824, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2187
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2188, time 46.82897734642029, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2188
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2189, time 46.84048557281494, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2189
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2190, time 47.019410610198975, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2190
goal_identified
goal_identified
=== ep: 2191, time 46.67983913421631, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2192, time 46.90207552909851, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2192
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2193, time 48.57788419723511, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2193
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2194, time 45.69919800758362, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2194
goal_identified
goal_identified
=== ep: 2195, time 46.36005926132202, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2195
goal_identified
goal_identified
=== ep: 2196, time 50.421101093292236, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2196
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2197, time 46.826441049575806, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2197
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2198, time 46.54031467437744, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2198
goal_identified
goal_identified
goal_identified
=== ep: 2199, time 50.572633266448975, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2199
goal_identified
=== ep: 2200, time 46.47526574134827, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2200
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2201, time 47.08413028717041, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2201
goal_identified
goal_identified
goal_identified
=== ep: 2202, time 50.483426570892334, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2202
goal_identified
goal_identified
goal_identified
=== ep: 2203, time 46.72939085960388, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2203
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2204, time 51.81311082839966, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2204
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2205, time 50.796897888183594, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2205
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2206, time 46.837164640426636, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2206
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2207, time 50.16138172149658, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2207
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2208, time 50.341896057128906, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2208
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2209, time 46.60958766937256, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2209
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2210, time 49.0669367313385, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2210
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2211, time 49.46612548828125, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2212, time 46.44316339492798, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2212
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2213, time 48.915682554244995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2214, time 49.17643094062805, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2214
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2215, time 46.71843862533569, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2215
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2216, time 47.948418617248535, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2216
goal_identified
goal_identified
goal_identified
=== ep: 2217, time 49.340590476989746, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2217
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2218, time 46.01724886894226, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2218
goal_identified
goal_identified
goal_identified
=== ep: 2219, time 47.314549684524536, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2219
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2220, time 48.4890775680542, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2220
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2221, time 46.44946098327637, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2221
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2222, time 46.641271114349365, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2222
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2223, time 48.137375593185425, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2223
goal_identified
goal_identified
=== ep: 2224, time 46.512911319732666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2224
goal_identified
goal_identified
=== ep: 2225, time 47.83174395561218, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2225
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2226, time 47.40026664733887, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2226
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2227, time 46.726157426834106, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2228, time 48.0366587638855, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2141
goal_identified
goal_identified
goal_identified
=== ep: 2229, time 47.61541199684143, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2229
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2230, time 46.72420310974121, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2230
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2231, time 48.779597759246826, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2231
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2232, time 47.509581565856934, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2232
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2233, time 46.695003032684326, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2233
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2234, time 49.19062900543213, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2235, time 46.979732036590576, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2235
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2236, time 46.54973387718201, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2236
goal_identified
goal_identified
goal_identified
=== ep: 2237, time 52.74315333366394, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2237
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2238, time 48.07443189620972, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2238
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2239, time 46.78159689903259, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2239
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2240, time 49.331419706344604, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2241, time 46.9375364780426, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2241
goal_identified
goal_identified
goal_identified
=== ep: 2242, time 46.92947292327881, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2242
goal_identified
goal_identified
=== ep: 2243, time 49.699790954589844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2244, time 46.476298809051514, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 46.768609046936035, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2246, time 50.156837701797485, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2246
goal_identified
goal_identified
=== ep: 2247, time 46.32287383079529, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2247
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2248, time 46.399010181427, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2162
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2249, time 49.85206961631775, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2228
goal_identified
goal_identified
=== ep: 2250, time 46.726202726364136, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2250
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2251, time 47.01837682723999, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2251
goal_identified
goal_identified
=== ep: 2252, time 51.70068883895874, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2252
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2253, time 47.08866739273071, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2253
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2254, time 47.01947784423828, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2254
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2255, time 49.94146704673767, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2255
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2256, time 46.707473278045654, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2256
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2257, time 46.864938259124756, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2257
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2258, time 49.24544024467468, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2258
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2259, time 46.468929290771484, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2259
goal_identified
goal_identified
goal_identified
=== ep: 2260, time 48.166284799575806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2260
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2261, time 49.27725577354431, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2261
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2262, time 46.29565978050232, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2262
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2263, time 48.181219816207886, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2264, time 48.93921971321106, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2264
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2265, time 46.73153018951416, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2265
goal_identified
goal_identified
goal_identified
=== ep: 2266, time 50.18894720077515, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2267, time 47.815327405929565, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2268, time 46.49405002593994, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2268
=== ep: 2269, time 49.38206958770752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2269
goal_identified
goal_identified
goal_identified
=== ep: 2270, time 48.580039501190186, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2270
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2271, time 50.004019260406494, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2271
goal_identified
goal_identified
=== ep: 2272, time 50.05779314041138, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2272
=== ep: 2273, time 48.37260913848877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2273
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2274, time 46.443129777908325, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2274
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2275, time 48.46248745918274, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 48.90557646751404, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2276
goal_identified
goal_identified
=== ep: 2277, time 46.38221788406372, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2277
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2278, time 45.82741093635559, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2278
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2279, time 49.10599756240845, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2280, time 46.49496650695801, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2280
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2281, time 46.48307251930237, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2281
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2282, time 49.827996015548706, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2282
goal_identified
goal_identified
=== ep: 2283, time 46.627248764038086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2283
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2284, time 46.748180627822876, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2284
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2285, time 49.94139289855957, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2285
goal_identified
goal_identified
goal_identified
=== ep: 2286, time 47.069983959198, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2286
goal_identified
goal_identified
goal_identified
=== ep: 2287, time 46.60136413574219, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2287
goal_identified
goal_identified
=== ep: 2288, time 49.99371910095215, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2289, time 46.24841642379761, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2289
goal_identified
goal_identified
goal_identified
=== ep: 2290, time 46.832526206970215, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2290
goal_identified
goal_identified
goal_identified
=== ep: 2291, time 51.204240560531616, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2291
goal_identified
goal_identified
goal_identified
=== ep: 2292, time 46.57344961166382, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2292
goal_identified
goal_identified
=== ep: 2293, time 46.69620227813721, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2293
goal_identified
goal_identified
=== ep: 2294, time 52.989563941955566, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2294
goal_identified
goal_identified
goal_identified
=== ep: 2295, time 46.72226166725159, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2295
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2296, time 46.93006658554077, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2296
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2297, time 53.10767436027527, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2297
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2298, time 46.90650653839111, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2298
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2299, time 47.02573275566101, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 53.75413680076599, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2300
goal_identified
goal_identified
goal_identified
=== ep: 2301, time 47.26520919799805, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2301
goal_identified
goal_identified
goal_identified
=== ep: 2302, time 47.01115655899048, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2303, time 53.485997676849365, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2303
goal_identified
goal_identified
=== ep: 2304, time 51.06520438194275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2304
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2305, time 47.19248366355896, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2248
goal_identified
goal_identified
=== ep: 2306, time 52.23496890068054, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2306
goal_identified
goal_identified
goal_identified
=== ep: 2307, time 47.39598083496094, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2307
goal_identified
goal_identified
=== ep: 2308, time 46.5137140750885, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2308
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2309, time 50.25100922584534, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2309
goal_identified
goal_identified
=== ep: 2310, time 46.61927533149719, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2310
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2311, time 46.779797077178955, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2311
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2312, time 49.71933436393738, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2312
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2313, time 46.84238815307617, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2313
goal_identified
=== ep: 2314, time 46.13584542274475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2314
goal_identified
goal_identified
goal_identified
=== ep: 2315, time 48.58101677894592, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2315
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2316, time 47.651989221572876, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2316
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2317, time 46.33597540855408, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2317
goal_identified
goal_identified
=== ep: 2318, time 47.41105628013611, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2318
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2319, time 47.23340320587158, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2319
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2320, time 46.89417600631714, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2320
goal_identified
goal_identified
=== ep: 2321, time 47.02862739562988, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2322, time 47.1781120300293, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2322
goal_identified
goal_identified
goal_identified
=== ep: 2323, time 46.952502489089966, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2323
goal_identified
goal_identified
goal_identified
=== ep: 2324, time 47.067291498184204, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2324
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2325, time 48.18899345397949, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2325
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2326, time 46.09326457977295, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2326
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2327, time 46.95634388923645, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2327
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2328, time 50.759968280792236, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2328
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2329, time 47.06899857521057, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2329
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2330, time 47.05991744995117, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2330
goal_identified
goal_identified
goal_identified
=== ep: 2331, time 50.409663915634155, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2331
goal_identified
goal_identified
goal_identified
=== ep: 2332, time 46.81688070297241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2332
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2333, time 45.76996731758118, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2333
goal_identified
goal_identified
=== ep: 2334, time 48.78826117515564, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2335, time 47.31315064430237, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2336, time 46.65357756614685, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2336
goal_identified
goal_identified
goal_identified
=== ep: 2337, time 47.1846718788147, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2337
goal_identified
=== ep: 2338, time 51.25310015678406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2338
goal_identified
goal_identified
goal_identified
=== ep: 2339, time 46.647483348846436, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2339
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2340, time 47.29299354553223, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2340
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2341, time 47.05083703994751, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2341
goal_identified
goal_identified
goal_identified
=== ep: 2342, time 47.44450926780701, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2342
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2343, time 46.96719765663147, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2343
goal_identified
goal_identified
=== ep: 2344, time 47.325199365615845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2344
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2345, time 47.11403131484985, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2345
goal_identified
goal_identified
goal_identified
=== ep: 2346, time 46.987035512924194, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2346
goal_identified
goal_identified
=== ep: 2347, time 46.99075388908386, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2348, time 47.391666650772095, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2348
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2349, time 46.930495738983154, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2349
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2350, time 47.32772731781006, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2350
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2351, time 47.09361505508423, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2351
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2352, time 46.62013030052185, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2352
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2353, time 49.732576847076416, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2353
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2354, time 47.257508516311646, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2354
goal_identified
goal_identified
=== ep: 2355, time 47.449063539505005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2355
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2356, time 49.88159966468811, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2356
goal_identified
goal_identified
goal_identified
=== ep: 2357, time 46.54161548614502, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2357
goal_identified
goal_identified
=== ep: 2358, time 47.41279435157776, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2358
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2359, time 50.32472634315491, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2359
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2360, time 46.498687744140625, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2360
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2361, time 48.06730318069458, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2361
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2362, time 51.320496797561646, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2362
goal_identified
goal_identified
goal_identified
=== ep: 2363, time 47.08940386772156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2363
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2364, time 47.16809582710266, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2365, time 50.53036189079285, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2365
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2366, time 46.71224308013916, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2366
goal_identified
goal_identified
goal_identified
=== ep: 2367, time 47.28852343559265, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2367
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2368, time 48.995835065841675, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2368
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2369, time 46.33970856666565, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2369
goal_identified
goal_identified
goal_identified
=== ep: 2370, time 47.2696008682251, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2370
goal_identified
goal_identified
goal_identified
=== ep: 2371, time 48.514623165130615, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2372, time 47.22412705421448, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2372
goal_identified
goal_identified
goal_identified
=== ep: 2373, time 46.79709839820862, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2373
goal_identified
goal_identified
goal_identified
=== ep: 2374, time 51.22770094871521, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2374
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2375, time 46.8567898273468, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2375
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2376, time 47.30995798110962, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2376
goal_identified
goal_identified
goal_identified
=== ep: 2377, time 48.03977084159851, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2377
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2378, time 47.000133991241455, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2379, time 48.707639932632446, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2379
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2380, time 47.90236687660217, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2380
goal_identified
goal_identified
=== ep: 2381, time 46.32563614845276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2381
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2382, time 50.29128623008728, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2382
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2383, time 47.137113094329834, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2383
goal_identified
goal_identified
=== ep: 2384, time 47.094995975494385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2384
goal_identified
goal_identified
goal_identified
=== ep: 2385, time 51.44503116607666, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2385
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2386, time 47.59750533103943, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2387, time 47.8549120426178, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2387
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2388, time 49.019551038742065, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2388
goal_identified
goal_identified
goal_identified
=== ep: 2389, time 46.793779134750366, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2389
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2390, time 46.904263973236084, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2390
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2391, time 48.3523211479187, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2392, time 47.42115926742554, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2392
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2393, time 46.59140920639038, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2393
goal_identified
goal_identified
=== ep: 2394, time 48.285067558288574, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2394
goal_identified
goal_identified
=== ep: 2395, time 46.8839328289032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2395
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2396, time 47.37345623970032, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2397, time 47.14083409309387, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2397
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2398, time 47.2413432598114, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2398
goal_identified
=== ep: 2399, time 46.75037407875061, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2399
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2400, time 46.767229080200195, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2400
goal_identified
=== ep: 2401, time 46.910675287246704, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2401
=== ep: 2402, time 48.40857291221619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2402
goal_identified
goal_identified
=== ep: 2403, time 46.98099184036255, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2403
goal_identified
goal_identified
=== ep: 2404, time 46.83981251716614, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2404
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2405, time 49.5849711894989, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2405
goal_identified
goal_identified
goal_identified
=== ep: 2406, time 46.773746728897095, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2406
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2407, time 47.02519130706787, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 50.68208909034729, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2409, time 48.016016483306885, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 132/132)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2409
goal_identified
goal_identified
goal_identified
=== ep: 2410, time 46.69607067108154, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2411, time 51.30264902114868, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 47.686734437942505, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2412
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2413, time 47.19978356361389, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2414, time 49.897520542144775, eps 0.001, sum reward: 6, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2249
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2415, time 46.579946756362915, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2415
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2416, time 46.85234498977661, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 49.69021272659302, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2417
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2418, time 46.76421856880188, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2418
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2419, time 46.73369836807251, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2419
goal_identified
goal_identified
goal_identified
=== ep: 2420, time 47.287720680236816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2420
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2421, time 46.69908857345581, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2421
goal_identified
goal_identified
=== ep: 2422, time 47.230122089385986, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2423, time 46.369263648986816, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2423
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2424, time 46.67994689941406, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2424
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2425, time 50.35181164741516, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2426, time 47.45016598701477, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 136/136)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2426
goal_identified
goal_identified
goal_identified
=== ep: 2427, time 46.58467936515808, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2427
goal_identified
goal_identified
goal_identified
=== ep: 2428, time 50.4043505191803, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2428
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2429, time 46.97480750083923, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2429
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2430, time 47.19904375076294, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2431, time 49.450077533721924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2431
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2432, time 47.14554286003113, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2432
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2433, time 48.51448965072632, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2434, time 47.80352759361267, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2434
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2435, time 47.14211392402649, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2435
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2436, time 49.06383991241455, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2436
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2437, time 47.34553241729736, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2437
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2438, time 47.43964123725891, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2414
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2439, time 49.79566669464111, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2439
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2440, time 47.54697227478027, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2440
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2441, time 47.8364782333374, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2441
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2442, time 49.980329751968384, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2442
goal_identified
=== ep: 2443, time 47.86129665374756, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2443
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2444, time 48.08458733558655, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2444
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2445, time 51.635894536972046, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2445
goal_identified
goal_identified
=== ep: 2446, time 51.89347505569458, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2446
goal_identified
goal_identified
=== ep: 2447, time 48.05255699157715, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2448, time 50.39531421661377, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2448
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2449, time 47.8118257522583, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2449
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2450, time 47.982518911361694, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2451, time 49.09828495979309, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2451
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2452, time 48.218793630599976, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2453, time 48.11778783798218, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2453
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2454, time 50.57217621803284, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2454
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2455, time 47.792977809906006, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2455
goal_identified
goal_identified
goal_identified
=== ep: 2456, time 47.700979709625244, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2456
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2457, time 51.57517147064209, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2457
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2458, time 47.771106243133545, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2458
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2459, time 48.35046863555908, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2459
goal_identified
=== ep: 2460, time 48.87602400779724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2460
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2461, time 47.92215704917908, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2462, time 47.69510507583618, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2462
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2463, time 50.119115591049194, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2463
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2464, time 47.244131088256836, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2464
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2465, time 48.513938665390015, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2465
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2466, time 50.081393241882324, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2466
=== ep: 2467, time 47.97115468978882, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2467
goal_identified
goal_identified
=== ep: 2468, time 48.628886222839355, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2468
goal_identified
goal_identified
goal_identified
=== ep: 2469, time 51.536627531051636, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2469
goal_identified
=== ep: 2470, time 47.791954040527344, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2470
goal_identified
goal_identified
goal_identified
=== ep: 2471, time 47.40973472595215, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2471
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2472, time 50.2003653049469, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2473, time 47.49742865562439, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2473
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2474, time 47.7744414806366, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2475, time 51.388911962509155, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2475
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2476, time 47.45174241065979, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2476
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2477, time 47.58223581314087, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2478, time 51.738383054733276, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2478
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2479, time 52.74277901649475, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2479
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2480, time 47.62601280212402, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2480
goal_identified
goal_identified
goal_identified
=== ep: 2481, time 51.01634764671326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2481
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2482, time 49.03093600273132, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2482
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2483, time 47.56975173950195, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2483
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2484, time 49.89987802505493, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2484
goal_identified
goal_identified
=== ep: 2485, time 48.945300340652466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2485
goal_identified
=== ep: 2486, time 47.60382342338562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2487, time 48.54395651817322, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2488, time 48.579275369644165, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2489, time 48.22332549095154, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2489
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2490, time 48.60885787010193, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2490
goal_identified
goal_identified
=== ep: 2491, time 49.48048257827759, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2491
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2492, time 48.627336263656616, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2492
goal_identified
goal_identified
goal_identified
=== ep: 2493, time 47.639578342437744, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2493
goal_identified
goal_identified
goal_identified
=== ep: 2494, time 48.54735016822815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2494
goal_identified
goal_identified
=== ep: 2495, time 48.05853867530823, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2495
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2496, time 47.147180795669556, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2496
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2497, time 48.98568773269653, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2497
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2498, time 47.99024844169617, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 47.81108069419861, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2499
goal_identified
goal_identified
goal_identified
=== ep: 2500, time 47.56388235092163, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2500
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2501, time 48.155449628829956, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2502, time 47.76201605796814, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2502
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2503, time 48.14990162849426, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2503
goal_identified
goal_identified
goal_identified
=== ep: 2504, time 47.85911989212036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2504
goal_identified
goal_identified
=== ep: 2505, time 48.738471269607544, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2505
goal_identified
goal_identified
goal_identified
=== ep: 2506, time 46.77333641052246, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2506
goal_identified
goal_identified
=== ep: 2507, time 48.03205394744873, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2507
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2508, time 48.804038286209106, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2508
goal_identified
goal_identified
goal_identified
=== ep: 2509, time 47.85558533668518, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2509
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2510, time 48.41243648529053, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2510
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2511, time 49.11718010902405, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2512, time 47.81763696670532, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2512
goal_identified
goal_identified
=== ep: 2513, time 49.52983570098877, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2514, time 47.306694984436035, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2514
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2515, time 47.689056396484375, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2515
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2516, time 51.776771545410156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2516
goal_identified
goal_identified
goal_identified
=== ep: 2517, time 48.03587746620178, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2517
goal_identified
goal_identified
=== ep: 2518, time 47.80974817276001, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2518
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2519, time 48.317466259002686, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2520, time 47.4395387172699, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2520
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2521, time 47.338372230529785, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2521
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2522, time 47.38307785987854, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2523, time 48.19998121261597, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2523
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2524, time 48.52502179145813, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2524
goal_identified
goal_identified
goal_identified
=== ep: 2525, time 47.503984689712524, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2526, time 48.37740135192871, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2526
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2527, time 48.6362190246582, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2527
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2528, time 48.04616379737854, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2438
goal_identified
goal_identified
=== ep: 2529, time 48.03287148475647, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2529
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2530, time 49.49928879737854, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2530
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2531, time 48.34389877319336, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2531
goal_identified
goal_identified
goal_identified
=== ep: 2532, time 48.40658688545227, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2532
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2533, time 48.51648497581482, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2533
goal_identified
goal_identified
goal_identified
=== ep: 2534, time 47.40259671211243, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2534
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2535, time 48.7025887966156, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2535
goal_identified
goal_identified
=== ep: 2536, time 47.28019094467163, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2536
goal_identified
goal_identified
goal_identified
=== ep: 2537, time 47.873870849609375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2537
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2538, time 49.40154814720154, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2538
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2539, time 48.83046746253967, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2539
goal_identified
goal_identified
goal_identified
=== ep: 2540, time 47.071412086486816, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2540
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2541, time 52.28552055358887, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2542, time 47.238234519958496, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2542
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2543, time 47.85795187950134, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2543
goal_identified
goal_identified
=== ep: 2544, time 50.707154273986816, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2544
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2545, time 48.048099517822266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2545
goal_identified
goal_identified
goal_identified
=== ep: 2546, time 47.843846797943115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2546
goal_identified
goal_identified
=== ep: 2547, time 48.89515042304993, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2547
goal_identified
goal_identified
=== ep: 2548, time 48.17672300338745, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2548
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2549, time 49.24943447113037, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2549
goal_identified
goal_identified
=== ep: 2550, time 49.23311638832092, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2550
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2551, time 47.99837017059326, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2551
goal_identified
goal_identified
goal_identified
=== ep: 2552, time 53.08709001541138, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2552
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2553, time 49.96175456047058, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2554, time 48.03448295593262, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2554
goal_identified
goal_identified
goal_identified
=== ep: 2555, time 47.73923444747925, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2556, time 49.1244261264801, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2556
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2557, time 48.12870812416077, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2558, time 49.03937649726868, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2558
goal_identified
goal_identified
goal_identified
=== ep: 2559, time 47.96505165100098, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2559
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2560, time 48.23319125175476, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2560
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2561, time 49.732526779174805, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2561
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2562, time 48.17935013771057, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2562
goal_identified
goal_identified
=== ep: 2563, time 47.87406516075134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2563
goal_identified
goal_identified
=== ep: 2564, time 50.83174777030945, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2564
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2565, time 48.10158610343933, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2565
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2566, time 48.69075155258179, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2566
goal_identified
goal_identified
goal_identified
=== ep: 2567, time 49.35332107543945, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2567
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2568, time 48.55190634727478, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2568
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2569, time 48.71195721626282, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 49.465182065963745, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2570
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2571, time 47.73479747772217, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2572, time 48.54708480834961, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2572
goal_identified
goal_identified
goal_identified
=== ep: 2573, time 50.461026430130005, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2573
goal_identified
goal_identified
goal_identified
=== ep: 2574, time 48.070565938949585, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2574
goal_identified
goal_identified
goal_identified
=== ep: 2575, time 49.39223599433899, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2576, time 50.46345019340515, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2576
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2577, time 48.04193305969238, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2577
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2578, time 50.01342749595642, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2578
goal_identified
goal_identified
=== ep: 2579, time 50.90336894989014, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2579
goal_identified
goal_identified
goal_identified
=== ep: 2580, time 47.68071389198303, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2580
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2581, time 50.6384015083313, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2581
goal_identified
goal_identified
goal_identified
=== ep: 2582, time 51.26618766784668, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2582
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2583, time 48.20928716659546, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2583
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2584, time 52.15535593032837, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2584
goal_identified
goal_identified
goal_identified
=== ep: 2585, time 51.46971607208252, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2586, time 47.92199659347534, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2586
goal_identified
=== ep: 2587, time 49.87855410575867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2587
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2588, time 50.940722942352295, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2588
goal_identified
goal_identified
goal_identified
=== ep: 2589, time 46.60150361061096, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2590, time 49.856773376464844, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2591, time 53.42771649360657, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2591
goal_identified
goal_identified
goal_identified
=== ep: 2592, time 47.733415842056274, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2592
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2593, time 50.865222454071045, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2593
goal_identified
=== ep: 2594, time 49.841137170791626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2594
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2595, time 47.75950527191162, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2596, time 47.96503686904907, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2597, time 49.31050515174866, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2598, time 48.205684661865234, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2598
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2599, time 48.82691144943237, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2600, time 49.07071900367737, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2600
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2601, time 48.474953413009644, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2601
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2602, time 48.08556270599365, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2602
goal_identified
goal_identified
goal_identified
=== ep: 2603, time 49.10720920562744, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2603
goal_identified
goal_identified
=== ep: 2604, time 47.479766845703125, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2604
goal_identified
goal_identified
goal_identified
=== ep: 2605, time 47.95638394355774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2605
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2606, time 50.830567598342896, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2606
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2607, time 47.68169593811035, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2607
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2608, time 47.8620765209198, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2608
goal_identified
=== ep: 2609, time 52.06421685218811, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2609
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2610, time 48.057297706604004, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2610
goal_identified
goal_identified
goal_identified
=== ep: 2611, time 48.45192813873291, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2611
goal_identified
goal_identified
goal_identified
=== ep: 2612, time 50.16642451286316, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2612
goal_identified
goal_identified
goal_identified
=== ep: 2613, time 47.94736838340759, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2614, time 48.6193745136261, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2614
goal_identified
goal_identified
=== ep: 2615, time 48.745877265930176, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2615
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2616, time 48.427186489105225, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2616
goal_identified
goal_identified
goal_identified
=== ep: 2617, time 48.26946806907654, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2617
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2618, time 47.81823182106018, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2618
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2619, time 48.1946702003479, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2619
goal_identified
goal_identified
goal_identified
=== ep: 2620, time 47.967182874679565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2620
=== ep: 2621, time 47.632020235061646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2622, time 52.18756580352783, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2622
goal_identified
goal_identified
=== ep: 2623, time 47.91864275932312, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2624, time 47.970168113708496, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2624
=== ep: 2625, time 54.02570128440857, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2626, time 47.93950080871582, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2626
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2627, time 48.13281178474426, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2627
goal_identified
goal_identified
goal_identified
=== ep: 2628, time 54.99624514579773, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2628
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2629, time 47.88144779205322, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2629
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2630, time 47.86865973472595, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2630
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2631, time 53.960991859436035, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2632, time 52.332561016082764, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2632
goal_identified
goal_identified
goal_identified
=== ep: 2633, time 48.56606459617615, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2633
goal_identified
goal_identified
=== ep: 2634, time 52.99269700050354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2634
goal_identified
goal_identified
goal_identified
=== ep: 2635, time 48.42539310455322, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2635
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2636, time 47.346457719802856, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2636
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2637, time 49.339462995529175, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2637
goal_identified
goal_identified
goal_identified
=== ep: 2638, time 48.433183431625366, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2638
goal_identified
goal_identified
goal_identified
=== ep: 2639, time 47.90388751029968, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2639
goal_identified
goal_identified
goal_identified
=== ep: 2640, time 47.999046087265015, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2640
goal_identified
goal_identified
goal_identified
=== ep: 2641, time 49.148104429244995, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2641
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2642, time 48.41317057609558, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2631
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2643, time 48.09239912033081, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2643
goal_identified
goal_identified
=== ep: 2644, time 49.72291278839111, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2644
goal_identified
goal_identified
goal_identified
=== ep: 2645, time 48.871689319610596, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2645
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2646, time 47.960291624069214, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2646
goal_identified
goal_identified
goal_identified
=== ep: 2647, time 52.704641580581665, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2647
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2648, time 48.48352360725403, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2648
=== ep: 2649, time 47.050132274627686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2649
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2650, time 51.58623266220093, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2650
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2651, time 47.59296655654907, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2651
goal_identified
goal_identified
goal_identified
=== ep: 2652, time 48.15545630455017, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2652
goal_identified
goal_identified
=== ep: 2653, time 50.89590501785278, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2653
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2654, time 47.63889265060425, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2654
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2655, time 48.061625957489014, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2655
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2656, time 51.15225601196289, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2656
goal_identified
goal_identified
goal_identified
=== ep: 2657, time 48.46328949928284, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2657
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2658, time 48.53495121002197, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2658
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2659, time 53.41385197639465, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2660, time 48.11501669883728, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2661, time 48.623111963272095, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2661
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2662, time 50.87143850326538, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2662
goal_identified
goal_identified
goal_identified
=== ep: 2663, time 47.97007703781128, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2664, time 48.072879791259766, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2664
goal_identified
goal_identified
goal_identified
=== ep: 2665, time 48.859503984451294, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2665
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2666, time 49.61790442466736, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2667, time 48.00994086265564, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2667
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2668, time 49.39573907852173, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2668
goal_identified
goal_identified
=== ep: 2669, time 52.46619153022766, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2669
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2670, time 48.96817708015442, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2671, time 47.905771017074585, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2671
goal_identified
goal_identified
goal_identified
=== ep: 2672, time 50.960667848587036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2672
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2673, time 48.678476095199585, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2673
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2674, time 52.5913565158844, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2674
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2675, time 50.33781957626343, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2675
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2676, time 48.103055477142334, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2676
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2677, time 48.29982900619507, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2677
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2678, time 48.572200536727905, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2678
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2679, time 48.945526361465454, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2679
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2680, time 48.96198272705078, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2680
=== ep: 2681, time 49.6694130897522, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2681
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2682, time 48.034339427948, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2682
goal_identified
goal_identified
goal_identified
=== ep: 2683, time 48.45434379577637, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2683
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2684, time 50.8706259727478, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2684
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2685, time 51.91207551956177, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2685
goal_identified
goal_identified
goal_identified
=== ep: 2686, time 47.88196778297424, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2686
goal_identified
goal_identified
=== ep: 2687, time 52.114643812179565, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2687
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2688, time 52.488157749176025, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2688
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2689, time 47.98840522766113, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2689
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2690, time 50.736368894577026, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2691, time 49.48985028266907, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2692, time 48.208990812301636, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2692
goal_identified
goal_identified
goal_identified
=== ep: 2693, time 51.960087060928345, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2693
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2694, time 48.0945930480957, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2694
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2695, time 48.070955991744995, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2695
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2696, time 50.92116403579712, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2696
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2697, time 48.59880781173706, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2697
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2698, time 50.77530312538147, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2698
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2699, time 51.10612654685974, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2660
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2700, time 48.71218776702881, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2701, time 51.613200187683105, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2702, time 50.17859721183777, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2702
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2703, time 48.58575248718262, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2704, time 52.39642906188965, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2704
goal_identified
goal_identified
=== ep: 2705, time 50.36602163314819, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2706, time 48.189406394958496, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2706
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2707, time 49.711352586746216, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2708, time 49.947468280792236, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2708
goal_identified
goal_identified
=== ep: 2709, time 47.9486825466156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2709
goal_identified
goal_identified
goal_identified
=== ep: 2710, time 47.94867730140686, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2710
goal_identified
goal_identified
goal_identified
=== ep: 2711, time 49.50298714637756, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2711
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2712, time 52.57826900482178, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2713, time 48.98442554473877, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2713
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2714, time 49.741273403167725, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2714
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2715, time 48.223883390426636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2715
goal_identified
goal_identified
=== ep: 2716, time 48.86850309371948, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2716
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2717, time 50.86146831512451, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2699
goal_identified
goal_identified
goal_identified
=== ep: 2718, time 48.54576921463013, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2718
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2719, time 48.151912689208984, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2719
goal_identified
=== ep: 2720, time 51.07772421836853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2720
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2721, time 48.2502658367157, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2722, time 48.731640338897705, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2723, time 48.75906682014465, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2717
goal_identified
goal_identified
=== ep: 2724, time 49.10077500343323, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2724
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2725, time 48.774128675460815, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2725
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2726, time 48.81178641319275, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2726
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2727, time 48.76157593727112, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2727
goal_identified
goal_identified
goal_identified
=== ep: 2728, time 48.81212902069092, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2728
=== ep: 2729, time 48.323124170303345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2729
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2730, time 50.28658676147461, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2730
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2731, time 48.53173637390137, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2731
goal_identified
=== ep: 2732, time 48.28526520729065, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2732
goal_identified
goal_identified
goal_identified
=== ep: 2733, time 51.94227194786072, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2733
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2734, time 48.42670917510986, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2734
goal_identified
goal_identified
=== ep: 2735, time 47.685604095458984, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2735
goal_identified
goal_identified
=== ep: 2736, time 55.951518297195435, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2736
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2737, time 48.888864278793335, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2737
goal_identified
goal_identified
goal_identified
=== ep: 2738, time 48.529924392700195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2738
goal_identified
=== ep: 2739, time 54.75043606758118, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2739
goal_identified
goal_identified
=== ep: 2740, time 49.29722571372986, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 48.8214271068573, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2742, time 53.93212580680847, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2742
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2743, time 48.11005425453186, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2743
goal_identified
goal_identified
=== ep: 2744, time 48.21970772743225, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2744
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2745, time 53.18059158325195, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2745
goal_identified
goal_identified
goal_identified
=== ep: 2746, time 47.77958536148071, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2746
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2747, time 48.91868495941162, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2747
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2748, time 52.285305976867676, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2748
goal_identified
goal_identified
goal_identified
=== ep: 2749, time 48.721747398376465, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2749
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2750, time 52.87717843055725, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2750
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2751, time 51.623685359954834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2751
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2752, time 48.96039795875549, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2753, time 48.74765110015869, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2753
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2754, time 51.98530721664429, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2755, time 49.104827880859375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2755
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2756, time 50.00657796859741, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2756
goal_identified
goal_identified
goal_identified
=== ep: 2757, time 50.616679668426514, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2757
goal_identified
goal_identified
=== ep: 2758, time 48.373862743377686, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2758
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2759, time 48.32195019721985, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2759
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2760, time 50.36677026748657, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2760
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2761, time 48.65258717536926, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2762, time 48.55262851715088, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2762
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2763, time 50.20050406455994, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2763
goal_identified
goal_identified
goal_identified
=== ep: 2764, time 48.52426743507385, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2764
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2765, time 48.32361459732056, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2765
goal_identified
goal_identified
goal_identified
=== ep: 2766, time 50.54470109939575, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2766
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2767, time 48.87634515762329, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2767
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2768, time 48.42305278778076, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2769, time 51.787877798080444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2769
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2770, time 50.18791198730469, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2770
goal_identified
goal_identified
goal_identified
=== ep: 2771, time 48.83623170852661, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2771
goal_identified
goal_identified
=== ep: 2772, time 50.645397663116455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2772
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2773, time 51.00886344909668, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2773
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2774, time 48.326483726501465, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2774
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2775, time 50.83633637428284, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2775
goal_identified
goal_identified
=== ep: 2776, time 51.829866886138916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2776
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2777, time 48.20243692398071, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2777
goal_identified
goal_identified
goal_identified
=== ep: 2778, time 50.55783486366272, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2778
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2779, time 52.0095477104187, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2779
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2780, time 48.43593716621399, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2780
goal_identified
goal_identified
=== ep: 2781, time 50.33358287811279, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2781
goal_identified
goal_identified
=== ep: 2782, time 52.13036227226257, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2782
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2783, time 48.19233846664429, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2783
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2784, time 50.663915157318115, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2784
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2785, time 52.23573660850525, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2785
goal_identified
goal_identified
goal_identified
=== ep: 2786, time 48.095085859298706, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2786
goal_identified
goal_identified
goal_identified
=== ep: 2787, time 48.693267822265625, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2788, time 50.67553424835205, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2788
goal_identified
goal_identified
goal_identified
=== ep: 2789, time 52.827133655548096, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2789
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2790, time 48.57267141342163, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2790
=== ep: 2791, time 51.09217429161072, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2791
goal_identified
goal_identified
goal_identified
=== ep: 2792, time 48.13634490966797, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2792
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2793, time 48.53945183753967, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2793
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2794, time 49.6602725982666, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2795, time 48.62472629547119, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2795
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2796, time 49.03081965446472, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2797, time 48.602147340774536, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2797
=== ep: 2798, time 48.718567848205566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2798
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2799, time 48.596569299697876, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2799
goal_identified
=== ep: 2800, time 48.3457465171814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2800
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2801, time 49.40394568443298, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2801
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2802, time 48.581881284713745, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2802
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2803, time 48.761969804763794, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2804, time 49.43320059776306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2804
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2805, time 47.759602069854736, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2805
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2806, time 47.78225541114807, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2807, time 52.204153537750244, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2807
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2808, time 48.39020013809204, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2808
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2809, time 49.255298137664795, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2809
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2810, time 52.080217361450195, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2810
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2811, time 48.05525827407837, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2811
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2812, time 48.568790435791016, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2812
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2813, time 48.73586392402649, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2813
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2814, time 49.00207018852234, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2814
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2815, time 48.408464193344116, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2816, time 49.09310269355774, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2817, time 51.022053480148315, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2817
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2818, time 48.01767873764038, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2818
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2819, time 48.261542320251465, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2819
goal_identified
goal_identified
goal_identified
=== ep: 2820, time 52.76905703544617, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2820
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2821, time 48.99936819076538, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2821
goal_identified
goal_identified
goal_identified
=== ep: 2822, time 48.697914600372314, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2822
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2823, time 53.91816854476929, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2823
goal_identified
goal_identified
goal_identified
=== ep: 2824, time 48.25609827041626, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2824
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2825, time 48.80903601646423, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2825
goal_identified
goal_identified
goal_identified
=== ep: 2826, time 50.13005518913269, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2826
goal_identified
goal_identified
goal_identified
=== ep: 2827, time 48.15468978881836, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2827
goal_identified
goal_identified
=== ep: 2828, time 52.97835183143616, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2828
=== ep: 2829, time 46.79089593887329, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2829
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2830, time 48.98375916481018, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
goal_identified
goal_identified
goal_identified
=== ep: 2831, time 48.65599775314331, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2831
goal_identified
goal_identified
goal_identified
=== ep: 2832, time 48.6677987575531, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2832
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2833, time 48.74940824508667, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2833
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2834, time 49.039493799209595, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2834
goal_identified
goal_identified
goal_identified
=== ep: 2835, time 51.55842137336731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2835
goal_identified
goal_identified
goal_identified
=== ep: 2836, time 50.21678805351257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2836
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2837, time 49.0644805431366, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2837
goal_identified
goal_identified
=== ep: 2838, time 50.357364654541016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2838
goal_identified
goal_identified
=== ep: 2839, time 50.2114999294281, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2839
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2840, time 49.02818489074707, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 50.02519154548645, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2841
goal_identified
=== ep: 2842, time 50.45061707496643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2842
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2843, time 48.68872261047363, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2843
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2844, time 50.367908239364624, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2844
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2845, time 51.382123708724976, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 48.566895484924316, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2847, time 51.643330574035645, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2847
goal_identified
goal_identified
goal_identified
=== ep: 2848, time 53.25473952293396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2848
goal_identified
goal_identified
goal_identified
=== ep: 2849, time 48.99599862098694, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2849
goal_identified
goal_identified
goal_identified
=== ep: 2850, time 49.58019208908081, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2850
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2851, time 51.71251153945923, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2851
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2852, time 48.702757835388184, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2852
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2853, time 48.76324510574341, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2853
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2854, time 47.61266255378723, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2854
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2855, time 48.13052487373352, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2855
goal_identified
goal_identified
=== ep: 2856, time 49.64593243598938, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2857, time 49.04876232147217, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2857
goal_identified
goal_identified
goal_identified
=== ep: 2858, time 50.742695331573486, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2859, time 48.592772006988525, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2860, time 48.56176710128784, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 52.11603236198425, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2861
goal_identified
goal_identified
goal_identified
=== ep: 2862, time 49.618399143218994, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2862
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2863, time 48.651426553726196, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2864, time 52.73434090614319, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2865, time 48.724645376205444, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2865
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2866, time 48.7064425945282, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2866
goal_identified
goal_identified
goal_identified
=== ep: 2867, time 52.47749900817871, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2867
goal_identified
goal_identified
=== ep: 2868, time 53.76012706756592, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2869, time 48.74414610862732, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2869
goal_identified
goal_identified
=== ep: 2870, time 51.12410354614258, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2871, time 48.911782026290894, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2871
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2872, time 48.774651765823364, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2872
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2873, time 49.19713282585144, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2873
goal_identified
goal_identified
goal_identified
=== ep: 2874, time 48.58154082298279, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2874
goal_identified
goal_identified
=== ep: 2875, time 48.39120841026306, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2875
goal_identified
goal_identified
goal_identified
=== ep: 2876, time 48.85250782966614, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2876
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2877, time 52.94969582557678, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2877
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2878, time 49.74342083930969, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2879, time 48.790422201156616, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2879
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2880, time 52.35049867630005, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2881, time 48.85536074638367, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2881
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2882, time 49.03653264045715, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2882
goal_identified
goal_identified
goal_identified
=== ep: 2883, time 51.18847942352295, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2883
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2884, time 48.3577823638916, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2884
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2885, time 48.59284543991089, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2885
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2886, time 48.946677446365356, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2886
goal_identified
goal_identified
goal_identified
=== ep: 2887, time 49.918975591659546, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2887
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2888, time 48.866249322891235, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2888
goal_identified
goal_identified
=== ep: 2889, time 48.76311421394348, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2889
goal_identified
goal_identified
=== ep: 2890, time 48.79785490036011, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2890
goal_identified
goal_identified
goal_identified
=== ep: 2891, time 48.5104820728302, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2891
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2892, time 48.62557125091553, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2892
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2893, time 51.48522996902466, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2893
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2894, time 48.568655014038086, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2894
goal_identified
goal_identified
=== ep: 2895, time 48.325682401657104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2895
goal_identified
goal_identified
goal_identified
=== ep: 2896, time 53.18352031707764, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2896
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2897, time 49.14490103721619, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2898, time 48.9925491809845, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2898
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2899, time 51.67428541183472, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2899
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2900, time 48.825705766677856, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2900
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2901, time 48.50823354721069, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2901
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2902, time 48.8162784576416, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2902
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2903, time 48.809485912323, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2903
goal_identified
goal_identified
goal_identified
=== ep: 2904, time 48.9849636554718, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2904
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2905, time 48.6153244972229, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2905
goal_identified
goal_identified
goal_identified
=== ep: 2906, time 49.34532380104065, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2906
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2907, time 49.573862075805664, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2907
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2908, time 54.232593297958374, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2908
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2909, time 52.13062882423401, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2909
goal_identified
goal_identified
=== ep: 2910, time 48.6133599281311, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2911, time 48.65458011627197, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2911
goal_identified
goal_identified
goal_identified
=== ep: 2912, time 52.424654483795166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2913, time 50.213449001312256, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2913
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2914, time 49.00391721725464, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
goal_identified
goal_identified
goal_identified
=== ep: 2915, time 53.52710223197937, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2915
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2916, time 50.46652579307556, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 49.0984206199646, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2918, time 50.085339307785034, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2918
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2919, time 51.26574993133545, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2920, time 49.28230428695679, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2920
goal_identified
goal_identified
goal_identified
=== ep: 2921, time 49.49168109893799, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2921
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2922, time 51.934709548950195, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2922
goal_identified
goal_identified
=== ep: 2923, time 48.98623013496399, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2923
goal_identified
goal_identified
=== ep: 2924, time 48.83820080757141, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2924
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2925, time 54.53893303871155, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2925
goal_identified
goal_identified
goal_identified
=== ep: 2926, time 49.391690492630005, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2926
goal_identified
goal_identified
=== ep: 2927, time 49.10717415809631, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2928, time 57.084789991378784, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 50.16150164604187, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2929
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2930, time 49.32005333900452, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2930
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2931, time 58.96393156051636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2931
goal_identified
goal_identified
goal_identified
=== ep: 2932, time 49.34143090248108, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2932
goal_identified
goal_identified
goal_identified
=== ep: 2933, time 49.187325954437256, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2933
goal_identified
goal_identified
=== ep: 2934, time 56.49385166168213, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2934
goal_identified
=== ep: 2935, time 49.343684911727905, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2935
goal_identified
goal_identified
=== ep: 2936, time 48.9173526763916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2936
goal_identified
goal_identified
goal_identified
=== ep: 2937, time 54.458858251571655, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2937
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2938, time 49.61955714225769, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2938
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2939, time 47.80397081375122, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2939
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2940, time 52.24467062950134, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2940
goal_identified
goal_identified
goal_identified
=== ep: 2941, time 48.952046632766724, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2941
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2942, time 49.16925644874573, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2942
goal_identified
goal_identified
goal_identified
=== ep: 2943, time 51.7968270778656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2943
goal_identified
goal_identified
=== ep: 2944, time 50.18377065658569, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2944
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2945, time 49.441770792007446, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2945
goal_identified
goal_identified
goal_identified
=== ep: 2946, time 51.86324191093445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2946
goal_identified
goal_identified
goal_identified
=== ep: 2947, time 51.007680892944336, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2947
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2948, time 49.0802686214447, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2948
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2949, time 55.67906880378723, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2949
goal_identified
goal_identified
goal_identified
=== ep: 2950, time 51.029640674591064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2950
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2951, time 48.96368050575256, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2951
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2952, time 50.095497369766235, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2952
goal_identified
goal_identified
=== ep: 2953, time 52.82454061508179, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2953
goal_identified
goal_identified
goal_identified
=== ep: 2954, time 48.90352249145508, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2954
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2955, time 50.765401124954224, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2955
goal_identified
=== ep: 2956, time 53.47190308570862, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2956
goal_identified
=== ep: 2957, time 48.867961406707764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2958, time 51.3147177696228, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2958
goal_identified
goal_identified
=== ep: 2959, time 52.86699414253235, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2959
goal_identified
=== ep: 2960, time 49.750418186187744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2961, time 51.39673829078674, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2961
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2962, time 53.7836799621582, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2962
goal_identified
goal_identified
goal_identified
=== ep: 2963, time 48.97306704521179, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2963
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2964, time 52.02516436576843, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2964
goal_identified
goal_identified
=== ep: 2965, time 53.714163303375244, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2965
goal_identified
goal_identified
=== ep: 2966, time 48.94166398048401, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2966
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2967, time 53.360334396362305, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2967
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2968, time 53.584680795669556, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2968
goal_identified
goal_identified
goal_identified
=== ep: 2969, time 50.24145698547363, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2969
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2970, time 52.91267800331116, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2970
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2971, time 51.129095792770386, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2971
goal_identified
goal_identified
goal_identified
=== ep: 2972, time 49.301108598709106, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2972
goal_identified
goal_identified
=== ep: 2973, time 52.32903027534485, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2973
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2974, time 49.06918239593506, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2974
goal_identified
goal_identified
=== ep: 2975, time 48.75655436515808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2976, time 52.53302073478699, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2976
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2977, time 48.972174406051636, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2978, time 50.384259939193726, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2978
goal_identified
goal_identified
goal_identified
=== ep: 2979, time 52.841817140579224, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2979
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2980, time 49.317002296447754, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2980
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2981, time 51.08543372154236, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2981
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2982, time 52.677128314971924, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2982
goal_identified
goal_identified
goal_identified
=== ep: 2983, time 49.360828161239624, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2983
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2984, time 51.78822183609009, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2984
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2985, time 52.73018169403076, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2985
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2986, time 48.98268699645996, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2986
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2987, time 54.097007274627686, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2987
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2988, time 53.8854923248291, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2988
goal_identified
goal_identified
goal_identified
=== ep: 2989, time 49.33553194999695, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2989
goal_identified
goal_identified
goal_identified
=== ep: 2990, time 54.14958095550537, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2990
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2991, time 58.21938943862915, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2991
goal_identified
goal_identified
=== ep: 2992, time 49.01466226577759, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2993, time 52.81395387649536, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2993
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2994, time 49.76936364173889, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2995, time 49.61262369155884, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2995
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2996, time 49.9853355884552, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2996
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2997, time 51.99555706977844, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2997
goal_identified
goal_identified
=== ep: 2998, time 49.2624351978302, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2998
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2999, time 50.0201518535614, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2999
goal_identified
goal_identified
goal_identified
=== ep: 3000, time 53.97703695297241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3000
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3001, time 48.52248954772949, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3001
goal_identified
goal_identified
goal_identified
=== ep: 3002, time 49.967546701431274, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3002
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3003, time 54.83690929412842, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3004, time 49.50521516799927, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3004
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3005, time 49.04727602005005, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3005
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3006, time 57.73171806335449, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3007, time 48.97742176055908, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3007
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3008, time 48.9017014503479, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3008
goal_identified
goal_identified
=== ep: 3009, time 58.170891761779785, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3009
goal_identified
goal_identified
goal_identified
=== ep: 3010, time 49.59403324127197, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3010
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3011, time 49.09386920928955, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3011
goal_identified
goal_identified
goal_identified
=== ep: 3012, time 55.08888864517212, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3012
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3013, time 49.49148106575012, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3013
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3014, time 49.590479135513306, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3014
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3015, time 52.79286026954651, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3015
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3016, time 49.211350440979004, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3016
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3017, time 49.26786756515503, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3017
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3018, time 49.10558891296387, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3018
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3019, time 52.63098502159119, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3019
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3020, time 49.104958295822144, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3020
goal_identified
goal_identified
goal_identified
=== ep: 3021, time 49.47300171852112, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3021
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3022, time 53.958757400512695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3022
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3023, time 48.57346487045288, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3023
goal_identified
goal_identified
=== ep: 3024, time 49.4158194065094, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3024
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3025, time 53.10134816169739, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3025
goal_identified
goal_identified
=== ep: 3026, time 48.80534744262695, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3026
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3027, time 50.39945435523987, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3028, time 52.544694900512695, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3028
goal_identified
goal_identified
goal_identified
=== ep: 3029, time 49.33153009414673, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3029
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3030, time 48.659215688705444, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3030
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3031, time 51.08989357948303, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3031
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3032, time 49.54442477226257, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3032
goal_identified
goal_identified
=== ep: 3033, time 56.207587242126465, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3033
goal_identified
goal_identified
goal_identified
=== ep: 3034, time 51.64894413948059, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3034
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3035, time 49.27857756614685, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3035
goal_identified
goal_identified
goal_identified
=== ep: 3036, time 50.35265588760376, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3036
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3037, time 49.87049412727356, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3038, time 50.00949954986572, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3038
goal_identified
goal_identified
goal_identified
=== ep: 3039, time 50.694984436035156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3039
goal_identified
goal_identified
=== ep: 3040, time 50.022011041641235, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 50.216811180114746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3041
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3042, time 50.610300064086914, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3042
goal_identified
goal_identified
=== ep: 3043, time 49.425867557525635, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3043
goal_identified
goal_identified
=== ep: 3044, time 52.85371661186218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3044
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3045, time 51.99849581718445, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3045
goal_identified
goal_identified
=== ep: 3046, time 48.817527055740356, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3047, time 54.45186734199524, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3047
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3048, time 52.94082522392273, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3048
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3049, time 49.539252281188965, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3049
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3050, time 53.75578165054321, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3050
goal_identified
goal_identified
=== ep: 3051, time 53.33685803413391, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3051
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3052, time 49.574336767196655, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3052
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3053, time 53.87762713432312, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3053
goal_identified
goal_identified
=== ep: 3054, time 51.58503699302673, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3054
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3055, time 49.547035455703735, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3055
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3056, time 49.89128875732422, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3056
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3057, time 53.22381639480591, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3058, time 49.36627554893494, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3058
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3059, time 49.30419588088989, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3059
goal_identified
goal_identified
goal_identified
=== ep: 3060, time 53.783790588378906, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3060
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3061, time 49.66628837585449, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3061
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3062, time 49.85166072845459, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3062
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3063, time 54.46689248085022, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3063
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3064, time 49.31788730621338, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3064
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3065, time 49.25828766822815, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3065
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3066, time 55.64968943595886, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3066
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3067, time 49.40882420539856, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3067
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3068, time 49.234567165374756, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3068
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3069, time 59.33773946762085, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3070, time 49.571425437927246, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3070
goal_identified
goal_identified
=== ep: 3071, time 49.620463132858276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3071
goal_identified
goal_identified
goal_identified
=== ep: 3072, time 57.01806926727295, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3072
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3073, time 49.24193000793457, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3073
goal_identified
goal_identified
goal_identified
=== ep: 3074, time 49.28860521316528, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3074
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3075, time 54.948549032211304, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3075
goal_identified
goal_identified
goal_identified
=== ep: 3076, time 54.08190608024597, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3076
goal_identified
goal_identified
goal_identified
=== ep: 3077, time 48.81257128715515, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3077
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3078, time 52.80731749534607, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3079, time 49.104520082473755, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3079
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3080, time 49.55198836326599, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3080
goal_identified
goal_identified
goal_identified
=== ep: 3081, time 51.63133263587952, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3081
goal_identified
goal_identified
goal_identified
=== ep: 3082, time 50.92167639732361, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3082
goal_identified
goal_identified
=== ep: 3083, time 49.40333938598633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3083
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3084, time 51.80427670478821, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3084
goal_identified
goal_identified
=== ep: 3085, time 51.16844606399536, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3085
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3086, time 49.890055656433105, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3086
goal_identified
goal_identified
goal_identified
=== ep: 3087, time 50.94903087615967, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3087
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3088, time 52.893672943115234, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3088
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3089, time 49.30273675918579, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3089
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3090, time 50.84066724777222, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3090
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3091, time 54.31895923614502, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3091
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3092, time 49.144792795181274, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3092
goal_identified
goal_identified
=== ep: 3093, time 50.328590393066406, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3093
goal_identified
=== ep: 3094, time 53.566994190216064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3094
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3095, time 49.99175763130188, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3095
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3096, time 51.65481901168823, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3096
goal_identified
goal_identified
=== ep: 3097, time 52.850340604782104, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3097
goal_identified
goal_identified
=== ep: 3098, time 49.330185651779175, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3098
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3099, time 51.288540840148926, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3099
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3100, time 51.69404315948486, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3100
goal_identified
goal_identified
goal_identified
=== ep: 3101, time 49.341023206710815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3101
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3102, time 49.655070304870605, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3102
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3103, time 49.99465847015381, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3103
goal_identified
goal_identified
goal_identified
=== ep: 3104, time 49.498932123184204, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3104
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3105, time 51.11690831184387, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3105
goal_identified
goal_identified
goal_identified
=== ep: 3106, time 49.44077253341675, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3106
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3107, time 50.414466857910156, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3107
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3108, time 49.80259037017822, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3109, time 49.51298403739929, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3110, time 52.983681440353394, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3110
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3111, time 49.627493381500244, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3112, time 49.84270119667053, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3112
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3113, time 53.9369535446167, eps 0.001, sum reward: 5, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3113
goal_identified
goal_identified
goal_identified
=== ep: 3114, time 47.983492851257324, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3114
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3115, time 49.5512330532074, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3115
goal_identified
goal_identified
goal_identified
=== ep: 3116, time 52.1524555683136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3116
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3117, time 49.833659172058105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3117
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3118, time 49.44987654685974, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3119, time 54.58582305908203, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3119
goal_identified
goal_identified
goal_identified
=== ep: 3120, time 49.780710220336914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3120
goal_identified
=== ep: 3121, time 49.41677403450012, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3121
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3122, time 49.53129720687866, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3122
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3123, time 53.68018627166748, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3123
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3124, time 49.50398850440979, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3124
goal_identified
goal_identified
goal_identified
=== ep: 3125, time 49.4786491394043, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3126, time 54.39462423324585, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3126
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3127, time 49.371185541152954, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3127
goal_identified
goal_identified
=== ep: 3128, time 49.35157322883606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3129, time 56.751556396484375, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3129
goal_identified
=== ep: 3130, time 49.5328094959259, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3130
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3131, time 49.44250845909119, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3132, time 55.91647124290466, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3132
goal_identified
goal_identified
goal_identified
=== ep: 3133, time 49.25459361076355, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3133
goal_identified
goal_identified
=== ep: 3134, time 49.460474729537964, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3134
goal_identified
goal_identified
=== ep: 3135, time 52.36970067024231, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3135
goal_identified
goal_identified
goal_identified
=== ep: 3136, time 49.73329758644104, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3136
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3137, time 49.52352786064148, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3137
goal_identified
goal_identified
=== ep: 3138, time 51.669140577316284, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3138
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3139, time 50.39424443244934, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3139
goal_identified
=== ep: 3140, time 49.850478410720825, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3140
goal_identified
goal_identified
goal_identified
=== ep: 3141, time 51.234166622161865, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3141
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3142, time 51.95220947265625, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3142
goal_identified
goal_identified
goal_identified
=== ep: 3143, time 49.059823751449585, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3143
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3144, time 49.576858043670654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3144
goal_identified
goal_identified
goal_identified
=== ep: 3145, time 53.497884035110474, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3145
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3146, time 49.273208141326904, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3146
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3147, time 51.038469314575195, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3148, time 54.748526096343994, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3148
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3149, time 50.16635823249817, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3149
goal_identified
goal_identified
=== ep: 3150, time 49.805161476135254, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3150
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3151, time 51.56666684150696, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3151
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3152, time 49.74664878845215, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3152
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3153, time 49.689515113830566, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3153
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3154, time 48.93006348609924, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3154
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3155, time 49.999237298965454, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3155
goal_identified
goal_identified
=== ep: 3156, time 49.381189584732056, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3156
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3157, time 49.24722933769226, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3157
goal_identified
goal_identified
goal_identified
=== ep: 3158, time 52.25870680809021, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3158
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3159, time 49.20789647102356, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3160, time 50.06146311759949, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3160
goal_identified
goal_identified
goal_identified
=== ep: 3161, time 52.401851654052734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3162, time 49.548412799835205, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3162
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3163, time 54.44232702255249, eps 0.001, sum reward: 5, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3163
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3164, time 54.814034938812256, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3164
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3165, time 49.669010400772095, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3166, time 49.946898460388184, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 55.069347858428955, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3167
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3168, time 49.65473794937134, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3168
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3169, time 49.67186737060547, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3169
goal_identified
goal_identified
goal_identified
=== ep: 3170, time 56.078484535217285, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3170
goal_identified
goal_identified
goal_identified
=== ep: 3171, time 49.611369609832764, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3171
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3172, time 49.48767971992493, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3173, time 53.86284875869751, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3173
goal_identified
goal_identified
=== ep: 3174, time 49.494779109954834, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3174
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3175, time 49.68536567687988, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3175
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3176, time 50.080225229263306, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3176
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3177, time 51.890052318573, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3177
goal_identified
goal_identified
goal_identified
=== ep: 3178, time 49.701165199279785, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3178
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3179, time 49.680912256240845, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3180, time 53.21072292327881, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3180
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3181, time 49.54082465171814, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3181
goal_identified
goal_identified
=== ep: 3182, time 49.39744162559509, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3182
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3183, time 52.968164682388306, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3183
goal_identified
goal_identified
goal_identified
=== ep: 3184, time 49.6626660823822, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3184
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3185, time 52.89986801147461, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3185
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3186, time 53.12074565887451, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3186
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3187, time 50.05439782142639, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3187
goal_identified
goal_identified
goal_identified
=== ep: 3188, time 51.89248442649841, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3188
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3189, time 50.40395951271057, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3190, time 50.26935005187988, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3190
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3191, time 50.874181270599365, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3191
goal_identified
goal_identified
goal_identified
=== ep: 3192, time 50.433655977249146, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3192
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3193, time 49.378986835479736, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3193
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3194, time 51.623069286346436, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3194
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3195, time 49.623695850372314, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3195
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3196, time 50.95911455154419, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3196
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3197, time 50.093847036361694, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3197
goal_identified
goal_identified
goal_identified
=== ep: 3198, time 50.05069971084595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3198
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3199, time 51.19077920913696, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3199
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3200, time 52.435086250305176, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3200
goal_identified
=== ep: 3201, time 49.86635637283325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3201
goal_identified
goal_identified
=== ep: 3202, time 53.47626829147339, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3202
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3203, time 51.567410469055176, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3203
goal_identified
=== ep: 3204, time 49.39406085014343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3204
goal_identified
goal_identified
goal_identified
=== ep: 3205, time 53.872480154037476, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3205
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3206, time 52.679818868637085, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3206
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3207, time 49.92365884780884, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3207
goal_identified
goal_identified
=== ep: 3208, time 59.170148849487305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3208
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3209, time 53.6713604927063, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3209
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3210, time 50.216394662857056, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3210
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3211, time 54.885934352874756, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3211
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3212, time 52.40114092826843, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3212
goal_identified
goal_identified
=== ep: 3213, time 49.52256631851196, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3213
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3214, time 54.403852224349976, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3214
goal_identified
goal_identified
goal_identified
=== ep: 3215, time 51.71614861488342, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3215
goal_identified
goal_identified
goal_identified
=== ep: 3216, time 49.45156478881836, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3216
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3217, time 52.18504214286804, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3217
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3218, time 52.24791169166565, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3218
goal_identified
goal_identified
=== ep: 3219, time 49.466389656066895, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3219
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3220, time 51.63343667984009, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3220
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3221, time 52.51286864280701, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3221
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3222, time 49.984689235687256, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3222
goal_identified
goal_identified
goal_identified
=== ep: 3223, time 49.98647499084473, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3223
goal_identified
goal_identified
=== ep: 3224, time 52.950520277023315, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3224
goal_identified
goal_identified
goal_identified
=== ep: 3225, time 49.41866207122803, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3226, time 49.47107911109924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3226
goal_identified
goal_identified
goal_identified
=== ep: 3227, time 54.93643760681152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3227
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3228, time 49.82664179801941, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3228
=== ep: 3229, time 48.34462356567383, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3230, time 56.09186387062073, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3230
goal_identified
goal_identified
=== ep: 3231, time 49.44789218902588, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3231
goal_identified
goal_identified
goal_identified
=== ep: 3232, time 49.6887047290802, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3232
goal_identified
goal_identified
=== ep: 3233, time 57.054571866989136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3233
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3234, time 50.30953335762024, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3234
goal_identified
goal_identified
goal_identified
=== ep: 3235, time 49.20423650741577, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3235
goal_identified
goal_identified
goal_identified
=== ep: 3236, time 57.36566925048828, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3236
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3237, time 50.265865087509155, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3237
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3238, time 49.669411420822144, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3238
goal_identified
goal_identified
goal_identified
=== ep: 3239, time 55.200634717941284, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3239
goal_identified
goal_identified
=== ep: 3240, time 49.57780861854553, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3241, time 49.89967894554138, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3241
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3242, time 50.69378995895386, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3242
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3243, time 49.559070110321045, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3244, time 49.92670202255249, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3245, time 49.8122193813324, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3245
goal_identified
goal_identified
goal_identified
=== ep: 3246, time 52.23650097846985, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3246
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3247, time 50.188175439834595, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3247
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3248, time 50.459794998168945, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3248
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3249, time 54.51439666748047, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3249
goal_identified
goal_identified
goal_identified
=== ep: 3250, time 50.612629890441895, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3250
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3251, time 50.882797718048096, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3251
goal_identified
goal_identified
goal_identified
=== ep: 3252, time 53.184831857681274, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3252
goal_identified
goal_identified
=== ep: 3253, time 55.509965658187866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3253
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3254, time 49.92825198173523, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3254
goal_identified
goal_identified
goal_identified
=== ep: 3255, time 53.26321053504944, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3255
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3256, time 50.05947399139404, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3256
goal_identified
goal_identified
=== ep: 3257, time 49.4994101524353, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3257
goal_identified
goal_identified
=== ep: 3258, time 50.789809226989746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3258
goal_identified
goal_identified
goal_identified
=== ep: 3259, time 49.84498190879822, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3259
goal_identified
goal_identified
goal_identified
=== ep: 3260, time 50.478832721710205, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3260
goal_identified
goal_identified
goal_identified
=== ep: 3261, time 49.95502495765686, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3262, time 50.89902877807617, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3262
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3263, time 50.1586754322052, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3264, time 50.09720993041992, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3264
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3265, time 52.10854363441467, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3265
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3266, time 49.95521354675293, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3267, time 50.29068040847778, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3268, time 52.990581035614014, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3268
goal_identified
goal_identified
goal_identified
=== ep: 3269, time 51.33808922767639, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3269
goal_identified
goal_identified
=== ep: 3270, time 50.18892955780029, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3270
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3271, time 53.76304388046265, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3271
goal_identified
goal_identified
goal_identified
=== ep: 3272, time 52.75909638404846, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3272
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3273, time 49.60385274887085, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3274, time 54.318578481674194, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3274
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3275, time 52.76039409637451, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3275
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3276, time 50.28792214393616, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3276
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3277, time 53.27607989311218, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3277
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3278, time 53.38612484931946, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3278
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3279, time 49.48094129562378, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3280, time 53.68101716041565, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3280
goal_identified
goal_identified
goal_identified
=== ep: 3281, time 53.552849769592285, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3281
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3282, time 49.87185049057007, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3282
goal_identified
goal_identified
goal_identified
=== ep: 3283, time 51.606947898864746, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3283
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3284, time 54.018836975097656, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3284
goal_identified
goal_identified
goal_identified
=== ep: 3285, time 49.93673062324524, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3285
goal_identified
goal_identified
goal_identified
=== ep: 3286, time 51.875404596328735, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3286
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3287, time 52.59639358520508, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3287
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3288, time 49.40416359901428, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3289, time 49.70196843147278, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3289
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3290, time 53.93203020095825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3290
goal_identified
goal_identified
goal_identified
=== ep: 3291, time 50.455434799194336, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3291
goal_identified
goal_identified
goal_identified
=== ep: 3292, time 48.92466735839844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3292
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3293, time 52.742207765579224, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3293
goal_identified
goal_identified
=== ep: 3294, time 49.46151161193848, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3294
goal_identified
goal_identified
=== ep: 3295, time 49.68000364303589, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3295
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3296, time 55.194106578826904, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3296
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3297, time 49.98416709899902, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3297
goal_identified
goal_identified
=== ep: 3298, time 49.72183609008789, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3298
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3299, time 59.50800108909607, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3299
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3300, time 50.46423578262329, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3300
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3301, time 50.281753063201904, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3301
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3302, time 56.40416765213013, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3302
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3303, time 49.65762662887573, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3303
goal_identified
goal_identified
goal_identified
=== ep: 3304, time 49.424017906188965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3305, time 57.518107175827026, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3305
goal_identified
goal_identified
=== ep: 3306, time 51.17975115776062, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3306
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3307, time 50.019662618637085, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3307
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3308, time 55.72636675834656, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3308
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3309, time 49.8845796585083, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3309
goal_identified
goal_identified
=== ep: 3310, time 49.70923161506653, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3310
goal_identified
goal_identified
goal_identified
=== ep: 3311, time 56.526556730270386, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3311
goal_identified
goal_identified
goal_identified
=== ep: 3312, time 50.044615745544434, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3312
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3313, time 50.576456785202026, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3314, time 57.99932551383972, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3314
goal_identified
goal_identified
goal_identified
=== ep: 3315, time 50.16968011856079, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3315
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3316, time 49.68702936172485, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3316
goal_identified
=== ep: 3317, time 57.145766735076904, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 50.07997369766235, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3318
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3319, time 49.79591965675354, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3319
goal_identified
goal_identified
goal_identified
=== ep: 3320, time 56.023953676223755, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3320
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3321, time 50.16916275024414, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3321
goal_identified
=== ep: 3322, time 50.56431770324707, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3322
goal_identified
goal_identified
goal_identified
=== ep: 3323, time 55.57890725135803, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3324, time 50.28593564033508, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3324
goal_identified
goal_identified
=== ep: 3325, time 50.5920250415802, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3325
goal_identified
goal_identified
goal_identified
=== ep: 3326, time 53.96354579925537, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3326
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3327, time 50.10377502441406, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3327
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3328, time 50.44478893280029, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3328
goal_identified
goal_identified
goal_identified
=== ep: 3329, time 53.70321440696716, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3329
goal_identified
=== ep: 3330, time 49.57754039764404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3330
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3331, time 49.75711226463318, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3331
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3332, time 53.20535087585449, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3332
goal_identified
=== ep: 3333, time 49.758389711380005, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3333
goal_identified
goal_identified
goal_identified
=== ep: 3334, time 50.30872368812561, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3335, time 53.05636024475098, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3336, time 50.39320635795593, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3336
goal_identified
goal_identified
=== ep: 3337, time 50.11014103889465, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3338, time 53.82364273071289, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3338
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3339, time 49.76093602180481, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3339
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3340, time 49.911043643951416, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3340
goal_identified
goal_identified
=== ep: 3341, time 53.263951778411865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3341
goal_identified
=== ep: 3342, time 53.286240339279175, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3342
goal_identified
goal_identified
goal_identified
=== ep: 3343, time 50.057605028152466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3343
goal_identified
goal_identified
=== ep: 3344, time 53.67283010482788, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3344
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3345, time 52.50665521621704, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3345
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3346, time 55.559967279434204, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3346
goal_identified
goal_identified
goal_identified
=== ep: 3347, time 54.381309270858765, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3347
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3348, time 52.606693267822266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3349, time 50.35467314720154, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3349
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3350, time 54.22938418388367, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3350
goal_identified
goal_identified
goal_identified
=== ep: 3351, time 53.76235055923462, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3351
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3352, time 50.67727875709534, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3352
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3353, time 53.951618671417236, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3353
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3354, time 53.87881135940552, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3354
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3355, time 50.590601205825806, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3355
goal_identified
goal_identified
=== ep: 3356, time 53.74296832084656, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3356
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3357, time 55.34940242767334, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3357
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3358, time 50.207892417907715, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3358
goal_identified
goal_identified
goal_identified
=== ep: 3359, time 53.65256428718567, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3359
goal_identified
goal_identified
=== ep: 3360, time 54.6204137802124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3360
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3361, time 50.596818685531616, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3361
goal_identified
=== ep: 3362, time 53.977906227111816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3362
goal_identified
goal_identified
goal_identified
=== ep: 3363, time 53.7094931602478, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3363
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3364, time 51.17245101928711, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3364
goal_identified
goal_identified
goal_identified
=== ep: 3365, time 53.55094361305237, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3365
goal_identified
goal_identified
goal_identified
=== ep: 3366, time 53.22860026359558, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3366
goal_identified
goal_identified
goal_identified
=== ep: 3367, time 50.44224524497986, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3367
goal_identified
goal_identified
=== ep: 3368, time 53.6917028427124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3368
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3369, time 52.25544333457947, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3370, time 50.75694251060486, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3370
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3371, time 53.339890480041504, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3372, time 51.51499581336975, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3373, time 50.72034430503845, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3373
goal_identified
goal_identified
goal_identified
=== ep: 3374, time 52.47917461395264, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3374
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3375, time 50.22488069534302, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3375
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3376, time 52.26108479499817, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3376
goal_identified
goal_identified
=== ep: 3377, time 52.08427882194519, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3377
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3378, time 50.18390440940857, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3378
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3379, time 51.94869613647461, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3379
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3380, time 51.839242935180664, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3380
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3381, time 50.194987773895264, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3381
goal_identified
goal_identified
goal_identified
=== ep: 3382, time 54.403326988220215, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3382
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3383, time 52.09574246406555, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3383
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3384, time 51.955190658569336, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3384
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3385, time 55.07828903198242, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3385
=== ep: 3386, time 51.849934816360474, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3386
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3387, time 50.013046741485596, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3387
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3388, time 55.170268058776855, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3388
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3389, time 50.34604549407959, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3390, time 50.49541997909546, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3390
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3391, time 52.2278368473053, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3392, time 50.04840350151062, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3392
goal_identified
goal_identified
goal_identified
=== ep: 3393, time 55.82919788360596, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3393
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3394, time 51.59934663772583, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3394
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3395, time 50.550063610076904, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3396, time 50.18316698074341, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3397, time 51.01648926734924, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3397
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3398, time 51.410507917404175, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3398
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3399, time 50.52088451385498, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3399
goal_identified
goal_identified
=== ep: 3400, time 50.6120080947876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3400
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3401, time 54.055628538131714, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3401
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3402, time 50.77054738998413, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3402
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3403, time 50.225913524627686, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3403
goal_identified
goal_identified
goal_identified
=== ep: 3404, time 56.08627152442932, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3404
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3405, time 50.80810356140137, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3406, time 49.86278462409973, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3406
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3407, time 57.18334102630615, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3407
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3408, time 50.477598428726196, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3408
goal_identified
goal_identified
goal_identified
=== ep: 3409, time 50.688464403152466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3409
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3410, time 52.75910663604736, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3411, time 51.709842920303345, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3412, time 50.11324453353882, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3412
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3413, time 52.19322991371155, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3414, time 50.317660093307495, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3414
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3415, time 50.32981562614441, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3415
goal_identified
goal_identified
goal_identified
=== ep: 3416, time 50.30423355102539, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3416
goal_identified
goal_identified
=== ep: 3417, time 51.69612646102905, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3417
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3418, time 50.94651007652283, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3418
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3419, time 50.10800504684448, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
goal_identified
goal_identified
goal_identified
=== ep: 3420, time 52.64540648460388, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3420
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3421, time 51.77810263633728, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3422, time 51.5928156375885, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3423, time 53.000638484954834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3423
goal_identified
goal_identified
goal_identified
=== ep: 3424, time 52.218873023986816, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3424
goal_identified
goal_identified
=== ep: 3425, time 50.732568979263306, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3426, time 54.834898948669434, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3426
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3427, time 53.7463698387146, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3427
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3428, time 50.673110246658325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3428
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3429, time 55.50392031669617, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3430, time 53.8306679725647, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3431, time 50.00872230529785, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3432, time 52.600966691970825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3432
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3433, time 54.11074376106262, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3434, time 51.022351026535034, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3434
goal_identified
goal_identified
goal_identified
=== ep: 3435, time 52.97962045669556, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3435
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3436, time 54.70729064941406, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3436
=== ep: 3437, time 51.83930540084839, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3437
goal_identified
goal_identified
goal_identified
=== ep: 3438, time 53.432663440704346, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3438
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3439, time 54.1601185798645, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3439
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3440, time 55.82616639137268, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3440
goal_identified
goal_identified
=== ep: 3441, time 53.92915606498718, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3441
goal_identified
goal_identified
=== ep: 3442, time 53.662904262542725, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3442
goal_identified
goal_identified
goal_identified
=== ep: 3443, time 50.607815742492676, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3443
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3444, time 52.39054989814758, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3444
goal_identified
goal_identified
goal_identified
=== ep: 3445, time 51.77359199523926, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3446, time 49.19250679016113, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3446
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3447, time 52.42076516151428, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3448, time 52.117964029312134, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3448
goal_identified
goal_identified
=== ep: 3449, time 50.17417931556702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3449
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3450, time 50.6989369392395, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3450
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3451, time 52.377434730529785, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3451
goal_identified
goal_identified
goal_identified
=== ep: 3452, time 50.41703987121582, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3452
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3453, time 50.17695999145508, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3453
goal_identified
goal_identified
goal_identified
=== ep: 3454, time 53.23930335044861, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3454
goal_identified
goal_identified
goal_identified
=== ep: 3455, time 49.864221811294556, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3455
goal_identified
goal_identified
=== ep: 3456, time 51.53306770324707, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3457, time 56.70290565490723, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3457
goal_identified
goal_identified
=== ep: 3458, time 49.078327894210815, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3458
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3459, time 50.955408334732056, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3459
goal_identified
goal_identified
goal_identified
=== ep: 3460, time 55.00717616081238, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3460
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3461, time 51.13298797607422, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3461
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3462, time 50.68150472640991, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
goal_identified
=== ep: 3463, time 52.575563192367554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3464, time 50.515103578567505, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3464
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3465, time 50.909929513931274, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3465
goal_identified
goal_identified
goal_identified
=== ep: 3466, time 50.51889252662659, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3466
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3467, time 50.27809524536133, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3467
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3468, time 50.777344703674316, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3468
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3469, time 50.32738780975342, eps 0.001, sum reward: 10, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3470, time 51.81620669364929, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3470
goal_identified
goal_identified
goal_identified
=== ep: 3471, time 50.09925079345703, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3471
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3472, time 50.23126769065857, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3473, time 52.25631332397461, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3473
goal_identified
goal_identified
=== ep: 3474, time 50.42456531524658, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3475, time 50.2803840637207, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3475
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3476, time 54.70554280281067, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3476
goal_identified
goal_identified
goal_identified
=== ep: 3477, time 50.62790560722351, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 49.43707036972046, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3478
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3479, time 53.83984613418579, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3479
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3480, time 51.25627946853638, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3480
goal_identified
goal_identified
goal_identified
=== ep: 3481, time 50.97252893447876, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3481
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3482, time 51.5548460483551, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3482
goal_identified
goal_identified
goal_identified
=== ep: 3483, time 51.94183278083801, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3483
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3484, time 49.82550859451294, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3485, time 50.02054405212402, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3486, time 51.24699783325195, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3486
goal_identified
goal_identified
=== ep: 3487, time 50.76776170730591, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3487
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3488, time 50.47088646888733, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3488
goal_identified
goal_identified
goal_identified
=== ep: 3489, time 59.462263107299805, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3489
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3490, time 50.37553095817566, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3490
goal_identified
goal_identified
goal_identified
=== ep: 3491, time 50.24999189376831, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3491
goal_identified
goal_identified
goal_identified
=== ep: 3492, time 51.817315101623535, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3492
goal_identified
goal_identified
=== ep: 3493, time 49.98740077018738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3493
goal_identified
=== ep: 3494, time 50.427565574645996, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3494
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3495, time 53.56485056877136, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3495
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3496, time 50.47923445701599, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3496
goal_identified
goal_identified
goal_identified
=== ep: 3497, time 50.42171120643616, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3497
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3498, time 58.15753364562988, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3498
=== ep: 3499, time 49.75860404968262, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3499
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3500, time 50.10990285873413, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3500
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3501, time 56.82821846008301, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3502, time 50.08864426612854, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3502
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3503, time 50.84308862686157, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3503
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3504, time 54.23075604438782, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3504
goal_identified
goal_identified
goal_identified
=== ep: 3505, time 49.91869139671326, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3505
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3506, time 49.948230266571045, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3506
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3507, time 50.322983264923096, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3507
goal_identified
goal_identified
goal_identified
=== ep: 3508, time 51.52682828903198, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3508
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3509, time 50.439186334609985, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3509
goal_identified
goal_identified
goal_identified
=== ep: 3510, time 50.352073669433594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3511, time 53.742579221725464, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3512, time 50.03968691825867, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3512
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3513, time 50.44360852241516, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3514, time 55.09749674797058, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3514
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3515, time 50.12926006317139, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3515
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3516, time 52.19360661506653, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3516
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3517, time 53.75018000602722, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3518, time 50.05626082420349, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3518
goal_identified
goal_identified
goal_identified
=== ep: 3519, time 52.153990507125854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3520, time 52.33024454116821, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3520
goal_identified
goal_identified
goal_identified
=== ep: 3521, time 50.63192963600159, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3521
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3522, time 53.13602876663208, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3523, time 50.35965847969055, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3523
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3524, time 50.57417035102844, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3524
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3525, time 52.32938838005066, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3525
goal_identified
=== ep: 3526, time 50.00649094581604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3526
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3527, time 52.502832889556885, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3527
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3528, time 53.90890026092529, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3529, time 49.57947635650635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3529
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3530, time 53.015570640563965, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3530
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3531, time 53.76855254173279, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3531
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3532, time 49.587132692337036, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3532
goal_identified
goal_identified
goal_identified
=== ep: 3533, time 54.873106718063354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3533
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3534, time 53.44257378578186, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3534
goal_identified
=== ep: 3535, time 50.06037998199463, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3535
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3536, time 53.900245904922485, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3536
goal_identified
goal_identified
goal_identified
=== ep: 3537, time 53.67447829246521, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3537
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3538, time 49.987699031829834, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3538
goal_identified
goal_identified
=== ep: 3539, time 58.247588872909546, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3539
goal_identified
=== ep: 3540, time 53.37687373161316, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3540
goal_identified
goal_identified
=== ep: 3541, time 50.12129092216492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3541
goal_identified
goal_identified
goal_identified
=== ep: 3542, time 52.28960728645325, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3542
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3543, time 54.04982829093933, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3543
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3544, time 49.385419607162476, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3544
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3545, time 51.37199020385742, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1948
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3546, time 54.70742583274841, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3546
goal_identified
goal_identified
=== ep: 3547, time 50.090662717819214, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3547
goal_identified
goal_identified
=== ep: 3548, time 50.3669753074646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3548
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3549, time 54.32417058944702, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3549
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3550, time 50.592960357666016, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3550
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3551, time 49.98850774765015, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3551
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3552, time 57.86357665061951, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2016
goal_identified
goal_identified
=== ep: 3553, time 50.345049142837524, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3554, time 51.087138652801514, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3554
goal_identified
goal_identified
=== ep: 3555, time 56.059807777404785, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3556, time 50.75294637680054, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3556
=== ep: 3557, time 50.25302338600159, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3557
goal_identified
goal_identified
=== ep: 3558, time 58.01428174972534, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3558
goal_identified
=== ep: 3559, time 50.33200669288635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3559
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3560, time 50.543912410736084, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3560
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3561, time 59.78677439689636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3561
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3562, time 50.83460736274719, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3562
goal_identified
goal_identified
goal_identified
=== ep: 3563, time 50.38369178771973, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3564, time 57.5386598110199, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3564
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3565, time 50.93975615501404, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3565
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3566, time 50.89202809333801, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3566
goal_identified
goal_identified
goal_identified
=== ep: 3567, time 53.95695233345032, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3567
goal_identified
goal_identified
goal_identified
=== ep: 3568, time 50.20951724052429, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3568
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3569, time 50.429943561553955, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3569
goal_identified
goal_identified
goal_identified
=== ep: 3570, time 51.21593999862671, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3570
goal_identified
goal_identified
=== ep: 3571, time 50.82974457740784, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3572, time 51.100247859954834, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3572
goal_identified
goal_identified
goal_identified
=== ep: 3573, time 50.83786153793335, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3573
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3574, time 53.52084517478943, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3574
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3575, time 50.65279531478882, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3575
goal_identified
goal_identified
=== ep: 3576, time 51.88281536102295, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3576
goal_identified
goal_identified
goal_identified
=== ep: 3577, time 54.75817084312439, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3577
goal_identified
goal_identified
goal_identified
=== ep: 3578, time 50.421520948410034, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3578
goal_identified
=== ep: 3579, time 50.63003897666931, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3579
goal_identified
goal_identified
goal_identified
=== ep: 3580, time 53.17770552635193, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3580
goal_identified
goal_identified
=== ep: 3581, time 50.051106691360474, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3582, time 51.153700828552246, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3582
goal_identified
goal_identified
goal_identified
=== ep: 3583, time 50.222540616989136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3583
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3584, time 52.36476707458496, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3584
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3585, time 51.751041650772095, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3585
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3586, time 50.808642864227295, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3587, time 54.32322072982788, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 51.17262935638428, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3588
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3589, time 56.7357132434845, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3589
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3590, time 55.488155364990234, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3590
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3591, time 50.70294952392578, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 49.998493671417236, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3592
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3593, time 55.72160267829895, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3593
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3594, time 50.65364050865173, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3594
goal_identified
goal_identified
=== ep: 3595, time 51.37633728981018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3596, time 53.80548453330994, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3597, time 50.19670605659485, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3598, time 50.45036816596985, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3598
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3599, time 50.843252420425415, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3600, time 51.24675798416138, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3600
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3601, time 51.381850719451904, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3601
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3602, time 51.18719124794006, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3602
goal_identified
=== ep: 3603, time 54.091066122055054, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3603
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3604, time 50.67018246650696, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3604
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3605, time 51.340737104415894, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3605
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3606, time 55.90609550476074, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3606
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3607, time 50.138657093048096, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3607
goal_identified
goal_identified
goal_identified
=== ep: 3608, time 54.1241717338562, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3608
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3609, time 55.23432683944702, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3609
goal_identified
=== ep: 3610, time 50.765188694000244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3611, time 54.923983097076416, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3611
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3612, time 54.20711088180542, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3612
goal_identified
=== ep: 3613, time 50.78344440460205, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3614, time 54.49027919769287, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3614
goal_identified
goal_identified
=== ep: 3615, time 52.880348920822144, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3615
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3616, time 50.53915619850159, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2148
goal_identified
goal_identified
goal_identified
=== ep: 3617, time 53.294793367385864, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3617
goal_identified
goal_identified
=== ep: 3618, time 51.68689775466919, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3618
goal_identified
goal_identified
=== ep: 3619, time 52.17582559585571, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3619
goal_identified
goal_identified
=== ep: 3620, time 52.244874477386475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3620
goal_identified
goal_identified
=== ep: 3621, time 51.96183729171753, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3621
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3622, time 54.01237630844116, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3622
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3623, time 51.724135398864746, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3623
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3624, time 50.59534478187561, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3624
goal_identified
goal_identified
=== ep: 3625, time 55.872711181640625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3625
goal_identified
goal_identified
=== ep: 3626, time 52.55196976661682, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3626
goal_identified
goal_identified
goal_identified
=== ep: 3627, time 51.09766697883606, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3627
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3628, time 53.82756471633911, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3628
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3629, time 53.33090543746948, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3629
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3630, time 51.62324285507202, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3630
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3631, time 53.95128393173218, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3631
goal_identified
goal_identified
goal_identified
=== ep: 3632, time 54.49197268486023, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3632
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3633, time 51.394739866256714, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3634, time 51.27069067955017, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3634
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3635, time 53.87683463096619, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3635
=== ep: 3636, time 51.71379375457764, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3636
goal_identified
goal_identified
goal_identified
=== ep: 3637, time 51.23928642272949, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3637
goal_identified
goal_identified
=== ep: 3638, time 55.03795552253723, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3638
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3639, time 50.6640739440918, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3639
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3640, time 49.979572057724, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3640
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3641, time 54.79305100440979, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3641
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3642, time 51.02559041976929, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3642
goal_identified
goal_identified
goal_identified
=== ep: 3643, time 51.179972887039185, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3643
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3644, time 56.34374403953552, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3644
goal_identified
goal_identified
=== ep: 3645, time 51.2579243183136, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3645
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3646, time 51.09923982620239, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3646
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3647, time 58.487600564956665, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3647
goal_identified
goal_identified
=== ep: 3648, time 51.35521459579468, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3648
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3649, time 50.67388701438904, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3649
goal_identified
goal_identified
goal_identified
=== ep: 3650, time 57.137224435806274, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3650
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3651, time 51.28611922264099, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 52.84275412559509, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3652
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3653, time 54.663068771362305, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3653
goal_identified
=== ep: 3654, time 51.101627349853516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3654
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3655, time 51.687664270401, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3655
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3656, time 55.02054190635681, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3656
goal_identified
goal_identified
=== ep: 3657, time 51.21746349334717, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3657
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3658, time 51.88963341712952, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3658
goal_identified
goal_identified
goal_identified
=== ep: 3659, time 52.87085270881653, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3659
goal_identified
goal_identified
=== ep: 3660, time 51.90619683265686, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3660
goal_identified
goal_identified
goal_identified
=== ep: 3661, time 51.93041801452637, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3661
goal_identified
goal_identified
goal_identified
=== ep: 3662, time 52.214616537094116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3662
goal_identified
goal_identified
goal_identified
=== ep: 3663, time 51.700307846069336, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3664, time 52.940990924835205, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3665, time 51.15939545631409, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3665
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3666, time 51.793670415878296, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3667, time 52.16691517829895, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3667
goal_identified
goal_identified
goal_identified
=== ep: 3668, time 50.91729664802551, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3668
goal_identified
goal_identified
goal_identified
=== ep: 3669, time 52.47502017021179, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3669
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3670, time 52.82577133178711, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3671, time 51.44208598136902, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3672, time 54.74449706077576, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3672
goal_identified
=== ep: 3673, time 53.76033091545105, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3673
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3674, time 52.00382971763611, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3674
goal_identified
goal_identified
goal_identified
=== ep: 3675, time 55.259169816970825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3675
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3676, time 54.770004987716675, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3676
goal_identified
goal_identified
goal_identified
=== ep: 3677, time 51.76112461090088, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3678, time 55.30439472198486, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3678
goal_identified
=== ep: 3679, time 55.23729658126831, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3679
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3680, time 51.51663398742676, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3680
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3681, time 56.117764711380005, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3681
goal_identified
goal_identified
=== ep: 3682, time 54.46562385559082, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3682
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3683, time 51.25524044036865, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3684, time 56.22762322425842, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3684
goal_identified
=== ep: 3685, time 53.88801455497742, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3685
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3686, time 51.05144691467285, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3686
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3687, time 55.504714488983154, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3687
goal_identified
goal_identified
=== ep: 3688, time 52.47093963623047, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3688
goal_identified
goal_identified
=== ep: 3689, time 51.35229206085205, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3689
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3690, time 54.00698804855347, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3691, time 58.44702482223511, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3692, time 51.66757535934448, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3692
goal_identified
goal_identified
goal_identified
=== ep: 3693, time 55.845807790756226, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3693
goal_identified
goal_identified
goal_identified
=== ep: 3694, time 51.250882625579834, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3694
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3695, time 51.53556847572327, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3695
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3696, time 55.69267249107361, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3696
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3697, time 51.58510971069336, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3697
goal_identified
goal_identified
=== ep: 3698, time 50.8793261051178, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3698
goal_identified
goal_identified
=== ep: 3699, time 53.28724551200867, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3699
goal_identified
goal_identified
=== ep: 3700, time 51.84679412841797, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3700
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3701, time 51.291237115859985, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3701
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3702, time 50.47820425033569, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3702
goal_identified
goal_identified
goal_identified
=== ep: 3703, time 53.38891386985779, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3704, time 51.85766100883484, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3704
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3705, time 51.321566104888916, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3706, time 54.98250961303711, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3706
goal_identified
goal_identified
goal_identified
=== ep: 3707, time 51.96460223197937, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3707
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3708, time 51.21334433555603, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3708
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3709, time 56.161076068878174, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3709
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3710, time 51.39942216873169, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3710
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3711, time 52.02883195877075, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3711
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3712, time 54.78347182273865, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3713, time 51.46704149246216, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3713
=== ep: 3714, time 51.09380388259888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3714
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3715, time 52.394869327545166, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3715
goal_identified
goal_identified
goal_identified
=== ep: 3716, time 51.47406482696533, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3716
goal_identified
goal_identified
goal_identified
=== ep: 3717, time 52.254798889160156, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3717
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3718, time 51.56751894950867, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3718
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3719, time 51.66035795211792, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3719
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3720, time 52.02012896537781, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3721, time 52.37561845779419, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3721
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3722, time 53.14935064315796, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3722
goal_identified
goal_identified
goal_identified
=== ep: 3723, time 51.67332363128662, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3723
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3724, time 51.80710744857788, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3724
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3725, time 52.92473340034485, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3725
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3726, time 52.298131227493286, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3726
goal_identified
goal_identified
=== ep: 3727, time 50.72264766693115, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3727
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3728, time 55.23473048210144, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3728
goal_identified
goal_identified
goal_identified
=== ep: 3729, time 51.44791555404663, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3729
goal_identified
goal_identified
goal_identified
=== ep: 3730, time 52.0122652053833, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3730
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3731, time 56.108885526657104, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3732, time 51.45230484008789, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3732
goal_identified
goal_identified
goal_identified
=== ep: 3733, time 51.599875688552856, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3733
goal_identified
goal_identified
goal_identified
=== ep: 3734, time 54.55739951133728, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3734
goal_identified
goal_identified
goal_identified
=== ep: 3735, time 51.57687020301819, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3735
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3736, time 51.57727551460266, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3736
goal_identified
=== ep: 3737, time 53.766366481781006, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3737
goal_identified
goal_identified
=== ep: 3738, time 51.31475257873535, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3738
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3739, time 51.50790214538574, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3739
goal_identified
goal_identified
=== ep: 3740, time 52.10885405540466, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3741, time 52.34580183029175, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3741
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3742, time 51.26914191246033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3742
goal_identified
goal_identified
=== ep: 3743, time 57.93228888511658, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3743
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3744, time 54.83576822280884, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3744
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3745, time 52.1291139125824, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3745
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3746, time 53.07987403869629, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3746
goal_identified
goal_identified
=== ep: 3747, time 55.55487895011902, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3747
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3748, time 52.59183955192566, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3748
goal_identified
goal_identified
=== ep: 3749, time 51.39493942260742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3749
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3750, time 55.6041841506958, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 51.906688928604126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3751
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3752, time 51.62527847290039, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3753, time 55.20071506500244, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3753
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3754, time 51.84487795829773, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3755, time 51.84877800941467, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3755
goal_identified
goal_identified
goal_identified
=== ep: 3756, time 56.32535457611084, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3756
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3757, time 50.73804259300232, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3757
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3758, time 51.57211995124817, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3758
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3759, time 57.79111933708191, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3759
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3760, time 52.02912902832031, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3760
goal_identified
goal_identified
goal_identified
=== ep: 3761, time 51.501630783081055, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3762, time 56.612279415130615, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3762
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3763, time 52.30335855484009, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3763
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3764, time 51.76741576194763, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3764
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3765, time 53.56128692626953, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3765
goal_identified
goal_identified
=== ep: 3766, time 51.86974000930786, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3766
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3767, time 51.53949332237244, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3767
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3768, time 51.97196674346924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3768
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3769, time 52.743714332580566, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3769
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3770, time 52.12315583229065, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3770
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3771, time 51.911739349365234, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3771
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3772, time 53.556309938430786, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3772
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3773, time 52.16612505912781, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3773
goal_identified
goal_identified
goal_identified
=== ep: 3774, time 51.79590678215027, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3774
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3775, time 55.75818109512329, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3776, time 52.17737627029419, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3776
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3777, time 51.3801965713501, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3777
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3778, time 53.76915383338928, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3778
goal_identified
goal_identified
=== ep: 3779, time 51.78950786590576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 51.96249485015869, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3780
goal_identified
goal_identified
goal_identified
=== ep: 3781, time 54.1440794467926, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3781
goal_identified
goal_identified
=== ep: 3782, time 51.59702920913696, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3782
goal_identified
goal_identified
=== ep: 3783, time 51.71418023109436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3783
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3784, time 53.905240297317505, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3784
=== ep: 3785, time 51.425312519073486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3785
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3786, time 51.669153690338135, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3786
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3787, time 53.542988300323486, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3787
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 52.17840886116028, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3789, time 52.64195370674133, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3789
goal_identified
goal_identified
=== ep: 3790, time 56.55974531173706, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3791, time 52.187135457992554, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3791
goal_identified
=== ep: 3792, time 52.2447030544281, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3793, time 55.619473934173584, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3793
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3794, time 53.506471395492554, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3794
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3795, time 52.02925658226013, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3795
goal_identified
goal_identified
goal_identified
=== ep: 3796, time 61.076292514801025, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3796
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3797, time 53.09927225112915, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3798, time 52.25612688064575, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3798
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3799, time 55.62061142921448, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3799
goal_identified
goal_identified
goal_identified
=== ep: 3800, time 54.46130704879761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3800
goal_identified
=== ep: 3801, time 51.08973026275635, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3801
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3802, time 54.76140379905701, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3802
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3803, time 54.14576983451843, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3803
goal_identified
goal_identified
goal_identified
=== ep: 3804, time 51.95978283882141, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3804
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3805, time 55.23199963569641, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3805
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3806, time 55.86006450653076, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3806
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3807, time 51.72065353393555, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3807
goal_identified
goal_identified
goal_identified
=== ep: 3808, time 55.7463698387146, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3808
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3809, time 55.30741333961487, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3809
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3810, time 51.78820610046387, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3810
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3811, time 56.21875548362732, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3811
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3812, time 56.579219818115234, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3812
goal_identified
goal_identified
goal_identified
=== ep: 3813, time 50.39136552810669, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3813
goal_identified
goal_identified
goal_identified
=== ep: 3814, time 55.849958419799805, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3814
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3815, time 56.183300733566284, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3816, time 51.11884641647339, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3817, time 56.52770256996155, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3817
goal_identified
goal_identified
=== ep: 3818, time 55.75364661216736, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3818
goal_identified
goal_identified
goal_identified
=== ep: 3819, time 52.183271169662476, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3820, time 56.1771035194397, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3820
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3821, time 56.64621829986572, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3821
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3822, time 52.352476596832275, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3823, time 55.71116900444031, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3824, time 57.12797713279724, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3824
goal_identified
goal_identified
goal_identified
=== ep: 3825, time 50.74462127685547, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3826, time 55.64683985710144, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3826
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3827, time 56.200408935546875, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3827
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3828, time 52.094127893447876, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3828
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3829, time 56.11879253387451, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3829
goal_identified
goal_identified
goal_identified
=== ep: 3830, time 54.13615155220032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3830
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3831, time 52.460978507995605, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3831
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3832, time 56.17837834358215, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3832
goal_identified
=== ep: 3833, time 54.24717712402344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3833
goal_identified
goal_identified
goal_identified
=== ep: 3834, time 51.80559968948364, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3834
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3835, time 56.11234474182129, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3836, time 52.6249303817749, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3836
goal_identified
goal_identified
goal_identified
=== ep: 3837, time 52.28175926208496, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3838, time 56.595364570617676, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3838
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3839, time 51.99142670631409, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3839
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3840, time 53.01244878768921, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3840
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3841, time 56.33090400695801, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 52.492348432540894, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3843, time 53.41590189933777, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3843
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3844, time 55.26845049858093, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3844
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3845, time 53.2260684967041, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3845
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3846, time 52.700146436691284, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3846
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3847, time 55.26957654953003, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3847
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3848, time 52.530478954315186, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3848
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3849, time 59.716190576553345, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3849
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3850, time 55.195592641830444, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3850
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3851, time 52.61349034309387, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3851
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3852, time 54.91251230239868, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3852
goal_identified
goal_identified
=== ep: 3853, time 55.501875162124634, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3853
goal_identified
goal_identified
goal_identified
=== ep: 3854, time 52.549294233322144, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3854
goal_identified
goal_identified
goal_identified
=== ep: 3855, time 55.2972469329834, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3855
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3856, time 54.81233739852905, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3856
goal_identified
goal_identified
goal_identified
=== ep: 3857, time 52.440322160720825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3857
goal_identified
goal_identified
=== ep: 3858, time 55.30772542953491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3859, time 54.666667222976685, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3860, time 50.86871027946472, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3860
goal_identified
=== ep: 3861, time 57.57585620880127, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3862, time 53.948126554489136, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3862
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3863, time 52.39299702644348, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3864, time 55.695799827575684, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3864
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3865, time 53.63009834289551, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3865
goal_identified
goal_identified
=== ep: 3866, time 51.89051389694214, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3866
goal_identified
goal_identified
goal_identified
=== ep: 3867, time 55.02518820762634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3867
goal_identified
goal_identified
=== ep: 3868, time 53.33168697357178, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3868
goal_identified
=== ep: 3869, time 52.97220778465271, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3869
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3870, time 53.951223373413086, eps 0.001, sum reward: 12, score_diff 12, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2723
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3871, time 54.81047987937927, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3871
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3872, time 51.444453954696655, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3872
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3873, time 53.036370515823364, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3873
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3874, time 52.48790454864502, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3874
goal_identified
goal_identified
=== ep: 3875, time 52.14597749710083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3876, time 51.91748857498169, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3877, time 52.19557046890259, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3878, time 52.844629526138306, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2830
goal_identified
goal_identified
=== ep: 3879, time 52.04425883293152, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3879
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3880, time 52.546555280685425, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3880
goal_identified
goal_identified
goal_identified
=== ep: 3881, time 51.95735692977905, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3881
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3882, time 52.735517263412476, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3883, time 54.11949944496155, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3883
goal_identified
goal_identified
goal_identified
=== ep: 3884, time 52.18508958816528, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3884
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3885, time 52.081767320632935, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3885
goal_identified
goal_identified
=== ep: 3886, time 54.25369381904602, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3886
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3887, time 53.444955348968506, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3887
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3888, time 50.70910573005676, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3888
goal_identified
goal_identified
=== ep: 3889, time 55.61959886550903, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3889
goal_identified
goal_identified
goal_identified
=== ep: 3890, time 53.215014696121216, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3890
goal_identified
goal_identified
goal_identified
=== ep: 3891, time 51.724244356155396, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3891
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3892, time 55.63217258453369, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3892
goal_identified
goal_identified
goal_identified
=== ep: 3893, time 52.86996603012085, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3893
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3894, time 52.08967733383179, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3894
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3895, time 53.09131646156311, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3896, time 53.33107542991638, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3897, time 52.41990876197815, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3898, time 52.630414724349976, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3899, time 53.864930868148804, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3899
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3900, time 52.09895324707031, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3900
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3901, time 51.875938177108765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3901
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3902, time 56.4512825012207, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3902
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3903, time 57.586114168167114, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3903
goal_identified
goal_identified
goal_identified
=== ep: 3904, time 52.12103486061096, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3904
goal_identified
goal_identified
goal_identified
=== ep: 3905, time 56.850308895111084, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3905
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3906, time 51.81410598754883, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3906
goal_identified
=== ep: 3907, time 52.270679235458374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3907
goal_identified
goal_identified
goal_identified
=== ep: 3908, time 58.163647174835205, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3908
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3909, time 51.80558395385742, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3909
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3910, time 52.10463619232178, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3911, time 58.36925745010376, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3911
goal_identified
goal_identified
goal_identified
=== ep: 3912, time 51.58949851989746, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3913, time 50.830177545547485, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3913
goal_identified
goal_identified
=== ep: 3914, time 56.94336938858032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3914
goal_identified
=== ep: 3915, time 51.64112901687622, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3915
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3916, time 51.89729595184326, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3916
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3917, time 53.44719958305359, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3917
goal_identified
=== ep: 3918, time 51.670507192611694, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3918
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3919, time 52.338000774383545, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3919
goal_identified
goal_identified
goal_identified
=== ep: 3920, time 51.88716697692871, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3920
goal_identified
goal_identified
goal_identified
=== ep: 3921, time 55.65841722488403, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3921
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3922, time 52.55294752120972, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3922
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3923, time 53.07360577583313, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3923
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3924, time 56.617907762527466, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3924
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3925, time 52.99076008796692, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3925
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3926, time 50.885966300964355, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3926
goal_identified
goal_identified
goal_identified
=== ep: 3927, time 55.76226043701172, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3928, time 52.0537691116333, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3928
goal_identified
goal_identified
goal_identified
=== ep: 3929, time 52.09251594543457, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3929
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3930, time 54.531954288482666, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3930
goal_identified
goal_identified
goal_identified
=== ep: 3931, time 52.06410336494446, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3931
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3932, time 51.911112546920776, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3932
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3933, time 53.2721266746521, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3933
goal_identified
goal_identified
goal_identified
=== ep: 3934, time 52.11245679855347, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3934
goal_identified
goal_identified
=== ep: 3935, time 51.90819072723389, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 52.71801018714905, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3936
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3937, time 52.36944222450256, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3937
goal_identified
goal_identified
goal_identified
=== ep: 3938, time 50.01520800590515, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3938
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3939, time 50.161370038986206, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3939
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3940, time 51.569659948349, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3940
goal_identified
goal_identified
=== ep: 3941, time 49.76989531517029, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3941
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3942, time 51.610066652297974, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3942
goal_identified
=== ep: 3943, time 52.909586906433105, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3943
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3944, time 50.12958812713623, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2914
goal_identified
goal_identified
=== ep: 3945, time 51.645920515060425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3945
goal_identified
goal_identified
goal_identified
=== ep: 3946, time 54.85405993461609, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3946
goal_identified
goal_identified
goal_identified
=== ep: 3947, time 50.24205136299133, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3947
goal_identified
goal_identified
goal_identified
=== ep: 3948, time 52.862064599990845, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3948
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3949, time 53.64276361465454, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3949
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3950, time 50.34077787399292, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3950
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3951, time 53.11751055717468, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3951
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3952, time 52.32049250602722, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3953, time 50.58778738975525, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3953
goal_identified
goal_identified
goal_identified
=== ep: 3954, time 54.29101204872131, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3954
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3955, time 51.60781383514404, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3956, time 50.87490367889404, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3957, time 52.6684832572937, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3957
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3958, time 57.381301164627075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3958
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3959, time 50.0921311378479, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3959
goal_identified
goal_identified
=== ep: 3960, time 49.76858997344971, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3961, time 47.800047159194946, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3961
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3962, time 48.340980052948, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3962
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3963, time 49.13444375991821, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3963
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3964, time 47.36305069923401, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3964
goal_identified
goal_identified
=== ep: 3965, time 50.00911736488342, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3965
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3966, time 47.87376427650452, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3966
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3967, time 48.32094860076904, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3967
goal_identified
goal_identified
goal_identified
=== ep: 3968, time 50.89162087440491, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3968
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3969, time 47.72156476974487, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3969
goal_identified
goal_identified
goal_identified
=== ep: 3970, time 47.87726187705994, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3970
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3971, time 50.999900579452515, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3971
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3972, time 48.19133973121643, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3972
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3973, time 47.75862169265747, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3973
goal_identified
goal_identified
goal_identified
=== ep: 3974, time 48.30767464637756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3974
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3975, time 48.386537313461304, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3975
goal_identified
=== ep: 3976, time 48.00761818885803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3976
goal_identified
goal_identified
=== ep: 3977, time 47.63946723937988, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3977
goal_identified
goal_identified
goal_identified
=== ep: 3978, time 48.29871606826782, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3978
goal_identified
goal_identified
=== ep: 3979, time 50.618953227996826, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3979
goal_identified
goal_identified
goal_identified
=== ep: 3980, time 48.26100564002991, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3980
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3981, time 48.77446150779724, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3981
goal_identified
goal_identified
=== ep: 3982, time 50.885592460632324, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3982
goal_identified
goal_identified
=== ep: 3983, time 48.19710326194763, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3983
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3984, time 50.86554479598999, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3984
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3985, time 50.12811040878296, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3985
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3986, time 47.7817223072052, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3986
goal_identified
goal_identified
=== ep: 3987, time 52.41107702255249, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3987
goal_identified
goal_identified
=== ep: 3988, time 49.18411588668823, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3988
goal_identified
goal_identified
goal_identified
=== ep: 3989, time 48.38564467430115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3989
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3990, time 50.0451180934906, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3990
goal_identified
goal_identified
goal_identified
=== ep: 3991, time 48.65815472602844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3991
goal_identified
goal_identified
goal_identified
=== ep: 3992, time 48.16161012649536, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 48.166926860809326, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3993
goal_identified
goal_identified
goal_identified
=== ep: 3994, time 48.263447761535645, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3995, time 48.30738091468811, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3995
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3996, time 48.10193943977356, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3996
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3997, time 46.65723967552185, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3997
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3998, time 45.04070544242859, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3998
goal_identified
goal_identified
goal_identified
=== ep: 3999, time 44.02284049987793, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
