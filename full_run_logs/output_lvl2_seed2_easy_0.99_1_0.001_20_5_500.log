==> Playing in 11_vs_11_easy_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 27.33031964302063, eps 0.9, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
goal_identified
goal_identified
=== ep: 1, time 27.02711248397827, eps 0.8561552526261419, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
goal_identified
goal_identified
goal_identified
=== ep: 2, time 27.61353087425232, eps 0.8144488388143276, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
goal_identified
goal_identified
goal_identified
=== ep: 3, time 28.151817798614502, eps 0.774776470806127, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
goal_identified
=== ep: 4, time 31.14985418319702, eps 0.7370389470171057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
goal_identified
=== ep: 5, time 31.68277406692505, eps 0.701141903981193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
goal_identified
goal_identified
goal_identified
=== ep: 6, time 32.188284397125244, eps 0.6669955803928644, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
=== ep: 7, time 37.699896574020386, eps 0.6345145926571234, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
goal_identified
goal_identified
goal_identified
=== ep: 8, time 28.44583797454834, eps 0.6036177213860398, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
goal_identified
goal_identified
=== ep: 9, time 31.81509017944336, eps 0.5742277083079742, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
goal_identified
goal_identified
=== ep: 10, time 31.76335072517395, eps 0.5462710630816575, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
goal_identified
goal_identified
=== ep: 11, time 33.274444580078125, eps 0.5196778795320575, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
goal_identified
goal_identified
goal_identified
=== ep: 12, time 33.33557939529419, eps 0.49438166084852986, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
goal_identified
=== ep: 13, time 32.47060203552246, eps 0.47031915330815344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
goal_identified
goal_identified
goal_identified
=== ep: 14, time 36.60352683067322, eps 0.4474301881084772, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
goal_identified
=== ep: 15, time 36.32325053215027, eps 0.42565753091417224, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
goal_identified
=== ep: 16, time 36.59732913970947, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
goal_identified
goal_identified
=== ep: 17, time 40.120686769485474, eps 0.3852460238219053, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
goal_identified
goal_identified
=== ep: 18, time 42.225048780441284, eps 0.3665061241067986, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
goal_identified
=== ep: 19, time 40.06683683395386, eps 0.3486801800855966, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
=== ep: 20, time 41.65649890899658, eps 0.3317236176131267, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
goal_identified
=== ep: 21, time 43.03251910209656, eps 0.31559403645092865, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 22, time 41.82803130149841, eps 0.3002511042445735, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
goal_identified
goal_identified
=== ep: 23, time 37.20436954498291, eps 0.2856564556717689, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
=== ep: 24, time 39.441084146499634, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
=== ep: 25, time 41.232418060302734, eps 0.2585678123773109, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
goal_identified
=== ep: 26, time 43.04623198509216, eps 0.24600608193757734, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
=== ep: 27, time 45.85350513458252, eps 0.23405699432065646, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
goal_identified
=== ep: 28, time 45.29026746749878, eps 0.22269067058350425, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
=== ep: 29, time 46.62237882614136, eps 0.2118786889963241, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
=== ep: 30, time 43.2033634185791, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
goal_identified
goal_identified
=== ep: 31, time 41.322962522506714, eps 0.191810928470242, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
=== ep: 32, time 44.13374423980713, eps 0.1825049696771952, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 33, time 41.81024503707886, eps 0.17365286785005798, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
=== ep: 34, time 45.23582577705383, eps 0.16523248812340846, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
goal_identified
goal_identified
=== ep: 35, time 44.53415822982788, eps 0.15722277516195018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
goal_identified
=== ep: 36, time 46.50040817260742, eps 0.1496037005112063, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
goal_identified
=== ep: 37, time 42.03086614608765, eps 0.14235621251595124, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 38, time 55.911232709884644, eps 0.13546218868114893, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
=== ep: 39, time 55.31304097175598, eps 0.1289043903562757, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
=== ep: 40, time 56.391154050827026, eps 0.12266641962971482, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
goal_identified
goal_identified
goal_identified
=== ep: 41, time 50.23058533668518, eps 0.116732678325436, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
goal_identified
=== ep: 42, time 51.050886154174805, eps 0.11108832899943073, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
goal_identified
goal_identified
=== ep: 43, time 48.480712890625, eps 0.10571925783837377, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
=== ep: 44, time 52.72298192977905, eps 0.10061203936773815, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
goal_identified
=== ep: 45, time 52.3151752948761, eps 0.09575390288111604, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
=== ep: 46, time 53.119242906570435, eps 0.09113270050680057, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 47, time 61.259434938430786, eps 0.08673687683177911, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
=== ep: 48, time 64.05733108520508, eps 0.08255544000718185, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
goal_identified
goal_identified
=== ep: 49, time 66.56757497787476, eps 0.07857793426293408, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
goal_identified
=== ep: 50, time 58.811216831207275, eps 0.07479441376288502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
=== ep: 51, time 50.72678256034851, eps 0.0711954177350367, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
=== ep: 52, time 56.534740924835205, eps 0.06777194681468615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
goal_identified
=== ep: 53, time 57.84486961364746, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
=== ep: 54, time 52.478644132614136, eps 0.06141775595303503, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
goal_identified
goal_identified
goal_identified
=== ep: 55, time 58.54466986656189, eps 0.05847114722483011, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
=== ep: 56, time 74.72504663467407, eps 0.05566824630007096, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
=== ep: 57, time 68.75263810157776, eps 0.05300204446647978, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
=== ep: 58, time 55.15219068527222, eps 0.050465874830710106, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
goal_identified
goal_identified
goal_identified
=== ep: 59, time 57.76846957206726, eps 0.04805339564764071, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
goal_identified
=== ep: 60, time 53.12557411193848, eps 0.045758574462709686, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
=== ep: 61, time 56.86143612861633, eps 0.043575673027635695, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
=== ep: 62, time 52.158955335617065, eps 0.04149923295180846, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
goal_identified
=== ep: 63, time 65.46782422065735, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
goal_identified
=== ep: 64, time 67.40820264816284, eps 0.03764522137655123, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 72.75266218185425, eps 0.03585801284071809, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
=== ep: 66, time 64.72990250587463, eps 0.034157967493714775, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
=== ep: 67, time 62.16624712944031, eps 0.03254083433665968, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
=== ep: 68, time 58.08331036567688, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
=== ep: 69, time 63.53586196899414, eps 0.02953932710388308, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
=== ep: 70, time 58.11692428588867, eps 0.028147447696664333, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
=== ep: 71, time 63.7184784412384, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 67.3871841430664, eps 0.025564026480116013, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
goal_identified
goal_identified
=== ep: 73, time 74.58171033859253, eps 0.02436602477210106, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
=== ep: 74, time 71.29214358329773, eps 0.02322645029683511, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
goal_identified
=== ep: 75, time 75.86796164512634, eps 0.02214245352455219, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
=== ep: 76, time 79.16650462150574, eps 0.02111132389869288, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
=== ep: 77, time 82.04588270187378, eps 0.020130483058101077, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
=== ep: 78, time 87.8996844291687, eps 0.019197478389778148, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
=== ep: 79, time 94.54624915122986, eps 0.018309976896072843, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 80, time 89.37926578521729, eps 0.017465759360972027, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
=== ep: 81, time 71.591792345047, eps 0.01666271480090467, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
goal_identified
=== ep: 82, time 77.37449264526367, eps 0.015898835186183367, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
goal_identified
=== ep: 83, time 89.4770450592041, eps 0.015172210419884185, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
=== ep: 84, time 87.5274305343628, eps 0.014481023561609456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
=== ep: 85, time 93.28931760787964, eps 0.01382354628419033, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
goal_identified
=== ep: 86, time 104.86793994903564, eps 0.013198134551968641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
goal_identified
=== ep: 87, time 103.5172667503357, eps 0.012603224509851407, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 88, time 78.79448366165161, eps 0.012037328572858524, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
=== ep: 89, time 86.37590670585632, eps 0.011499031706385502, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
=== ep: 90, time 67.31268358230591, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
=== ep: 91, time 84.23096370697021, eps 0.010499916741083536, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
=== ep: 92, time 103.90852236747742, eps 0.010036600334425595, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
=== ep: 93, time 104.04405426979065, eps 0.00959588013555861, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
=== ep: 94, time 96.46346187591553, eps 0.009176654114424539, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
=== ep: 95, time 66.31123661994934, eps 0.00877787398760545, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
=== ep: 96, time 80.73239779472351, eps 0.008398542597069007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
=== ep: 97, time 80.6805329322815, eps 0.008037711416753971, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
goal_identified
=== ep: 98, time 99.38759779930115, eps 0.00769447818076098, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
=== ep: 99, time 93.92614030838013, eps 0.007367984627217855, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
=== ep: 100, time 90.10868787765503, eps 0.007057414352177835, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
goal_identified
goal_identified
=== ep: 101, time 87.64203596115112, eps 0.006761990768184489, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
goal_identified
goal_identified
=== ep: 102, time 84.33477210998535, eps 0.006480975162398559, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
=== ep: 103, time 90.06564688682556, eps 0.006213664849431085, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
=== ep: 104, time 95.1725664138794, eps 0.005959391414263934, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
goal_identified
goal_identified
=== ep: 105, time 98.54831123352051, eps 0.005717519040864065, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
=== ep: 106, time 92.50970077514648, eps 0.005487442922312285, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
=== ep: 107, time 95.6970362663269, eps 0.005268587748470919, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
=== ep: 108, time 100.14202404022217, eps 0.005060406267408787, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
=== ep: 109, time 99.69828009605408, eps 0.004862377916986354, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
goal_identified
=== ep: 110, time 109.86896324157715, eps 0.004674007523179196, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
=== ep: 111, time 98.4766640663147, eps 0.004494824061885041, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
goal_identified
=== ep: 112, time 92.67085433006287, eps 0.0043243794811181555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
goal_identified
=== ep: 113, time 80.81806111335754, eps 0.0041622475806460035, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
=== ep: 114, time 99.66929173469543, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
=== ep: 115, time 104.64487528800964, eps 0.0038613199360621906, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
=== ep: 116, time 96.08737015724182, eps 0.003721771716092858, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
goal_identified
=== ep: 117, time 92.3910870552063, eps 0.0035890293431213305, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
goal_identified
goal_identified
=== ep: 118, time 84.5947539806366, eps 0.0034627608920727634, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
goal_identified
=== ep: 119, time 72.96085619926453, eps 0.00334265062604924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
goal_identified
=== ep: 120, time 90.53721332550049, eps 0.0032283982068230565, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 100.15693974494934, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 92.7771303653717, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
=== ep: 123, time 86.07978129386902, eps 0.0029180001112638996, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
goal_identified
goal_identified
=== ep: 124, time 90.92991614341736, eps 0.002824458142029865, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
=== ep: 125, time 90.76578617095947, eps 0.0027354782684687108, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
goal_identified
=== ep: 126, time 97.42774701118469, eps 0.0026508379945489875, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 127, time 105.37351894378662, eps 0.0025703256754987464, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
=== ep: 128, time 81.21898436546326, eps 0.0024937399885833667, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
=== ep: 129, time 90.23732137680054, eps 0.0024208894296938593, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
goal_identified
=== ep: 130, time 90.50746583938599, eps 0.0023515918344868374, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
=== ep: 131, time 89.92207407951355, eps 0.002285673922878779, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
=== ep: 132, time 89.58835339546204, eps 0.0022229708657555565, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
goal_identified
=== ep: 133, time 69.39133763313293, eps 0.0021633258728137976, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
=== ep: 134, time 93.02244758605957, eps 0.0021065898005034594, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
goal_identified
=== ep: 135, time 81.94114804267883, eps 0.002052620779091266, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
=== ep: 136, time 73.9979841709137, eps 0.0020012838579124784, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 88.49453973770142, eps 0.0019524506679239415, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
goal_identified
goal_identified
=== ep: 138, time 106.4198784828186, eps 0.001905999100714611, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
goal_identified
=== ep: 139, time 100.2393548488617, eps 0.001861813003170924, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 140, time 84.71110582351685, eps 0.0018197818870335101, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
=== ep: 141, time 99.667151927948, eps 0.0017798006526189953, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
goal_identified
=== ep: 142, time 118.5654673576355, eps 0.0017417693260160481, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
goal_identified
=== ep: 143, time 101.53290748596191, eps 0.0017055928090985275, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
=== ep: 144, time 105.49690389633179, eps 0.0016711806417306348, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
=== ep: 145, time 109.74137377738953, eps 0.0016384467755694515, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 94.40311455726624, eps 0.0016073093588992661, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
goal_identified
=== ep: 147, time 117.45434975624084, eps 0.0015776905319596466, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
goal_identified
goal_identified
goal_identified
=== ep: 148, time 109.00394296646118, eps 0.0015495162322554856, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 149, time 101.9749367237091, eps 0.0015227160093621863, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
=== ep: 150, time 102.93200421333313, eps 0.0014972228487629025, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
=== ep: 151, time 110.76700067520142, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
=== ep: 152, time 107.89359211921692, eps 0.001449905838663109, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
=== ep: 153, time 109.22446203231812, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 154, time 108.96268963813782, eps 0.0014070916374152305, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
=== ep: 155, time 98.24899458885193, eps 0.001387237543977543, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 156, time 88.14118695259094, eps 0.0013683517461028282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
=== ep: 157, time 119.08269095420837, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 106.30607843399048, eps 0.0013332984428727204, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
goal_identified
=== ep: 159, time 104.88082551956177, eps 0.001317043286000802, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
=== ep: 160, time 95.44442963600159, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 93.9402539730072, eps 0.0012868726283106018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
goal_identified
goal_identified
=== ep: 162, time 129.23192763328552, eps 0.0012728816851329014, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
=== ep: 163, time 103.52446055412292, eps 0.0012595730883057546, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
=== ep: 164, time 103.58306503295898, eps 0.001246913559404956, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
=== ep: 165, time 109.65221285820007, eps 0.0012348714430141991, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
goal_identified
=== ep: 166, time 109.67715835571289, eps 0.0012234166275700486, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
goal_identified
=== ep: 167, time 123.96130633354187, eps 0.001212520470067348, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
goal_identified
goal_identified
goal_identified
=== ep: 168, time 97.3559000492096, eps 0.0012021557244367845, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
=== ep: 169, time 105.47263956069946, eps 0.0011922964734155277, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
=== ep: 170, time 110.5316891670227, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 171, time 121.40761923789978, eps 0.0011739970445027263, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
goal_identified
=== ep: 172, time 124.13448572158813, eps 0.0011655111085071537, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
goal_identified
goal_identified
goal_identified
=== ep: 173, time 118.333993434906, eps 0.001157439036493735, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
=== ep: 174, time 119.97919774055481, eps 0.0011497606440778825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
goal_identified
goal_identified
=== ep: 175, time 115.03239274024963, eps 0.0011424567312790603, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
goal_identified
=== ep: 176, time 132.877769947052, eps 0.0011355090345108335, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
goal_identified
=== ep: 177, time 123.04286408424377, eps 0.0011289001809123877, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
goal_identified
=== ep: 178, time 117.0295135974884, eps 0.0011226136449073282, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
=== ep: 179, time 112.29149842262268, eps 0.001116633706881133, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
goal_identified
=== ep: 180, time 122.94582486152649, eps 0.001110945413873925, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
=== ep: 181, time 139.2390205860138, eps 0.001105534542190287, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
goal_identified
goal_identified
=== ep: 182, time 107.91340851783752, eps 0.0011003875618326132, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
goal_identified
=== ep: 183, time 130.10995483398438, eps 0.0010954916026690664, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
=== ep: 184, time 122.12564516067505, eps 0.001090834422251547, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
goal_identified
goal_identified
goal_identified
=== ep: 185, time 141.2546408176422, eps 0.0010864043752031938, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
=== ep: 186, time 114.77242803573608, eps 0.0010821903840988777, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
=== ep: 187, time 124.15103793144226, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
=== ep: 188, time 125.8980758190155, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 146.3426730632782, eps 0.0010707419191793434, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
=== ep: 190, time 137.66922283172607, eps 0.0010672917950690429, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
goal_identified
=== ep: 191, time 110.65325570106506, eps 0.0010640099354971456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
goal_identified
=== ep: 192, time 131.26222944259644, eps 0.0010608881341052777, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
=== ep: 193, time 140.33760333061218, eps 0.0010579185847638855, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 194, time 127.26662921905518, eps 0.0010550938620528466, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
=== ep: 195, time 126.45936107635498, eps 0.001052406902694051, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
=== ep: 196, time 125.23855209350586, eps 0.001049850987889527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
goal_identified
goal_identified
=== ep: 197, time 133.54717087745667, eps 0.0010474197265209469, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
=== ep: 198, time 124.28135347366333, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
goal_identified
goal_identified
=== ep: 199, time 135.1891987323761, eps 0.001042907142909185, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
goal_identified
=== ep: 200, time 121.82552552223206, eps 0.001040814536856474, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
goal_identified
goal_identified
=== ep: 201, time 138.6721796989441, eps 0.0010388239884052469, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
=== ep: 202, time 125.67495346069336, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
goal_identified
=== ep: 203, time 112.57800364494324, eps 0.0010351293974264616, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
=== ep: 204, time 123.46048188209534, eps 0.00103341611649703, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
goal_identified
=== ep: 205, time 149.02644658088684, eps 0.0010317863932645186, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
goal_identified
goal_identified
=== ep: 206, time 121.49880766868591, eps 0.0010302361525719613, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
=== ep: 207, time 126.74369215965271, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
=== ep: 208, time 129.51204705238342, eps 0.001027358802224555, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
=== ep: 209, time 141.89390087127686, eps 0.0010260244976950921, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
goal_identified
=== ep: 210, time 128.61439657211304, eps 0.0010247552679654227, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
goal_identified
goal_identified
=== ep: 211, time 116.13443756103516, eps 0.00102354793930011, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
goal_identified
goal_identified
goal_identified
=== ep: 212, time 124.97267293930054, eps 0.0010223994927486214, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
=== ep: 213, time 149.00310897827148, eps 0.001021307056596379, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
goal_identified
goal_identified
=== ep: 214, time 116.61596369743347, eps 0.0010202678991839778, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
=== ep: 215, time 128.04354095458984, eps 0.0010192794220766138, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
=== ep: 216, time 122.99241495132446, eps 0.0010183391535666436, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
=== ep: 217, time 138.82523655891418, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
goal_identified
goal_identified
=== ep: 218, time 120.10094785690308, eps 0.0010165939523622068, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
goal_identified
=== ep: 219, time 132.68439722061157, eps 0.0010157846557556941, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 117.32923412322998, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
=== ep: 221, time 144.9888026714325, eps 0.0010142825471585687, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
=== ep: 222, time 122.47019600868225, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
=== ep: 223, time 114.10266184806824, eps 0.0010129233830939361, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
goal_identified
=== ep: 224, time 129.96444416046143, eps 0.0010122931022630473, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
goal_identified
goal_identified
=== ep: 225, time 129.72432446479797, eps 0.001011693560591007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
=== ep: 226, time 135.45716857910156, eps 0.0010111232589113477, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
=== ep: 227, time 130.23641967773438, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
=== ep: 228, time 124.39004945755005, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 142.21470618247986, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
goal_identified
=== ep: 230, time 126.31843590736389, eps 0.001009106954145169, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
=== ep: 231, time 116.74279189109802, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
=== ep: 232, time 134.690043926239, eps 0.0010082403128748867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
=== ep: 233, time 142.64836525917053, eps 0.0010078384280736842, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
goal_identified
=== ep: 234, time 126.1317138671875, eps 0.001007456143425521, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
=== ep: 235, time 133.78362464904785, eps 0.001007092503019653, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
goal_identified
=== ep: 236, time 121.46144509315491, eps 0.001006746597565654, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
=== ep: 237, time 148.26181936264038, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
=== ep: 238, time 131.32657027244568, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
=== ep: 239, time 128.47578883171082, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
=== ep: 240, time 126.47181844711304, eps 0.001005523646905642, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
=== ep: 241, time 142.08111453056335, eps 0.001005254255467199, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
goal_identified
goal_identified
goal_identified
=== ep: 242, time 120.3500623703003, eps 0.0010049980024042435, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
goal_identified
=== ep: 243, time 130.59020972251892, eps 0.0010047542469506416, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
=== ep: 244, time 127.06381440162659, eps 0.0010045223795907931, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
=== ep: 245, time 150.58327555656433, eps 0.001004301820535524, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
=== ep: 246, time 130.9654541015625, eps 0.0010040920182723119, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
goal_identified
=== ep: 247, time 125.36090207099915, eps 0.0010038924481862177, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
goal_identified
goal_identified
=== ep: 248, time 138.02851843833923, eps 0.0010037026112480747, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
goal_identified
goal_identified
=== ep: 249, time 152.6325466632843, eps 0.0010035220327666559, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
goal_identified
goal_identified
=== ep: 250, time 131.5797348022461, eps 0.0010033502612016988, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
=== ep: 251, time 137.66342902183533, eps 0.001003186867034819, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
goal_identified
=== ep: 252, time 157.5296754837036, eps 0.001003031441695491, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
=== ep: 253, time 135.90002369880676, eps 0.0010028835965394094, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
=== ep: 254, time 139.45701146125793, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
goal_identified
=== ep: 255, time 150.06843447685242, eps 0.0010026091860473767, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
=== ep: 256, time 168.58732771873474, eps 0.0010024819345422614, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
=== ep: 257, time 135.61370658874512, eps 0.0010023608891662839, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
goal_identified
=== ep: 258, time 162.11345434188843, eps 0.001002245747242954, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
=== ep: 259, time 133.38039374351501, eps 0.0010021362208574892, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
=== ep: 260, time 142.72023057937622, eps 0.001002032036136876, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
goal_identified
=== ep: 261, time 149.87589406967163, eps 0.0010019329325650452, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 131.84406065940857, eps 0.0010018386623314465, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
goal_identified
=== ep: 263, time 139.38398504257202, eps 0.0010017489897113931, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
=== ep: 264, time 150.63150644302368, eps 0.0010016636904766263, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
=== ep: 265, time 139.39005303382874, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
=== ep: 266, time 140.44153475761414, eps 0.0010015053693952815, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
=== ep: 267, time 131.78829789161682, eps 0.0010014319516635345, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
goal_identified
=== ep: 268, time 144.85305500030518, eps 0.0010013621145568167, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
=== ep: 269, time 136.09219074249268, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 132.86219263076782, eps 0.0010012324922186594, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
goal_identified
goal_identified
goal_identified
=== ep: 271, time 137.91279292106628, eps 0.001001172382863857, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
=== ep: 272, time 134.16932559013367, eps 0.0010011152050768812, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
goal_identified
=== ep: 273, time 140.77211236953735, eps 0.0010010608158834819, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
goal_identified
goal_identified
=== ep: 274, time 127.17555212974548, eps 0.0010010090792823456, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
goal_identified
=== ep: 275, time 155.25517010688782, eps 0.0010009598659050213, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
goal_identified
goal_identified
=== ep: 276, time 130.29998445510864, eps 0.0010009130526924313, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
=== ep: 277, time 129.83133172988892, eps 0.0010008685225871602, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
goal_identified
=== ep: 278, time 143.34309792518616, eps 0.0010008261642407504, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
goal_identified
=== ep: 279, time 142.29886174201965, eps 0.001000785871735272, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
goal_identified
goal_identified
goal_identified
=== ep: 280, time 133.97429084777832, eps 0.0010007475443184742, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 281, time 140.10110068321228, eps 0.001000711086151851, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
goal_identified
goal_identified
goal_identified
=== ep: 282, time 153.64527893066406, eps 0.0010006764060709957, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
goal_identified
=== ep: 283, time 130.62498998641968, eps 0.001000643417357642, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
goal_identified
=== ep: 284, time 140.9535117149353, eps 0.0010006120375228235, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
goal_identified
=== ep: 285, time 146.77876567840576, eps 0.0010005821881006083, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 136.3364691734314, eps 0.0010005537944518927, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 132.10621690750122, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 129.89228892326355, eps 0.0010005010939419733, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
=== ep: 289, time 129.79140782356262, eps 0.001000476655302044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
=== ep: 290, time 139.47060585021973, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 123.65818190574646, eps 0.0010004312955527947, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
=== ep: 292, time 134.04626202583313, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
=== ep: 293, time 129.14795446395874, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
goal_identified
goal_identified
goal_identified
=== ep: 294, time 127.41087365150452, eps 0.0010003712195224871, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
=== ep: 295, time 134.61555671691895, eps 0.0010003531149327387, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
=== ep: 296, time 124.09780597686768, eps 0.0010003358933142518, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
goal_identified
=== ep: 297, time 134.96087980270386, eps 0.0010003195116040093, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
goal_identified
goal_identified
goal_identified
=== ep: 298, time 132.51270031929016, eps 0.0010003039288392032, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
=== ep: 299, time 137.1514811515808, eps 0.0010002891060548044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
=== ep: 300, time 130.5825264453888, eps 0.0010002750061861312, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
goal_identified
goal_identified
=== ep: 301, time 136.62724208831787, eps 0.0010002615939761676, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
goal_identified
goal_identified
=== ep: 302, time 134.9258542060852, eps 0.001000248835887403, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
=== ep: 303, time 131.11484456062317, eps 0.0010002367000179694, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
goal_identified
=== ep: 304, time 130.34991550445557, eps 0.0010002251560218723, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
goal_identified
=== ep: 305, time 148.31389164924622, eps 0.0010002141750331084, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
goal_identified
goal_identified
=== ep: 306, time 134.52426099777222, eps 0.0010002037295934862, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
goal_identified
goal_identified
=== ep: 307, time 139.3844509124756, eps 0.0010001937935839656, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 308, time 131.30646896362305, eps 0.0010001843421593476, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
goal_identified
goal_identified
=== ep: 309, time 137.0588035583496, eps 0.0010001753516861473, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
=== ep: 310, time 135.71865558624268, eps 0.0010001667996834991, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
=== ep: 311, time 131.80217266082764, eps 0.001000158664766942, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
goal_identified
goal_identified
=== ep: 312, time 130.09557271003723, eps 0.0010001509265949466, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
goal_identified
=== ep: 313, time 135.2450647354126, eps 0.001000143565818053, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
=== ep: 314, time 137.8083839416504, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
=== ep: 315, time 129.8588523864746, eps 0.0010001299037241253, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
goal_identified
goal_identified
=== ep: 316, time 127.18934798240662, eps 0.0010001235682447402, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
goal_identified
=== ep: 317, time 128.2144057750702, eps 0.0010001175417503308, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 141.36590385437012, eps 0.0010001118091715218, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 123.85257339477539, eps 0.0010001063561738807, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
goal_identified
goal_identified
=== ep: 320, time 139.4187092781067, eps 0.0010001011691220727, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
goal_identified
goal_identified
=== ep: 321, time 131.48987221717834, eps 0.0010000962350457665, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
=== ep: 322, time 135.1235203742981, eps 0.0010000915416072012, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
=== ep: 323, time 122.63387131690979, eps 0.0010000870770703358, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
=== ep: 324, time 127.00044250488281, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
=== ep: 325, time 132.30877447128296, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 126.20889735221863, eps 0.0010000749479290019, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
=== ep: 327, time 134.63843202590942, eps 0.001000071292675372, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
goal_identified
=== ep: 328, time 135.0347330570221, eps 0.001000067815690565, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
=== ep: 329, time 122.32591843605042, eps 0.0010000645082803084, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
=== ep: 330, time 124.82744288444519, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
goal_identified
=== ep: 331, time 139.79284834861755, eps 0.0010000583695057963, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 332, time 125.13879823684692, eps 0.0010000555227914069, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
goal_identified
=== ep: 333, time 133.1154408454895, eps 0.0010000528149129166, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 122.25239419937134, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
=== ep: 335, time 137.40495371818542, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
goal_identified
=== ep: 336, time 133.75288248062134, eps 0.0010000454582168217, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
goal_identified
goal_identified
goal_identified
=== ep: 337, time 138.79170751571655, eps 0.001000043241193426, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
=== ep: 338, time 132.24321913719177, eps 0.0010000411322955373, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 339, time 124.69892978668213, eps 0.0010000391262498123, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 127.11090326309204, eps 0.001000037218040092, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
=== ep: 341, time 139.31723737716675, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
goal_identified
goal_identified
=== ep: 342, time 126.8630621433258, eps 0.0010000336762753012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
=== ep: 343, time 128.7949607372284, eps 0.001000032033863974, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
goal_identified
=== ep: 344, time 123.11325907707214, eps 0.0010000304715539925, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
goal_identified
=== ep: 345, time 116.84844279289246, eps 0.001000028985438768, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
=== ep: 346, time 128.23873734474182, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 129.67291498184204, eps 0.0010000262271095755, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
=== ep: 348, time 131.0152862071991, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
goal_identified
=== ep: 349, time 136.56620335578918, eps 0.0010000237312701107, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
goal_identified
=== ep: 350, time 126.95688819885254, eps 0.00100002257388241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
goal_identified
=== ep: 351, time 134.60716676712036, eps 0.0010000214729411737, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
goal_identified
=== ep: 352, time 118.88891959190369, eps 0.0010000204256934752, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 132.51652669906616, eps 0.0010000194295206493, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
goal_identified
=== ep: 354, time 125.50209021568298, eps 0.0010000184819317455, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
goal_identified
goal_identified
=== ep: 355, time 124.0405101776123, eps 0.001000017580557298, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
goal_identified
goal_identified
goal_identified
=== ep: 356, time 130.23305320739746, eps 0.001000016723143401, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
goal_identified
goal_identified
=== ep: 357, time 132.28470587730408, eps 0.0010000159075460732, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
=== ep: 358, time 131.98309874534607, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
goal_identified
=== ep: 359, time 130.63931727409363, eps 0.0010000143937429161, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 132.94308614730835, eps 0.0010000136917517905, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
=== ep: 361, time 141.5268681049347, eps 0.001000013023997176, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
goal_identified
goal_identified
goal_identified
=== ep: 362, time 120.25899314880371, eps 0.0010000123888093385, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
=== ep: 363, time 131.25967955589294, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
=== ep: 364, time 135.84206581115723, eps 0.0010000112098582543, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
=== ep: 365, time 129.63739943504333, eps 0.001000010663147016, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
=== ep: 366, time 123.5841851234436, eps 0.0010000101430991996, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
goal_identified
=== ep: 367, time 135.61937737464905, eps 0.0010000096484144142, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
=== ep: 368, time 135.3320460319519, eps 0.0010000091778556905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 131.55097794532776, eps 0.0010000087302463867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
goal_identified
=== ep: 370, time 124.9611086845398, eps 0.001000008304467246, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
goal_identified
goal_identified
goal_identified
=== ep: 371, time 138.485027551651, eps 0.0010000078994535993, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
goal_identified
=== ep: 372, time 136.3692591190338, eps 0.0010000075141927012, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
=== ep: 373, time 139.52513718605042, eps 0.0010000071477211988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
goal_identified
goal_identified
goal_identified
=== ep: 374, time 134.67636251449585, eps 0.0010000067991227223, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
goal_identified
goal_identified
=== ep: 375, time 137.5363311767578, eps 0.0010000064675255943, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
goal_identified
=== ep: 376, time 136.32897472381592, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
goal_identified
=== ep: 377, time 120.11274242401123, eps 0.0010000058520591598, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
goal_identified
goal_identified
=== ep: 378, time 139.94479846954346, eps 0.0010000055666508666, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
goal_identified
=== ep: 379, time 141.4453980922699, eps 0.0010000052951621003, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
goal_identified
=== ep: 380, time 142.57492351531982, eps 0.0010000050369139975, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
=== ep: 381, time 137.07597017288208, eps 0.001000004791260803, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
=== ep: 382, time 141.22937202453613, eps 0.0010000045575882562, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 102.39374566078186, eps 0.001000004335312054, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
=== ep: 384, time 96.0513916015625, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
goal_identified
=== ep: 385, time 100.223956823349, eps 0.0010000039227525655, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
goal_identified
=== ep: 386, time 100.7293291091919, eps 0.0010000037314376652, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 101.60385489463806, eps 0.001000003549453303, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
=== ep: 388, time 102.37637495994568, eps 0.0010000033763444226, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 103.86835885047913, eps 0.001000003211678162, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
goal_identified
goal_identified
=== ep: 390, time 105.38546562194824, eps 0.0010000030550427698, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 105.94092559814453, eps 0.0010000029060465757, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
=== ep: 392, time 103.61445879936218, eps 0.0010000027643170119, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
goal_identified
goal_identified
=== ep: 393, time 98.39315986633301, eps 0.0010000026294996803, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
=== ep: 394, time 103.83077478408813, eps 0.0010000025012574677, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
=== ep: 395, time 103.48354268074036, eps 0.0010000023792697014, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
goal_identified
=== ep: 396, time 101.89011216163635, eps 0.0010000022632313489, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
=== ep: 397, time 107.72664904594421, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
=== ep: 398, time 97.54763460159302, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
goal_identified
=== ep: 399, time 99.47447514533997, eps 0.0010000019479812744, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 97.61186814308167, eps 0.0010000018529771066, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
=== ep: 401, time 99.15277123451233, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
=== ep: 402, time 107.69264578819275, eps 0.0010000016766430208, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
goal_identified
=== ep: 403, time 98.69695115089417, eps 0.0010000015948721758, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 103.94869685173035, eps 0.001000001517089342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
goal_identified
goal_identified
=== ep: 405, time 103.38924264907837, eps 0.0010000014431000217, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
=== ep: 406, time 101.90837740898132, eps 0.001000001372719203, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
=== ep: 407, time 100.34490728378296, eps 0.0010000013057708975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
goal_identified
goal_identified
=== ep: 408, time 108.99670386314392, eps 0.0010000012420876994, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
goal_identified
=== ep: 409, time 100.91590452194214, eps 0.0010000011815103674, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
=== ep: 410, time 97.71007227897644, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
goal_identified
=== ep: 411, time 96.61103868484497, eps 0.0010000010690747903, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
=== ep: 412, time 102.63133907318115, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
=== ep: 413, time 107.11783385276794, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
=== ep: 414, time 102.11005806922913, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
=== ep: 415, time 104.26420640945435, eps 0.0010000008752844081, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
=== ep: 416, time 108.00618124008179, eps 0.0010000008325962838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
goal_identified
goal_identified
goal_identified
=== ep: 417, time 101.23125314712524, eps 0.001000000791990084, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
goal_identified
=== ep: 418, time 101.4850640296936, eps 0.0010000007533642718, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
=== ep: 419, time 100.92325568199158, eps 0.0010000007166222626, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
=== ep: 420, time 98.07519626617432, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
=== ep: 421, time 98.29871845245361, eps 0.001000000648426638, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
goal_identified
goal_identified
=== ep: 422, time 92.98344278335571, eps 0.0010000006168024976, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
=== ep: 423, time 94.74510669708252, eps 0.0010000005867206849, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
goal_identified
=== ep: 424, time 100.39324688911438, eps 0.0010000005581059794, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
goal_identified
=== ep: 425, time 95.9443724155426, eps 0.0010000005308868295, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
goal_identified
=== ep: 426, time 106.94477677345276, eps 0.0010000005049951733, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
goal_identified
=== ep: 427, time 96.06635165214539, eps 0.001000000480366268, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
goal_identified
goal_identified
=== ep: 428, time 99.71487140655518, eps 0.0010000004569385287, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 92.62346076965332, eps 0.0010000004346533736, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
goal_identified
goal_identified
=== ep: 430, time 92.95937418937683, eps 0.0010000004134550786, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
goal_identified
=== ep: 431, time 97.74294376373291, eps 0.0010000003932906364, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
goal_identified
=== ep: 432, time 99.22486209869385, eps 0.0010000003741096257, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
=== ep: 433, time 104.96819019317627, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
goal_identified
goal_identified
=== ep: 434, time 98.89448428153992, eps 0.0010000003385083878, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
goal_identified
goal_identified
=== ep: 435, time 101.51081156730652, eps 0.001000000321999139, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
goal_identified
=== ep: 436, time 99.0161988735199, eps 0.0010000003062950555, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
goal_identified
=== ep: 437, time 97.37687397003174, eps 0.0010000002913568694, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
=== ep: 438, time 92.46247363090515, eps 0.0010000002771472273, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
=== ep: 439, time 101.70335721969604, eps 0.0010000002636305976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
goal_identified
=== ep: 440, time 98.93331480026245, eps 0.0010000002507731815, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
=== ep: 441, time 101.18118238449097, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
goal_identified
=== ep: 442, time 100.13485074043274, eps 0.0010000002269089582, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
=== ep: 443, time 100.81261873245239, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
goal_identified
goal_identified
=== ep: 444, time 101.54945039749146, eps 0.0010000002053157158, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
=== ep: 445, time 97.08510661125183, eps 0.0010000001953023503, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
goal_identified
=== ep: 446, time 99.69878625869751, eps 0.001000000185777342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
=== ep: 447, time 121.39230370521545, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
=== ep: 448, time 92.28576135635376, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
goal_identified
=== ep: 449, time 92.13185667991638, eps 0.0010000001599000403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
goal_identified
goal_identified
=== ep: 450, time 104.29726886749268, eps 0.0010000001521016232, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
goal_identified
=== ep: 451, time 98.50463676452637, eps 0.0010000001446835395, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
goal_identified
=== ep: 452, time 105.65566110610962, eps 0.0010000001376272401, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
goal_identified
=== ep: 453, time 101.28358960151672, eps 0.0010000001309150804, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
goal_identified
goal_identified
=== ep: 454, time 98.39138841629028, eps 0.0010000001245302765, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
=== ep: 455, time 98.22164058685303, eps 0.0010000001184568633, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
goal_identified
goal_identified
=== ep: 456, time 97.90000653266907, eps 0.0010000001126796538, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
goal_identified
=== ep: 457, time 95.55106043815613, eps 0.0010000001071842023, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
=== ep: 458, time 99.43988108634949, eps 0.001000000101956767, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
=== ep: 459, time 96.89805722236633, eps 0.001000000096984277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
=== ep: 460, time 102.64555239677429, eps 0.001000000092254298, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
=== ep: 461, time 99.51931500434875, eps 0.0010000000877550027, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
=== ep: 462, time 109.8663341999054, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
=== ep: 463, time 90.4525728225708, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
=== ep: 464, time 103.941171169281, eps 0.0010000000755314307, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
goal_identified
=== ep: 465, time 95.95258140563965, eps 0.0010000000718477194, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
=== ep: 466, time 94.97197794914246, eps 0.0010000000683436647, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
goal_identified
=== ep: 467, time 101.99516367912292, eps 0.001000000065010505, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
goal_identified
=== ep: 468, time 99.2471113204956, eps 0.0010000000618399052, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
goal_identified
=== ep: 469, time 94.98727488517761, eps 0.0010000000588239375, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
=== ep: 470, time 105.0229377746582, eps 0.0010000000559550603, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
=== ep: 471, time 98.23301434516907, eps 0.0010000000532260998, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
=== ep: 472, time 103.96817278862, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
goal_identified
goal_identified
=== ep: 473, time 90.12963962554932, eps 0.0010000000481609666, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
=== ep: 474, time 98.1273741722107, eps 0.0010000000458121286, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
=== ep: 475, time 120.01773190498352, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
goal_identified
=== ep: 476, time 104.01720237731934, eps 0.001000000041452528, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
=== ep: 477, time 100.44860696792603, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
goal_identified
=== ep: 478, time 104.75014543533325, eps 0.0010000000375077985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
goal_identified
goal_identified
goal_identified
=== ep: 479, time 101.59884142875671, eps 0.0010000000356785216, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
=== ep: 480, time 98.43446636199951, eps 0.0010000000339384595, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 115.36925601959229, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
goal_identified
goal_identified
=== ep: 482, time 100.1002049446106, eps 0.0010000000307087882, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 99.15000176429749, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
goal_identified
=== ep: 484, time 103.91999650001526, eps 0.0010000000277864607, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
=== ep: 485, time 102.1451051235199, eps 0.0010000000264312988, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
goal_identified
=== ep: 486, time 97.64815354347229, eps 0.0010000000251422292, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
=== ep: 487, time 96.29539442062378, eps 0.0010000000239160282, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
=== ep: 488, time 94.29216003417969, eps 0.00100000002274963, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
goal_identified
goal_identified
=== ep: 489, time 93.16635632514954, eps 0.0010000000216401172, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
goal_identified
goal_identified
=== ep: 490, time 101.29387092590332, eps 0.0010000000205847162, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
=== ep: 491, time 100.58857369422913, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
=== ep: 492, time 103.11796736717224, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
=== ep: 493, time 97.80359983444214, eps 0.0010000000177174295, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
=== ep: 494, time 101.00980234146118, eps 0.0010000000168533404, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
goal_identified
=== ep: 495, time 102.29284906387329, eps 0.0010000000160313932, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
goal_identified
goal_identified
=== ep: 496, time 95.6087007522583, eps 0.001000000015249533, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
goal_identified
goal_identified
goal_identified
=== ep: 497, time 94.38258171081543, eps 0.0010000000145058043, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
=== ep: 498, time 94.47778129577637, eps 0.001000000013798348, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
goal_identified
=== ep: 499, time 95.86498999595642, eps 0.0010000000131253947, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
=== ep: 500, time 84.47376942634583, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
=== ep: 501, time 95.37813329696655, eps 0.0010000000118763482, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
goal_identified
goal_identified
=== ep: 502, time 94.11450004577637, eps 0.0010000000112971319, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
goal_identified
goal_identified
=== ep: 503, time 97.78389930725098, eps 0.0010000000107461642, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
=== ep: 504, time 93.08970832824707, eps 0.0010000000102220676, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
=== ep: 505, time 101.16057062149048, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
=== ep: 506, time 101.4352593421936, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
=== ep: 507, time 101.5519528388977, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
=== ep: 508, time 93.44841837882996, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
=== ep: 509, time 101.74830651283264, eps 0.0010000000079609542, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
goal_identified
=== ep: 510, time 97.06811428070068, eps 0.001000000007572694, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
goal_identified
goal_identified
=== ep: 511, time 92.84311318397522, eps 0.0010000000072033692, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
goal_identified
=== ep: 512, time 95.37762975692749, eps 0.001000000006852057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
=== ep: 513, time 95.1114661693573, eps 0.001000000006517878, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
goal_identified
=== ep: 514, time 98.51736402511597, eps 0.0010000000061999974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
=== ep: 515, time 126.43146061897278, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
goal_identified
=== ep: 516, time 91.0785722732544, eps 0.0010000000056099897, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
=== ep: 517, time 97.17588782310486, eps 0.0010000000053363872, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
=== ep: 518, time 92.80732297897339, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
=== ep: 519, time 94.70313382148743, eps 0.001000000004828563, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
=== ep: 520, time 95.17872953414917, eps 0.001000000004593071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
=== ep: 521, time 95.90161538124084, eps 0.0010000000043690644, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
goal_identified
goal_identified
goal_identified
=== ep: 522, time 97.93798208236694, eps 0.0010000000041559827, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
=== ep: 523, time 92.27676248550415, eps 0.0010000000039532928, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
=== ep: 524, time 95.56245636940002, eps 0.0010000000037604885, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 525, time 91.3403992652893, eps 0.0010000000035770874, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
=== ep: 526, time 105.75272345542908, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
=== ep: 527, time 93.2784571647644, eps 0.0010000000032366824, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 94.24936318397522, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
goal_identified
goal_identified
goal_identified
=== ep: 529, time 95.4167127609253, eps 0.0010000000029286714, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
goal_identified
=== ep: 530, time 99.65024304389954, eps 0.0010000000027858384, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
goal_identified
goal_identified
=== ep: 531, time 100.29291653633118, eps 0.0010000000026499714, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
goal_identified
=== ep: 532, time 96.03167080879211, eps 0.0010000000025207308, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
goal_identified
goal_identified
=== ep: 533, time 93.60220050811768, eps 0.0010000000023977934, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
=== ep: 534, time 111.21221733093262, eps 0.0010000000022808515, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
=== ep: 535, time 87.53232502937317, eps 0.0010000000021696133, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
=== ep: 536, time 84.2101399898529, eps 0.0010000000020637999, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
goal_identified
goal_identified
=== ep: 537, time 89.47662830352783, eps 0.0010000000019631471, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
goal_identified
goal_identified
goal_identified
=== ep: 538, time 88.58709979057312, eps 0.0010000000018674034, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
=== ep: 539, time 92.51668977737427, eps 0.001000000001776329, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
=== ep: 540, time 97.04557275772095, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
=== ep: 541, time 88.66152095794678, eps 0.001000000001607289, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
goal_identified
=== ep: 542, time 93.70867824554443, eps 0.0010000000015289005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
=== ep: 543, time 93.31483507156372, eps 0.0010000000014543352, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
=== ep: 544, time 90.16765666007996, eps 0.0010000000013834064, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
=== ep: 545, time 90.57156991958618, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 95.28212237358093, eps 0.0010000000012517578, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
goal_identified
=== ep: 547, time 96.23175954818726, eps 0.001000000001190709, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
goal_identified
=== ep: 548, time 98.74110555648804, eps 0.0010000000011326374, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
=== ep: 549, time 94.21507358551025, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
=== ep: 550, time 95.82626438140869, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
goal_identified
goal_identified
=== ep: 551, time 93.01445937156677, eps 0.00100000000097487, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 95.31967401504517, eps 0.001000000000927325, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
=== ep: 553, time 94.2121422290802, eps 0.0010000000008820989, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
goal_identified
goal_identified
=== ep: 554, time 93.10860252380371, eps 0.0010000000008390784, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
goal_identified
goal_identified
=== ep: 555, time 91.22345757484436, eps 0.001000000000798156, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
goal_identified
=== ep: 556, time 94.90217733383179, eps 0.0010000000007592295, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
=== ep: 557, time 97.51042795181274, eps 0.0010000000007222014, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
goal_identified
goal_identified
=== ep: 558, time 94.74569344520569, eps 0.0010000000006869794, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
=== ep: 559, time 97.45146679878235, eps 0.001000000000653475, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
goal_identified
goal_identified
=== ep: 560, time 95.95148921012878, eps 0.0010000000006216046, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
goal_identified
goal_identified
=== ep: 561, time 99.41670179367065, eps 0.0010000000005912885, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
goal_identified
goal_identified
=== ep: 562, time 100.01171827316284, eps 0.0010000000005624511, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
goal_identified
=== ep: 563, time 92.61608052253723, eps 0.00100000000053502, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
goal_identified
=== ep: 564, time 90.42755913734436, eps 0.001000000000508927, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
goal_identified
=== ep: 565, time 92.37698912620544, eps 0.001000000000484106, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
goal_identified
=== ep: 566, time 93.14649939537048, eps 0.001000000000460496, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
goal_identified
goal_identified
=== ep: 567, time 90.77330541610718, eps 0.0010000000004380374, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
=== ep: 568, time 94.4793610572815, eps 0.001000000000416674, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
=== ep: 569, time 95.10693860054016, eps 0.0010000000003963527, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 92.95452737808228, eps 0.0010000000003770222, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
goal_identified
=== ep: 571, time 94.53763103485107, eps 0.0010000000003586346, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
goal_identified
=== ep: 572, time 94.89389538764954, eps 0.0010000000003411438, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
=== ep: 573, time 96.65585732460022, eps 0.001000000000324506, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
goal_identified
=== ep: 574, time 94.43033075332642, eps 0.0010000000003086798, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
=== ep: 575, time 94.0663115978241, eps 0.0010000000002936252, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
=== ep: 576, time 95.56272339820862, eps 0.001000000000279305, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
goal_identified
=== ep: 577, time 89.54800724983215, eps 0.0010000000002656831, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
goal_identified
goal_identified
=== ep: 578, time 93.32003855705261, eps 0.0010000000002527256, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
goal_identified
goal_identified
=== ep: 579, time 92.40378165245056, eps 0.0010000000002404, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
goal_identified
=== ep: 580, time 94.07164716720581, eps 0.0010000000002286756, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
goal_identified
=== ep: 581, time 94.01203274726868, eps 0.0010000000002175229, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
goal_identified
=== ep: 582, time 96.85929751396179, eps 0.0010000000002069142, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
goal_identified
goal_identified
=== ep: 583, time 92.22042441368103, eps 0.0010000000001968228, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
=== ep: 584, time 103.06892228126526, eps 0.0010000000001872237, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
goal_identified
goal_identified
=== ep: 585, time 94.31455588340759, eps 0.0010000000001780928, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
=== ep: 586, time 95.40035676956177, eps 0.001000000000169407, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
goal_identified
goal_identified
=== ep: 587, time 98.4898476600647, eps 0.001000000000161145, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
=== ep: 588, time 92.46204566955566, eps 0.0010000000001532858, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
=== ep: 589, time 103.56237554550171, eps 0.00100000000014581, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
=== ep: 590, time 101.44377207756042, eps 0.0010000000001386988, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
goal_identified
=== ep: 591, time 95.26849842071533, eps 0.0010000000001319344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 89.7317271232605, eps 0.0010000000001255, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
=== ep: 593, time 94.2987732887268, eps 0.0010000000001193791, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
goal_identified
=== ep: 594, time 97.08131575584412, eps 0.001000000000113557, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
goal_identified
=== ep: 595, time 95.19112491607666, eps 0.0010000000001080186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
goal_identified
=== ep: 596, time 92.10022950172424, eps 0.0010000000001027505, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
goal_identified
=== ep: 597, time 90.81553864479065, eps 0.0010000000000977393, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
goal_identified
goal_identified
=== ep: 598, time 88.59922242164612, eps 0.0010000000000929725, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
=== ep: 599, time 88.58223366737366, eps 0.0010000000000884382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
goal_identified
goal_identified
=== ep: 600, time 94.59982013702393, eps 0.001000000000084125, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
=== ep: 601, time 88.837069272995, eps 0.0010000000000800222, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 94.18979811668396, eps 0.0010000000000761195, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
goal_identified
goal_identified
=== ep: 603, time 97.61335635185242, eps 0.0010000000000724072, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
goal_identified
goal_identified
=== ep: 604, time 91.67765188217163, eps 0.0010000000000688757, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
goal_identified
=== ep: 605, time 84.88158440589905, eps 0.0010000000000655166, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
=== ep: 606, time 88.73050832748413, eps 0.0010000000000623215, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 85.79536533355713, eps 0.001000000000059282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
goal_identified
=== ep: 608, time 86.0581419467926, eps 0.0010000000000563907, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
=== ep: 609, time 87.79792761802673, eps 0.0010000000000536405, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
goal_identified
=== ep: 610, time 89.66752099990845, eps 0.0010000000000510245, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 88.70014882087708, eps 0.0010000000000485358, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
=== ep: 612, time 87.35138463973999, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 91.73243165016174, eps 0.0010000000000439171, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
=== ep: 614, time 99.49537706375122, eps 0.0010000000000417752, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
=== ep: 615, time 92.31745100021362, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 615
=== ep: 616, time 95.72743225097656, eps 0.0010000000000377999, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 93.03117728233337, eps 0.0010000000000359563, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
goal_identified
=== ep: 618, time 86.0990686416626, eps 0.0010000000000342027, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
goal_identified
=== ep: 619, time 96.22304487228394, eps 0.0010000000000325345, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
=== ep: 620, time 90.09264206886292, eps 0.001000000000030948, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
=== ep: 621, time 95.57852458953857, eps 0.0010000000000294385, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
=== ep: 622, time 88.97583985328674, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 94.48527121543884, eps 0.0010000000000266371, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
=== ep: 624, time 92.45439076423645, eps 0.001000000000025338, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
=== ep: 625, time 96.05010747909546, eps 0.0010000000000241023, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
=== ep: 626, time 95.47307181358337, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
=== ep: 627, time 92.81646823883057, eps 0.0010000000000218085, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
goal_identified
=== ep: 628, time 92.79918646812439, eps 0.001000000000020745, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 97.65870785713196, eps 0.0010000000000197332, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
goal_identified
=== ep: 630, time 91.55568242073059, eps 0.0010000000000187708, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
goal_identified
goal_identified
=== ep: 631, time 92.0718138217926, eps 0.0010000000000178553, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
goal_identified
=== ep: 632, time 91.72498154640198, eps 0.0010000000000169845, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
=== ep: 633, time 89.10240316390991, eps 0.0010000000000161562, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
=== ep: 634, time 96.68774485588074, eps 0.0010000000000153684, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
goal_identified
=== ep: 635, time 73.4014778137207, eps 0.0010000000000146188, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
=== ep: 636, time 98.45311093330383, eps 0.0010000000000139058, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
goal_identified
=== ep: 637, time 85.85253620147705, eps 0.0010000000000132275, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
=== ep: 638, time 94.00735068321228, eps 0.0010000000000125824, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
=== ep: 639, time 91.25449848175049, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
=== ep: 640, time 93.67032313346863, eps 0.001000000000011385, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
goal_identified
goal_identified
=== ep: 641, time 94.32817363739014, eps 0.00100000000001083, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
goal_identified
goal_identified
=== ep: 642, time 95.29335713386536, eps 0.0010000000000103017, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 642
=== ep: 643, time 96.05405139923096, eps 0.0010000000000097993, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
goal_identified
goal_identified
=== ep: 644, time 95.2056577205658, eps 0.0010000000000093213, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
goal_identified
=== ep: 645, time 96.13728094100952, eps 0.0010000000000088666, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
=== ep: 646, time 95.16701030731201, eps 0.0010000000000084342, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
=== ep: 647, time 91.18363952636719, eps 0.001000000000008023, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
goal_identified
=== ep: 648, time 98.34098815917969, eps 0.0010000000000076317, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
goal_identified
goal_identified
=== ep: 649, time 94.87020373344421, eps 0.0010000000000072594, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
goal_identified
=== ep: 650, time 100.02102947235107, eps 0.0010000000000069055, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
goal_identified
goal_identified
=== ep: 651, time 102.51392602920532, eps 0.0010000000000065686, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
goal_identified
goal_identified
=== ep: 652, time 106.65907502174377, eps 0.0010000000000062483, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
goal_identified
=== ep: 653, time 101.86756181716919, eps 0.0010000000000059436, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 96.20834374427795, eps 0.0010000000000056537, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
goal_identified
=== ep: 655, time 96.91685914993286, eps 0.0010000000000053779, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
goal_identified
goal_identified
goal_identified
=== ep: 656, time 98.68358302116394, eps 0.0010000000000051157, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
goal_identified
goal_identified
=== ep: 657, time 103.33598518371582, eps 0.0010000000000048661, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
goal_identified
=== ep: 658, time 96.47028994560242, eps 0.001000000000004629, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
goal_identified
=== ep: 659, time 100.0097291469574, eps 0.0010000000000044032, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
=== ep: 660, time 105.40481352806091, eps 0.0010000000000041883, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
goal_identified
goal_identified
goal_identified
=== ep: 661, time 104.41188216209412, eps 0.001000000000003984, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
=== ep: 662, time 107.42823719978333, eps 0.0010000000000037897, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
goal_identified
=== ep: 663, time 100.28311705589294, eps 0.001000000000003605, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
=== ep: 664, time 105.23705387115479, eps 0.0010000000000034291, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
goal_identified
=== ep: 665, time 102.95118832588196, eps 0.001000000000003262, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
goal_identified
goal_identified
=== ep: 666, time 94.7436933517456, eps 0.0010000000000031028, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 102.35068225860596, eps 0.0010000000000029514, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
goal_identified
=== ep: 668, time 105.83751177787781, eps 0.0010000000000028075, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 669, time 100.99909591674805, eps 0.0010000000000026706, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
=== ep: 670, time 100.08017444610596, eps 0.0010000000000025403, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
=== ep: 671, time 102.25088000297546, eps 0.0010000000000024165, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
=== ep: 672, time 101.66694188117981, eps 0.0010000000000022985, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 673, time 104.3223807811737, eps 0.0010000000000021864, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 674, time 103.1753466129303, eps 0.00100000000000208, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
goal_identified
goal_identified
=== ep: 675, time 100.61141657829285, eps 0.0010000000000019785, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
goal_identified
goal_identified
=== ep: 676, time 103.30396938323975, eps 0.001000000000001882, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
goal_identified
goal_identified
=== ep: 677, time 100.38545727729797, eps 0.0010000000000017903, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
=== ep: 678, time 94.94693946838379, eps 0.0010000000000017029, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
goal_identified
=== ep: 679, time 99.45096468925476, eps 0.0010000000000016198, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
goal_identified
goal_identified
goal_identified
=== ep: 680, time 103.62755608558655, eps 0.0010000000000015409, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
goal_identified
=== ep: 681, time 98.22259497642517, eps 0.0010000000000014656, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
=== ep: 682, time 99.19889068603516, eps 0.0010000000000013943, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
goal_identified
goal_identified
=== ep: 683, time 107.25318789482117, eps 0.0010000000000013262, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
=== ep: 684, time 106.49131989479065, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
goal_identified
=== ep: 685, time 106.12687683105469, eps 0.0010000000000012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
goal_identified
goal_identified
goal_identified
=== ep: 686, time 104.4047794342041, eps 0.0010000000000011415, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
=== ep: 687, time 92.33462858200073, eps 0.0010000000000010857, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
=== ep: 688, time 101.63765692710876, eps 0.0010000000000010328, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
goal_identified
=== ep: 689, time 99.68900966644287, eps 0.0010000000000009825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
goal_identified
=== ep: 690, time 95.52977871894836, eps 0.0010000000000009346, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
goal_identified
goal_identified
=== ep: 691, time 95.0202407836914, eps 0.001000000000000889, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 98.19459080696106, eps 0.0010000000000008457, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 693, time 97.73840427398682, eps 0.0010000000000008045, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
=== ep: 694, time 96.32664227485657, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
goal_identified
=== ep: 695, time 96.4175660610199, eps 0.0010000000000007277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
=== ep: 696, time 92.84068179130554, eps 0.0010000000000006924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
goal_identified
=== ep: 697, time 98.64456534385681, eps 0.0010000000000006586, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
=== ep: 698, time 96.70988059043884, eps 0.0010000000000006265, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
=== ep: 699, time 97.49820256233215, eps 0.001000000000000596, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
goal_identified
goal_identified
goal_identified
=== ep: 700, time 91.69417953491211, eps 0.0010000000000005668, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
goal_identified
=== ep: 701, time 95.00456213951111, eps 0.0010000000000005393, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 702, time 93.58782029151917, eps 0.0010000000000005128, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
goal_identified
goal_identified
=== ep: 703, time 96.54661893844604, eps 0.001000000000000488, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
=== ep: 704, time 98.86376523971558, eps 0.001000000000000464, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
goal_identified
=== ep: 705, time 99.44815731048584, eps 0.0010000000000004415, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
=== ep: 706, time 95.65359425544739, eps 0.00100000000000042, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
goal_identified
goal_identified
=== ep: 707, time 94.97025489807129, eps 0.0010000000000003994, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
=== ep: 708, time 91.17339086532593, eps 0.00100000000000038, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
goal_identified
goal_identified
=== ep: 709, time 89.20300364494324, eps 0.0010000000000003615, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
goal_identified
goal_identified
=== ep: 710, time 97.49217104911804, eps 0.0010000000000003437, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
goal_identified
goal_identified
goal_identified
=== ep: 711, time 91.76441240310669, eps 0.001000000000000327, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
goal_identified
=== ep: 712, time 93.406090259552, eps 0.0010000000000003112, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 98.64753890037537, eps 0.001000000000000296, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
=== ep: 714, time 97.65228724479675, eps 0.0010000000000002815, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
goal_identified
=== ep: 715, time 89.85501623153687, eps 0.0010000000000002678, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
goal_identified
=== ep: 716, time 103.58012056350708, eps 0.0010000000000002548, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
=== ep: 717, time 110.27799272537231, eps 0.0010000000000002422, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
goal_identified
=== ep: 718, time 96.1379599571228, eps 0.0010000000000002305, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
goal_identified
goal_identified
=== ep: 719, time 102.37221097946167, eps 0.0010000000000002192, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
goal_identified
=== ep: 720, time 101.25405383110046, eps 0.0010000000000002086, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
goal_identified
=== ep: 721, time 97.35740780830383, eps 0.0010000000000001984, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
goal_identified
goal_identified
=== ep: 722, time 104.65639162063599, eps 0.0010000000000001887, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
goal_identified
=== ep: 723, time 99.28147888183594, eps 0.0010000000000001796, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
=== ep: 724, time 102.94377493858337, eps 0.0010000000000001707, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
=== ep: 725, time 91.25284075737, eps 0.0010000000000001624, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
goal_identified
=== ep: 726, time 101.52122187614441, eps 0.0010000000000001544, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
=== ep: 727, time 94.2743513584137, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
=== ep: 728, time 95.80421853065491, eps 0.0010000000000001399, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
goal_identified
goal_identified
goal_identified
=== ep: 729, time 94.56316590309143, eps 0.001000000000000133, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
goal_identified
=== ep: 730, time 93.9176549911499, eps 0.0010000000000001264, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
goal_identified
=== ep: 731, time 87.64025378227234, eps 0.0010000000000001204, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
=== ep: 732, time 91.38025689125061, eps 0.0010000000000001145, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
goal_identified
goal_identified
=== ep: 733, time 90.90305352210999, eps 0.0010000000000001089, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
=== ep: 734, time 90.92022943496704, eps 0.0010000000000001037, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
goal_identified
=== ep: 735, time 89.3166151046753, eps 0.0010000000000000985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
goal_identified
=== ep: 736, time 94.81393480300903, eps 0.0010000000000000937, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
=== ep: 737, time 94.14588904380798, eps 0.0010000000000000891, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
=== ep: 738, time 94.55327725410461, eps 0.0010000000000000848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
goal_identified
=== ep: 739, time 91.71838402748108, eps 0.0010000000000000807, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
=== ep: 740, time 95.5090503692627, eps 0.0010000000000000768, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
goal_identified
=== ep: 741, time 99.18813872337341, eps 0.001000000000000073, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 95.09517025947571, eps 0.0010000000000000694, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
=== ep: 743, time 99.49867486953735, eps 0.001000000000000066, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
=== ep: 744, time 109.67777872085571, eps 0.001000000000000063, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
goal_identified
goal_identified
goal_identified
=== ep: 745, time 93.73688960075378, eps 0.0010000000000000599, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
goal_identified
=== ep: 746, time 100.07022333145142, eps 0.0010000000000000568, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
goal_identified
goal_identified
goal_identified
=== ep: 747, time 101.57916307449341, eps 0.001000000000000054, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
goal_identified
goal_identified
=== ep: 748, time 104.379061460495, eps 0.0010000000000000514, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
=== ep: 749, time 94.23860168457031, eps 0.001000000000000049, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
goal_identified
=== ep: 750, time 90.66438174247742, eps 0.0010000000000000466, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
=== ep: 751, time 94.44330263137817, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
goal_identified
=== ep: 752, time 93.91366767883301, eps 0.001000000000000042, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
=== ep: 753, time 94.63914179801941, eps 0.0010000000000000401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
=== ep: 754, time 90.70817542076111, eps 0.0010000000000000382, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
=== ep: 755, time 91.49601364135742, eps 0.0010000000000000362, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
goal_identified
=== ep: 756, time 92.55862498283386, eps 0.0010000000000000345, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
=== ep: 757, time 93.10524296760559, eps 0.0010000000000000328, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 88.45289373397827, eps 0.0010000000000000312, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
=== ep: 759, time 90.23861622810364, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
goal_identified
=== ep: 760, time 86.12728476524353, eps 0.0010000000000000282, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
goal_identified
=== ep: 761, time 89.03043985366821, eps 0.001000000000000027, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
goal_identified
=== ep: 762, time 87.3497085571289, eps 0.0010000000000000256, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
goal_identified
=== ep: 763, time 90.94019341468811, eps 0.0010000000000000243, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
=== ep: 764, time 92.1948094367981, eps 0.0010000000000000232, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
goal_identified
=== ep: 765, time 93.72383284568787, eps 0.001000000000000022, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
=== ep: 766, time 95.69245791435242, eps 0.0010000000000000208, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
goal_identified
goal_identified
goal_identified
=== ep: 767, time 95.96514368057251, eps 0.00100000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
=== ep: 768, time 92.75878190994263, eps 0.0010000000000000189, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
=== ep: 769, time 115.85431337356567, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
goal_identified
goal_identified
goal_identified
=== ep: 770, time 100.80886769294739, eps 0.0010000000000000172, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
=== ep: 771, time 99.29166197776794, eps 0.0010000000000000163, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
goal_identified
=== ep: 772, time 103.18503713607788, eps 0.0010000000000000154, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
=== ep: 773, time 98.80454015731812, eps 0.0010000000000000148, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
=== ep: 774, time 100.65788078308105, eps 0.0010000000000000141, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
=== ep: 775, time 103.5573787689209, eps 0.0010000000000000132, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
=== ep: 776, time 103.27523350715637, eps 0.0010000000000000126, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
=== ep: 777, time 98.95483064651489, eps 0.0010000000000000122, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
=== ep: 778, time 90.1530647277832, eps 0.0010000000000000115, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
=== ep: 779, time 86.33499240875244, eps 0.0010000000000000109, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 2/2)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
goal_identified
=== ep: 780, time 94.79010558128357, eps 0.0010000000000000104, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
=== ep: 781, time 83.49084401130676, eps 0.00100000000000001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
goal_identified
=== ep: 782, time 93.6245129108429, eps 0.0010000000000000093, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
=== ep: 783, time 85.57956647872925, eps 0.001000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
goal_identified
=== ep: 784, time 98.85834217071533, eps 0.0010000000000000085, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
goal_identified
goal_identified
=== ep: 785, time 87.66984248161316, eps 0.001000000000000008, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
goal_identified
=== ep: 786, time 97.50501871109009, eps 0.0010000000000000076, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
=== ep: 787, time 99.50254273414612, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
goal_identified
goal_identified
goal_identified
=== ep: 788, time 100.15843033790588, eps 0.001000000000000007, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
=== ep: 789, time 100.1398355960846, eps 0.0010000000000000067, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
=== ep: 790, time 101.68116331100464, eps 0.0010000000000000063, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
goal_identified
=== ep: 791, time 97.55136895179749, eps 0.001000000000000006, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
goal_identified
=== ep: 792, time 98.70914149284363, eps 0.0010000000000000057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
goal_identified
goal_identified
=== ep: 793, time 98.86284136772156, eps 0.0010000000000000054, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
goal_identified
goal_identified
=== ep: 794, time 101.6957049369812, eps 0.0010000000000000052, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
=== ep: 795, time 99.40115857124329, eps 0.001000000000000005, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
goal_identified
=== ep: 796, time 105.88031578063965, eps 0.0010000000000000048, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
goal_identified
goal_identified
=== ep: 797, time 103.14246964454651, eps 0.0010000000000000044, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
=== ep: 798, time 101.60709571838379, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
goal_identified
goal_identified
=== ep: 799, time 103.1978120803833, eps 0.0010000000000000041, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
=== ep: 800, time 101.559561252594, eps 0.001000000000000004, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 93.52809619903564, eps 0.0010000000000000037, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
=== ep: 802, time 102.01586127281189, eps 0.0010000000000000035, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
goal_identified
=== ep: 803, time 99.88791155815125, eps 0.0010000000000000033, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
=== ep: 804, time 104.31372833251953, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
goal_identified
=== ep: 805, time 103.98265957832336, eps 0.001000000000000003, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 806, time 94.48710942268372, eps 0.0010000000000000028, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
goal_identified
goal_identified
=== ep: 807, time 103.04345440864563, eps 0.0010000000000000026, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
goal_identified
goal_identified
goal_identified
=== ep: 808, time 100.1293363571167, eps 0.0010000000000000026, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
goal_identified
=== ep: 809, time 105.48109817504883, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
=== ep: 810, time 102.67901182174683, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
=== ep: 811, time 98.17755436897278, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
=== ep: 812, time 109.0189254283905, eps 0.0010000000000000022, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
=== ep: 813, time 94.44723653793335, eps 0.001000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
goal_identified
goal_identified
=== ep: 814, time 100.5984001159668, eps 0.001000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
goal_identified
=== ep: 815, time 91.10621285438538, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
=== ep: 816, time 95.66450524330139, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
goal_identified
goal_identified
=== ep: 817, time 93.76558494567871, eps 0.0010000000000000018, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
goal_identified
=== ep: 818, time 97.12159943580627, eps 0.0010000000000000015, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
goal_identified
goal_identified
=== ep: 819, time 97.69241523742676, eps 0.0010000000000000015, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
=== ep: 820, time 97.49052715301514, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
=== ep: 821, time 102.41707611083984, eps 0.0010000000000000013, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
goal_identified
=== ep: 822, time 97.95889973640442, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 103.06733727455139, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
goal_identified
goal_identified
=== ep: 824, time 97.843994140625, eps 0.001000000000000001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
goal_identified
=== ep: 825, time 104.22600984573364, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
goal_identified
=== ep: 826, time 108.10724234580994, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
=== ep: 827, time 94.1042046546936, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
goal_identified
=== ep: 828, time 99.24945068359375, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 829, time 96.05458784103394, eps 0.0010000000000000009, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
=== ep: 830, time 104.23112988471985, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
=== ep: 831, time 103.96030402183533, eps 0.0010000000000000009, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
=== ep: 832, time 99.92121887207031, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
goal_identified
goal_identified
=== ep: 833, time 100.46454620361328, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
goal_identified
=== ep: 834, time 102.15705347061157, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
=== ep: 835, time 103.03055882453918, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
goal_identified
goal_identified
=== ep: 836, time 103.73770260810852, eps 0.0010000000000000007, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
=== ep: 837, time 102.23435544967651, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
=== ep: 838, time 107.05276441574097, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
=== ep: 839, time 100.92772054672241, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
=== ep: 840, time 97.12226128578186, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
goal_identified
=== ep: 841, time 107.09563827514648, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
=== ep: 842, time 93.38510704040527, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
goal_identified
=== ep: 843, time 99.32439517974854, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
goal_identified
=== ep: 844, time 93.35149955749512, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
goal_identified
=== ep: 845, time 89.77014136314392, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
=== ep: 846, time 95.13398671150208, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
=== ep: 847, time 97.64446353912354, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
=== ep: 848, time 97.36497831344604, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
goal_identified
=== ep: 849, time 100.96244668960571, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
goal_identified
goal_identified
=== ep: 850, time 102.13341975212097, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
goal_identified
=== ep: 851, time 99.56516814231873, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
=== ep: 852, time 101.12801933288574, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
goal_identified
=== ep: 853, time 102.52555179595947, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
goal_identified
=== ep: 854, time 104.07419323921204, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
=== ep: 855, time 105.31134462356567, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
=== ep: 856, time 103.30929064750671, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
goal_identified
goal_identified
=== ep: 857, time 103.63146305084229, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
=== ep: 858, time 100.10450744628906, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
=== ep: 859, time 96.58520174026489, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
=== ep: 860, time 97.46267557144165, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
goal_identified
=== ep: 861, time 94.37299156188965, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
=== ep: 862, time 91.5684220790863, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
goal_identified
=== ep: 863, time 92.19047951698303, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
goal_identified
=== ep: 864, time 96.65260577201843, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
=== ep: 865, time 96.92538571357727, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
goal_identified
=== ep: 866, time 101.02664399147034, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
=== ep: 867, time 101.95897221565247, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
goal_identified
goal_identified
=== ep: 868, time 97.18282723426819, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
=== ep: 869, time 97.87448143959045, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
goal_identified
=== ep: 870, time 93.120765209198, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 102.63305068016052, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
goal_identified
=== ep: 872, time 98.57627415657043, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
goal_identified
=== ep: 873, time 106.57617425918579, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
=== ep: 874, time 102.65954065322876, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
goal_identified
goal_identified
goal_identified
=== ep: 875, time 103.35548400878906, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
goal_identified
goal_identified
=== ep: 876, time 107.70650291442871, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
goal_identified
=== ep: 877, time 99.60912156105042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
=== ep: 878, time 104.38551259040833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
goal_identified
=== ep: 879, time 103.03981375694275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
=== ep: 880, time 100.68929743766785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
=== ep: 881, time 103.39477252960205, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
goal_identified
=== ep: 882, time 98.096608877182, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
goal_identified
=== ep: 883, time 88.71086192131042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
=== ep: 884, time 93.53697204589844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
=== ep: 885, time 96.39760446548462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
goal_identified
=== ep: 886, time 96.46004939079285, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
=== ep: 887, time 101.52895474433899, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
=== ep: 888, time 103.43457984924316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
=== ep: 889, time 104.83447504043579, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 103.95261693000793, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
goal_identified
=== ep: 891, time 98.75097751617432, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
=== ep: 892, time 101.45306134223938, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
=== ep: 893, time 105.37392115592957, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 894, time 103.49189400672913, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
=== ep: 895, time 104.49895858764648, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
goal_identified
=== ep: 896, time 101.63398241996765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
goal_identified
=== ep: 897, time 96.20342063903809, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 898, time 102.86583423614502, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
goal_identified
goal_identified
=== ep: 899, time 104.51021528244019, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
goal_identified
=== ep: 900, time 105.00008869171143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
goal_identified
=== ep: 901, time 98.02084946632385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
goal_identified
goal_identified
goal_identified
=== ep: 902, time 98.65354347229004, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
=== ep: 903, time 105.4581081867218, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
goal_identified
goal_identified
=== ep: 904, time 111.5259256362915, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
goal_identified
goal_identified
=== ep: 905, time 103.67199802398682, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
=== ep: 906, time 105.47649669647217, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
goal_identified
=== ep: 907, time 101.74979400634766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
goal_identified
=== ep: 908, time 99.56285119056702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
=== ep: 909, time 99.92116355895996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
goal_identified
goal_identified
goal_identified
=== ep: 910, time 102.93624305725098, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
=== ep: 911, time 97.85157346725464, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
=== ep: 912, time 92.98996639251709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
=== ep: 913, time 113.47783637046814, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
=== ep: 914, time 102.25792050361633, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
goal_identified
goal_identified
goal_identified
=== ep: 915, time 101.44425702095032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 916, time 97.72328209877014, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
=== ep: 917, time 105.69021773338318, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
=== ep: 918, time 98.2761480808258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
=== ep: 919, time 97.39358854293823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
=== ep: 920, time 96.70464372634888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
goal_identified
=== ep: 921, time 102.34311962127686, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
goal_identified
=== ep: 922, time 95.89808201789856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
goal_identified
=== ep: 923, time 95.01872849464417, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
=== ep: 924, time 99.18227863311768, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
goal_identified
=== ep: 925, time 96.1041476726532, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
=== ep: 926, time 102.86940860748291, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
goal_identified
goal_identified
=== ep: 927, time 95.9359769821167, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
goal_identified
=== ep: 928, time 103.78558564186096, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
goal_identified
=== ep: 929, time 90.97349786758423, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
goal_identified
=== ep: 930, time 99.9702525138855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
=== ep: 931, time 97.02803564071655, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
goal_identified
=== ep: 932, time 102.7670350074768, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
=== ep: 933, time 97.68722009658813, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
goal_identified
=== ep: 934, time 98.36271810531616, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
goal_identified
=== ep: 935, time 103.66435670852661, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
goal_identified
goal_identified
=== ep: 936, time 104.4916045665741, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
goal_identified
=== ep: 937, time 104.20889902114868, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
=== ep: 938, time 99.08780431747437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
goal_identified
=== ep: 939, time 98.0918824672699, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
goal_identified
goal_identified
=== ep: 940, time 98.08735156059265, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
=== ep: 941, time 95.88983130455017, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
goal_identified
=== ep: 942, time 95.62992763519287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
=== ep: 943, time 101.63822627067566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
=== ep: 944, time 113.17012023925781, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
=== ep: 945, time 97.04332542419434, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
goal_identified
goal_identified
goal_identified
=== ep: 946, time 98.9709951877594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
goal_identified
=== ep: 947, time 97.74021625518799, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
=== ep: 948, time 94.21113228797913, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
=== ep: 949, time 92.78455018997192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
=== ep: 950, time 87.15280270576477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
goal_identified
goal_identified
=== ep: 951, time 84.97691249847412, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 27/27)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
goal_identified
goal_identified
goal_identified
=== ep: 952, time 91.50767755508423, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
goal_identified
goal_identified
goal_identified
=== ep: 953, time 92.21451950073242, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
=== ep: 954, time 95.97928953170776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
=== ep: 955, time 95.00427842140198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
=== ep: 956, time 94.99268460273743, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
=== ep: 957, time 96.9142701625824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 88.94839239120483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
=== ep: 959, time 95.51377272605896, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
goal_identified
goal_identified
=== ep: 960, time 94.78472399711609, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
goal_identified
=== ep: 961, time 94.34187841415405, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
goal_identified
=== ep: 962, time 100.85473299026489, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
=== ep: 963, time 96.45830607414246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
=== ep: 964, time 111.87482500076294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 965, time 97.94091629981995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
=== ep: 966, time 98.07583522796631, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
=== ep: 967, time 98.60935735702515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
=== ep: 968, time 100.11223530769348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
goal_identified
goal_identified
=== ep: 969, time 90.93609476089478, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
goal_identified
=== ep: 970, time 100.69414186477661, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
goal_identified
=== ep: 971, time 96.17709755897522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
goal_identified
=== ep: 972, time 96.03038287162781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
goal_identified
goal_identified
=== ep: 973, time 104.43860197067261, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
=== ep: 974, time 94.59793186187744, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
goal_identified
=== ep: 975, time 95.03268098831177, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
goal_identified
=== ep: 976, time 86.21066236495972, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
goal_identified
goal_identified
goal_identified
=== ep: 977, time 95.37971329689026, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
goal_identified
goal_identified
=== ep: 978, time 95.9634747505188, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
goal_identified
=== ep: 979, time 93.60538959503174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
=== ep: 980, time 96.98570704460144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
goal_identified
=== ep: 981, time 94.95742797851562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
goal_identified
goal_identified
=== ep: 982, time 99.43029022216797, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
=== ep: 983, time 102.78467535972595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
goal_identified
goal_identified
=== ep: 984, time 94.85504198074341, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
=== ep: 985, time 96.58856773376465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
goal_identified
=== ep: 986, time 97.83396100997925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
goal_identified
goal_identified
=== ep: 987, time 94.5367865562439, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
goal_identified
=== ep: 988, time 94.93213438987732, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
=== ep: 989, time 100.0550320148468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
goal_identified
goal_identified
=== ep: 990, time 96.95218992233276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
goal_identified
goal_identified
=== ep: 991, time 101.01785802841187, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
=== ep: 992, time 104.06137776374817, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
goal_identified
=== ep: 993, time 106.07716345787048, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
goal_identified
goal_identified
=== ep: 994, time 94.72647643089294, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
goal_identified
goal_identified
=== ep: 995, time 101.74241590499878, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
=== ep: 996, time 92.0445806980133, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
=== ep: 997, time 83.3932273387909, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
=== ep: 998, time 87.40013337135315, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
=== ep: 999, time 93.72729897499084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
goal_identified
goal_identified
goal_identified
=== ep: 1000, time 86.48476052284241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
=== ep: 1001, time 91.77136325836182, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
goal_identified
=== ep: 1002, time 97.6221649646759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
=== ep: 1003, time 95.79028558731079, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
goal_identified
=== ep: 1004, time 102.01337027549744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
=== ep: 1005, time 93.00952410697937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
goal_identified
=== ep: 1006, time 94.62692260742188, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
=== ep: 1007, time 94.69171738624573, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
=== ep: 1008, time 94.13195157051086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
goal_identified
=== ep: 1009, time 89.94983077049255, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
goal_identified
=== ep: 1010, time 95.38181328773499, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
goal_identified
goal_identified
=== ep: 1011, time 96.20998573303223, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
=== ep: 1012, time 99.6726975440979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
=== ep: 1013, time 96.10993242263794, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
=== ep: 1014, time 99.59465622901917, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
goal_identified
=== ep: 1015, time 97.67696905136108, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
goal_identified
goal_identified
=== ep: 1016, time 95.04150891304016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
=== ep: 1017, time 93.91341090202332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
=== ep: 1018, time 98.3302321434021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
=== ep: 1019, time 92.86847996711731, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
goal_identified
=== ep: 1020, time 101.54575157165527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
=== ep: 1021, time 101.52652740478516, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
goal_identified
=== ep: 1022, time 99.0529420375824, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
=== ep: 1023, time 97.73726105690002, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
goal_identified
=== ep: 1024, time 102.32116723060608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
goal_identified
goal_identified
=== ep: 1025, time 97.27577233314514, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
goal_identified
goal_identified
=== ep: 1026, time 102.39137673377991, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
goal_identified
=== ep: 1027, time 100.18772435188293, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
goal_identified
goal_identified
=== ep: 1028, time 100.30136060714722, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
=== ep: 1029, time 96.88480496406555, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
goal_identified
=== ep: 1030, time 101.1094856262207, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
=== ep: 1031, time 101.63029742240906, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
=== ep: 1032, time 101.01516604423523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
goal_identified
goal_identified
=== ep: 1033, time 100.81408071517944, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
goal_identified
goal_identified
=== ep: 1034, time 99.46365571022034, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
=== ep: 1035, time 107.33948373794556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
goal_identified
goal_identified
=== ep: 1036, time 104.61593723297119, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
=== ep: 1037, time 94.91940569877625, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
=== ep: 1038, time 94.94767355918884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
=== ep: 1039, time 102.75187802314758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
=== ep: 1040, time 99.82456135749817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
=== ep: 1041, time 95.30648851394653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
=== ep: 1042, time 107.63151288032532, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
goal_identified
=== ep: 1043, time 102.36472535133362, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
=== ep: 1044, time 93.33218312263489, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
goal_identified
goal_identified
=== ep: 1045, time 99.06554937362671, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
goal_identified
=== ep: 1046, time 104.00309944152832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
goal_identified
=== ep: 1047, time 99.02942156791687, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
=== ep: 1048, time 101.39883470535278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
=== ep: 1049, time 103.70894384384155, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
goal_identified
=== ep: 1050, time 99.95015072822571, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
=== ep: 1051, time 100.37937188148499, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
goal_identified
goal_identified
goal_identified
=== ep: 1052, time 106.2289228439331, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
=== ep: 1053, time 102.23275899887085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
=== ep: 1054, time 101.70931386947632, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
goal_identified
=== ep: 1055, time 96.93915629386902, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
=== ep: 1056, time 102.9204511642456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
=== ep: 1057, time 103.06110429763794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
=== ep: 1058, time 104.17632842063904, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
goal_identified
=== ep: 1059, time 104.81004190444946, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
=== ep: 1060, time 97.21888089179993, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
goal_identified
goal_identified
=== ep: 1061, time 86.615642786026, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
goal_identified
=== ep: 1062, time 95.25629377365112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
goal_identified
=== ep: 1063, time 96.27851271629333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
goal_identified
goal_identified
=== ep: 1064, time 99.40782570838928, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
goal_identified
=== ep: 1065, time 96.70052719116211, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
goal_identified
goal_identified
=== ep: 1066, time 98.68656468391418, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
goal_identified
goal_identified
=== ep: 1067, time 93.08837723731995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
=== ep: 1068, time 93.67008090019226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
=== ep: 1069, time 98.54207992553711, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
goal_identified
goal_identified
=== ep: 1070, time 100.71211123466492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
goal_identified
=== ep: 1071, time 91.8522720336914, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
goal_identified
=== ep: 1072, time 88.71245050430298, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1072
=== ep: 1073, time 85.51253151893616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
=== ep: 1074, time 96.99245262145996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
goal_identified
goal_identified
=== ep: 1075, time 96.60261297225952, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
=== ep: 1076, time 96.22163701057434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
goal_identified
=== ep: 1077, time 94.93393588066101, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
=== ep: 1078, time 91.49682664871216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
goal_identified
goal_identified
=== ep: 1079, time 88.64180898666382, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
goal_identified
=== ep: 1080, time 98.23343539237976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
=== ep: 1081, time 95.33836841583252, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
=== ep: 1082, time 99.3389458656311, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
goal_identified
goal_identified
=== ep: 1083, time 91.24344539642334, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
goal_identified
=== ep: 1084, time 103.25970816612244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
goal_identified
=== ep: 1085, time 90.719979763031, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
=== ep: 1086, time 95.01507210731506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
=== ep: 1087, time 94.73049116134644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
=== ep: 1088, time 98.13677287101746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
goal_identified
=== ep: 1089, time 90.53953146934509, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
=== ep: 1090, time 96.39544939994812, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
=== ep: 1091, time 93.4084701538086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
=== ep: 1092, time 93.27862334251404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
goal_identified
goal_identified
=== ep: 1093, time 94.61799716949463, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
=== ep: 1094, time 107.12696838378906, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
goal_identified
goal_identified
=== ep: 1095, time 94.75520157814026, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
goal_identified
=== ep: 1096, time 93.28590726852417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
goal_identified
goal_identified
=== ep: 1097, time 97.95198011398315, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
goal_identified
=== ep: 1098, time 102.38223361968994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
=== ep: 1099, time 96.19941568374634, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
=== ep: 1100, time 93.99190282821655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
goal_identified
goal_identified
=== ep: 1101, time 103.29180669784546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
=== ep: 1102, time 96.88826322555542, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
goal_identified
=== ep: 1103, time 100.06633973121643, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
=== ep: 1104, time 95.92023348808289, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
=== ep: 1105, time 99.97213053703308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
goal_identified
goal_identified
=== ep: 1106, time 100.85093092918396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
goal_identified
=== ep: 1107, time 100.31721591949463, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
goal_identified
goal_identified
goal_identified
=== ep: 1108, time 105.15133619308472, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
goal_identified
=== ep: 1109, time 100.16627740859985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
goal_identified
=== ep: 1110, time 101.5909104347229, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
=== ep: 1111, time 101.65145921707153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
=== ep: 1112, time 84.37260460853577, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
goal_identified
=== ep: 1113, time 95.23999238014221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
goal_identified
=== ep: 1114, time 94.00997138023376, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
goal_identified
=== ep: 1115, time 91.68481492996216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
goal_identified
=== ep: 1116, time 88.90405631065369, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
=== ep: 1117, time 88.40142846107483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
=== ep: 1118, time 98.55542278289795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
goal_identified
goal_identified
=== ep: 1119, time 94.17528057098389, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
goal_identified
=== ep: 1120, time 93.88328075408936, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
=== ep: 1121, time 94.92103028297424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
goal_identified
goal_identified
=== ep: 1122, time 98.12747025489807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 94.37515258789062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
goal_identified
goal_identified
=== ep: 1124, time 101.21585702896118, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
goal_identified
=== ep: 1125, time 95.81037712097168, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
=== ep: 1126, time 122.70852446556091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
=== ep: 1127, time 93.39289140701294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
goal_identified
=== ep: 1128, time 104.07954525947571, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
goal_identified
goal_identified
goal_identified
=== ep: 1129, time 98.06787419319153, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
goal_identified
=== ep: 1130, time 92.25549817085266, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
=== ep: 1131, time 100.13995504379272, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
=== ep: 1132, time 97.13560199737549, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
goal_identified
=== ep: 1133, time 94.69756317138672, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
goal_identified
=== ep: 1134, time 94.88997650146484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1135, time 97.48058128356934, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 1136, time 92.2818295955658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
=== ep: 1137, time 94.98214673995972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
=== ep: 1138, time 102.0128903388977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
goal_identified
=== ep: 1139, time 103.24596905708313, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1140, time 101.58863687515259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
=== ep: 1141, time 113.16635656356812, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
goal_identified
=== ep: 1142, time 107.0999391078949, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
goal_identified
=== ep: 1143, time 102.21187138557434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
=== ep: 1144, time 115.42892074584961, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
=== ep: 1145, time 108.95301032066345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
=== ep: 1146, time 99.22274327278137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
goal_identified
goal_identified
goal_identified
=== ep: 1147, time 112.77231788635254, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
goal_identified
goal_identified
=== ep: 1148, time 114.77223014831543, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
goal_identified
goal_identified
goal_identified
=== ep: 1149, time 108.81947946548462, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
=== ep: 1150, time 103.71688938140869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
=== ep: 1151, time 103.76863837242126, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
goal_identified
=== ep: 1152, time 103.22079181671143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
goal_identified
=== ep: 1153, time 101.344318151474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
=== ep: 1154, time 100.22425580024719, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
=== ep: 1155, time 103.16903066635132, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
goal_identified
=== ep: 1156, time 107.32706308364868, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
goal_identified
goal_identified
=== ep: 1157, time 101.71278429031372, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
=== ep: 1158, time 103.2532114982605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
goal_identified
goal_identified
=== ep: 1159, time 107.4400634765625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
goal_identified
goal_identified
=== ep: 1160, time 101.39942860603333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
goal_identified
=== ep: 1161, time 100.86337518692017, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
goal_identified
goal_identified
=== ep: 1162, time 98.29080319404602, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
goal_identified
goal_identified
goal_identified
=== ep: 1163, time 104.30975794792175, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
=== ep: 1164, time 111.4917516708374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
=== ep: 1165, time 94.27036643028259, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 2/2)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
=== ep: 1166, time 103.65860867500305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
goal_identified
=== ep: 1167, time 100.19819116592407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
goal_identified
goal_identified
=== ep: 1168, time 94.56335258483887, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
=== ep: 1169, time 103.62124681472778, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
goal_identified
=== ep: 1170, time 102.75250720977783, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
goal_identified
=== ep: 1171, time 108.0594494342804, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
goal_identified
=== ep: 1172, time 102.23769116401672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
goal_identified
=== ep: 1173, time 101.12542724609375, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
=== ep: 1174, time 99.20195984840393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
goal_identified
goal_identified
=== ep: 1175, time 102.91823720932007, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
goal_identified
goal_identified
goal_identified
=== ep: 1176, time 104.08085060119629, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
=== ep: 1177, time 89.91775417327881, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
goal_identified
goal_identified
=== ep: 1178, time 105.84390377998352, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
=== ep: 1179, time 102.60576748847961, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
=== ep: 1180, time 101.22040963172913, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
goal_identified
goal_identified
goal_identified
=== ep: 1181, time 102.42673635482788, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
goal_identified
goal_identified
=== ep: 1182, time 100.32354402542114, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
=== ep: 1183, time 101.2566430568695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
goal_identified
=== ep: 1184, time 108.59869384765625, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1185, time 101.59268689155579, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
=== ep: 1186, time 95.54612374305725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
goal_identified
goal_identified
=== ep: 1187, time 103.34847021102905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
=== ep: 1188, time 105.46573948860168, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
goal_identified
goal_identified
goal_identified
=== ep: 1189, time 98.17027258872986, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
goal_identified
=== ep: 1190, time 104.822194814682, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
=== ep: 1191, time 104.06861066818237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
goal_identified
=== ep: 1192, time 107.28031897544861, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
=== ep: 1193, time 110.61125063896179, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
=== ep: 1194, time 129.94162344932556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 103.3667631149292, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
goal_identified
=== ep: 1196, time 99.11762404441833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
=== ep: 1197, time 102.35700345039368, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
=== ep: 1198, time 105.41183257102966, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
goal_identified
goal_identified
=== ep: 1199, time 106.50080871582031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
=== ep: 1200, time 111.98238182067871, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
=== ep: 1201, time 110.68705606460571, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
goal_identified
goal_identified
=== ep: 1202, time 108.15768671035767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
=== ep: 1203, time 103.9897677898407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
goal_identified
=== ep: 1204, time 108.60361409187317, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
=== ep: 1205, time 114.82200241088867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
=== ep: 1206, time 104.00175189971924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
goal_identified
=== ep: 1207, time 105.61976766586304, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
goal_identified
goal_identified
=== ep: 1208, time 108.57031989097595, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
goal_identified
goal_identified
=== ep: 1209, time 98.98470830917358, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
=== ep: 1210, time 102.85529255867004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
goal_identified
goal_identified
goal_identified
=== ep: 1211, time 95.77528476715088, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
goal_identified
goal_identified
goal_identified
=== ep: 1212, time 98.87148094177246, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
goal_identified
goal_identified
=== ep: 1213, time 100.38681364059448, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
goal_identified
=== ep: 1214, time 106.81271147727966, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
=== ep: 1215, time 102.922128200531, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
=== ep: 1216, time 104.36726331710815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
=== ep: 1217, time 105.53375911712646, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
goal_identified
=== ep: 1218, time 95.73618173599243, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
goal_identified
goal_identified
goal_identified
=== ep: 1219, time 105.11222696304321, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
goal_identified
=== ep: 1220, time 100.8497941493988, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
=== ep: 1221, time 106.59872436523438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
=== ep: 1222, time 108.16258072853088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
=== ep: 1223, time 98.6766836643219, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
goal_identified
=== ep: 1224, time 110.01182007789612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
goal_identified
=== ep: 1225, time 110.26432228088379, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
goal_identified
goal_identified
=== ep: 1226, time 102.76988649368286, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
goal_identified
goal_identified
=== ep: 1227, time 103.80458307266235, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
=== ep: 1228, time 101.13904190063477, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
=== ep: 1229, time 103.12313055992126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
goal_identified
=== ep: 1230, time 100.39177441596985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
=== ep: 1231, time 97.9938235282898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
goal_identified
goal_identified
=== ep: 1232, time 93.77903199195862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
=== ep: 1233, time 103.3220145702362, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
goal_identified
=== ep: 1234, time 106.78329277038574, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
goal_identified
=== ep: 1235, time 99.64247131347656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
=== ep: 1236, time 102.38126134872437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1236
=== ep: 1237, time 113.73226523399353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
goal_identified
=== ep: 1238, time 101.31255173683167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
goal_identified
=== ep: 1239, time 102.94847536087036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
goal_identified
=== ep: 1240, time 101.55655360221863, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
=== ep: 1241, time 101.19304370880127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
=== ep: 1242, time 108.82947158813477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
goal_identified
=== ep: 1243, time 93.93456983566284, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
goal_identified
=== ep: 1244, time 102.98061108589172, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
=== ep: 1245, time 97.79597330093384, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
=== ep: 1246, time 104.86038303375244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
goal_identified
=== ep: 1247, time 94.84806799888611, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
=== ep: 1248, time 100.59726142883301, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
goal_identified
=== ep: 1249, time 101.26501178741455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
=== ep: 1250, time 104.51303029060364, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
=== ep: 1251, time 112.77237272262573, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
goal_identified
goal_identified
=== ep: 1252, time 111.19192624092102, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
goal_identified
=== ep: 1253, time 108.34087324142456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
goal_identified
=== ep: 1254, time 112.3909707069397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
goal_identified
goal_identified
=== ep: 1255, time 108.65216302871704, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
goal_identified
goal_identified
=== ep: 1256, time 105.50148940086365, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
goal_identified
=== ep: 1257, time 108.71285891532898, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
goal_identified
goal_identified
=== ep: 1258, time 105.26366639137268, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
=== ep: 1259, time 99.73763418197632, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
goal_identified
=== ep: 1260, time 101.677405834198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
goal_identified
=== ep: 1261, time 98.70725011825562, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
goal_identified
=== ep: 1262, time 94.62021732330322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
goal_identified
goal_identified
=== ep: 1263, time 103.45452332496643, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
goal_identified
=== ep: 1264, time 104.75030326843262, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
=== ep: 1265, time 101.09435772895813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
goal_identified
goal_identified
goal_identified
=== ep: 1266, time 103.31851434707642, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
=== ep: 1267, time 104.74962282180786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
=== ep: 1268, time 113.80053114891052, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
goal_identified
goal_identified
=== ep: 1269, time 97.57191109657288, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
=== ep: 1270, time 113.44886422157288, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
=== ep: 1271, time 104.74243927001953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
goal_identified
goal_identified
goal_identified
=== ep: 1272, time 106.32274007797241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
=== ep: 1273, time 107.42288160324097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
=== ep: 1274, time 116.58673620223999, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
goal_identified
=== ep: 1275, time 108.91041493415833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
=== ep: 1276, time 106.72788763046265, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
goal_identified
goal_identified
=== ep: 1277, time 99.41679000854492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
goal_identified
=== ep: 1278, time 100.11152744293213, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
=== ep: 1279, time 104.67624044418335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
goal_identified
=== ep: 1280, time 105.97178220748901, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
goal_identified
=== ep: 1281, time 102.4231185913086, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
goal_identified
=== ep: 1282, time 100.96108484268188, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
=== ep: 1283, time 97.91578316688538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
goal_identified
=== ep: 1284, time 99.95574951171875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
=== ep: 1285, time 100.84619069099426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
=== ep: 1286, time 103.20650029182434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
=== ep: 1287, time 99.67635083198547, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
=== ep: 1288, time 107.55208396911621, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
=== ep: 1289, time 100.47514128684998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
goal_identified
=== ep: 1290, time 104.32573890686035, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
goal_identified
=== ep: 1291, time 105.57795929908752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
goal_identified
=== ep: 1292, time 101.30047369003296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
=== ep: 1293, time 107.90962934494019, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1293
goal_identified
goal_identified
=== ep: 1294, time 101.16539072990417, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
=== ep: 1295, time 107.22705864906311, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
=== ep: 1296, time 112.25039839744568, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
goal_identified
goal_identified
=== ep: 1297, time 111.33673191070557, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
goal_identified
=== ep: 1298, time 98.21268391609192, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 106.51602411270142, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
goal_identified
=== ep: 1300, time 94.15197706222534, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
goal_identified
goal_identified
goal_identified
=== ep: 1301, time 102.27872967720032, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
=== ep: 1302, time 105.17539286613464, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
goal_identified
=== ep: 1303, time 102.54516124725342, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
goal_identified
=== ep: 1304, time 100.03318047523499, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
goal_identified
=== ep: 1305, time 102.01309847831726, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
=== ep: 1306, time 106.9176561832428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
=== ep: 1307, time 97.89350700378418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
=== ep: 1308, time 101.11183619499207, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1309, time 98.02727627754211, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
=== ep: 1310, time 105.80680441856384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
=== ep: 1311, time 109.24240064620972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
goal_identified
goal_identified
=== ep: 1312, time 108.22854399681091, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
goal_identified
=== ep: 1313, time 105.07889699935913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
goal_identified
goal_identified
=== ep: 1314, time 112.57107353210449, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
goal_identified
=== ep: 1315, time 104.44019317626953, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
goal_identified
goal_identified
=== ep: 1316, time 106.29291892051697, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
goal_identified
goal_identified
=== ep: 1317, time 114.92249131202698, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
goal_identified
goal_identified
=== ep: 1318, time 102.7332935333252, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
goal_identified
=== ep: 1319, time 105.87740159034729, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
goal_identified
=== ep: 1320, time 115.43147659301758, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
=== ep: 1321, time 108.58404278755188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
=== ep: 1322, time 102.51693034172058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
=== ep: 1323, time 105.70603680610657, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
=== ep: 1324, time 101.97829413414001, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
=== ep: 1325, time 94.40154671669006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
goal_identified
=== ep: 1326, time 99.96390175819397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
goal_identified
=== ep: 1327, time 102.33282947540283, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
goal_identified
=== ep: 1328, time 99.80080103874207, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
=== ep: 1329, time 104.9442138671875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
=== ep: 1330, time 102.04293084144592, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
goal_identified
=== ep: 1331, time 106.01789283752441, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
=== ep: 1332, time 104.35864901542664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
goal_identified
goal_identified
=== ep: 1333, time 105.02356839179993, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
=== ep: 1334, time 105.50518536567688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
goal_identified
=== ep: 1335, time 102.55311417579651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
goal_identified
=== ep: 1336, time 106.2124285697937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
=== ep: 1337, time 107.28096032142639, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1338, time 112.87938094139099, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
=== ep: 1339, time 113.45799398422241, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
=== ep: 1340, time 108.91674542427063, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
=== ep: 1341, time 103.45545625686646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
=== ep: 1342, time 109.28829002380371, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
goal_identified
goal_identified
=== ep: 1343, time 107.03361988067627, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
=== ep: 1344, time 100.58183026313782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
=== ep: 1345, time 106.66618776321411, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
=== ep: 1346, time 105.27460217475891, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
=== ep: 1347, time 95.99855375289917, eps 0.001, sum reward: 1, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
=== ep: 1348, time 107.15443110466003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
goal_identified
=== ep: 1349, time 93.11128306388855, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
=== ep: 1350, time 100.36427783966064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
goal_identified
=== ep: 1351, time 99.12227320671082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
goal_identified
=== ep: 1352, time 107.16610383987427, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
=== ep: 1353, time 107.14225339889526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
=== ep: 1354, time 97.43673872947693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
=== ep: 1355, time 106.94117522239685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
goal_identified
=== ep: 1356, time 107.45934557914734, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
goal_identified
goal_identified
=== ep: 1357, time 103.25513100624084, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
=== ep: 1358, time 110.67015194892883, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
goal_identified
=== ep: 1359, time 103.58000898361206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
=== ep: 1360, time 104.06498456001282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
goal_identified
goal_identified
=== ep: 1361, time 105.63571810722351, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
=== ep: 1362, time 112.73080229759216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
goal_identified
goal_identified
=== ep: 1363, time 114.95334935188293, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
goal_identified
=== ep: 1364, time 112.06961679458618, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
=== ep: 1365, time 112.73447704315186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
=== ep: 1366, time 113.35477781295776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
goal_identified
=== ep: 1367, time 110.25756812095642, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
=== ep: 1368, time 106.66229557991028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
goal_identified
=== ep: 1369, time 100.9834771156311, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
goal_identified
goal_identified
=== ep: 1370, time 109.05221366882324, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
goal_identified
goal_identified
=== ep: 1371, time 106.86860918998718, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
=== ep: 1372, time 108.94534087181091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
=== ep: 1373, time 106.21277928352356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
goal_identified
=== ep: 1374, time 103.34319305419922, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
=== ep: 1375, time 95.16802835464478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
goal_identified
=== ep: 1376, time 96.73783111572266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1376
goal_identified
=== ep: 1377, time 103.73854780197144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
goal_identified
=== ep: 1378, time 102.77765321731567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
=== ep: 1379, time 97.39778733253479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
=== ep: 1380, time 105.14314246177673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
=== ep: 1381, time 104.82190561294556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
=== ep: 1382, time 102.01068329811096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
=== ep: 1383, time 106.12748980522156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
=== ep: 1384, time 112.43296432495117, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
goal_identified
=== ep: 1385, time 109.15108513832092, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
goal_identified
=== ep: 1386, time 108.71316313743591, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
goal_identified
goal_identified
goal_identified
=== ep: 1387, time 108.24462556838989, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
goal_identified
=== ep: 1388, time 113.2594587802887, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
goal_identified
=== ep: 1389, time 108.93890237808228, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
=== ep: 1390, time 110.13406872749329, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
=== ep: 1391, time 112.66684007644653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
goal_identified
=== ep: 1392, time 114.16472291946411, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
=== ep: 1393, time 112.44706082344055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
goal_identified
=== ep: 1394, time 110.55552816390991, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
=== ep: 1395, time 106.69076633453369, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
goal_identified
=== ep: 1396, time 108.65067625045776, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
goal_identified
=== ep: 1397, time 98.72400784492493, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
=== ep: 1398, time 107.098867893219, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
goal_identified
=== ep: 1399, time 99.78174901008606, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
goal_identified
goal_identified
goal_identified
=== ep: 1400, time 100.37669706344604, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
goal_identified
=== ep: 1401, time 95.47362327575684, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
goal_identified
goal_identified
=== ep: 1402, time 101.89583396911621, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
goal_identified
goal_identified
=== ep: 1403, time 103.04020762443542, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
=== ep: 1404, time 108.65089178085327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
=== ep: 1405, time 102.13437080383301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
=== ep: 1406, time 110.7525405883789, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
goal_identified
=== ep: 1407, time 108.16264247894287, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
=== ep: 1408, time 104.20815014839172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
goal_identified
=== ep: 1409, time 113.20040988922119, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
goal_identified
goal_identified
=== ep: 1410, time 109.08531737327576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
goal_identified
goal_identified
=== ep: 1411, time 110.33032035827637, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
goal_identified
=== ep: 1412, time 110.68579125404358, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
goal_identified
=== ep: 1413, time 108.68522930145264, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
=== ep: 1414, time 104.22073888778687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
=== ep: 1415, time 106.50037145614624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
=== ep: 1416, time 115.54283332824707, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
goal_identified
goal_identified
=== ep: 1417, time 108.92432236671448, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
goal_identified
goal_identified
=== ep: 1418, time 109.00806879997253, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
goal_identified
goal_identified
=== ep: 1419, time 111.75833487510681, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
goal_identified
goal_identified
=== ep: 1420, time 107.15857172012329, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 106.80545496940613, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
goal_identified
goal_identified
=== ep: 1422, time 104.43602180480957, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
goal_identified
=== ep: 1423, time 107.77766537666321, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 100.34780097007751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
goal_identified
=== ep: 1425, time 101.87973952293396, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
goal_identified
goal_identified
=== ep: 1426, time 107.03104567527771, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
goal_identified
=== ep: 1427, time 100.42678308486938, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
=== ep: 1428, time 101.94803071022034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
goal_identified
=== ep: 1429, time 102.03184628486633, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
=== ep: 1430, time 111.61201047897339, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
goal_identified
goal_identified
goal_identified
=== ep: 1431, time 105.50503587722778, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
=== ep: 1432, time 109.35388159751892, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
goal_identified
=== ep: 1433, time 112.67291641235352, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
=== ep: 1434, time 111.32295370101929, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
goal_identified
=== ep: 1435, time 107.53118300437927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
goal_identified
=== ep: 1436, time 103.90182685852051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
goal_identified
goal_identified
goal_identified
=== ep: 1437, time 108.27634739875793, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
goal_identified
=== ep: 1438, time 112.11778664588928, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
=== ep: 1439, time 116.22536373138428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
goal_identified
goal_identified
goal_identified
=== ep: 1440, time 111.62832832336426, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
=== ep: 1441, time 111.91007876396179, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
goal_identified
=== ep: 1442, time 108.18865728378296, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
=== ep: 1443, time 110.65649080276489, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
goal_identified
goal_identified
=== ep: 1444, time 106.9919159412384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
=== ep: 1445, time 101.35646748542786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
=== ep: 1446, time 98.90764784812927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
goal_identified
=== ep: 1447, time 102.51149702072144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
=== ep: 1448, time 103.97323751449585, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
=== ep: 1449, time 105.30784320831299, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
=== ep: 1450, time 105.66980504989624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
goal_identified
=== ep: 1451, time 103.33193039894104, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
goal_identified
goal_identified
=== ep: 1452, time 106.0102710723877, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
goal_identified
goal_identified
=== ep: 1453, time 102.54054379463196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
goal_identified
=== ep: 1454, time 109.27173280715942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1454
=== ep: 1455, time 110.53877067565918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
goal_identified
=== ep: 1456, time 109.76814007759094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
goal_identified
=== ep: 1457, time 108.65903282165527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
=== ep: 1458, time 105.41586661338806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
=== ep: 1459, time 104.19630694389343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
goal_identified
=== ep: 1460, time 111.65541672706604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
goal_identified
goal_identified
=== ep: 1461, time 108.35921239852905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
=== ep: 1462, time 111.19456577301025, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
goal_identified
=== ep: 1463, time 109.59354019165039, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
=== ep: 1464, time 102.7688934803009, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
goal_identified
goal_identified
=== ep: 1465, time 105.41518235206604, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
goal_identified
=== ep: 1466, time 105.64665746688843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
goal_identified
goal_identified
goal_identified
=== ep: 1467, time 105.74764823913574, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
=== ep: 1468, time 101.80785179138184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
=== ep: 1469, time 104.22718214988708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
=== ep: 1470, time 104.29508352279663, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
goal_identified
goal_identified
=== ep: 1471, time 104.77343654632568, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
=== ep: 1472, time 98.62708497047424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
goal_identified
=== ep: 1473, time 110.6582407951355, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
=== ep: 1474, time 116.12851619720459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
goal_identified
=== ep: 1475, time 114.29469108581543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
=== ep: 1476, time 101.665536403656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
=== ep: 1477, time 106.16462445259094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
=== ep: 1478, time 106.76023316383362, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
goal_identified
goal_identified
=== ep: 1479, time 90.53517961502075, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
=== ep: 1480, time 106.78851366043091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
=== ep: 1481, time 102.50411701202393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
=== ep: 1482, time 106.65018606185913, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
goal_identified
goal_identified
=== ep: 1483, time 104.83304858207703, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
goal_identified
goal_identified
=== ep: 1484, time 106.14504051208496, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
goal_identified
=== ep: 1485, time 109.86049628257751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
goal_identified
=== ep: 1486, time 97.5608241558075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
=== ep: 1487, time 109.8331527709961, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
=== ep: 1488, time 115.99423623085022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
=== ep: 1489, time 115.78676581382751, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
=== ep: 1490, time 103.24194312095642, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
=== ep: 1491, time 105.80793595314026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
goal_identified
=== ep: 1492, time 111.72172355651855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
goal_identified
goal_identified
goal_identified
=== ep: 1493, time 109.5671751499176, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
goal_identified
=== ep: 1494, time 103.74902582168579, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
goal_identified
=== ep: 1495, time 101.57502222061157, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
=== ep: 1496, time 103.09026217460632, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
goal_identified
=== ep: 1497, time 106.81107068061829, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
=== ep: 1498, time 103.88769936561584, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
=== ep: 1499, time 107.37001395225525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
=== ep: 1500, time 110.65041708946228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
=== ep: 1501, time 94.49208760261536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
goal_identified
=== ep: 1502, time 110.2693419456482, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
goal_identified
=== ep: 1503, time 111.82860016822815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
goal_identified
goal_identified
=== ep: 1504, time 109.08494877815247, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
=== ep: 1505, time 109.11752772331238, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
=== ep: 1506, time 103.98292684555054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
goal_identified
=== ep: 1507, time 102.46138668060303, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
=== ep: 1508, time 105.35573482513428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
=== ep: 1509, time 106.24699354171753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
goal_identified
=== ep: 1510, time 101.91003680229187, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
goal_identified
=== ep: 1511, time 112.27329230308533, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
goal_identified
goal_identified
=== ep: 1512, time 97.48889541625977, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
goal_identified
goal_identified
=== ep: 1513, time 104.21664071083069, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
=== ep: 1514, time 115.76092147827148, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
goal_identified
goal_identified
goal_identified
=== ep: 1515, time 116.61888313293457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
goal_identified
=== ep: 1516, time 110.14165735244751, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1516
goal_identified
goal_identified
=== ep: 1517, time 109.13575649261475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
=== ep: 1518, time 112.65960717201233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
goal_identified
=== ep: 1519, time 118.28131079673767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
goal_identified
goal_identified
goal_identified
=== ep: 1520, time 112.01331520080566, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
=== ep: 1521, time 101.04199433326721, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
goal_identified
=== ep: 1522, time 101.9475519657135, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
goal_identified
goal_identified
=== ep: 1523, time 104.57702994346619, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
=== ep: 1524, time 102.57775163650513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
goal_identified
=== ep: 1525, time 100.45284223556519, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
goal_identified
goal_identified
=== ep: 1526, time 104.15100717544556, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
goal_identified
goal_identified
=== ep: 1527, time 112.05730056762695, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
goal_identified
=== ep: 1528, time 107.54208517074585, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
goal_identified
goal_identified
=== ep: 1529, time 104.0539219379425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
goal_identified
goal_identified
=== ep: 1530, time 111.51320552825928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
=== ep: 1531, time 111.70345783233643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
=== ep: 1532, time 103.86676621437073, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
goal_identified
=== ep: 1533, time 110.46866106987, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
goal_identified
=== ep: 1534, time 105.6705470085144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
=== ep: 1535, time 102.83581781387329, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
=== ep: 1536, time 110.21133351325989, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
goal_identified
goal_identified
=== ep: 1537, time 107.12504410743713, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
=== ep: 1538, time 108.81936049461365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
goal_identified
=== ep: 1539, time 103.7109305858612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
goal_identified
=== ep: 1540, time 107.57699584960938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
=== ep: 1541, time 101.30991697311401, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
=== ep: 1542, time 101.13534927368164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
goal_identified
=== ep: 1543, time 111.73151326179504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
=== ep: 1544, time 105.76320576667786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
=== ep: 1545, time 113.34899640083313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
goal_identified
=== ep: 1546, time 112.00795316696167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
=== ep: 1547, time 109.1281566619873, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
=== ep: 1548, time 115.47541928291321, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
=== ep: 1549, time 107.80446791648865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
goal_identified
=== ep: 1550, time 112.17745804786682, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
=== ep: 1551, time 111.35043859481812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
goal_identified
=== ep: 1552, time 107.81808662414551, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
=== ep: 1553, time 101.07819056510925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
=== ep: 1554, time 103.90791201591492, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
goal_identified
goal_identified
=== ep: 1555, time 103.60116338729858, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
=== ep: 1556, time 110.63611173629761, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
=== ep: 1557, time 102.31357407569885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
goal_identified
goal_identified
=== ep: 1558, time 105.597332239151, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
goal_identified
goal_identified
=== ep: 1559, time 87.15262985229492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
goal_identified
=== ep: 1560, time 110.39228463172913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
=== ep: 1561, time 116.92996191978455, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
=== ep: 1562, time 118.91416072845459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
goal_identified
=== ep: 1563, time 104.73355436325073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
=== ep: 1564, time 109.0538318157196, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
=== ep: 1565, time 110.3513195514679, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
=== ep: 1566, time 101.75233769416809, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
goal_identified
=== ep: 1567, time 103.3268175125122, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
goal_identified
=== ep: 1568, time 105.50086069107056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
goal_identified
=== ep: 1569, time 105.20966696739197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
goal_identified
goal_identified
=== ep: 1570, time 114.51402544975281, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
=== ep: 1571, time 103.14585494995117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
=== ep: 1572, time 110.36356401443481, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
goal_identified
=== ep: 1573, time 103.49752831459045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
=== ep: 1574, time 106.35344505310059, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
goal_identified
=== ep: 1575, time 105.92012190818787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
=== ep: 1576, time 121.7662444114685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
goal_identified
=== ep: 1577, time 110.99154686927795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
=== ep: 1578, time 109.6230640411377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
=== ep: 1579, time 111.89148545265198, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
=== ep: 1580, time 105.36719107627869, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
goal_identified
goal_identified
goal_identified
=== ep: 1581, time 108.45684027671814, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
goal_identified
=== ep: 1582, time 93.90939545631409, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
goal_identified
goal_identified
=== ep: 1583, time 108.42552947998047, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 106.51108717918396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
goal_identified
=== ep: 1585, time 104.32710790634155, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
goal_identified
=== ep: 1586, time 101.1200635433197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
goal_identified
=== ep: 1587, time 101.1778016090393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
goal_identified
goal_identified
=== ep: 1588, time 113.46786689758301, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
=== ep: 1589, time 99.59885716438293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
=== ep: 1590, time 111.40530776977539, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
goal_identified
goal_identified
=== ep: 1591, time 106.42682814598083, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
goal_identified
goal_identified
=== ep: 1592, time 105.65922999382019, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
goal_identified
goal_identified
=== ep: 1593, time 113.34201645851135, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
goal_identified
goal_identified
=== ep: 1594, time 108.54927897453308, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
goal_identified
=== ep: 1595, time 110.36139225959778, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
=== ep: 1596, time 110.62321949005127, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
goal_identified
=== ep: 1597, time 103.43214559555054, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
goal_identified
goal_identified
goal_identified
=== ep: 1598, time 101.00695204734802, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
=== ep: 1599, time 107.4541130065918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1600, time 103.00384378433228, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
=== ep: 1601, time 104.75094819068909, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
=== ep: 1602, time 115.43537425994873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
=== ep: 1603, time 104.51746225357056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
=== ep: 1604, time 108.0180914402008, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
goal_identified
goal_identified
=== ep: 1605, time 112.77283835411072, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
=== ep: 1606, time 111.75921440124512, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
goal_identified
goal_identified
=== ep: 1607, time 104.60800981521606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
=== ep: 1608, time 108.21203923225403, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
=== ep: 1609, time 106.6680235862732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
goal_identified
=== ep: 1610, time 106.85856556892395, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1610
=== ep: 1611, time 104.92716073989868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
=== ep: 1612, time 112.32014179229736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
goal_identified
=== ep: 1613, time 102.0672242641449, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
goal_identified
=== ep: 1614, time 103.39607214927673, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
=== ep: 1615, time 112.02836632728577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1615
=== ep: 1616, time 107.40940833091736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
=== ep: 1617, time 112.71783471107483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
goal_identified
goal_identified
=== ep: 1618, time 108.60846185684204, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
=== ep: 1619, time 116.10355305671692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
goal_identified
goal_identified
=== ep: 1620, time 116.53927230834961, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
goal_identified
=== ep: 1621, time 113.37604308128357, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
=== ep: 1622, time 112.49236416816711, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
=== ep: 1623, time 107.64350390434265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
goal_identified
=== ep: 1624, time 109.456298828125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
goal_identified
=== ep: 1625, time 107.78133654594421, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
=== ep: 1626, time 107.48063158988953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1627, time 100.789724111557, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
goal_identified
=== ep: 1628, time 103.9438910484314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 105.00202369689941, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
=== ep: 1630, time 107.11572241783142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
goal_identified
=== ep: 1631, time 112.73927760124207, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
=== ep: 1632, time 114.04528880119324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
goal_identified
=== ep: 1633, time 101.09508967399597, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
=== ep: 1634, time 105.67004418373108, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1634
=== ep: 1635, time 112.45952868461609, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 100.43439173698425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
goal_identified
=== ep: 1637, time 101.47381663322449, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
goal_identified
=== ep: 1638, time 101.80373096466064, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
goal_identified
=== ep: 1639, time 99.33507800102234, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1639
goal_identified
=== ep: 1640, time 101.02023315429688, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
goal_identified
=== ep: 1641, time 105.08902049064636, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1641
=== ep: 1642, time 103.39408826828003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
=== ep: 1643, time 103.45149326324463, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
goal_identified
goal_identified
=== ep: 1644, time 109.81151604652405, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
=== ep: 1645, time 105.8393337726593, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
goal_identified
=== ep: 1646, time 99.40010046958923, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
=== ep: 1647, time 97.5947778224945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
=== ep: 1648, time 97.7519302368164, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 103.45015811920166, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
goal_identified
=== ep: 1650, time 103.68255424499512, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
goal_identified
goal_identified
goal_identified
=== ep: 1651, time 107.33789992332458, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
=== ep: 1652, time 103.11583995819092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
goal_identified
goal_identified
=== ep: 1653, time 106.33121085166931, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
goal_identified
=== ep: 1654, time 101.77284073829651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
=== ep: 1655, time 92.2847831249237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
goal_identified
goal_identified
=== ep: 1656, time 96.46499848365784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
goal_identified
goal_identified
=== ep: 1657, time 103.2349443435669, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
goal_identified
=== ep: 1658, time 101.962327003479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
goal_identified
goal_identified
=== ep: 1659, time 102.10914969444275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
goal_identified
=== ep: 1660, time 109.95872974395752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
goal_identified
=== ep: 1661, time 96.74056243896484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
goal_identified
goal_identified
=== ep: 1662, time 106.77214026451111, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
goal_identified
=== ep: 1663, time 103.5289478302002, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
goal_identified
=== ep: 1664, time 102.73927736282349, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
=== ep: 1665, time 96.98700499534607, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
=== ep: 1666, time 104.05359649658203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
=== ep: 1667, time 104.8692307472229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
=== ep: 1668, time 105.08900141716003, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
goal_identified
goal_identified
goal_identified
=== ep: 1669, time 114.39836502075195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
=== ep: 1670, time 110.91792368888855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
goal_identified
=== ep: 1671, time 100.1615526676178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
goal_identified
=== ep: 1672, time 99.88368439674377, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
goal_identified
goal_identified
goal_identified
=== ep: 1673, time 89.41603207588196, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
goal_identified
=== ep: 1674, time 97.98997664451599, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
goal_identified
=== ep: 1675, time 106.58456611633301, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
=== ep: 1676, time 110.26012396812439, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
=== ep: 1677, time 97.19210481643677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
goal_identified
=== ep: 1678, time 107.41226506233215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
goal_identified
goal_identified
=== ep: 1679, time 101.54277181625366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
=== ep: 1680, time 98.05670142173767, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
goal_identified
goal_identified
goal_identified
=== ep: 1681, time 98.29851913452148, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
=== ep: 1682, time 102.24083995819092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
=== ep: 1683, time 105.93049120903015, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
=== ep: 1684, time 113.27645206451416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
=== ep: 1685, time 107.9904305934906, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
goal_identified
=== ep: 1686, time 102.94364929199219, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
goal_identified
goal_identified
goal_identified
=== ep: 1687, time 105.12316846847534, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
=== ep: 1688, time 109.42612767219543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
=== ep: 1689, time 97.38284397125244, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
goal_identified
goal_identified
=== ep: 1690, time 104.13585138320923, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
goal_identified
goal_identified
=== ep: 1691, time 102.00087785720825, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
goal_identified
=== ep: 1692, time 100.68925786018372, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
=== ep: 1693, time 103.44770932197571, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
=== ep: 1694, time 110.85682702064514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
=== ep: 1695, time 108.56248188018799, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
=== ep: 1696, time 99.33216762542725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
goal_identified
=== ep: 1697, time 99.65977215766907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
goal_identified
=== ep: 1698, time 98.49420046806335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
=== ep: 1699, time 104.22467613220215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 103.15787482261658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
goal_identified
=== ep: 1701, time 101.31156802177429, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
=== ep: 1702, time 127.82595682144165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
goal_identified
=== ep: 1703, time 106.96670842170715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
goal_identified
=== ep: 1704, time 101.82656931877136, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
=== ep: 1705, time 108.19735956192017, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
goal_identified
=== ep: 1706, time 106.67685580253601, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
goal_identified
=== ep: 1707, time 110.46255350112915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
=== ep: 1708, time 105.06096982955933, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
=== ep: 1709, time 100.36474657058716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
goal_identified
goal_identified
=== ep: 1710, time 102.03074550628662, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1711, time 97.39171934127808, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
=== ep: 1712, time 104.5195562839508, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
goal_identified
=== ep: 1713, time 100.62558460235596, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
goal_identified
goal_identified
=== ep: 1714, time 108.8726634979248, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
=== ep: 1715, time 106.72535181045532, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
goal_identified
=== ep: 1716, time 104.97393703460693, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
goal_identified
=== ep: 1717, time 108.96557927131653, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
=== ep: 1718, time 102.74049425125122, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
goal_identified
=== ep: 1719, time 107.13775992393494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
goal_identified
goal_identified
=== ep: 1720, time 101.87674856185913, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
=== ep: 1721, time 99.03687047958374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
=== ep: 1722, time 103.32021307945251, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
goal_identified
goal_identified
=== ep: 1723, time 105.28563475608826, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
goal_identified
goal_identified
=== ep: 1724, time 104.15946173667908, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
=== ep: 1725, time 111.92976641654968, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
=== ep: 1726, time 106.86859846115112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
goal_identified
goal_identified
=== ep: 1727, time 104.50723314285278, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
goal_identified
goal_identified
=== ep: 1728, time 101.68877387046814, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
goal_identified
goal_identified
goal_identified
=== ep: 1729, time 104.41449332237244, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
=== ep: 1730, time 99.98073196411133, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
=== ep: 1731, time 133.89996361732483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
goal_identified
goal_identified
goal_identified
=== ep: 1732, time 98.17882347106934, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
=== ep: 1733, time 96.52346563339233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
=== ep: 1734, time 102.24611496925354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
goal_identified
goal_identified
=== ep: 1735, time 104.43998622894287, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
=== ep: 1736, time 107.96024823188782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
=== ep: 1737, time 104.16634511947632, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
goal_identified
=== ep: 1738, time 109.90590739250183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
goal_identified
=== ep: 1739, time 102.71923995018005, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
=== ep: 1740, time 94.77209091186523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
goal_identified
=== ep: 1741, time 100.72137379646301, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
goal_identified
=== ep: 1742, time 106.29545783996582, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
goal_identified
goal_identified
=== ep: 1743, time 104.13301539421082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
goal_identified
=== ep: 1744, time 102.52238464355469, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
=== ep: 1745, time 110.48260736465454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
=== ep: 1746, time 107.36448955535889, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
=== ep: 1747, time 99.86143279075623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
goal_identified
=== ep: 1748, time 102.82379007339478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
=== ep: 1749, time 94.6113395690918, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
goal_identified
=== ep: 1750, time 100.03232836723328, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
goal_identified
goal_identified
=== ep: 1751, time 106.59207391738892, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
goal_identified
=== ep: 1752, time 113.27375674247742, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
=== ep: 1753, time 106.12869715690613, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
=== ep: 1754, time 115.060054063797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
goal_identified
=== ep: 1755, time 113.83286738395691, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
=== ep: 1756, time 102.53469586372375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
goal_identified
goal_identified
=== ep: 1757, time 94.3975658416748, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
=== ep: 1758, time 103.67334222793579, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
goal_identified
=== ep: 1759, time 103.24756693840027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
goal_identified
goal_identified
=== ep: 1760, time 103.51388788223267, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
=== ep: 1761, time 104.37708687782288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
=== ep: 1762, time 112.4512050151825, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
goal_identified
=== ep: 1763, time 101.25761795043945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
goal_identified
=== ep: 1764, time 101.81387543678284, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
goal_identified
goal_identified
=== ep: 1765, time 96.65143704414368, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
goal_identified
goal_identified
=== ep: 1766, time 102.61798477172852, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
=== ep: 1767, time 87.8070456981659, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
=== ep: 1768, time 110.23836994171143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
goal_identified
=== ep: 1769, time 105.58670973777771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
=== ep: 1770, time 106.3894579410553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
=== ep: 1771, time 107.30187892913818, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
goal_identified
goal_identified
=== ep: 1772, time 94.12931084632874, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
goal_identified
goal_identified
=== ep: 1773, time 102.63019037246704, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
goal_identified
=== ep: 1774, time 98.41287088394165, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
=== ep: 1775, time 102.88927388191223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
=== ep: 1776, time 105.19780468940735, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
=== ep: 1777, time 108.8960862159729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
goal_identified
goal_identified
=== ep: 1778, time 107.69684863090515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
goal_identified
=== ep: 1779, time 90.21318006515503, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
goal_identified
=== ep: 1780, time 105.18760561943054, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
=== ep: 1781, time 99.13164520263672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
=== ep: 1782, time 103.270920753479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
goal_identified
goal_identified
=== ep: 1783, time 106.45818090438843, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
goal_identified
goal_identified
goal_identified
=== ep: 1784, time 108.19830679893494, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
=== ep: 1785, time 102.54773616790771, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
goal_identified
=== ep: 1786, time 105.85507869720459, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
goal_identified
=== ep: 1787, time 111.53760838508606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
goal_identified
=== ep: 1788, time 100.8210768699646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 99.6254243850708, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
goal_identified
=== ep: 1790, time 97.77926659584045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
goal_identified
=== ep: 1791, time 96.67683839797974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
=== ep: 1792, time 99.54000520706177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
=== ep: 1793, time 106.48864984512329, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
goal_identified
=== ep: 1794, time 103.50989508628845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
goal_identified
goal_identified
goal_identified
=== ep: 1795, time 110.47388982772827, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
goal_identified
=== ep: 1796, time 103.09100151062012, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
=== ep: 1797, time 108.66157865524292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
goal_identified
goal_identified
=== ep: 1798, time 109.0718674659729, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
goal_identified
=== ep: 1799, time 98.9464042186737, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
=== ep: 1800, time 96.79583430290222, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
=== ep: 1801, time 100.0313355922699, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 102.45160913467407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
=== ep: 1803, time 109.63932275772095, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
=== ep: 1804, time 110.84162902832031, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
=== ep: 1805, time 106.08007597923279, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
=== ep: 1806, time 108.90756106376648, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1807, time 105.40687465667725, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
=== ep: 1808, time 109.03491759300232, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 104.1412284374237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
goal_identified
=== ep: 1810, time 103.4010374546051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
=== ep: 1811, time 105.40291213989258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
=== ep: 1812, time 102.78144526481628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
=== ep: 1813, time 109.1893105506897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
goal_identified
=== ep: 1814, time 106.73466277122498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 114.3605420589447, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
goal_identified
=== ep: 1816, time 113.28390765190125, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
goal_identified
goal_identified
=== ep: 1817, time 102.8619737625122, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
goal_identified
=== ep: 1818, time 95.03184580802917, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
goal_identified
goal_identified
=== ep: 1819, time 100.29167604446411, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
goal_identified
goal_identified
goal_identified
=== ep: 1820, time 104.33618330955505, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
goal_identified
goal_identified
=== ep: 1821, time 101.19543194770813, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
goal_identified
=== ep: 1822, time 107.77039813995361, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
goal_identified
goal_identified
goal_identified
=== ep: 1823, time 105.56970357894897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
=== ep: 1824, time 111.04734897613525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
goal_identified
goal_identified
=== ep: 1825, time 105.11889839172363, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
goal_identified
=== ep: 1826, time 103.61755466461182, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
goal_identified
goal_identified
=== ep: 1827, time 103.07564043998718, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
goal_identified
=== ep: 1828, time 100.42576217651367, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
goal_identified
goal_identified
=== ep: 1829, time 103.3387291431427, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
=== ep: 1830, time 102.27401232719421, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
goal_identified
=== ep: 1831, time 100.85351157188416, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
goal_identified
goal_identified
=== ep: 1832, time 109.58270311355591, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
=== ep: 1833, time 103.93766617774963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
goal_identified
goal_identified
goal_identified
=== ep: 1834, time 106.61426496505737, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
=== ep: 1835, time 106.88127779960632, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
goal_identified
goal_identified
goal_identified
=== ep: 1836, time 97.01402711868286, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
goal_identified
goal_identified
goal_identified
=== ep: 1837, time 99.86829352378845, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
=== ep: 1838, time 105.5363302230835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
=== ep: 1839, time 103.73158740997314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
goal_identified
=== ep: 1840, time 104.99201273918152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1841, time 109.86946892738342, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
=== ep: 1842, time 118.00119209289551, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
goal_identified
goal_identified
=== ep: 1843, time 102.45992612838745, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
=== ep: 1844, time 101.41823530197144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
goal_identified
goal_identified
=== ep: 1845, time 104.81723237037659, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
goal_identified
=== ep: 1846, time 101.89915800094604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1847, time 93.93015170097351, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1848, time 107.0694272518158, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
=== ep: 1849, time 107.68115782737732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
goal_identified
=== ep: 1850, time 109.70093297958374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
goal_identified
=== ep: 1851, time 108.5263683795929, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
goal_identified
goal_identified
goal_identified
=== ep: 1852, time 117.16016149520874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
goal_identified
=== ep: 1853, time 108.78329396247864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
goal_identified
goal_identified
=== ep: 1854, time 116.88197469711304, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
=== ep: 1855, time 116.05965161323547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
=== ep: 1856, time 111.72973155975342, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
=== ep: 1857, time 100.18989539146423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
goal_identified
=== ep: 1858, time 104.03370308876038, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
goal_identified
=== ep: 1859, time 108.00802540779114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
=== ep: 1860, time 108.7307620048523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1861, time 104.471684217453, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 101.85834169387817, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
=== ep: 1863, time 115.50028777122498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
goal_identified
=== ep: 1864, time 116.92970156669617, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
goal_identified
=== ep: 1865, time 117.33731746673584, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
goal_identified
goal_identified
=== ep: 1866, time 109.00352048873901, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
=== ep: 1867, time 109.49581122398376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
=== ep: 1868, time 111.55290079116821, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
goal_identified
=== ep: 1869, time 104.85780811309814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
=== ep: 1870, time 93.19562315940857, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
=== ep: 1871, time 102.58000683784485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
goal_identified
=== ep: 1872, time 108.2921838760376, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
goal_identified
goal_identified
=== ep: 1873, time 110.34559655189514, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
=== ep: 1874, time 109.51103925704956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
goal_identified
goal_identified
=== ep: 1875, time 113.70599627494812, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
goal_identified
=== ep: 1876, time 116.53306651115417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
=== ep: 1877, time 109.60228133201599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
=== ep: 1878, time 103.3071219921112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
goal_identified
goal_identified
=== ep: 1879, time 103.74807238578796, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
goal_identified
goal_identified
=== ep: 1880, time 104.16002202033997, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
=== ep: 1881, time 99.59391450881958, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
goal_identified
goal_identified
=== ep: 1882, time 110.02207446098328, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
goal_identified
=== ep: 1883, time 104.62763667106628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
goal_identified
=== ep: 1884, time 112.9010546207428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
goal_identified
goal_identified
goal_identified
=== ep: 1885, time 107.77774453163147, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
goal_identified
=== ep: 1886, time 111.79769539833069, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
goal_identified
=== ep: 1887, time 113.43049502372742, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
goal_identified
goal_identified
=== ep: 1888, time 107.12968158721924, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
goal_identified
=== ep: 1889, time 107.1570553779602, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 112.37560367584229, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
=== ep: 1891, time 109.9228765964508, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
=== ep: 1892, time 99.60693335533142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
=== ep: 1893, time 105.21583294868469, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
goal_identified
=== ep: 1894, time 101.29000926017761, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
goal_identified
=== ep: 1895, time 110.56186318397522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
goal_identified
goal_identified
=== ep: 1896, time 105.8939208984375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
=== ep: 1897, time 104.28318738937378, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
goal_identified
=== ep: 1898, time 111.61594319343567, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
goal_identified
=== ep: 1899, time 96.85843086242676, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1899
goal_identified
goal_identified
=== ep: 1900, time 111.7683777809143, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1900
goal_identified
=== ep: 1901, time 104.67406940460205, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1901
goal_identified
goal_identified
goal_identified
=== ep: 1902, time 114.25237250328064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1902
=== ep: 1903, time 108.61198019981384, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1903
goal_identified
=== ep: 1904, time 114.12522029876709, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1904
=== ep: 1905, time 111.35365891456604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1905
goal_identified
=== ep: 1906, time 111.52024149894714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1906
=== ep: 1907, time 103.39720368385315, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1907
=== ep: 1908, time 102.37684035301208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1908
goal_identified
=== ep: 1909, time 107.03802037239075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1909
=== ep: 1910, time 104.6848201751709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1910
goal_identified
=== ep: 1911, time 104.34902048110962, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1911
goal_identified
goal_identified
goal_identified
=== ep: 1912, time 106.82477378845215, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1912
=== ep: 1913, time 112.70770502090454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1913
goal_identified
=== ep: 1914, time 105.22716760635376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1914
goal_identified
=== ep: 1915, time 108.7954740524292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1915
=== ep: 1916, time 112.04856824874878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1916
=== ep: 1917, time 111.74936008453369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1917
=== ep: 1918, time 103.52576732635498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1918
=== ep: 1919, time 107.95204854011536, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1919
goal_identified
=== ep: 1920, time 118.4103491306305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1920
=== ep: 1921, time 107.75311017036438, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1921
goal_identified
goal_identified
=== ep: 1922, time 109.30859923362732, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1922
goal_identified
=== ep: 1923, time 102.41137504577637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1923
goal_identified
=== ep: 1924, time 102.38699889183044, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1924
=== ep: 1925, time 98.68868041038513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1925
goal_identified
=== ep: 1926, time 103.99037480354309, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1926
goal_identified
goal_identified
=== ep: 1927, time 106.44834065437317, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1927
goal_identified
=== ep: 1928, time 109.22924017906189, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1928
goal_identified
=== ep: 1929, time 113.19679188728333, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1929
=== ep: 1930, time 109.08539986610413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1930
=== ep: 1931, time 114.31685066223145, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1931
=== ep: 1932, time 108.7172429561615, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1932
=== ep: 1933, time 104.55609917640686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1933
goal_identified
goal_identified
=== ep: 1934, time 104.42443752288818, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1934
goal_identified
=== ep: 1935, time 103.55783438682556, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1935
=== ep: 1936, time 103.81804299354553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1936
goal_identified
=== ep: 1937, time 106.48809576034546, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1937
=== ep: 1938, time 107.50296473503113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1938
=== ep: 1939, time 101.70668268203735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1939
=== ep: 1940, time 111.580885887146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1940
goal_identified
=== ep: 1941, time 113.06061363220215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1941
=== ep: 1942, time 110.90307760238647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1942
goal_identified
goal_identified
=== ep: 1943, time 108.41095232963562, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1943
goal_identified
goal_identified
=== ep: 1944, time 108.15306901931763, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1944
goal_identified
=== ep: 1945, time 104.94654178619385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1945
goal_identified
goal_identified
=== ep: 1946, time 108.52314829826355, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1946
=== ep: 1947, time 105.86432671546936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1947
goal_identified
=== ep: 1948, time 107.59591960906982, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1948
goal_identified
goal_identified
=== ep: 1949, time 103.73539471626282, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1949
goal_identified
=== ep: 1950, time 100.54575371742249, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1950
goal_identified
goal_identified
=== ep: 1951, time 106.62292170524597, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1951
=== ep: 1952, time 111.498783826828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1952
=== ep: 1953, time 107.98532724380493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1953
goal_identified
goal_identified
goal_identified
=== ep: 1954, time 109.87657451629639, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
goal_identified
goal_identified
goal_identified
=== ep: 1955, time 109.71848440170288, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1955
goal_identified
goal_identified
=== ep: 1956, time 107.48102164268494, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1956
=== ep: 1957, time 107.26948142051697, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1957
=== ep: 1958, time 99.54890060424805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1958
goal_identified
=== ep: 1959, time 95.21709656715393, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1959
goal_identified
goal_identified
goal_identified
=== ep: 1960, time 104.92196583747864, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1960
=== ep: 1961, time 103.43115210533142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1961
goal_identified
goal_identified
goal_identified
=== ep: 1962, time 113.49123549461365, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1962
=== ep: 1963, time 102.90817427635193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1963
=== ep: 1964, time 104.54249382019043, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1964
goal_identified
=== ep: 1965, time 109.31726717948914, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1965
goal_identified
goal_identified
=== ep: 1966, time 114.71610879898071, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1966
goal_identified
goal_identified
=== ep: 1967, time 112.63735055923462, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1967
=== ep: 1968, time 100.72141742706299, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1968
goal_identified
=== ep: 1969, time 103.7715630531311, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1969
goal_identified
=== ep: 1970, time 101.34584188461304, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1970
goal_identified
goal_identified
=== ep: 1971, time 108.37482452392578, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1971
goal_identified
=== ep: 1972, time 105.44068574905396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1972
goal_identified
=== ep: 1973, time 108.14090466499329, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1973
=== ep: 1974, time 98.33798837661743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1974
goal_identified
=== ep: 1975, time 106.96323442459106, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1975
=== ep: 1976, time 107.4456946849823, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1976
goal_identified
=== ep: 1977, time 110.89922046661377, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1977
=== ep: 1978, time 115.201336145401, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1978
=== ep: 1979, time 106.66767001152039, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1979
=== ep: 1980, time 109.12304306030273, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1980
=== ep: 1981, time 111.34884071350098, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1981
goal_identified
=== ep: 1982, time 113.8249990940094, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1982
goal_identified
=== ep: 1983, time 102.87101864814758, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1983
goal_identified
goal_identified
=== ep: 1984, time 99.278076171875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1984
goal_identified
goal_identified
=== ep: 1985, time 103.91239786148071, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1985
goal_identified
goal_identified
goal_identified
=== ep: 1986, time 101.91218447685242, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1986
=== ep: 1987, time 105.2222809791565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1987
goal_identified
=== ep: 1988, time 111.4853184223175, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1988
=== ep: 1989, time 103.34247088432312, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1989
=== ep: 1990, time 111.5773515701294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1990
goal_identified
=== ep: 1991, time 108.40251326560974, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1991
=== ep: 1992, time 105.66876339912415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1992
=== ep: 1993, time 113.08144903182983, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1993
=== ep: 1994, time 107.30657005310059, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1994
goal_identified
goal_identified
=== ep: 1995, time 106.67962670326233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1995
=== ep: 1996, time 106.16216397285461, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1996
goal_identified
goal_identified
=== ep: 1997, time 102.55200242996216, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1997
=== ep: 1998, time 106.49879837036133, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1998
goal_identified
goal_identified
=== ep: 1999, time 102.5497350692749, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1999
goal_identified
=== ep: 2000, time 101.21735501289368, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2000
goal_identified
=== ep: 2001, time 105.69876551628113, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2001
goal_identified
goal_identified
goal_identified
=== ep: 2002, time 110.93359637260437, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2002
=== ep: 2003, time 113.03032660484314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2003
=== ep: 2004, time 107.83180260658264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2004
=== ep: 2005, time 106.48992848396301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2005
goal_identified
goal_identified
=== ep: 2006, time 107.58879947662354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2006
goal_identified
goal_identified
=== ep: 2007, time 105.61165285110474, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2007
=== ep: 2008, time 101.42807197570801, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2008
=== ep: 2009, time 107.86013913154602, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2009
=== ep: 2010, time 99.45473575592041, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2010
=== ep: 2011, time 118.82171273231506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2011
goal_identified
=== ep: 2012, time 108.3433268070221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2012
goal_identified
=== ep: 2013, time 113.68290662765503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2013
goal_identified
goal_identified
=== ep: 2014, time 106.44995975494385, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2014
=== ep: 2015, time 114.39636063575745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2015
goal_identified
=== ep: 2016, time 104.64422607421875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2016
goal_identified
=== ep: 2017, time 111.88688778877258, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2017
=== ep: 2018, time 110.02388191223145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2018
goal_identified
=== ep: 2019, time 109.18945026397705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2019
=== ep: 2020, time 106.31045317649841, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2020
goal_identified
=== ep: 2021, time 104.52253890037537, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2021
goal_identified
goal_identified
goal_identified
=== ep: 2022, time 101.76304793357849, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2022
goal_identified
=== ep: 2023, time 98.88677263259888, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2023
goal_identified
=== ep: 2024, time 108.69168877601624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2024
=== ep: 2025, time 101.13352370262146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2025
goal_identified
=== ep: 2026, time 104.12097430229187, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2026
=== ep: 2027, time 110.12902164459229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2027
=== ep: 2028, time 106.26369309425354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2028
=== ep: 2029, time 91.54594087600708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2029
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2030, time 106.3304078578949, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
goal_identified
=== ep: 2031, time 98.14574217796326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2031
goal_identified
=== ep: 2032, time 103.33606791496277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2032
=== ep: 2033, time 100.61317610740662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2033
=== ep: 2034, time 97.62389612197876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2034
=== ep: 2035, time 105.299644947052, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2035
goal_identified
=== ep: 2036, time 104.48611664772034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2036
goal_identified
goal_identified
=== ep: 2037, time 92.23165941238403, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2037
=== ep: 2038, time 100.58254051208496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2038
goal_identified
=== ep: 2039, time 100.80542731285095, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2039
=== ep: 2040, time 98.7086591720581, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2040
goal_identified
goal_identified
=== ep: 2041, time 102.10795879364014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2041
goal_identified
=== ep: 2042, time 102.01223182678223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2042
goal_identified
goal_identified
=== ep: 2043, time 108.92371606826782, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2043
=== ep: 2044, time 101.30250453948975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2044
goal_identified
goal_identified
=== ep: 2045, time 99.79097890853882, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2045
=== ep: 2046, time 88.9311740398407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2046
=== ep: 2047, time 106.37956261634827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2047
=== ep: 2048, time 98.81501340866089, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2048
goal_identified
=== ep: 2049, time 100.80979943275452, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2049
goal_identified
=== ep: 2050, time 105.54727625846863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2050
goal_identified
goal_identified
=== ep: 2051, time 102.80583214759827, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2051
=== ep: 2052, time 105.641184091568, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2052
goal_identified
goal_identified
=== ep: 2053, time 109.00678706169128, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2053
=== ep: 2054, time 97.79527354240417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2054
=== ep: 2055, time 107.429607629776, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2055
=== ep: 2056, time 99.20252799987793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2056
goal_identified
=== ep: 2057, time 99.43397784233093, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2057
goal_identified
=== ep: 2058, time 100.41165566444397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2058
goal_identified
goal_identified
=== ep: 2059, time 100.36860704421997, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2059
goal_identified
goal_identified
=== ep: 2060, time 105.57812476158142, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2060
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2061, time 104.9147481918335, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2062, time 105.71396398544312, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
goal_identified
goal_identified
=== ep: 2063, time 107.87972259521484, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2063
goal_identified
goal_identified
goal_identified
=== ep: 2064, time 98.90236306190491, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2064
goal_identified
=== ep: 2065, time 102.36149477958679, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2065
goal_identified
=== ep: 2066, time 96.89771914482117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2066
goal_identified
=== ep: 2067, time 95.43331813812256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2067
=== ep: 2068, time 111.03961944580078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2068
goal_identified
goal_identified
=== ep: 2069, time 107.23433375358582, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2069
=== ep: 2070, time 105.73725509643555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2070
=== ep: 2071, time 100.39366698265076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2071
goal_identified
=== ep: 2072, time 98.70166993141174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2072
=== ep: 2073, time 93.38773036003113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2073
goal_identified
=== ep: 2074, time 97.82789397239685, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2074
goal_identified
goal_identified
=== ep: 2075, time 101.68586373329163, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2075
goal_identified
goal_identified
=== ep: 2076, time 103.6523962020874, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 2076
=== ep: 2077, time 112.59236931800842, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2077
goal_identified
=== ep: 2078, time 104.30603075027466, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2078
=== ep: 2079, time 98.48024082183838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2079
goal_identified
=== ep: 2080, time 103.03158926963806, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2080
=== ep: 2081, time 90.70557117462158, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2081
=== ep: 2082, time 96.82657217979431, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2082
goal_identified
goal_identified
goal_identified
=== ep: 2083, time 98.71721839904785, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2083
goal_identified
goal_identified
=== ep: 2084, time 104.55724954605103, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2084
goal_identified
=== ep: 2085, time 110.09749412536621, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2085
goal_identified
=== ep: 2086, time 105.71449756622314, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2086
goal_identified
goal_identified
goal_identified
=== ep: 2087, time 98.95650219917297, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2087
goal_identified
goal_identified
=== ep: 2088, time 101.99409747123718, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2088
goal_identified
goal_identified
goal_identified
=== ep: 2089, time 99.84290838241577, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2089
=== ep: 2090, time 93.52967596054077, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2090
goal_identified
goal_identified
=== ep: 2091, time 101.74362707138062, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2091
goal_identified
=== ep: 2092, time 105.58447980880737, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2092
goal_identified
goal_identified
goal_identified
=== ep: 2093, time 103.84629368782043, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2093
goal_identified
=== ep: 2094, time 105.11767268180847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2094
goal_identified
=== ep: 2095, time 103.20841026306152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2095
=== ep: 2096, time 92.18596124649048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2096
=== ep: 2097, time 100.0723717212677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2097
goal_identified
=== ep: 2098, time 103.59744930267334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2098
=== ep: 2099, time 106.83607745170593, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2099
goal_identified
goal_identified
=== ep: 2100, time 110.16555523872375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2100
goal_identified
=== ep: 2101, time 101.11913442611694, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2101
=== ep: 2102, time 109.07627606391907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2102
=== ep: 2103, time 99.74792146682739, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2103
goal_identified
goal_identified
goal_identified
=== ep: 2104, time 97.10095715522766, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2104
goal_identified
=== ep: 2105, time 106.02554535865784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2105
goal_identified
=== ep: 2106, time 93.24633598327637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2106
goal_identified
goal_identified
=== ep: 2107, time 87.01733207702637, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2107
=== ep: 2108, time 100.036696434021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2108
goal_identified
=== ep: 2109, time 109.99173879623413, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2109
goal_identified
=== ep: 2110, time 110.21748566627502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2110
goal_identified
goal_identified
=== ep: 2111, time 101.69246411323547, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2111
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2112, time 97.28186297416687, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
goal_identified
=== ep: 2113, time 86.8888611793518, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2113
=== ep: 2114, time 95.46745896339417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2114
=== ep: 2115, time 93.95918822288513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2115
=== ep: 2116, time 103.33138132095337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2116
goal_identified
goal_identified
goal_identified
=== ep: 2117, time 99.91046404838562, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2117
=== ep: 2118, time 90.49306869506836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2118
=== ep: 2119, time 95.69399452209473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2119
=== ep: 2120, time 89.45053744316101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2120
=== ep: 2121, time 98.9554705619812, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2121
goal_identified
=== ep: 2122, time 99.49756073951721, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2122
=== ep: 2123, time 99.12153840065002, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2123
=== ep: 2124, time 91.04827523231506, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2124
goal_identified
=== ep: 2125, time 98.41435837745667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2125
goal_identified
=== ep: 2126, time 97.91879868507385, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 99.08510494232178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2127
=== ep: 2128, time 102.4071741104126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2128
=== ep: 2129, time 103.95605516433716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2129
goal_identified
=== ep: 2130, time 88.7503821849823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2130
goal_identified
=== ep: 2131, time 97.37942361831665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2131
goal_identified
goal_identified
=== ep: 2132, time 98.3054256439209, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2132
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2133, time 99.1663088798523, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
goal_identified
goal_identified
=== ep: 2134, time 102.1393940448761, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2134
goal_identified
=== ep: 2135, time 104.2185742855072, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2135
goal_identified
goal_identified
=== ep: 2136, time 101.89500761032104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2136
=== ep: 2137, time 98.55487012863159, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2137
=== ep: 2138, time 102.83513164520264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2138
goal_identified
goal_identified
=== ep: 2139, time 97.43926286697388, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2139
goal_identified
goal_identified
=== ep: 2140, time 98.0360836982727, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2140
goal_identified
=== ep: 2141, time 96.1653413772583, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2141
goal_identified
goal_identified
=== ep: 2142, time 107.98335599899292, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2142
goal_identified
goal_identified
=== ep: 2143, time 105.6109561920166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2143
goal_identified
goal_identified
goal_identified
=== ep: 2144, time 103.08004093170166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2144
goal_identified
goal_identified
=== ep: 2145, time 104.01708674430847, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2145
goal_identified
goal_identified
=== ep: 2146, time 92.50933337211609, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2146
goal_identified
goal_identified
=== ep: 2147, time 100.96992301940918, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2147
goal_identified
=== ep: 2148, time 107.44890832901001, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2148
goal_identified
=== ep: 2149, time 102.49839997291565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2149
goal_identified
goal_identified
goal_identified
=== ep: 2150, time 105.02139782905579, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2150
goal_identified
goal_identified
=== ep: 2151, time 99.15796852111816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2151
goal_identified
=== ep: 2152, time 104.69263887405396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2152
=== ep: 2153, time 101.2562928199768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2153
=== ep: 2154, time 98.07328033447266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2154
=== ep: 2155, time 94.09379601478577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2155
goal_identified
=== ep: 2156, time 105.43335747718811, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2156
goal_identified
goal_identified
goal_identified
=== ep: 2157, time 105.20797181129456, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2157
goal_identified
goal_identified
=== ep: 2158, time 106.75149083137512, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2158
goal_identified
goal_identified
=== ep: 2159, time 102.23352813720703, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2159
=== ep: 2160, time 107.14866280555725, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2160
goal_identified
goal_identified
=== ep: 2161, time 102.52386832237244, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2161
goal_identified
=== ep: 2162, time 95.33925652503967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2162
goal_identified
=== ep: 2163, time 97.78021931648254, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2163
=== ep: 2164, time 102.30745315551758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2164
goal_identified
goal_identified
=== ep: 2165, time 102.73837685585022, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2165
goal_identified
=== ep: 2166, time 105.91836476325989, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2166
=== ep: 2167, time 106.46569991111755, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2167
goal_identified
goal_identified
=== ep: 2168, time 101.56888914108276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2168
goal_identified
=== ep: 2169, time 96.83000874519348, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2169
goal_identified
=== ep: 2170, time 83.8359739780426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2170
goal_identified
=== ep: 2171, time 97.71973133087158, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2171
goal_identified
=== ep: 2172, time 98.67849373817444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2172
goal_identified
goal_identified
=== ep: 2173, time 106.91203188896179, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2173
goal_identified
=== ep: 2174, time 102.94728493690491, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2174
goal_identified
goal_identified
=== ep: 2175, time 97.09522199630737, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2175
=== ep: 2176, time 94.12901043891907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2176
goal_identified
=== ep: 2177, time 90.57428669929504, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2178, time 99.62454342842102, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
goal_identified
=== ep: 2179, time 101.87212038040161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2179
=== ep: 2180, time 105.50953149795532, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2180
goal_identified
=== ep: 2181, time 104.76793599128723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2181
goal_identified
=== ep: 2182, time 98.08425903320312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2182
goal_identified
goal_identified
=== ep: 2183, time 93.12620568275452, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2183
goal_identified
=== ep: 2184, time 96.1666738986969, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2184
=== ep: 2185, time 101.12197041511536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2185
=== ep: 2186, time 102.54034304618835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2186
goal_identified
=== ep: 2187, time 104.48510265350342, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2187
=== ep: 2188, time 96.53317475318909, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2188
goal_identified
=== ep: 2189, time 97.27362966537476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2189
=== ep: 2190, time 88.55029249191284, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2190
goal_identified
goal_identified
=== ep: 2191, time 96.54546523094177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2191
goal_identified
=== ep: 2192, time 100.26266527175903, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2192
goal_identified
goal_identified
=== ep: 2193, time 106.8345136642456, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2193
goal_identified
=== ep: 2194, time 102.48014545440674, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2194
goal_identified
goal_identified
=== ep: 2195, time 96.04122161865234, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2195
goal_identified
goal_identified
=== ep: 2196, time 92.02463507652283, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2196
=== ep: 2197, time 100.74694871902466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2197
=== ep: 2198, time 90.48520588874817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2198
=== ep: 2199, time 96.14165377616882, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2199
goal_identified
goal_identified
=== ep: 2200, time 85.53429532051086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2200
goal_identified
=== ep: 2201, time 96.21967315673828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2201
goal_identified
=== ep: 2202, time 92.14435195922852, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2202
goal_identified
goal_identified
=== ep: 2203, time 103.06296467781067, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2203
goal_identified
=== ep: 2204, time 109.05705952644348, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2204
goal_identified
goal_identified
=== ep: 2205, time 102.23811054229736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2205
=== ep: 2206, time 100.5952980518341, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2206
=== ep: 2207, time 98.2204749584198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2207
goal_identified
=== ep: 2208, time 95.50684237480164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2208
goal_identified
goal_identified
goal_identified
=== ep: 2209, time 92.99199557304382, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2209
goal_identified
=== ep: 2210, time 103.30924534797668, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2210
goal_identified
=== ep: 2211, time 101.8514711856842, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2211
=== ep: 2212, time 97.52368712425232, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2212
=== ep: 2213, time 90.71851992607117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2213
goal_identified
goal_identified
=== ep: 2214, time 96.10289573669434, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2214
goal_identified
goal_identified
=== ep: 2215, time 94.65225672721863, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2215
=== ep: 2216, time 97.61084604263306, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2216
=== ep: 2217, time 108.71864604949951, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2217
goal_identified
=== ep: 2218, time 99.9139518737793, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2218
=== ep: 2219, time 95.02107238769531, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2219
=== ep: 2220, time 90.74910044670105, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2220
goal_identified
goal_identified
=== ep: 2221, time 97.01395988464355, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2221
=== ep: 2222, time 103.49696588516235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2222
goal_identified
=== ep: 2223, time 100.72675251960754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2223
=== ep: 2224, time 100.62999892234802, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2224
=== ep: 2225, time 102.40583992004395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2225
goal_identified
=== ep: 2226, time 87.18492531776428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2226
=== ep: 2227, time 97.36165833473206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2227
=== ep: 2228, time 93.40150618553162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2228
goal_identified
=== ep: 2229, time 100.56533813476562, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2229
goal_identified
=== ep: 2230, time 101.15853500366211, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2230
goal_identified
goal_identified
=== ep: 2231, time 103.08307552337646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2231
=== ep: 2232, time 97.07882642745972, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2232
=== ep: 2233, time 89.30664348602295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2233
=== ep: 2234, time 101.96421694755554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2234
goal_identified
goal_identified
=== ep: 2235, time 96.58176279067993, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2235
=== ep: 2236, time 131.822092294693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2236
=== ep: 2237, time 102.51834869384766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2237
=== ep: 2238, time 102.4432303905487, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2238
goal_identified
=== ep: 2239, time 98.81696105003357, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2239
goal_identified
goal_identified
=== ep: 2240, time 99.56078910827637, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2240
=== ep: 2241, time 93.1782214641571, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2241
goal_identified
goal_identified
goal_identified
=== ep: 2242, time 97.79712724685669, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2242
=== ep: 2243, time 106.60217881202698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2243
goal_identified
=== ep: 2244, time 103.80613589286804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 101.66245031356812, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2245
goal_identified
=== ep: 2246, time 87.83873677253723, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2246
=== ep: 2247, time 100.43354892730713, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2247
=== ep: 2248, time 97.10828566551208, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2248
=== ep: 2249, time 94.88954091072083, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2249
goal_identified
goal_identified
=== ep: 2250, time 103.96125674247742, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2250
goal_identified
goal_identified
=== ep: 2251, time 105.69215297698975, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2251
goal_identified
goal_identified
goal_identified
=== ep: 2252, time 100.6495566368103, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2252
goal_identified
=== ep: 2253, time 87.69686198234558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2253
goal_identified
goal_identified
goal_identified
=== ep: 2254, time 98.48754262924194, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2254
=== ep: 2255, time 91.77902388572693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2255
=== ep: 2256, time 101.45324325561523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2256
goal_identified
=== ep: 2257, time 105.24753022193909, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2257
goal_identified
=== ep: 2258, time 100.54214453697205, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2258
goal_identified
=== ep: 2259, time 93.23888850212097, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2259
=== ep: 2260, time 94.3016095161438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2260
goal_identified
goal_identified
=== ep: 2261, time 98.48747873306274, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2261
goal_identified
goal_identified
goal_identified
=== ep: 2262, time 104.65003538131714, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2262
goal_identified
=== ep: 2263, time 102.07412886619568, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2263
goal_identified
goal_identified
goal_identified
=== ep: 2264, time 102.25796413421631, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2264
=== ep: 2265, time 98.4567039012909, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2265
=== ep: 2266, time 112.62129211425781, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2266
=== ep: 2267, time 94.41946816444397, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2267
goal_identified
=== ep: 2268, time 87.43422222137451, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2268
goal_identified
=== ep: 2269, time 95.55387806892395, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2269
goal_identified
=== ep: 2270, time 101.34092473983765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2270
=== ep: 2271, time 104.42425394058228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2271
=== ep: 2272, time 100.10427284240723, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2272
goal_identified
goal_identified
=== ep: 2273, time 102.67422723770142, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2273
=== ep: 2274, time 96.11924242973328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2274
=== ep: 2275, time 103.23158073425293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 94.2960262298584, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2276
goal_identified
goal_identified
=== ep: 2277, time 96.67402267456055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2277
goal_identified
=== ep: 2278, time 97.36523008346558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2278
=== ep: 2279, time 93.56156349182129, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 2/2)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2279
goal_identified
goal_identified
goal_identified
=== ep: 2280, time 94.14067912101746, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2280
goal_identified
goal_identified
goal_identified
=== ep: 2281, time 97.12207841873169, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2281
goal_identified
=== ep: 2282, time 101.43347644805908, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2282
=== ep: 2283, time 100.3662383556366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2283
=== ep: 2284, time 100.9370949268341, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2284
goal_identified
goal_identified
goal_identified
=== ep: 2285, time 93.59622931480408, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2285
goal_identified
goal_identified
goal_identified
=== ep: 2286, time 98.85109210014343, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2286
=== ep: 2287, time 90.68142223358154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2287
goal_identified
goal_identified
=== ep: 2288, time 100.57230710983276, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2288
goal_identified
=== ep: 2289, time 97.560231924057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2289
=== ep: 2290, time 103.67247819900513, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2290
goal_identified
goal_identified
=== ep: 2291, time 94.60977554321289, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2291
goal_identified
=== ep: 2292, time 90.32928848266602, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2292
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2293, time 89.01770401000977, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
=== ep: 2294, time 87.57832837104797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2294
goal_identified
=== ep: 2295, time 91.19052529335022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2295
goal_identified
=== ep: 2296, time 97.47410082817078, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2296
goal_identified
=== ep: 2297, time 92.92176485061646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2297
goal_identified
goal_identified
=== ep: 2298, time 89.36422872543335, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2298
=== ep: 2299, time 89.75941848754883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 78.78223967552185, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2300
=== ep: 2301, time 93.43420481681824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2301
goal_identified
goal_identified
=== ep: 2302, time 92.65900135040283, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2302
goal_identified
=== ep: 2303, time 86.39508056640625, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2303
=== ep: 2304, time 80.32081913948059, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2304
=== ep: 2305, time 87.50085043907166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2305
=== ep: 2306, time 90.61177825927734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2306
=== ep: 2307, time 89.13543105125427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2307
goal_identified
goal_identified
=== ep: 2308, time 96.56399655342102, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2308
goal_identified
goal_identified
=== ep: 2309, time 94.89119410514832, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2309
goal_identified
goal_identified
=== ep: 2310, time 83.40784335136414, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2310
goal_identified
=== ep: 2311, time 81.9414427280426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2311
=== ep: 2312, time 91.88766837120056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2312
goal_identified
=== ep: 2313, time 95.00717687606812, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2313
goal_identified
=== ep: 2314, time 92.0621452331543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2314
goal_identified
goal_identified
goal_identified
=== ep: 2315, time 92.54151320457458, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2315
goal_identified
=== ep: 2316, time 88.27326345443726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2316
=== ep: 2317, time 80.10658884048462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2317
goal_identified
=== ep: 2318, time 87.4048011302948, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2318
=== ep: 2319, time 92.28431725502014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2319
goal_identified
=== ep: 2320, time 99.62420415878296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2320
=== ep: 2321, time 89.88712167739868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2321
=== ep: 2322, time 84.2292799949646, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2322
=== ep: 2323, time 88.76802015304565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2323
goal_identified
=== ep: 2324, time 86.60480785369873, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2324
goal_identified
=== ep: 2325, time 90.59536695480347, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2325
goal_identified
=== ep: 2326, time 93.88187885284424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2326
goal_identified
goal_identified
=== ep: 2327, time 88.32410430908203, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2327
goal_identified
=== ep: 2328, time 81.38953733444214, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2328
goal_identified
=== ep: 2329, time 82.16980075836182, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2329
goal_identified
=== ep: 2330, time 89.4206714630127, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2330
=== ep: 2331, time 93.93345713615417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2331
goal_identified
goal_identified
=== ep: 2332, time 89.65667486190796, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2332
=== ep: 2333, time 94.30432605743408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2333
goal_identified
goal_identified
=== ep: 2334, time 90.9889326095581, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2334
=== ep: 2335, time 86.87055897712708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2335
=== ep: 2336, time 88.967698097229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2336
goal_identified
=== ep: 2337, time 78.33174538612366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2337
=== ep: 2338, time 89.03032517433167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2338
goal_identified
=== ep: 2339, time 92.14307594299316, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2339
goal_identified
=== ep: 2340, time 90.86098456382751, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2340
goal_identified
=== ep: 2341, time 86.59440684318542, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2341
goal_identified
=== ep: 2342, time 78.6983540058136, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2342
=== ep: 2343, time 79.13576626777649, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2344, time 76.04688167572021, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
=== ep: 2345, time 84.70514345169067, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2345
goal_identified
=== ep: 2346, time 84.15053701400757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2346
=== ep: 2347, time 88.63861107826233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2347
goal_identified
goal_identified
=== ep: 2348, time 84.17175006866455, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2348
goal_identified
goal_identified
=== ep: 2349, time 68.37420511245728, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2349
goal_identified
=== ep: 2350, time 80.00209379196167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2350
goal_identified
goal_identified
=== ep: 2351, time 84.11202383041382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2351
goal_identified
=== ep: 2352, time 82.4409670829773, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2352
goal_identified
=== ep: 2353, time 85.76329326629639, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2353
=== ep: 2354, time 84.31878662109375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2354
=== ep: 2355, time 87.9229199886322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2355
goal_identified
goal_identified
=== ep: 2356, time 78.29137682914734, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2356
=== ep: 2357, time 81.44481253623962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2357
=== ep: 2358, time 78.78768992424011, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2358
goal_identified
=== ep: 2359, time 86.09176015853882, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2359
goal_identified
=== ep: 2360, time 88.11144638061523, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2360
=== ep: 2361, time 87.65380263328552, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2361
goal_identified
=== ep: 2362, time 81.98403739929199, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2362
=== ep: 2363, time 82.71838283538818, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2363
goal_identified
=== ep: 2364, time 76.13934707641602, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2364
=== ep: 2365, time 80.95243620872498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2365
goal_identified
=== ep: 2366, time 85.12834668159485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2366
=== ep: 2367, time 84.83355522155762, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2367
goal_identified
=== ep: 2368, time 87.49645662307739, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2368
=== ep: 2369, time 85.18311762809753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2369
goal_identified
=== ep: 2370, time 72.30445551872253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2370
goal_identified
=== ep: 2371, time 72.6872193813324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2371
=== ep: 2372, time 83.34486174583435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2372
goal_identified
goal_identified
=== ep: 2373, time 79.20238256454468, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2373
goal_identified
=== ep: 2374, time 81.97045397758484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2374
goal_identified
=== ep: 2375, time 85.82262849807739, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2375
goal_identified
goal_identified
=== ep: 2376, time 88.5030882358551, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2376
goal_identified
goal_identified
=== ep: 2377, time 84.15268063545227, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2377
goal_identified
=== ep: 2378, time 79.78122806549072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2378
=== ep: 2379, time 70.01207399368286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2379
goal_identified
=== ep: 2380, time 82.07178473472595, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2380
goal_identified
=== ep: 2381, time 77.14402222633362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2381
goal_identified
goal_identified
=== ep: 2382, time 88.68583369255066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2382
goal_identified
goal_identified
=== ep: 2383, time 87.59870529174805, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2383
goal_identified
=== ep: 2384, time 87.2058036327362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2384
=== ep: 2385, time 78.78525137901306, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2385
=== ep: 2386, time 71.74375081062317, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2386
goal_identified
goal_identified
=== ep: 2387, time 83.1661319732666, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2387
goal_identified
=== ep: 2388, time 82.58389043807983, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2388
=== ep: 2389, time 90.36162161827087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2389
goal_identified
=== ep: 2390, time 88.05326676368713, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2390
=== ep: 2391, time 80.48557806015015, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2391
goal_identified
=== ep: 2392, time 78.33391046524048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2392
=== ep: 2393, time 76.20428895950317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2393
goal_identified
=== ep: 2394, time 82.25654339790344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2394
goal_identified
goal_identified
=== ep: 2395, time 86.21224308013916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2395
goal_identified
=== ep: 2396, time 86.50891089439392, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2396
goal_identified
goal_identified
=== ep: 2397, time 83.27486371994019, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2397
=== ep: 2398, time 78.44396018981934, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2398
goal_identified
=== ep: 2399, time 75.01259565353394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2399
=== ep: 2400, time 62.19810652732849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2400
=== ep: 2401, time 77.3674681186676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2401
goal_identified
goal_identified
=== ep: 2402, time 77.01559138298035, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2402
goal_identified
goal_identified
=== ep: 2403, time 69.9305624961853, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2403
=== ep: 2404, time 72.80684471130371, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2404
=== ep: 2405, time 76.96031379699707, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2405
goal_identified
=== ep: 2406, time 76.08454179763794, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2406
goal_identified
=== ep: 2407, time 67.26440644264221, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 66.47145509719849, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2408
goal_identified
=== ep: 2409, time 69.78384947776794, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2409
goal_identified
=== ep: 2410, time 63.08009600639343, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2410
=== ep: 2411, time 68.43440699577332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 72.04207730293274, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
goal_identified
=== ep: 2413, time 73.8105571269989, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2413
goal_identified
goal_identified
goal_identified
=== ep: 2414, time 69.22469925880432, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
=== ep: 2415, time 74.23403120040894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2415
goal_identified
=== ep: 2416, time 77.3156189918518, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2416
=== ep: 2417, time 74.74969816207886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2417
=== ep: 2418, time 71.98102974891663, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2418
goal_identified
=== ep: 2419, time 69.82973885536194, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2419
goal_identified
goal_identified
=== ep: 2420, time 65.60618853569031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2420
goal_identified
goal_identified
=== ep: 2421, time 66.8250904083252, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2421
goal_identified
=== ep: 2422, time 70.4616334438324, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2422
=== ep: 2423, time 63.921626567840576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2423
goal_identified
goal_identified
=== ep: 2424, time 73.30504012107849, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2424
goal_identified
goal_identified
=== ep: 2425, time 72.1066575050354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2425
goal_identified
=== ep: 2426, time 72.07454800605774, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2426
goal_identified
goal_identified
=== ep: 2427, time 72.16498041152954, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2427
goal_identified
goal_identified
goal_identified
=== ep: 2428, time 75.7855818271637, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2428
goal_identified
goal_identified
=== ep: 2429, time 78.58254742622375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2429
goal_identified
goal_identified
=== ep: 2430, time 72.4167640209198, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2430
=== ep: 2431, time 74.45530247688293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2431
=== ep: 2432, time 71.59238266944885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2432
goal_identified
=== ep: 2433, time 67.53343558311462, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2433
goal_identified
=== ep: 2434, time 67.94996666908264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2434
goal_identified
goal_identified
=== ep: 2435, time 69.78143811225891, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2435
=== ep: 2436, time 67.6661319732666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2436
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2437, time 65.82195448875427, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
=== ep: 2438, time 70.5266101360321, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2438
=== ep: 2439, time 66.67688941955566, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2439
goal_identified
goal_identified
=== ep: 2440, time 71.4224967956543, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2440
=== ep: 2441, time 72.49932622909546, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2441
goal_identified
goal_identified
=== ep: 2442, time 73.44704127311707, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2442
goal_identified
goal_identified
=== ep: 2443, time 71.9161159992218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2443
goal_identified
=== ep: 2444, time 76.50709700584412, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2444
=== ep: 2445, time 75.21952748298645, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2445
=== ep: 2446, time 72.66986393928528, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2446
=== ep: 2447, time 91.50421452522278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2447
goal_identified
goal_identified
goal_identified
=== ep: 2448, time 73.98973727226257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2448
=== ep: 2449, time 69.81043124198914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2449
goal_identified
goal_identified
goal_identified
=== ep: 2450, time 68.22471785545349, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2450
goal_identified
=== ep: 2451, time 64.77689790725708, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2451
goal_identified
goal_identified
goal_identified
=== ep: 2452, time 67.71866416931152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2452
goal_identified
=== ep: 2453, time 79.72341585159302, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2453
goal_identified
=== ep: 2454, time 58.99925899505615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2454
=== ep: 2455, time 68.67262935638428, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2455
=== ep: 2456, time 73.71606373786926, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2456
goal_identified
=== ep: 2457, time 74.69828391075134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2457
=== ep: 2458, time 75.1117193698883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2458
goal_identified
=== ep: 2459, time 77.0736825466156, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2459
goal_identified
=== ep: 2460, time 72.45312714576721, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2460
goal_identified
=== ep: 2461, time 74.15813326835632, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2461
goal_identified
goal_identified
goal_identified
=== ep: 2462, time 68.77490425109863, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2462
goal_identified
=== ep: 2463, time 59.97645711898804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2463
=== ep: 2464, time 67.3325092792511, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2464
=== ep: 2465, time 69.27896213531494, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2465
goal_identified
=== ep: 2466, time 67.48089671134949, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2466
goal_identified
goal_identified
goal_identified
=== ep: 2467, time 69.74127197265625, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2467
goal_identified
goal_identified
=== ep: 2468, time 72.20044827461243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2468
goal_identified
goal_identified
=== ep: 2469, time 73.68359470367432, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2469
goal_identified
goal_identified
=== ep: 2470, time 75.34411072731018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2470
=== ep: 2471, time 80.76731872558594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2471
goal_identified
goal_identified
=== ep: 2472, time 73.93739891052246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2472
goal_identified
=== ep: 2473, time 70.4907693862915, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2473
goal_identified
goal_identified
=== ep: 2474, time 72.08470749855042, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2474
goal_identified
goal_identified
=== ep: 2475, time 69.07995843887329, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2475
goal_identified
=== ep: 2476, time 65.81667017936707, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2476
goal_identified
goal_identified
=== ep: 2477, time 66.0788984298706, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2477
goal_identified
goal_identified
goal_identified
=== ep: 2478, time 69.42467284202576, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2478
=== ep: 2479, time 65.73188495635986, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2479
=== ep: 2480, time 65.33562922477722, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2480
goal_identified
=== ep: 2481, time 73.83904552459717, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2481
=== ep: 2482, time 73.40629625320435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2482
goal_identified
goal_identified
=== ep: 2483, time 74.41639137268066, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2483
goal_identified
=== ep: 2484, time 77.16748929023743, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2484
goal_identified
=== ep: 2485, time 70.73704028129578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2485
=== ep: 2486, time 68.32665085792542, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2486
goal_identified
=== ep: 2487, time 67.13832306861877, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2487
goal_identified
=== ep: 2488, time 70.9556336402893, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2488
goal_identified
goal_identified
=== ep: 2489, time 69.38184380531311, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2489
goal_identified
goal_identified
goal_identified
=== ep: 2490, time 68.1063768863678, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2490
goal_identified
=== ep: 2491, time 65.59187340736389, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2491
goal_identified
goal_identified
=== ep: 2492, time 70.91954231262207, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1954
goal_identified
goal_identified
=== ep: 2493, time 65.31486535072327, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2493
=== ep: 2494, time 65.7555775642395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2494
=== ep: 2495, time 75.72256231307983, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2495
goal_identified
=== ep: 2496, time 70.6774914264679, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2496
=== ep: 2497, time 73.64298868179321, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2497
=== ep: 2498, time 73.29591774940491, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 68.12010383605957, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2499
=== ep: 2500, time 69.86029553413391, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2500
goal_identified
=== ep: 2501, time 69.96335983276367, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2501
goal_identified
=== ep: 2502, time 68.89506125450134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2502
goal_identified
goal_identified
goal_identified
=== ep: 2503, time 71.66971802711487, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2503
goal_identified
goal_identified
=== ep: 2504, time 71.89693784713745, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2504
=== ep: 2505, time 66.10571146011353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2505
=== ep: 2506, time 69.76644659042358, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2506
goal_identified
goal_identified
=== ep: 2507, time 68.45660781860352, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2507
goal_identified
goal_identified
=== ep: 2508, time 68.39213347434998, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2508
goal_identified
=== ep: 2509, time 65.37132024765015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2509
goal_identified
=== ep: 2510, time 66.19140529632568, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2510
goal_identified
goal_identified
=== ep: 2511, time 65.03813457489014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2511
=== ep: 2512, time 57.621870279312134, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2512
=== ep: 2513, time 62.25607466697693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2513
goal_identified
goal_identified
=== ep: 2514, time 65.99350261688232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2514
=== ep: 2515, time 60.5875084400177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2515
goal_identified
goal_identified
=== ep: 2516, time 59.04655623435974, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2516
goal_identified
goal_identified
=== ep: 2517, time 66.59817123413086, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2517
=== ep: 2518, time 66.82110691070557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2518
goal_identified
=== ep: 2519, time 60.950575828552246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2519
goal_identified
=== ep: 2520, time 61.71034383773804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2520
goal_identified
goal_identified
goal_identified
=== ep: 2521, time 68.70533084869385, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2521
goal_identified
goal_identified
=== ep: 2522, time 64.72981214523315, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2522
=== ep: 2523, time 62.302788734436035, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2523
goal_identified
goal_identified
goal_identified
=== ep: 2524, time 66.53623986244202, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2524
goal_identified
goal_identified
=== ep: 2525, time 68.6136360168457, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2525
=== ep: 2526, time 69.16566300392151, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2526
=== ep: 2527, time 69.61383962631226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2527
goal_identified
=== ep: 2528, time 71.5437524318695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2528
goal_identified
goal_identified
=== ep: 2529, time 68.40291452407837, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2529
=== ep: 2530, time 74.08482599258423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2530
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2531, time 71.41594076156616, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2030
=== ep: 2532, time 69.51982879638672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2532
goal_identified
goal_identified
=== ep: 2533, time 70.15409922599792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2533
goal_identified
=== ep: 2534, time 66.70271253585815, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2534
goal_identified
goal_identified
goal_identified
=== ep: 2535, time 61.91887187957764, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2535
goal_identified
=== ep: 2536, time 62.41022348403931, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2536
goal_identified
=== ep: 2537, time 66.44794416427612, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2537
=== ep: 2538, time 63.16273331642151, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2538
goal_identified
=== ep: 2539, time 57.718658447265625, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2539
goal_identified
goal_identified
=== ep: 2540, time 63.1438512802124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2540
goal_identified
=== ep: 2541, time 66.27486801147461, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2541
goal_identified
goal_identified
=== ep: 2542, time 66.57207226753235, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2542
=== ep: 2543, time 59.330057859420776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2543
=== ep: 2544, time 65.66075944900513, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2544
goal_identified
=== ep: 2545, time 65.93076419830322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2545
goal_identified
goal_identified
=== ep: 2546, time 59.333390951156616, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2546
goal_identified
goal_identified
goal_identified
=== ep: 2547, time 60.235947608947754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2547
goal_identified
=== ep: 2548, time 66.15917682647705, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2548
=== ep: 2549, time 67.65155029296875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2549
=== ep: 2550, time 63.45266342163086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2550
goal_identified
=== ep: 2551, time 62.60862469673157, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2551
=== ep: 2552, time 66.37793850898743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2552
goal_identified
goal_identified
goal_identified
=== ep: 2553, time 69.63523721694946, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2553
=== ep: 2554, time 67.0210440158844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2554
=== ep: 2555, time 69.28078389167786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2555
=== ep: 2556, time 66.55202603340149, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2556
=== ep: 2557, time 66.10921263694763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2557
goal_identified
=== ep: 2558, time 71.21952557563782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2558
=== ep: 2559, time 69.29106187820435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2559
=== ep: 2560, time 70.28120756149292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2560
goal_identified
=== ep: 2561, time 73.82830500602722, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2561
goal_identified
=== ep: 2562, time 69.27548742294312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2562
goal_identified
goal_identified
goal_identified
=== ep: 2563, time 70.22216534614563, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2563
goal_identified
goal_identified
=== ep: 2564, time 68.88280963897705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2564
=== ep: 2565, time 84.74658918380737, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2565
goal_identified
goal_identified
goal_identified
=== ep: 2566, time 72.17991638183594, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2566
goal_identified
goal_identified
=== ep: 2567, time 71.24583911895752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2567
goal_identified
goal_identified
=== ep: 2568, time 71.72547054290771, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2568
=== ep: 2569, time 68.63874983787537, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2569
goal_identified
=== ep: 2570, time 67.99986362457275, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2570
goal_identified
goal_identified
goal_identified
=== ep: 2571, time 66.92966198921204, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2571
=== ep: 2572, time 68.56475162506104, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2572
goal_identified
goal_identified
=== ep: 2573, time 61.849165201187134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2573
=== ep: 2574, time 59.59398818016052, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2574
goal_identified
=== ep: 2575, time 63.5395028591156, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2575
goal_identified
=== ep: 2576, time 64.08352780342102, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2576
=== ep: 2577, time 66.09634280204773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2577
=== ep: 2578, time 61.501269817352295, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2578
goal_identified
=== ep: 2579, time 65.8057267665863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2579
=== ep: 2580, time 65.41578197479248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2580
=== ep: 2581, time 60.61332178115845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2581
=== ep: 2582, time 59.922752380371094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2582
goal_identified
=== ep: 2583, time 64.21845269203186, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2583
goal_identified
=== ep: 2584, time 65.31015086174011, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2584
=== ep: 2585, time 63.595027923583984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2585
=== ep: 2586, time 60.402384519577026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2586
goal_identified
=== ep: 2587, time 67.84469962120056, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2587
=== ep: 2588, time 63.95369791984558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2588
goal_identified
=== ep: 2589, time 64.06425189971924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2589
=== ep: 2590, time 56.934675216674805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2590
=== ep: 2591, time 67.40770864486694, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2591
=== ep: 2592, time 63.57625102996826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2592
=== ep: 2593, time 62.55419111251831, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2593
=== ep: 2594, time 64.99548888206482, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2594
goal_identified
goal_identified
goal_identified
=== ep: 2595, time 69.46902465820312, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2595
goal_identified
=== ep: 2596, time 65.42562913894653, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2596
goal_identified
goal_identified
=== ep: 2597, time 65.23831629753113, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2597
goal_identified
goal_identified
=== ep: 2598, time 69.84923386573792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2598
=== ep: 2599, time 67.98770642280579, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2599
goal_identified
=== ep: 2600, time 67.3171546459198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2600
goal_identified
=== ep: 2601, time 72.39548802375793, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2601
goal_identified
=== ep: 2602, time 67.56363987922668, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2602
=== ep: 2603, time 65.52291941642761, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2603
goal_identified
=== ep: 2604, time 69.50331163406372, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2604
goal_identified
goal_identified
=== ep: 2605, time 70.04834032058716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2605
=== ep: 2606, time 69.91477513313293, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2606
=== ep: 2607, time 69.85837030410767, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2607
=== ep: 2608, time 74.78949880599976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2608
=== ep: 2609, time 68.76310634613037, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2609
goal_identified
goal_identified
=== ep: 2610, time 68.86170363426208, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2610
=== ep: 2611, time 69.14650416374207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2611
goal_identified
goal_identified
=== ep: 2612, time 64.26722407341003, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2612
goal_identified
=== ep: 2613, time 64.96409797668457, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2613
goal_identified
goal_identified
goal_identified
=== ep: 2614, time 65.39104080200195, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2614
goal_identified
goal_identified
=== ep: 2615, time 67.95613646507263, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2615
=== ep: 2616, time 60.556816816329956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2616
goal_identified
goal_identified
=== ep: 2617, time 61.66025471687317, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2617
goal_identified
goal_identified
=== ep: 2618, time 65.8863115310669, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2618
goal_identified
goal_identified
=== ep: 2619, time 67.47040271759033, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2619
=== ep: 2620, time 72.07665872573853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2620
=== ep: 2621, time 63.502460956573486, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2621
goal_identified
=== ep: 2622, time 71.18512558937073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2622
goal_identified
goal_identified
=== ep: 2623, time 63.2430739402771, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2623
=== ep: 2624, time 57.43146347999573, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2624
goal_identified
=== ep: 2625, time 61.19681978225708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2625
=== ep: 2626, time 66.53516435623169, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2626
goal_identified
=== ep: 2627, time 66.00983834266663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2627
goal_identified
goal_identified
goal_identified
=== ep: 2628, time 63.025490522384644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2628
goal_identified
=== ep: 2629, time 60.39035654067993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2629
goal_identified
=== ep: 2630, time 65.68833804130554, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2630
goal_identified
=== ep: 2631, time 66.88961720466614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2631
=== ep: 2632, time 63.08491563796997, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2632
goal_identified
=== ep: 2633, time 62.29082655906677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2633
goal_identified
=== ep: 2634, time 67.37595009803772, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2634
=== ep: 2635, time 77.67299318313599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2635
=== ep: 2636, time 60.192758321762085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2636
goal_identified
=== ep: 2637, time 70.11492204666138, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2637
=== ep: 2638, time 62.683063983917236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2638
=== ep: 2639, time 72.72667384147644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2639
goal_identified
=== ep: 2640, time 56.86319613456726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2640
goal_identified
=== ep: 2641, time 63.024155616760254, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2641
goal_identified
=== ep: 2642, time 65.61552333831787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2642
goal_identified
=== ep: 2643, time 62.6284921169281, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2643
goal_identified
goal_identified
=== ep: 2644, time 60.68041014671326, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2644
=== ep: 2645, time 64.4812741279602, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2645
goal_identified
=== ep: 2646, time 67.09138035774231, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2646
=== ep: 2647, time 63.5280282497406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2647
goal_identified
=== ep: 2648, time 61.994580030441284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2648
=== ep: 2649, time 69.72144365310669, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2649
goal_identified
=== ep: 2650, time 70.62614917755127, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2650
goal_identified
=== ep: 2651, time 65.88041806221008, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2651
goal_identified
=== ep: 2652, time 69.56026196479797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2652
goal_identified
goal_identified
=== ep: 2653, time 70.76965808868408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2653
goal_identified
goal_identified
=== ep: 2654, time 69.87098932266235, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2654
=== ep: 2655, time 71.52350187301636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2655
=== ep: 2656, time 70.93549275398254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2656
=== ep: 2657, time 68.41767644882202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2657
=== ep: 2658, time 76.57787156105042, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2658
=== ep: 2659, time 67.11747336387634, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2659
goal_identified
=== ep: 2660, time 66.59765243530273, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2660
=== ep: 2661, time 66.84707713127136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2661
=== ep: 2662, time 66.66599893569946, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2662
=== ep: 2663, time 59.50012230873108, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2663
goal_identified
=== ep: 2664, time 60.82847213745117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2664
goal_identified
goal_identified
=== ep: 2665, time 63.87419939041138, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2665
goal_identified
=== ep: 2666, time 65.01212310791016, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2666
goal_identified
goal_identified
=== ep: 2667, time 64.72915577888489, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2667
=== ep: 2668, time 58.48624229431152, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2668
goal_identified
goal_identified
=== ep: 2669, time 62.53574323654175, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2669
goal_identified
=== ep: 2670, time 67.84246301651001, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2670
goal_identified
goal_identified
=== ep: 2671, time 67.16844868659973, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2671
=== ep: 2672, time 60.17643928527832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2672
goal_identified
goal_identified
=== ep: 2673, time 60.74701142311096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2673
goal_identified
=== ep: 2674, time 71.1145339012146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2674
goal_identified
goal_identified
=== ep: 2675, time 61.52473473548889, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2675
goal_identified
=== ep: 2676, time 57.94522309303284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2676
=== ep: 2677, time 63.36727690696716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2677
goal_identified
=== ep: 2678, time 66.45623230934143, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2678
goal_identified
goal_identified
=== ep: 2679, time 63.111952781677246, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2679
=== ep: 2680, time 61.15224266052246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2680
goal_identified
goal_identified
=== ep: 2681, time 66.34553718566895, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2681
=== ep: 2682, time 65.3462598323822, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2682
goal_identified
=== ep: 2683, time 65.54435753822327, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2683
=== ep: 2684, time 61.653454542160034, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2684
goal_identified
=== ep: 2685, time 65.43465685844421, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2685
=== ep: 2686, time 71.3719847202301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2686
goal_identified
goal_identified
=== ep: 2687, time 64.33173441886902, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2687
=== ep: 2688, time 60.052998542785645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2688
=== ep: 2689, time 59.64402747154236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2689
goal_identified
goal_identified
=== ep: 2690, time 68.84961080551147, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2690
goal_identified
=== ep: 2691, time 67.69662141799927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2691
goal_identified
=== ep: 2692, time 64.38405990600586, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2692
goal_identified
=== ep: 2693, time 71.52088832855225, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2693
goal_identified
=== ep: 2694, time 70.78818869590759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2694
goal_identified
=== ep: 2695, time 68.58046460151672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2695
goal_identified
=== ep: 2696, time 70.18958568572998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2696
=== ep: 2697, time 70.65523934364319, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2697
=== ep: 2698, time 69.048410654068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2698
=== ep: 2699, time 72.80070972442627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2699
goal_identified
goal_identified
=== ep: 2700, time 71.58390545845032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2700
=== ep: 2701, time 69.09143853187561, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2701
goal_identified
goal_identified
=== ep: 2702, time 68.35260319709778, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2702
goal_identified
goal_identified
=== ep: 2703, time 69.73210501670837, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2703
=== ep: 2704, time 62.97978734970093, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2704
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2705, time 63.12273097038269, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2061
goal_identified
=== ep: 2706, time 66.15502309799194, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2706
goal_identified
=== ep: 2707, time 66.50755596160889, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2707
=== ep: 2708, time 61.87065100669861, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2708
goal_identified
=== ep: 2709, time 62.72998809814453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2709
goal_identified
=== ep: 2710, time 70.66057562828064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2710
goal_identified
=== ep: 2711, time 67.94018983840942, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2711
goal_identified
=== ep: 2712, time 61.88703632354736, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2712
=== ep: 2713, time 66.01956152915955, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2713
=== ep: 2714, time 65.87365341186523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2714
=== ep: 2715, time 60.11855936050415, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2715
goal_identified
=== ep: 2716, time 57.773293018341064, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2716
=== ep: 2717, time 79.11068534851074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2717
goal_identified
goal_identified
=== ep: 2718, time 60.95239472389221, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2718
goal_identified
goal_identified
goal_identified
=== ep: 2719, time 56.613667726516724, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2719
=== ep: 2720, time 61.70406436920166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2720
goal_identified
goal_identified
=== ep: 2721, time 67.22794103622437, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2721
=== ep: 2722, time 61.080349922180176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2722
goal_identified
=== ep: 2723, time 57.677961349487305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2723
=== ep: 2724, time 67.19031977653503, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2724
goal_identified
=== ep: 2725, time 64.37427616119385, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2725
goal_identified
goal_identified
=== ep: 2726, time 60.17948627471924, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2726
goal_identified
=== ep: 2727, time 62.52128553390503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2727
goal_identified
=== ep: 2728, time 65.18581652641296, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2728
goal_identified
=== ep: 2729, time 65.26730465888977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2729
goal_identified
goal_identified
=== ep: 2730, time 61.98478293418884, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2730
goal_identified
goal_identified
goal_identified
=== ep: 2731, time 62.895684480667114, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2731
goal_identified
=== ep: 2732, time 68.09741878509521, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2732
goal_identified
goal_identified
=== ep: 2733, time 62.683696269989014, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2733
=== ep: 2734, time 62.906832218170166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2734
goal_identified
=== ep: 2735, time 68.36525654792786, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2735
goal_identified
goal_identified
=== ep: 2736, time 67.09636044502258, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2736
goal_identified
=== ep: 2737, time 66.71012449264526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2737
goal_identified
goal_identified
=== ep: 2738, time 68.59214973449707, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2738
goal_identified
=== ep: 2739, time 64.93816685676575, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2739
=== ep: 2740, time 64.9765887260437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2740
goal_identified
=== ep: 2741, time 67.2693738937378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2741
=== ep: 2742, time 65.59769749641418, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 65.40205073356628, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2743
goal_identified
goal_identified
=== ep: 2744, time 67.98780345916748, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2744
goal_identified
goal_identified
=== ep: 2745, time 67.33594870567322, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2745
=== ep: 2746, time 65.6583104133606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2746
=== ep: 2747, time 71.76067209243774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2747
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2748, time 72.25457978248596, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2062
goal_identified
=== ep: 2749, time 72.05724453926086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2749
goal_identified
=== ep: 2750, time 74.85383105278015, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 71.1802875995636, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2751
goal_identified
goal_identified
goal_identified
=== ep: 2752, time 72.11769223213196, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2752
=== ep: 2753, time 69.00469613075256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2753
=== ep: 2754, time 69.71788787841797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2754
=== ep: 2755, time 70.03440046310425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2755
=== ep: 2756, time 64.70890426635742, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2756
goal_identified
=== ep: 2757, time 68.68284964561462, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2757
goal_identified
=== ep: 2758, time 68.31169080734253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2758
=== ep: 2759, time 63.881073236465454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2759
goal_identified
=== ep: 2760, time 65.1310658454895, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2760
goal_identified
goal_identified
=== ep: 2761, time 70.22230672836304, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2761
=== ep: 2762, time 69.22426652908325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2762
goal_identified
goal_identified
goal_identified
=== ep: 2763, time 67.38582134246826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2763
=== ep: 2764, time 67.40066456794739, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2764
=== ep: 2765, time 63.293853998184204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2765
goal_identified
=== ep: 2766, time 61.3372528553009, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2766
goal_identified
goal_identified
=== ep: 2767, time 64.07625794410706, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2767
goal_identified
=== ep: 2768, time 68.270099401474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2768
goal_identified
=== ep: 2769, time 60.98099064826965, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2769
goal_identified
=== ep: 2770, time 58.171592473983765, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2770
=== ep: 2771, time 59.342095136642456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2771
goal_identified
goal_identified
=== ep: 2772, time 67.27003383636475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2772
goal_identified
goal_identified
=== ep: 2773, time 65.01131939888, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2773
goal_identified
goal_identified
=== ep: 2774, time 58.30231237411499, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2774
goal_identified
=== ep: 2775, time 68.3926157951355, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2775
goal_identified
=== ep: 2776, time 65.85844230651855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2776
=== ep: 2777, time 61.68581414222717, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2777
goal_identified
=== ep: 2778, time 59.3728449344635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2778
=== ep: 2779, time 59.650145530700684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2779
goal_identified
goal_identified
goal_identified
=== ep: 2780, time 69.51718354225159, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2780
goal_identified
=== ep: 2781, time 64.35016512870789, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2781
=== ep: 2782, time 63.74032211303711, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2782
goal_identified
=== ep: 2783, time 69.16605186462402, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2783
=== ep: 2784, time 67.39503788948059, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2784
goal_identified
goal_identified
=== ep: 2785, time 67.24358749389648, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2785
=== ep: 2786, time 69.43100500106812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2786
goal_identified
=== ep: 2787, time 68.86715173721313, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2787
goal_identified
goal_identified
goal_identified
=== ep: 2788, time 70.85975313186646, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2788
goal_identified
=== ep: 2789, time 70.3544716835022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2789
=== ep: 2790, time 68.54601073265076, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2790
=== ep: 2791, time 69.50518202781677, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2791
goal_identified
=== ep: 2792, time 71.79699063301086, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2792
goal_identified
goal_identified
=== ep: 2793, time 71.32339406013489, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2793
=== ep: 2794, time 71.128427028656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2794
goal_identified
goal_identified
goal_identified
=== ep: 2795, time 72.60139489173889, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2795
goal_identified
=== ep: 2796, time 68.57425665855408, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2796
goal_identified
goal_identified
=== ep: 2797, time 70.00823760032654, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2797
=== ep: 2798, time 68.22436738014221, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2798
=== ep: 2799, time 66.01778817176819, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2799
goal_identified
=== ep: 2800, time 62.86211395263672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2800
goal_identified
goal_identified
=== ep: 2801, time 71.19418144226074, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2801
=== ep: 2802, time 66.15820455551147, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2802
goal_identified
=== ep: 2803, time 60.84303665161133, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2803
goal_identified
=== ep: 2804, time 59.979307651519775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2804
goal_identified
=== ep: 2805, time 64.5418028831482, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2805
goal_identified
=== ep: 2806, time 66.07255911827087, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2806
=== ep: 2807, time 58.88000273704529, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2807
goal_identified
=== ep: 2808, time 61.399096965789795, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2808
=== ep: 2809, time 65.26693034172058, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2809
goal_identified
=== ep: 2810, time 62.681649923324585, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2810
goal_identified
=== ep: 2811, time 59.58605623245239, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2811
=== ep: 2812, time 63.6905722618103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2812
=== ep: 2813, time 64.82351660728455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2813
goal_identified
goal_identified
goal_identified
=== ep: 2814, time 66.29338955879211, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2814
goal_identified
goal_identified
goal_identified
=== ep: 2815, time 61.52946996688843, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2815
goal_identified
goal_identified
=== ep: 2816, time 61.267815589904785, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2816
goal_identified
=== ep: 2817, time 63.48552656173706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2817
=== ep: 2818, time 67.47539114952087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2818
goal_identified
=== ep: 2819, time 68.33911204338074, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2819
goal_identified
goal_identified
=== ep: 2820, time 66.95308208465576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2820
goal_identified
goal_identified
=== ep: 2821, time 61.79011416435242, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2821
=== ep: 2822, time 57.7148494720459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2822
=== ep: 2823, time 55.54191184043884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2823
goal_identified
goal_identified
=== ep: 2824, time 60.50743556022644, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2824
goal_identified
=== ep: 2825, time 64.43808102607727, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2825
=== ep: 2826, time 63.73571753501892, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2826
goal_identified
=== ep: 2827, time 59.34229350090027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2827
=== ep: 2828, time 55.59304118156433, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2828
goal_identified
goal_identified
=== ep: 2829, time 61.4517924785614, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2829
=== ep: 2830, time 69.06636762619019, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2830
=== ep: 2831, time 65.88939929008484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2831
=== ep: 2832, time 67.3054826259613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2832
goal_identified
=== ep: 2833, time 66.77942252159119, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2833
=== ep: 2834, time 64.70644497871399, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2834
=== ep: 2835, time 65.50040602684021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2835
goal_identified
goal_identified
=== ep: 2836, time 64.55417346954346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2836
=== ep: 2837, time 54.41924500465393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2837
goal_identified
=== ep: 2838, time 55.44717621803284, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2838
=== ep: 2839, time 60.072144985198975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2839
=== ep: 2840, time 68.55796194076538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2840
goal_identified
goal_identified
=== ep: 2841, time 65.48529028892517, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2841
=== ep: 2842, time 63.661134481430054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2842
goal_identified
=== ep: 2843, time 64.67946267127991, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 67.15328216552734, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2844
goal_identified
=== ep: 2845, time 65.8697338104248, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2845
goal_identified
goal_identified
=== ep: 2846, time 67.02425456047058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2846
=== ep: 2847, time 62.077425718307495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2847
goal_identified
goal_identified
=== ep: 2848, time 55.39804220199585, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2848
goal_identified
=== ep: 2849, time 55.73270630836487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2849
goal_identified
=== ep: 2850, time 63.847904443740845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2850
goal_identified
=== ep: 2851, time 63.80138182640076, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2851
goal_identified
=== ep: 2852, time 61.06779074668884, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2852
goal_identified
=== ep: 2853, time 65.00735402107239, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2853
goal_identified
goal_identified
=== ep: 2854, time 60.70291471481323, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2854
goal_identified
=== ep: 2855, time 65.29747200012207, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2855
goal_identified
=== ep: 2856, time 68.24461340904236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2856
goal_identified
goal_identified
=== ep: 2857, time 67.19694924354553, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2857
goal_identified
=== ep: 2858, time 64.551344871521, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2858
goal_identified
goal_identified
=== ep: 2859, time 59.32297682762146, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2859
goal_identified
goal_identified
=== ep: 2860, time 53.7074077129364, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 58.934345722198486, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2861
goal_identified
goal_identified
=== ep: 2862, time 62.58575892448425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2862
=== ep: 2863, time 64.50463914871216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2863
=== ep: 2864, time 58.837881088256836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2864
=== ep: 2865, time 58.67671012878418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2865
goal_identified
=== ep: 2866, time 64.69440579414368, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2866
goal_identified
goal_identified
=== ep: 2867, time 65.53699803352356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2867
=== ep: 2868, time 66.75542426109314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2868
=== ep: 2869, time 66.48005676269531, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2869
=== ep: 2870, time 67.51822853088379, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2870
goal_identified
=== ep: 2871, time 64.8486921787262, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2871
goal_identified
goal_identified
=== ep: 2872, time 65.810476064682, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2872
=== ep: 2873, time 67.04874444007874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2873
goal_identified
goal_identified
=== ep: 2874, time 63.72772407531738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2874
goal_identified
=== ep: 2875, time 56.72377586364746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2875
goal_identified
=== ep: 2876, time 53.9305899143219, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2876
goal_identified
=== ep: 2877, time 61.68832349777222, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2877
=== ep: 2878, time 74.81489825248718, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2878
=== ep: 2879, time 63.6427903175354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2879
=== ep: 2880, time 58.31786608695984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2880
goal_identified
goal_identified
goal_identified
=== ep: 2881, time 64.51213645935059, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2881
goal_identified
=== ep: 2882, time 67.07641291618347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2882
=== ep: 2883, time 66.80214333534241, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2883
=== ep: 2884, time 66.01980304718018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2884
goal_identified
=== ep: 2885, time 67.20399713516235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2885
=== ep: 2886, time 63.17501878738403, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2886
goal_identified
=== ep: 2887, time 64.27530002593994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2887
goal_identified
goal_identified
goal_identified
=== ep: 2888, time 65.71153950691223, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2888
goal_identified
=== ep: 2889, time 56.05144786834717, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2889
goal_identified
goal_identified
=== ep: 2890, time 54.272114992141724, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2890
goal_identified
=== ep: 2891, time 61.73760366439819, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2891
goal_identified
goal_identified
goal_identified
=== ep: 2892, time 66.3462553024292, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2892
goal_identified
goal_identified
=== ep: 2893, time 67.65934586524963, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2893
goal_identified
goal_identified
=== ep: 2894, time 65.90509819984436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2894
=== ep: 2895, time 61.82814407348633, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2895
=== ep: 2896, time 71.16844749450684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2896
goal_identified
=== ep: 2897, time 67.9336769580841, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2897
goal_identified
=== ep: 2898, time 63.97166919708252, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2898
goal_identified
goal_identified
=== ep: 2899, time 57.14940595626831, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2899
goal_identified
goal_identified
=== ep: 2900, time 54.22145414352417, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2900
goal_identified
=== ep: 2901, time 63.585838317871094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2901
=== ep: 2902, time 65.27134847640991, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2902
=== ep: 2903, time 63.31742525100708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2903
=== ep: 2904, time 62.91226315498352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2904
goal_identified
goal_identified
goal_identified
=== ep: 2905, time 65.15197396278381, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2905
=== ep: 2906, time 71.87557315826416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2906
goal_identified
goal_identified
goal_identified
=== ep: 2907, time 68.27401661872864, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2907
goal_identified
=== ep: 2908, time 65.78158950805664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2908
goal_identified
=== ep: 2909, time 64.49505686759949, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2909
=== ep: 2910, time 61.131253242492676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2910
goal_identified
=== ep: 2911, time 62.63985729217529, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2911
=== ep: 2912, time 64.06442880630493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2912
=== ep: 2913, time 59.69401264190674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2913
=== ep: 2914, time 54.880451679229736, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2914
=== ep: 2915, time 58.91889977455139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2915
=== ep: 2916, time 64.34029841423035, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 67.29370284080505, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2112
goal_identified
goal_identified
=== ep: 2918, time 68.60216236114502, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2918
goal_identified
=== ep: 2919, time 71.02430248260498, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2919
goal_identified
goal_identified
=== ep: 2920, time 68.80601716041565, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2920
goal_identified
goal_identified
=== ep: 2921, time 69.27135133743286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2921
goal_identified
=== ep: 2922, time 68.62785649299622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2922
goal_identified
=== ep: 2923, time 70.54605555534363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2923
goal_identified
goal_identified
goal_identified
=== ep: 2924, time 69.8133864402771, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2924
=== ep: 2925, time 67.05617928504944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2925
=== ep: 2926, time 61.98614549636841, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2926
goal_identified
=== ep: 2927, time 60.87346291542053, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2927
goal_identified
=== ep: 2928, time 62.97846841812134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 66.0033929347992, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2929
=== ep: 2930, time 60.86800956726074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2930
goal_identified
goal_identified
=== ep: 2931, time 55.12389850616455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2931
goal_identified
=== ep: 2932, time 65.96943998336792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2932
goal_identified
=== ep: 2933, time 66.1394317150116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2933
goal_identified
=== ep: 2934, time 64.65265393257141, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2934
goal_identified
goal_identified
goal_identified
=== ep: 2935, time 61.56065368652344, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2133
goal_identified
=== ep: 2936, time 59.87190771102905, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2936
goal_identified
goal_identified
=== ep: 2937, time 62.64777731895447, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2937
goal_identified
=== ep: 2938, time 67.01595282554626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2938
goal_identified
goal_identified
=== ep: 2939, time 68.50666570663452, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2939
goal_identified
goal_identified
=== ep: 2940, time 70.52573108673096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2940
=== ep: 2941, time 76.65337371826172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2941
=== ep: 2942, time 67.4867193698883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2942
=== ep: 2943, time 67.83016920089722, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2943
=== ep: 2944, time 66.1253559589386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2944
goal_identified
goal_identified
=== ep: 2945, time 74.10523533821106, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2945
goal_identified
=== ep: 2946, time 67.26857471466064, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2946
=== ep: 2947, time 64.96670913696289, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2947
goal_identified
goal_identified
=== ep: 2948, time 61.01531481742859, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2948
goal_identified
goal_identified
goal_identified
=== ep: 2949, time 62.147557735443115, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2949
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2950, time 63.319486141204834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2178
goal_identified
goal_identified
=== ep: 2951, time 66.25703573226929, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2951
goal_identified
=== ep: 2952, time 59.17937159538269, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2952
=== ep: 2953, time 55.33335542678833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2953
goal_identified
goal_identified
=== ep: 2954, time 64.13716244697571, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2954
goal_identified
=== ep: 2955, time 67.63349914550781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2955
=== ep: 2956, time 64.44712400436401, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2956
goal_identified
=== ep: 2957, time 59.78126764297485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2957
goal_identified
=== ep: 2958, time 65.70871686935425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2958
=== ep: 2959, time 63.86311912536621, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2959
goal_identified
goal_identified
goal_identified
=== ep: 2960, time 69.33824276924133, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2960
goal_identified
=== ep: 2961, time 64.82815670967102, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2961
goal_identified
goal_identified
goal_identified
=== ep: 2962, time 62.78792142868042, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2962
=== ep: 2963, time 64.02624201774597, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2963
goal_identified
goal_identified
=== ep: 2964, time 69.17503762245178, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2964
goal_identified
=== ep: 2965, time 68.48145818710327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2965
goal_identified
=== ep: 2966, time 67.47300004959106, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2966
=== ep: 2967, time 66.98189783096313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2967
=== ep: 2968, time 69.0307183265686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2968
goal_identified
=== ep: 2969, time 71.04667115211487, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2969
=== ep: 2970, time 69.93252992630005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2970
=== ep: 2971, time 74.66467905044556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2971
=== ep: 2972, time 70.8676404953003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2972
=== ep: 2973, time 69.07613158226013, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2973
goal_identified
=== ep: 2974, time 67.00305819511414, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2974
goal_identified
=== ep: 2975, time 65.45166254043579, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2975
=== ep: 2976, time 66.21705794334412, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2976
goal_identified
goal_identified
=== ep: 2977, time 68.85090947151184, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2977
goal_identified
=== ep: 2978, time 62.828468322753906, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2978
goal_identified
goal_identified
goal_identified
=== ep: 2979, time 57.2647442817688, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2979
goal_identified
=== ep: 2980, time 60.517794132232666, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2980
goal_identified
=== ep: 2981, time 66.51685667037964, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2981
=== ep: 2982, time 65.3893232345581, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2982
goal_identified
goal_identified
=== ep: 2983, time 70.11042284965515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2983
goal_identified
=== ep: 2984, time 57.0668420791626, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2984
=== ep: 2985, time 60.032939434051514, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2985
goal_identified
=== ep: 2986, time 68.91620564460754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2986
=== ep: 2987, time 67.6587176322937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2987
goal_identified
=== ep: 2988, time 66.2112329006195, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2988
goal_identified
=== ep: 2989, time 66.82907390594482, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2989
=== ep: 2990, time 67.93078589439392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2990
goal_identified
goal_identified
=== ep: 2991, time 70.93246626853943, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2991
goal_identified
goal_identified
=== ep: 2992, time 71.71909022331238, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2992
=== ep: 2993, time 67.6828510761261, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2993
=== ep: 2994, time 65.76287722587585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2994
goal_identified
goal_identified
goal_identified
=== ep: 2995, time 63.138450384140015, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2995
goal_identified
goal_identified
=== ep: 2996, time 71.17829251289368, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2996
goal_identified
goal_identified
=== ep: 2997, time 64.82667851448059, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2997
=== ep: 2998, time 57.87471318244934, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2998
=== ep: 2999, time 56.38851976394653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2999
goal_identified
goal_identified
=== ep: 3000, time 65.18974280357361, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3000
goal_identified
=== ep: 3001, time 65.39682459831238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3001
goal_identified
goal_identified
=== ep: 3002, time 64.34581542015076, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3002
goal_identified
=== ep: 3003, time 62.833186626434326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3003
=== ep: 3004, time 64.89887428283691, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3004
=== ep: 3005, time 67.53511095046997, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3005
goal_identified
=== ep: 3006, time 71.44674324989319, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3006
=== ep: 3007, time 71.0584626197815, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3007
=== ep: 3008, time 68.48270440101624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3008
goal_identified
=== ep: 3009, time 75.61740589141846, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3009
goal_identified
=== ep: 3010, time 72.23075270652771, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3010
goal_identified
=== ep: 3011, time 69.68085718154907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3011
goal_identified
goal_identified
=== ep: 3012, time 66.89076375961304, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3012
goal_identified
goal_identified
goal_identified
=== ep: 3013, time 65.70082759857178, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3013
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3014, time 68.61334729194641, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3014
=== ep: 3015, time 63.70472478866577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3015
goal_identified
=== ep: 3016, time 59.374847650527954, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3016
=== ep: 3017, time 56.77305221557617, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3017
goal_identified
=== ep: 3018, time 64.94000291824341, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3018
=== ep: 3019, time 66.75814270973206, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3019
goal_identified
=== ep: 3020, time 67.66631960868835, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3020
goal_identified
=== ep: 3021, time 65.83746790885925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3021
goal_identified
=== ep: 3022, time 69.09785437583923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3022
goal_identified
=== ep: 3023, time 68.39148902893066, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3023
goal_identified
=== ep: 3024, time 71.23118734359741, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3024
goal_identified
=== ep: 3025, time 71.03134322166443, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3025
goal_identified
goal_identified
=== ep: 3026, time 68.73763489723206, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3026
goal_identified
=== ep: 3027, time 67.12184381484985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3027
=== ep: 3028, time 67.81272721290588, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3028
=== ep: 3029, time 68.35414266586304, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3029
goal_identified
=== ep: 3030, time 61.94869518280029, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3030
=== ep: 3031, time 57.669519901275635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3031
goal_identified
=== ep: 3032, time 60.82260203361511, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3032
goal_identified
=== ep: 3033, time 66.97071671485901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3033
=== ep: 3034, time 64.71738314628601, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3034
=== ep: 3035, time 61.81829309463501, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3035
goal_identified
=== ep: 3036, time 58.493295669555664, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3036
=== ep: 3037, time 67.36881899833679, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3037
=== ep: 3038, time 65.34193205833435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3038
=== ep: 3039, time 66.15461421012878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3039
goal_identified
=== ep: 3040, time 67.02411389350891, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 70.00047636032104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3041
goal_identified
goal_identified
=== ep: 3042, time 70.15690779685974, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3042
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3043, time 69.39294838905334, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2293
goal_identified
goal_identified
=== ep: 3044, time 73.00957107543945, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3044
=== ep: 3045, time 72.44107699394226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3045
goal_identified
=== ep: 3046, time 69.2138729095459, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3046
goal_identified
goal_identified
=== ep: 3047, time 68.63216352462769, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3047
goal_identified
=== ep: 3048, time 75.49517393112183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3048
=== ep: 3049, time 70.15929412841797, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3049
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3050, time 66.59570789337158, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2344
goal_identified
goal_identified
=== ep: 3051, time 66.95509028434753, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3051
=== ep: 3052, time 63.31581902503967, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3052
goal_identified
=== ep: 3053, time 54.36004066467285, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3053
goal_identified
=== ep: 3054, time 65.95995783805847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3054
goal_identified
goal_identified
goal_identified
=== ep: 3055, time 64.99842548370361, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3055
=== ep: 3056, time 63.50781202316284, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3056
goal_identified
goal_identified
=== ep: 3057, time 57.05166530609131, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3057
=== ep: 3058, time 59.571603536605835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3058
goal_identified
goal_identified
=== ep: 3059, time 65.75687885284424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3059
goal_identified
=== ep: 3060, time 67.53433632850647, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3060
goal_identified
goal_identified
=== ep: 3061, time 68.80582284927368, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3061
=== ep: 3062, time 68.16453170776367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3062
goal_identified
goal_identified
=== ep: 3063, time 69.63858771324158, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3063
goal_identified
goal_identified
=== ep: 3064, time 69.6050341129303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3064
=== ep: 3065, time 65.19956707954407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3065
goal_identified
goal_identified
=== ep: 3066, time 63.184515953063965, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3066
goal_identified
goal_identified
=== ep: 3067, time 64.48566603660583, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3067
goal_identified
=== ep: 3068, time 67.95722937583923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3068
goal_identified
goal_identified
=== ep: 3069, time 56.326849699020386, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3069
goal_identified
=== ep: 3070, time 56.65416765213013, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3070
goal_identified
=== ep: 3071, time 64.47820830345154, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3071
=== ep: 3072, time 62.71663737297058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3072
=== ep: 3073, time 62.034353494644165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3073
=== ep: 3074, time 61.7470703125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3074
goal_identified
goal_identified
=== ep: 3075, time 71.77558612823486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3075
goal_identified
goal_identified
=== ep: 3076, time 66.10405373573303, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3076
goal_identified
goal_identified
=== ep: 3077, time 66.67412614822388, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3077
goal_identified
=== ep: 3078, time 66.46154522895813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3078
goal_identified
=== ep: 3079, time 68.81118607521057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3079
goal_identified
goal_identified
=== ep: 3080, time 68.74162292480469, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3080
=== ep: 3081, time 64.00648021697998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3081
=== ep: 3082, time 59.67154598236084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3082
=== ep: 3083, time 68.24124431610107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3083
goal_identified
=== ep: 3084, time 65.46020007133484, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3084
=== ep: 3085, time 57.085952043533325, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3085
goal_identified
goal_identified
=== ep: 3086, time 56.372979402542114, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3086
goal_identified
goal_identified
goal_identified
=== ep: 3087, time 65.0574893951416, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3087
=== ep: 3088, time 65.2795341014862, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3088
=== ep: 3089, time 67.9839699268341, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3089
goal_identified
=== ep: 3090, time 67.260666847229, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3090
goal_identified
=== ep: 3091, time 66.96656775474548, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3091
goal_identified
=== ep: 3092, time 67.85139727592468, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3092
=== ep: 3093, time 69.21315217018127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3093
=== ep: 3094, time 68.6055018901825, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3094
goal_identified
=== ep: 3095, time 67.46128511428833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3095
=== ep: 3096, time 63.14585471153259, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3096
=== ep: 3097, time 62.671398639678955, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 60.997251749038696, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3098
=== ep: 3099, time 53.8825740814209, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3099
=== ep: 3100, time 60.864755392074585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3100
goal_identified
goal_identified
=== ep: 3101, time 66.25609827041626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3101
goal_identified
=== ep: 3102, time 63.092031717300415, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3102
=== ep: 3103, time 64.08954453468323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3103
=== ep: 3104, time 67.55197858810425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3104
=== ep: 3105, time 66.87245917320251, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3105
goal_identified
=== ep: 3106, time 65.575190782547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3106
goal_identified
=== ep: 3107, time 67.57290410995483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3107
goal_identified
goal_identified
=== ep: 3108, time 67.3136887550354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3108
goal_identified
goal_identified
=== ep: 3109, time 64.25737738609314, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3109
goal_identified
=== ep: 3110, time 60.003559827804565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3110
=== ep: 3111, time 61.390695333480835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3111
=== ep: 3112, time 64.70228147506714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3112
goal_identified
=== ep: 3113, time 57.95365524291992, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3113
goal_identified
goal_identified
=== ep: 3114, time 54.866780042648315, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3114
=== ep: 3115, time 63.98122692108154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3115
goal_identified
=== ep: 3116, time 63.589195728302, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3116
=== ep: 3117, time 64.6207947731018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3117
=== ep: 3118, time 64.38925695419312, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3119, time 66.71230745315552, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2414
goal_identified
goal_identified
=== ep: 3120, time 67.74056077003479, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3120
goal_identified
goal_identified
=== ep: 3121, time 68.55388164520264, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3121
goal_identified
=== ep: 3122, time 66.82360005378723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3122
=== ep: 3123, time 65.8512077331543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3123
goal_identified
=== ep: 3124, time 62.00500988960266, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3124
=== ep: 3125, time 61.54612159729004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3125
goal_identified
=== ep: 3126, time 64.49781012535095, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3126
=== ep: 3127, time 56.83257794380188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3127
goal_identified
=== ep: 3128, time 56.717318534851074, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3128
=== ep: 3129, time 63.73839592933655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3129
goal_identified
=== ep: 3130, time 63.54727578163147, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3130
goal_identified
=== ep: 3131, time 64.51114177703857, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3131
=== ep: 3132, time 70.46890425682068, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3132
goal_identified
goal_identified
=== ep: 3133, time 65.74686336517334, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3133
goal_identified
=== ep: 3134, time 63.99184775352478, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3134
goal_identified
goal_identified
goal_identified
=== ep: 3135, time 68.50525784492493, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3135
goal_identified
=== ep: 3136, time 71.56493306159973, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3136
goal_identified
goal_identified
=== ep: 3137, time 67.38215136528015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3137
goal_identified
=== ep: 3138, time 64.898353099823, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3138
goal_identified
goal_identified
=== ep: 3139, time 65.16658329963684, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3139
goal_identified
=== ep: 3140, time 61.74583911895752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3140
goal_identified
=== ep: 3141, time 53.79791307449341, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3141
goal_identified
=== ep: 3142, time 58.75143098831177, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 63.88168406486511, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3143
goal_identified
goal_identified
=== ep: 3144, time 61.056074142456055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3144
goal_identified
goal_identified
=== ep: 3145, time 68.64316272735596, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3145
goal_identified
=== ep: 3146, time 65.81170153617859, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3146
=== ep: 3147, time 65.07073736190796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3147
goal_identified
=== ep: 3148, time 66.77478504180908, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3148
=== ep: 3149, time 70.54725646972656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3149
goal_identified
=== ep: 3150, time 75.4422857761383, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3150
goal_identified
=== ep: 3151, time 68.97812533378601, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3151
=== ep: 3152, time 65.66070771217346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3152
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3153, time 63.710431814193726, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2492
goal_identified
=== ep: 3154, time 64.16172647476196, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3154
goal_identified
goal_identified
=== ep: 3155, time 63.19217801094055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3155
=== ep: 3156, time 52.931541204452515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3156
=== ep: 3157, time 65.49515628814697, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3157
goal_identified
=== ep: 3158, time 61.27790880203247, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3158
=== ep: 3159, time 62.871021032333374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3159
goal_identified
goal_identified
=== ep: 3160, time 58.139721393585205, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3160
goal_identified
goal_identified
=== ep: 3161, time 66.01060128211975, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3161
goal_identified
goal_identified
goal_identified
=== ep: 3162, time 63.30598998069763, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3162
goal_identified
=== ep: 3163, time 62.9116735458374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3163
goal_identified
=== ep: 3164, time 66.72044730186462, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3164
=== ep: 3165, time 69.13125658035278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3165
=== ep: 3166, time 66.73492550849915, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 66.07359266281128, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3167
goal_identified
goal_identified
=== ep: 3168, time 65.47570586204529, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3168
goal_identified
=== ep: 3169, time 60.39243459701538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3169
=== ep: 3170, time 51.75263500213623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3170
goal_identified
goal_identified
=== ep: 3171, time 62.139498233795166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3171
goal_identified
=== ep: 3172, time 63.138168811798096, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3172
goal_identified
=== ep: 3173, time 64.39768147468567, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3173
goal_identified
goal_identified
=== ep: 3174, time 63.55747747421265, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3174
goal_identified
goal_identified
goal_identified
=== ep: 3175, time 66.10456228256226, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3175
=== ep: 3176, time 65.67653179168701, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3176
goal_identified
=== ep: 3177, time 67.08839011192322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3177
=== ep: 3178, time 68.22373652458191, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3178
goal_identified
=== ep: 3179, time 66.97570204734802, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3179
=== ep: 3180, time 63.77966547012329, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3180
=== ep: 3181, time 62.55474901199341, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3181
=== ep: 3182, time 59.43276619911194, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3182
goal_identified
goal_identified
=== ep: 3183, time 55.20352077484131, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3183
goal_identified
=== ep: 3184, time 57.87473464012146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3184
goal_identified
=== ep: 3185, time 67.22736239433289, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3185
goal_identified
=== ep: 3186, time 63.69118046760559, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3186
goal_identified
=== ep: 3187, time 63.539013147354126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3187
=== ep: 3188, time 69.41606545448303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3188
goal_identified
goal_identified
=== ep: 3189, time 68.45913934707642, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3189
=== ep: 3190, time 65.98397588729858, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3190
=== ep: 3191, time 71.3573751449585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3191
goal_identified
goal_identified
goal_identified
=== ep: 3192, time 66.44726228713989, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3192
goal_identified
goal_identified
=== ep: 3193, time 59.17284083366394, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3193
=== ep: 3194, time 57.27513813972473, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3194
goal_identified
=== ep: 3195, time 62.02720522880554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3195
goal_identified
=== ep: 3196, time 61.74962782859802, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3196
goal_identified
=== ep: 3197, time 55.90432953834534, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3197
goal_identified
goal_identified
goal_identified
=== ep: 3198, time 58.73091459274292, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3198
goal_identified
=== ep: 3199, time 65.81656455993652, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3199
=== ep: 3200, time 67.88647174835205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3200
goal_identified
goal_identified
goal_identified
=== ep: 3201, time 66.056645154953, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3201
=== ep: 3202, time 66.40449786186218, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3202
goal_identified
goal_identified
=== ep: 3203, time 69.61232686042786, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3203
=== ep: 3204, time 57.71451020240784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3204
=== ep: 3205, time 57.12578749656677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3205
goal_identified
=== ep: 3206, time 55.49081087112427, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3206
goal_identified
=== ep: 3207, time 50.767231702804565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3207
goal_identified
goal_identified
=== ep: 3208, time 54.11265707015991, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3208
=== ep: 3209, time 55.66751980781555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3209
=== ep: 3210, time 55.212072134017944, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3210
=== ep: 3211, time 58.49379849433899, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3211
goal_identified
=== ep: 3212, time 59.871023654937744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3212
goal_identified
=== ep: 3213, time 61.685413122177124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3213
=== ep: 3214, time 64.8806517124176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3214
=== ep: 3215, time 63.482187032699585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3215
goal_identified
goal_identified
=== ep: 3216, time 63.614513874053955, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3216
goal_identified
=== ep: 3217, time 61.75617527961731, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3217
=== ep: 3218, time 60.792813539505005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3218
goal_identified
=== ep: 3219, time 55.45004200935364, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3219
goal_identified
goal_identified
=== ep: 3220, time 54.534241676330566, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3220
goal_identified
=== ep: 3221, time 50.63898277282715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3221
=== ep: 3222, time 57.79838252067566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3222
=== ep: 3223, time 52.24752998352051, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3223
goal_identified
=== ep: 3224, time 53.29454731941223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3224
=== ep: 3225, time 55.85287284851074, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3225
goal_identified
=== ep: 3226, time 57.15135622024536, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3226
=== ep: 3227, time 57.23686361312866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3227
goal_identified
=== ep: 3228, time 62.51767420768738, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3228
=== ep: 3229, time 62.5264310836792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3229
=== ep: 3230, time 61.838016510009766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3230
goal_identified
=== ep: 3231, time 58.27194333076477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3231
goal_identified
=== ep: 3232, time 55.81545662879944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3232
goal_identified
=== ep: 3233, time 60.43198037147522, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3233
goal_identified
goal_identified
=== ep: 3234, time 53.494678258895874, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3234
=== ep: 3235, time 52.01179313659668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3235
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3236, time 52.11917209625244, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2705
=== ep: 3237, time 53.76659083366394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3237
=== ep: 3238, time 56.32857823371887, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3238
goal_identified
=== ep: 3239, time 57.48163318634033, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3239
goal_identified
goal_identified
=== ep: 3240, time 58.57067513465881, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3240
goal_identified
goal_identified
=== ep: 3241, time 59.35126543045044, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3241
=== ep: 3242, time 60.80363392829895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3242
goal_identified
goal_identified
=== ep: 3243, time 60.707843542099, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3243
goal_identified
=== ep: 3244, time 65.55107760429382, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3244
goal_identified
=== ep: 3245, time 65.59366083145142, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3245
goal_identified
goal_identified
goal_identified
=== ep: 3246, time 64.63448762893677, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3246
=== ep: 3247, time 66.69845771789551, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3247
goal_identified
=== ep: 3248, time 63.785261154174805, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3248
goal_identified
=== ep: 3249, time 61.76646566390991, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3249
goal_identified
=== ep: 3250, time 61.20716881752014, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3250
goal_identified
goal_identified
=== ep: 3251, time 58.85361170768738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3251
goal_identified
=== ep: 3252, time 57.06465935707092, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3252
=== ep: 3253, time 53.30571532249451, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3253
=== ep: 3254, time 53.00198578834534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3254
goal_identified
=== ep: 3255, time 52.805153608322144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3255
=== ep: 3256, time 56.30111837387085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3256
goal_identified
goal_identified
=== ep: 3257, time 60.289167642593384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3257
=== ep: 3258, time 58.45968580245972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3258
=== ep: 3259, time 59.80317211151123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3259
goal_identified
=== ep: 3260, time 61.964866638183594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3260
goal_identified
=== ep: 3261, time 68.40035772323608, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3261
goal_identified
goal_identified
=== ep: 3262, time 61.25496506690979, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3262
=== ep: 3263, time 62.90903043746948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3263
goal_identified
goal_identified
=== ep: 3264, time 63.53756666183472, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3264
goal_identified
goal_identified
=== ep: 3265, time 65.27303385734558, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3265
goal_identified
=== ep: 3266, time 66.24445390701294, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3266
goal_identified
=== ep: 3267, time 63.40376615524292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3267
=== ep: 3268, time 59.50628113746643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3268
goal_identified
goal_identified
=== ep: 3269, time 58.593570709228516, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3269
=== ep: 3270, time 52.424384117126465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3270
goal_identified
=== ep: 3271, time 55.31909394264221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3271
goal_identified
goal_identified
goal_identified
=== ep: 3272, time 52.23433184623718, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3272
=== ep: 3273, time 52.83230686187744, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3273
goal_identified
=== ep: 3274, time 54.594823122024536, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3274
goal_identified
=== ep: 3275, time 59.67716693878174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3275
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3276, time 54.624021768569946, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2748
goal_identified
goal_identified
=== ep: 3277, time 54.34132719039917, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3277
goal_identified
=== ep: 3278, time 57.22092366218567, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3278
=== ep: 3279, time 59.04001784324646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3279
goal_identified
=== ep: 3280, time 61.47311043739319, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3280
=== ep: 3281, time 60.366955280303955, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3281
goal_identified
=== ep: 3282, time 61.64406657218933, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3282
=== ep: 3283, time 59.900824308395386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3283
=== ep: 3284, time 57.06742286682129, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3284
=== ep: 3285, time 54.421308755874634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3285
goal_identified
=== ep: 3286, time 51.145989656448364, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3286
goal_identified
=== ep: 3287, time 52.69774842262268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3287
goal_identified
=== ep: 3288, time 53.36616277694702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3288
goal_identified
goal_identified
goal_identified
=== ep: 3289, time 53.638118267059326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3289
goal_identified
goal_identified
=== ep: 3290, time 57.6713593006134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3290
=== ep: 3291, time 60.76302671432495, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3291
goal_identified
goal_identified
=== ep: 3292, time 57.98632287979126, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3292
=== ep: 3293, time 59.00452017784119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3293
=== ep: 3294, time 63.75386643409729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3294
=== ep: 3295, time 64.35562562942505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3295
goal_identified
=== ep: 3296, time 63.303966760635376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3296
goal_identified
goal_identified
=== ep: 3297, time 61.78907513618469, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3297
goal_identified
=== ep: 3298, time 59.98646521568298, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3298
goal_identified
goal_identified
=== ep: 3299, time 58.73864674568176, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3299
=== ep: 3300, time 57.75929927825928, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3300
goal_identified
goal_identified
goal_identified
=== ep: 3301, time 54.492587089538574, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3301
goal_identified
goal_identified
goal_identified
=== ep: 3302, time 51.86318111419678, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3302
goal_identified
=== ep: 3303, time 50.556777000427246, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3303
goal_identified
goal_identified
=== ep: 3304, time 54.10240292549133, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3304
goal_identified
=== ep: 3305, time 54.986549854278564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3305
goal_identified
=== ep: 3306, time 62.557307720184326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3306
=== ep: 3307, time 59.56966519355774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3307
goal_identified
=== ep: 3308, time 59.442060232162476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3308
goal_identified
=== ep: 3309, time 62.53678798675537, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3309
goal_identified
=== ep: 3310, time 64.60521173477173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3310
goal_identified
goal_identified
goal_identified
=== ep: 3311, time 63.24582600593567, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3311
=== ep: 3312, time 62.92902421951294, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3312
goal_identified
=== ep: 3313, time 59.66963243484497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3313
=== ep: 3314, time 55.96287751197815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3314
goal_identified
=== ep: 3315, time 56.269614696502686, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3315
goal_identified
goal_identified
=== ep: 3316, time 66.80478572845459, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3316
goal_identified
goal_identified
=== ep: 3317, time 56.13173842430115, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3317
goal_identified
=== ep: 3318, time 52.858083963394165, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3318
=== ep: 3319, time 52.764376640319824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3319
=== ep: 3320, time 55.52701663970947, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3320
=== ep: 3321, time 57.42629146575928, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3321
=== ep: 3322, time 56.567410707473755, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3322
=== ep: 3323, time 53.62886381149292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3323
goal_identified
=== ep: 3324, time 60.257596015930176, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3324
goal_identified
=== ep: 3325, time 62.31220245361328, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3325
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3326, time 63.07456088066101, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3326
goal_identified
goal_identified
=== ep: 3327, time 62.3215229511261, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3327
goal_identified
goal_identified
=== ep: 3328, time 62.13165473937988, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3328
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3329, time 58.949440002441406, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2917
goal_identified
goal_identified
=== ep: 3330, time 56.357431411743164, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3330
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3331, time 52.69433045387268, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2935
goal_identified
goal_identified
goal_identified
=== ep: 3332, time 50.13919115066528, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3332
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3333, time 52.752137660980225, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2950
goal_identified
goal_identified
=== ep: 3334, time 54.91379714012146, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3334
goal_identified
=== ep: 3335, time 57.84374165534973, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3335
goal_identified
=== ep: 3336, time 66.08676600456238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3336
goal_identified
goal_identified
=== ep: 3337, time 60.003217458724976, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3337
=== ep: 3338, time 60.88835859298706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3338
=== ep: 3339, time 56.00190758705139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3339
=== ep: 3340, time 53.36327767372131, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3340
=== ep: 3341, time 50.7657835483551, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3341
=== ep: 3342, time 49.368138790130615, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3342
goal_identified
goal_identified
goal_identified
=== ep: 3343, time 53.52289652824402, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3343
goal_identified
=== ep: 3344, time 55.84929633140564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3344
goal_identified
goal_identified
=== ep: 3345, time 60.23189043998718, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3345
=== ep: 3346, time 59.57261681556702, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3346
goal_identified
=== ep: 3347, time 57.487927198410034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3347
goal_identified
=== ep: 3348, time 57.85039043426514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3348
=== ep: 3349, time 53.66526126861572, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3349
goal_identified
goal_identified
goal_identified
=== ep: 3350, time 53.8965368270874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3350
goal_identified
goal_identified
=== ep: 3351, time 48.09576416015625, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3351
=== ep: 3352, time 51.7166383266449, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3352
=== ep: 3353, time 59.64900255203247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3353
goal_identified
=== ep: 3354, time 54.61312508583069, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3354
goal_identified
=== ep: 3355, time 57.28791928291321, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3355
goal_identified
goal_identified
=== ep: 3356, time 60.48801302909851, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3356
goal_identified
=== ep: 3357, time 59.30058431625366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3357
goal_identified
goal_identified
goal_identified
=== ep: 3358, time 57.13409066200256, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3358
=== ep: 3359, time 53.37396955490112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3359
=== ep: 3360, time 51.268943071365356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3360
=== ep: 3361, time 48.831117153167725, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3361
=== ep: 3362, time 52.47661232948303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3362
goal_identified
=== ep: 3363, time 54.332767724990845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3363
=== ep: 3364, time 57.194907903671265, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3364
goal_identified
=== ep: 3365, time 59.971383810043335, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3365
goal_identified
=== ep: 3366, time 61.00206160545349, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3366
goal_identified
goal_identified
=== ep: 3367, time 56.2998788356781, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3367
goal_identified
=== ep: 3368, time 54.54187870025635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3368
=== ep: 3369, time 51.66418981552124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3369
goal_identified
goal_identified
goal_identified
=== ep: 3370, time 49.02690863609314, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3370
goal_identified
goal_identified
=== ep: 3371, time 56.756216049194336, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3371
goal_identified
goal_identified
=== ep: 3372, time 54.53705191612244, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3372
=== ep: 3373, time 56.03216600418091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3373
=== ep: 3374, time 58.89596652984619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3374
goal_identified
=== ep: 3375, time 60.791311740875244, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3375
goal_identified
=== ep: 3376, time 58.98774552345276, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3376
goal_identified
=== ep: 3377, time 56.22649359703064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3377
goal_identified
goal_identified
=== ep: 3378, time 52.30219650268555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3378
goal_identified
goal_identified
goal_identified
=== ep: 3379, time 49.96391582489014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3379
goal_identified
=== ep: 3380, time 50.2101788520813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3380
goal_identified
goal_identified
=== ep: 3381, time 53.08723449707031, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3381
=== ep: 3382, time 54.13640642166138, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3382
goal_identified
=== ep: 3383, time 56.856117486953735, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3383
=== ep: 3384, time 59.45035934448242, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3384
goal_identified
=== ep: 3385, time 57.984761476516724, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3385
=== ep: 3386, time 54.70243978500366, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3386
goal_identified
=== ep: 3387, time 53.86949849128723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3387
=== ep: 3388, time 56.0306875705719, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3388
=== ep: 3389, time 49.12959027290344, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3389
goal_identified
=== ep: 3390, time 51.52874135971069, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3390
=== ep: 3391, time 52.95018124580383, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3391
=== ep: 3392, time 57.24909210205078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3392
=== ep: 3393, time 58.79535937309265, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3393
goal_identified
goal_identified
=== ep: 3394, time 59.97072887420654, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3394
=== ep: 3395, time 58.08661675453186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3395
=== ep: 3396, time 57.564736127853394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3396
goal_identified
goal_identified
=== ep: 3397, time 51.96814155578613, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3397
goal_identified
goal_identified
=== ep: 3398, time 49.315871238708496, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3398
goal_identified
goal_identified
=== ep: 3399, time 51.30838370323181, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3399
=== ep: 3400, time 57.17342233657837, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3400
goal_identified
=== ep: 3401, time 57.29207754135132, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3401
goal_identified
goal_identified
=== ep: 3402, time 60.22215962409973, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3402
goal_identified
=== ep: 3403, time 59.54959487915039, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3403
goal_identified
=== ep: 3404, time 57.013927936553955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3404
goal_identified
goal_identified
=== ep: 3405, time 58.445027351379395, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3405
=== ep: 3406, time 52.05883812904358, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3406
=== ep: 3407, time 49.29827690124512, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3407
goal_identified
goal_identified
=== ep: 3408, time 50.61538624763489, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3408
goal_identified
=== ep: 3409, time 52.6914222240448, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3409
goal_identified
goal_identified
goal_identified
=== ep: 3410, time 56.77130699157715, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3410
goal_identified
=== ep: 3411, time 58.978681564331055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3411
=== ep: 3412, time 60.59817934036255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3412
goal_identified
goal_identified
=== ep: 3413, time 57.65397119522095, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3413
=== ep: 3414, time 55.315508127212524, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3414
goal_identified
=== ep: 3415, time 51.698469161987305, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3415
goal_identified
goal_identified
goal_identified
=== ep: 3416, time 49.66658091545105, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3416
goal_identified
=== ep: 3417, time 51.56783699989319, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3417
goal_identified
=== ep: 3418, time 53.92787957191467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3418
=== ep: 3419, time 55.61135768890381, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3419
=== ep: 3420, time 59.39855456352234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3420
goal_identified
=== ep: 3421, time 60.26589894294739, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3421
goal_identified
goal_identified
=== ep: 3422, time 64.76326370239258, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3422
goal_identified
goal_identified
goal_identified
=== ep: 3423, time 58.695048809051514, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3423
=== ep: 3424, time 55.90393805503845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3424
=== ep: 3425, time 53.40214967727661, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3425
goal_identified
goal_identified
=== ep: 3426, time 48.39449858665466, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3426
goal_identified
=== ep: 3427, time 52.952282667160034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3427
goal_identified
=== ep: 3428, time 55.38535809516907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3428
goal_identified
=== ep: 3429, time 59.733562707901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3429
goal_identified
=== ep: 3430, time 59.54726696014404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3430
goal_identified
goal_identified
=== ep: 3431, time 57.88543486595154, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3431
=== ep: 3432, time 56.02039575576782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3432
=== ep: 3433, time 52.15320372581482, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3433
=== ep: 3434, time 48.69596028327942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3434
=== ep: 3435, time 51.41150403022766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3435
goal_identified
goal_identified
=== ep: 3436, time 53.80735468864441, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3436
=== ep: 3437, time 55.701441287994385, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3437
goal_identified
=== ep: 3438, time 57.1661114692688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3438
goal_identified
goal_identified
=== ep: 3439, time 60.8442816734314, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3439
goal_identified
=== ep: 3440, time 65.82084488868713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3440
goal_identified
goal_identified
=== ep: 3441, time 58.65815758705139, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3441
goal_identified
goal_identified
=== ep: 3442, time 55.61939263343811, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3442
goal_identified
=== ep: 3443, time 58.45288586616516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3444, time 51.13821578025818, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3043
goal_identified
=== ep: 3445, time 48.14233613014221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3445
goal_identified
goal_identified
=== ep: 3446, time 50.43707585334778, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3446
goal_identified
=== ep: 3447, time 54.55453944206238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 58.98616099357605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 3448
goal_identified
goal_identified
goal_identified
=== ep: 3449, time 58.612713098526, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3050
goal_identified
goal_identified
goal_identified
=== ep: 3450, time 57.11610150337219, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3450
goal_identified
goal_identified
=== ep: 3451, time 52.64219927787781, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3451
goal_identified
=== ep: 3452, time 48.91195106506348, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3452
goal_identified
=== ep: 3453, time 50.08434057235718, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3453
goal_identified
=== ep: 3454, time 53.02106976509094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3454
goal_identified
goal_identified
=== ep: 3455, time 57.66382384300232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3455
=== ep: 3456, time 59.69175100326538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3456
=== ep: 3457, time 55.619088649749756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3457
goal_identified
goal_identified
=== ep: 3458, time 59.3786506652832, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3458
goal_identified
=== ep: 3459, time 48.88359093666077, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3459
=== ep: 3460, time 48.389143228530884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3460
goal_identified
=== ep: 3461, time 52.33362650871277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3461
=== ep: 3462, time 56.02880573272705, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3462
goal_identified
goal_identified
=== ep: 3463, time 58.42056918144226, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3463
goal_identified
=== ep: 3464, time 60.54493737220764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3464
=== ep: 3465, time 56.80375838279724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3465
goal_identified
=== ep: 3466, time 52.76761198043823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3466
goal_identified
goal_identified
=== ep: 3467, time 47.86400294303894, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3467
=== ep: 3468, time 50.66347098350525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3468
goal_identified
=== ep: 3469, time 54.94165658950806, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3469
goal_identified
goal_identified
=== ep: 3470, time 59.66048860549927, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3470
goal_identified
=== ep: 3471, time 59.542640924453735, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3471
goal_identified
goal_identified
=== ep: 3472, time 56.022296667099, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3472
goal_identified
goal_identified
goal_identified
=== ep: 3473, time 52.75318002700806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3473
goal_identified
=== ep: 3474, time 48.22449803352356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3474
goal_identified
goal_identified
goal_identified
=== ep: 3475, time 49.74080562591553, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3475
goal_identified
goal_identified
=== ep: 3476, time 53.27049803733826, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3476
=== ep: 3477, time 61.56658434867859, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3477
=== ep: 3478, time 59.756866455078125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3478
goal_identified
goal_identified
=== ep: 3479, time 58.19828796386719, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3479
=== ep: 3480, time 54.901851415634155, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3480
goal_identified
=== ep: 3481, time 51.7703115940094, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3481
=== ep: 3482, time 48.8481388092041, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3482
goal_identified
=== ep: 3483, time 52.85671663284302, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3483
=== ep: 3484, time 57.087066411972046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3484
goal_identified
=== ep: 3485, time 60.168509006500244, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3485
goal_identified
=== ep: 3486, time 66.25022578239441, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3486
goal_identified
=== ep: 3487, time 58.80291724205017, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3487
goal_identified
=== ep: 3488, time 55.357579946517944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3488
=== ep: 3489, time 50.73918175697327, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3489
=== ep: 3490, time 47.789299964904785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3490
goal_identified
goal_identified
goal_identified
=== ep: 3491, time 51.033196449279785, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3491
=== ep: 3492, time 56.10805130004883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3492
goal_identified
goal_identified
=== ep: 3493, time 60.28938817977905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3493
goal_identified
=== ep: 3494, time 58.980255365371704, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3494
goal_identified
goal_identified
=== ep: 3495, time 62.01338315010071, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3495
=== ep: 3496, time 54.4340603351593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3496
goal_identified
goal_identified
goal_identified
=== ep: 3497, time 47.89013147354126, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3497
goal_identified
=== ep: 3498, time 49.507120847702026, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3498
goal_identified
=== ep: 3499, time 53.334189653396606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3499
=== ep: 3500, time 57.995484828948975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3500
=== ep: 3501, time 59.61458683013916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3501
=== ep: 3502, time 64.26808667182922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3502
=== ep: 3503, time 55.170231342315674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3503
goal_identified
=== ep: 3504, time 50.06641387939453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3504
goal_identified
goal_identified
goal_identified
=== ep: 3505, time 48.907994747161865, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3505
goal_identified
goal_identified
=== ep: 3506, time 51.01006746292114, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3506
goal_identified
=== ep: 3507, time 55.73661255836487, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3507
goal_identified
=== ep: 3508, time 57.614673137664795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3508
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3509, time 60.22578454017639, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3509
goal_identified
goal_identified
goal_identified
=== ep: 3510, time 58.72492337226868, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3510
=== ep: 3511, time 50.92340660095215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3511
goal_identified
=== ep: 3512, time 48.42235493659973, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3512
=== ep: 3513, time 55.623528480529785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3513
goal_identified
=== ep: 3514, time 52.688023805618286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3514
=== ep: 3515, time 54.65242028236389, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3515
goal_identified
goal_identified
goal_identified
=== ep: 3516, time 57.803504943847656, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3516
goal_identified
=== ep: 3517, time 60.382142543792725, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3517
goal_identified
=== ep: 3518, time 56.65245008468628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3518
=== ep: 3519, time 53.78986859321594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3519
goal_identified
=== ep: 3520, time 49.6178240776062, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3520
goal_identified
=== ep: 3521, time 48.85182166099548, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3521
goal_identified
=== ep: 3522, time 51.22423028945923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3522
goal_identified
goal_identified
=== ep: 3523, time 56.1600136756897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3523
goal_identified
=== ep: 3524, time 58.07306885719299, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3524
goal_identified
goal_identified
goal_identified
=== ep: 3525, time 57.23637104034424, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3525
goal_identified
=== ep: 3526, time 54.37893581390381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3526
=== ep: 3527, time 52.06979751586914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3527
goal_identified
=== ep: 3528, time 48.50938105583191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3528
goal_identified
=== ep: 3529, time 51.916906118392944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3529
=== ep: 3530, time 55.042386531829834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3530
goal_identified
=== ep: 3531, time 57.659239768981934, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3531
=== ep: 3532, time 63.463276624679565, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3532
goal_identified
=== ep: 3533, time 58.8011360168457, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3533
goal_identified
=== ep: 3534, time 57.95711541175842, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3534
goal_identified
goal_identified
=== ep: 3535, time 53.10702681541443, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3535
goal_identified
goal_identified
goal_identified
=== ep: 3536, time 48.795947551727295, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3536
=== ep: 3537, time 49.32359766960144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3537
=== ep: 3538, time 52.67367434501648, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3538
goal_identified
goal_identified
=== ep: 3539, time 54.8144416809082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3539
=== ep: 3540, time 57.99848771095276, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3540
goal_identified
goal_identified
=== ep: 3541, time 56.88219690322876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3541
goal_identified
goal_identified
=== ep: 3542, time 52.886025190353394, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3542
=== ep: 3543, time 48.63421392440796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3543
=== ep: 3544, time 49.26372671127319, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3544
=== ep: 3545, time 56.171366691589355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3545
goal_identified
=== ep: 3546, time 52.39315366744995, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3546
=== ep: 3547, time 56.83448886871338, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3547
goal_identified
=== ep: 3548, time 59.23241567611694, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3548
=== ep: 3549, time 58.121618032455444, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3549
=== ep: 3550, time 55.32320499420166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3550
goal_identified
goal_identified
=== ep: 3551, time 58.882834911346436, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3551
=== ep: 3552, time 47.90737795829773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3552
goal_identified
goal_identified
=== ep: 3553, time 48.83636164665222, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3553
=== ep: 3554, time 53.27626943588257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3554
=== ep: 3555, time 59.302627086639404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3555
=== ep: 3556, time 58.82860541343689, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3556
=== ep: 3557, time 57.985660552978516, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3557
goal_identified
goal_identified
goal_identified
=== ep: 3558, time 55.502429485321045, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3558
goal_identified
=== ep: 3559, time 50.73435568809509, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3559
goal_identified
=== ep: 3560, time 47.556663036346436, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3560
=== ep: 3561, time 52.52592158317566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3561
goal_identified
goal_identified
=== ep: 3562, time 54.31210780143738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3562
=== ep: 3563, time 58.50861644744873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3563
goal_identified
=== ep: 3564, time 58.320855379104614, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3564
goal_identified
=== ep: 3565, time 56.022433042526245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3565
=== ep: 3566, time 53.02646017074585, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3566
goal_identified
goal_identified
=== ep: 3567, time 47.45775866508484, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3567
goal_identified
=== ep: 3568, time 50.82602834701538, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3568
goal_identified
=== ep: 3569, time 53.717692136764526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3569
goal_identified
goal_identified
=== ep: 3570, time 61.304348945617676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3570
=== ep: 3571, time 59.93813633918762, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3571
goal_identified
=== ep: 3572, time 60.59900259971619, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3572
goal_identified
=== ep: 3573, time 56.756428718566895, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3573
=== ep: 3574, time 52.27069163322449, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3574
goal_identified
goal_identified
=== ep: 3575, time 47.803157329559326, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3575
goal_identified
goal_identified
goal_identified
=== ep: 3576, time 50.79339361190796, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3576
goal_identified
=== ep: 3577, time 52.86379075050354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3577
=== ep: 3578, time 57.89685559272766, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3578
=== ep: 3579, time 59.71616506576538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3579
=== ep: 3580, time 58.13088655471802, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3580
goal_identified
=== ep: 3581, time 53.349979400634766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3581
goal_identified
goal_identified
=== ep: 3582, time 50.15003967285156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3582
goal_identified
=== ep: 3583, time 51.77718687057495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3583
goal_identified
=== ep: 3584, time 55.09104537963867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3584
goal_identified
goal_identified
goal_identified
=== ep: 3585, time 56.86278247833252, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3585
goal_identified
goal_identified
=== ep: 3586, time 59.81258797645569, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3586
=== ep: 3587, time 52.12661647796631, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 54.033886194229126, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3588
goal_identified
=== ep: 3589, time 48.29551100730896, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3589
goal_identified
=== ep: 3590, time 50.41729784011841, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3590
goal_identified
=== ep: 3591, time 53.254249572753906, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3591
=== ep: 3592, time 57.67991805076599, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3592
=== ep: 3593, time 58.962498903274536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3593
=== ep: 3594, time 58.015584230422974, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3594
goal_identified
=== ep: 3595, time 54.1809766292572, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3595
goal_identified
=== ep: 3596, time 50.81431245803833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3596
goal_identified
=== ep: 3597, time 49.439114570617676, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3597
goal_identified
goal_identified
goal_identified
=== ep: 3598, time 55.11032223701477, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3598
goal_identified
=== ep: 3599, time 58.69875621795654, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3599
=== ep: 3600, time 59.463979959487915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3600
goal_identified
=== ep: 3601, time 56.74448275566101, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3601
=== ep: 3602, time 52.36080598831177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3602
goal_identified
=== ep: 3603, time 46.23152995109558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3603
goal_identified
=== ep: 3604, time 50.075650453567505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3604
=== ep: 3605, time 54.46933150291443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3605
=== ep: 3606, time 58.84681296348572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3606
goal_identified
=== ep: 3607, time 65.0193600654602, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3607
=== ep: 3608, time 58.75449252128601, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3608
goal_identified
=== ep: 3609, time 55.16094470024109, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3609
goal_identified
=== ep: 3610, time 51.705790996551514, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3610
goal_identified
=== ep: 3611, time 48.80806255340576, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3611
=== ep: 3612, time 52.92930555343628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3612
goal_identified
goal_identified
=== ep: 3613, time 57.69334125518799, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3613
=== ep: 3614, time 59.64316701889038, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3614
goal_identified
goal_identified
=== ep: 3615, time 57.91981053352356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3615
goal_identified
=== ep: 3616, time 52.864745140075684, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3616
goal_identified
=== ep: 3617, time 49.75093221664429, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3617
goal_identified
goal_identified
=== ep: 3618, time 48.303900718688965, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3618
goal_identified
goal_identified
=== ep: 3619, time 51.496615171432495, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3619
goal_identified
goal_identified
=== ep: 3620, time 55.34173846244812, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3620
goal_identified
=== ep: 3621, time 57.63660740852356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3621
goal_identified
=== ep: 3622, time 58.71702742576599, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3622
=== ep: 3623, time 54.988282918930054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3623
=== ep: 3624, time 52.235095500946045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3624
goal_identified
=== ep: 3625, time 45.31611466407776, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3625
goal_identified
=== ep: 3626, time 53.97290229797363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3626
=== ep: 3627, time 60.16351819038391, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3627
goal_identified
=== ep: 3628, time 59.50813388824463, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3628
goal_identified
goal_identified
=== ep: 3629, time 60.00557327270508, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3629
=== ep: 3630, time 55.84662413597107, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3630
goal_identified
goal_identified
=== ep: 3631, time 54.06906819343567, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3631
goal_identified
=== ep: 3632, time 49.96972727775574, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3632
goal_identified
=== ep: 3633, time 47.94589400291443, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3633
=== ep: 3634, time 52.36448788642883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3634
=== ep: 3635, time 54.87577676773071, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3635
goal_identified
goal_identified
=== ep: 3636, time 60.07271671295166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3636
goal_identified
=== ep: 3637, time 59.36084294319153, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3637
=== ep: 3638, time 55.999287366867065, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3638
=== ep: 3639, time 49.79357600212097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3639
goal_identified
goal_identified
=== ep: 3640, time 48.417216777801514, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3640
=== ep: 3641, time 50.32769751548767, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3641
goal_identified
=== ep: 3642, time 57.10042476654053, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3642
goal_identified
goal_identified
=== ep: 3643, time 59.8323016166687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3643
=== ep: 3644, time 58.77814865112305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3644
goal_identified
goal_identified
=== ep: 3645, time 56.195446252822876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3645
=== ep: 3646, time 62.127275228500366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3646
goal_identified
=== ep: 3647, time 51.316951274871826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3647
goal_identified
=== ep: 3648, time 48.62802243232727, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3648
goal_identified
goal_identified
=== ep: 3649, time 49.43882417678833, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3649
goal_identified
=== ep: 3650, time 53.40969920158386, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3650
goal_identified
goal_identified
=== ep: 3651, time 56.72205853462219, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3651
=== ep: 3652, time 59.87981176376343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3652
=== ep: 3653, time 58.07300138473511, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3653
=== ep: 3654, time 55.15997838973999, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3654
goal_identified
goal_identified
=== ep: 3655, time 49.1304771900177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3655
goal_identified
=== ep: 3656, time 48.39538550376892, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3656
=== ep: 3657, time 54.05439305305481, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3657
goal_identified
=== ep: 3658, time 55.705546855926514, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3658
goal_identified
=== ep: 3659, time 59.58765363693237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3659
goal_identified
=== ep: 3660, time 57.79021954536438, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3660
=== ep: 3661, time 55.74386286735535, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3661
=== ep: 3662, time 51.32984685897827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3662
goal_identified
goal_identified
goal_identified
=== ep: 3663, time 48.424487590789795, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3663
=== ep: 3664, time 57.41528129577637, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3664
=== ep: 3665, time 55.27882719039917, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3665
=== ep: 3666, time 51.49327778816223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3666
goal_identified
goal_identified
=== ep: 3667, time 54.076176166534424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3667
goal_identified
goal_identified
=== ep: 3668, time 57.908705949783325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3668
=== ep: 3669, time 59.31896901130676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3669
=== ep: 3670, time 60.17220449447632, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3670
goal_identified
=== ep: 3671, time 54.4196834564209, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3671
=== ep: 3672, time 50.39212679862976, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3672
=== ep: 3673, time 47.854963302612305, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3673
=== ep: 3674, time 51.699089765548706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3674
goal_identified
goal_identified
goal_identified
=== ep: 3675, time 55.640788316726685, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3675
=== ep: 3676, time 59.26384353637695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3676
=== ep: 3677, time 58.240684270858765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3677
goal_identified
=== ep: 3678, time 55.421727657318115, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3678
=== ep: 3679, time 50.81730604171753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3679
goal_identified
goal_identified
=== ep: 3680, time 49.04646062850952, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3680
goal_identified
goal_identified
=== ep: 3681, time 49.78048610687256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3681
=== ep: 3682, time 53.392794132232666, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3682
=== ep: 3683, time 56.589333295822144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3683
goal_identified
=== ep: 3684, time 63.030776023864746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3684
goal_identified
goal_identified
=== ep: 3685, time 59.269771099090576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3685
goal_identified
goal_identified
goal_identified
=== ep: 3686, time 58.99343776702881, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3686
=== ep: 3687, time 52.03092432022095, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3687
=== ep: 3688, time 48.70782542228699, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3688
goal_identified
goal_identified
=== ep: 3689, time 51.2119882106781, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3689
goal_identified
=== ep: 3690, time 50.61734485626221, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3690
=== ep: 3691, time 54.59108233451843, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3691
goal_identified
goal_identified
=== ep: 3692, time 58.813939571380615, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3692
goal_identified
=== ep: 3693, time 59.511964321136475, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3693
goal_identified
=== ep: 3694, time 57.96809792518616, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3694
goal_identified
=== ep: 3695, time 54.330559968948364, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3695
=== ep: 3696, time 66.95956945419312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3696
goal_identified
=== ep: 3697, time 51.358641147613525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3697
goal_identified
goal_identified
=== ep: 3698, time 49.2657744884491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3698
=== ep: 3699, time 50.189791440963745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3699
=== ep: 3700, time 52.05635094642639, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3700
=== ep: 3701, time 60.74458646774292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3701
=== ep: 3702, time 59.53177618980408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3702
goal_identified
goal_identified
=== ep: 3703, time 64.34440159797668, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3703
goal_identified
=== ep: 3704, time 57.471285581588745, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3704
=== ep: 3705, time 53.34804010391235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3705
=== ep: 3706, time 51.95145320892334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3706
goal_identified
=== ep: 3707, time 49.42137932777405, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3707
goal_identified
=== ep: 3708, time 51.454174757003784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3708
goal_identified
goal_identified
goal_identified
=== ep: 3709, time 53.22498869895935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3709
=== ep: 3710, time 56.13464283943176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3710
goal_identified
=== ep: 3711, time 58.91715693473816, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3711
=== ep: 3712, time 58.74082374572754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3712
goal_identified
goal_identified
=== ep: 3713, time 57.02908658981323, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3713
=== ep: 3714, time 53.27723407745361, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3714
goal_identified
=== ep: 3715, time 47.38625121116638, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3715
=== ep: 3716, time 51.03891634941101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3716
goal_identified
=== ep: 3717, time 54.879674673080444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3717
=== ep: 3718, time 57.326359272003174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3718
=== ep: 3719, time 58.90161967277527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3719
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3720, time 58.54289793968201, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3153
goal_identified
=== ep: 3721, time 54.27182054519653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3721
goal_identified
goal_identified
goal_identified
=== ep: 3722, time 51.16023540496826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3722
goal_identified
goal_identified
=== ep: 3723, time 55.67774486541748, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3723
=== ep: 3724, time 47.96877932548523, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3724
=== ep: 3725, time 50.35937023162842, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3725
goal_identified
goal_identified
=== ep: 3726, time 55.285046100616455, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3726
goal_identified
=== ep: 3727, time 56.885932207107544, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3727
goal_identified
goal_identified
=== ep: 3728, time 59.298572301864624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3728
=== ep: 3729, time 65.48065090179443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3729
goal_identified
=== ep: 3730, time 55.640589237213135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3730
=== ep: 3731, time 52.73045253753662, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3731
=== ep: 3732, time 52.95733857154846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3732
goal_identified
=== ep: 3733, time 48.47680330276489, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3733
goal_identified
=== ep: 3734, time 51.70952749252319, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3734
goal_identified
goal_identified
=== ep: 3735, time 53.89624333381653, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3735
goal_identified
goal_identified
=== ep: 3736, time 58.81272625923157, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3736
=== ep: 3737, time 59.99404573440552, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3737
goal_identified
goal_identified
goal_identified
=== ep: 3738, time 58.91448760032654, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3738
=== ep: 3739, time 56.144437313079834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3739
goal_identified
goal_identified
=== ep: 3740, time 52.95472168922424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3740
goal_identified
=== ep: 3741, time 49.72841429710388, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3741
goal_identified
=== ep: 3742, time 48.393678188323975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3742
=== ep: 3743, time 56.54412841796875, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3743
goal_identified
=== ep: 3744, time 53.14774751663208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3744
=== ep: 3745, time 55.7327036857605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3745
goal_identified
=== ep: 3746, time 58.720088720321655, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3746
=== ep: 3747, time 56.48697781562805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 55.610180139541626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3748
goal_identified
=== ep: 3749, time 51.85179281234741, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3749
=== ep: 3750, time 48.77950572967529, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3750
=== ep: 3751, time 49.65619516372681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3751
=== ep: 3752, time 52.143343925476074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3752
goal_identified
=== ep: 3753, time 56.18832445144653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3753
goal_identified
=== ep: 3754, time 58.89566779136658, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3754
=== ep: 3755, time 58.494935035705566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3755
goal_identified
=== ep: 3756, time 55.184964179992676, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3756
=== ep: 3757, time 51.47668790817261, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3757
goal_identified
=== ep: 3758, time 48.76470613479614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3758
goal_identified
goal_identified
goal_identified
=== ep: 3759, time 50.208136320114136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3759
=== ep: 3760, time 53.021284103393555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3760
goal_identified
goal_identified
=== ep: 3761, time 55.46520137786865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3761
goal_identified
goal_identified
=== ep: 3762, time 60.474719285964966, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3762
goal_identified
goal_identified
=== ep: 3763, time 63.27796149253845, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3763
goal_identified
=== ep: 3764, time 55.87495446205139, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3764
goal_identified
=== ep: 3765, time 53.28805446624756, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3765
=== ep: 3766, time 50.50921940803528, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3766
goal_identified
goal_identified
goal_identified
=== ep: 3767, time 48.12492799758911, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3767
=== ep: 3768, time 50.36119246482849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3768
=== ep: 3769, time 62.74674034118652, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3769
=== ep: 3770, time 51.63316011428833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3770
goal_identified
=== ep: 3771, time 55.71457242965698, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3771
goal_identified
=== ep: 3772, time 58.993464946746826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3772
goal_identified
=== ep: 3773, time 59.47441267967224, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3773
=== ep: 3774, time 56.53155589103699, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3774
goal_identified
goal_identified
=== ep: 3775, time 52.98514986038208, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3775
goal_identified
goal_identified
=== ep: 3776, time 49.725247859954834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3776
goal_identified
=== ep: 3777, time 50.25941181182861, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3777
=== ep: 3778, time 53.7242648601532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3778
goal_identified
=== ep: 3779, time 56.84940457344055, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 58.82179045677185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3780
=== ep: 3781, time 58.54732656478882, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3781
goal_identified
=== ep: 3782, time 55.03694677352905, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3782
goal_identified
=== ep: 3783, time 60.14531636238098, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3783
goal_identified
=== ep: 3784, time 52.129613161087036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3784
goal_identified
=== ep: 3785, time 48.39074087142944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3785
goal_identified
=== ep: 3786, time 50.483482360839844, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3786
goal_identified
=== ep: 3787, time 52.822099685668945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3787
goal_identified
=== ep: 3788, time 55.0345721244812, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3788
=== ep: 3789, time 59.546457052230835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3789
=== ep: 3790, time 60.66152882575989, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3790
=== ep: 3791, time 57.31676888465881, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3791
goal_identified
goal_identified
goal_identified
=== ep: 3792, time 51.876447439193726, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3792
goal_identified
=== ep: 3793, time 48.421430349349976, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3793
goal_identified
=== ep: 3794, time 50.41061568260193, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3794
goal_identified
=== ep: 3795, time 51.522619009017944, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3795
=== ep: 3796, time 55.074652671813965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3796
goal_identified
goal_identified
goal_identified
=== ep: 3797, time 58.80974292755127, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3797
goal_identified
goal_identified
=== ep: 3798, time 59.264089822769165, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3798
goal_identified
=== ep: 3799, time 56.72050428390503, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3799
=== ep: 3800, time 49.7989137172699, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3800
=== ep: 3801, time 58.95588135719299, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3801
goal_identified
=== ep: 3802, time 53.82711172103882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3802
=== ep: 3803, time 48.25006175041199, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3803
goal_identified
=== ep: 3804, time 51.442782163619995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3804
=== ep: 3805, time 50.871442794799805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3805
=== ep: 3806, time 57.98683309555054, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3806
=== ep: 3807, time 56.092031955718994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3807
goal_identified
=== ep: 3808, time 54.57771563529968, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3808
goal_identified
=== ep: 3809, time 51.1444046497345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3809
=== ep: 3810, time 47.8827486038208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3810
goal_identified
=== ep: 3811, time 50.77184057235718, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3811
goal_identified
=== ep: 3812, time 53.28744101524353, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3812
goal_identified
=== ep: 3813, time 61.752358198165894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3813
goal_identified
=== ep: 3814, time 54.636783599853516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3814
goal_identified
=== ep: 3815, time 56.93199920654297, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3815
=== ep: 3816, time 59.39799666404724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3816
=== ep: 3817, time 58.54384636878967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3817
goal_identified
=== ep: 3818, time 55.78671431541443, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3818
goal_identified
=== ep: 3819, time 63.993664264678955, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3819
=== ep: 3820, time 52.6241672039032, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3820
goal_identified
goal_identified
=== ep: 3821, time 48.9739625453949, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3821
goal_identified
=== ep: 3822, time 56.165013551712036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3822
=== ep: 3823, time 50.684884786605835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3823
=== ep: 3824, time 54.03206920623779, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3824
goal_identified
goal_identified
=== ep: 3825, time 57.375314474105835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3825
=== ep: 3826, time 62.85283398628235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3826
=== ep: 3827, time 59.09076499938965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3827
=== ep: 3828, time 56.78559398651123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3828
goal_identified
=== ep: 3829, time 54.545137882232666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3829
goal_identified
=== ep: 3830, time 49.0257568359375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3830
goal_identified
=== ep: 3831, time 48.575297355651855, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3831
goal_identified
=== ep: 3832, time 51.47658586502075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3833, time 56.385186195373535, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3236
=== ep: 3834, time 57.84354567527771, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3834
goal_identified
goal_identified
goal_identified
=== ep: 3835, time 60.77691125869751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3835
=== ep: 3836, time 57.485419273376465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3836
goal_identified
goal_identified
goal_identified
=== ep: 3837, time 55.25675392150879, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3837
goal_identified
=== ep: 3838, time 47.867841720581055, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3838
=== ep: 3839, time 49.750505447387695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3839
=== ep: 3840, time 54.41648459434509, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3840
goal_identified
=== ep: 3841, time 56.78769302368164, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 58.67408728599548, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3842
goal_identified
=== ep: 3843, time 65.53182125091553, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3843
goal_identified
=== ep: 3844, time 59.02608799934387, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3844
goal_identified
=== ep: 3845, time 55.99763798713684, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3845
goal_identified
=== ep: 3846, time 52.95503044128418, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3846
goal_identified
=== ep: 3847, time 49.179367542266846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3847
goal_identified
=== ep: 3848, time 50.31605768203735, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3848
goal_identified
goal_identified
goal_identified
=== ep: 3849, time 51.8314151763916, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3849
goal_identified
=== ep: 3850, time 53.93795037269592, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3850
=== ep: 3851, time 57.38870048522949, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3851
=== ep: 3852, time 59.248905658721924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3852
=== ep: 3853, time 60.36759972572327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3853
goal_identified
=== ep: 3854, time 53.75278162956238, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3854
=== ep: 3855, time 51.25829029083252, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3855
goal_identified
=== ep: 3856, time 49.386759519577026, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3856
goal_identified
goal_identified
=== ep: 3857, time 49.91549205780029, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3857
=== ep: 3858, time 63.12613224983215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3859, time 49.54586744308472, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3276
=== ep: 3860, time 53.51893973350525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3860
goal_identified
goal_identified
=== ep: 3861, time 57.08267092704773, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3861
goal_identified
=== ep: 3862, time 58.9071683883667, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3862
goal_identified
=== ep: 3863, time 64.88586401939392, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3863
goal_identified
=== ep: 3864, time 57.9554979801178, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3864
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3865, time 56.04224371910095, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3865
goal_identified
goal_identified
goal_identified
=== ep: 3866, time 52.1875376701355, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3866
goal_identified
=== ep: 3867, time 48.53746724128723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3867
=== ep: 3868, time 48.78367495536804, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3868
goal_identified
=== ep: 3869, time 54.37406015396118, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3869
goal_identified
goal_identified
goal_identified
=== ep: 3870, time 57.31596851348877, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3870
goal_identified
goal_identified
=== ep: 3871, time 57.298980951309204, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3871
goal_identified
goal_identified
=== ep: 3872, time 56.5262017250061, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3872
goal_identified
goal_identified
=== ep: 3873, time 53.06537222862244, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3873
=== ep: 3874, time 49.440866231918335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3874
goal_identified
=== ep: 3875, time 47.9074866771698, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3875
goal_identified
goal_identified
=== ep: 3876, time 51.149230003356934, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3876
=== ep: 3877, time 54.31870675086975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3877
goal_identified
=== ep: 3878, time 55.394237995147705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3878
goal_identified
=== ep: 3879, time 60.04318714141846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3879
=== ep: 3880, time 69.20418572425842, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3880
goal_identified
=== ep: 3881, time 59.720688343048096, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3881
goal_identified
=== ep: 3882, time 58.777517795562744, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3882
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3883, time 59.79397177696228, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3329
goal_identified
=== ep: 3884, time 51.742599964141846, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3884
goal_identified
=== ep: 3885, time 48.18460822105408, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3885
goal_identified
=== ep: 3886, time 50.172919034957886, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3886
=== ep: 3887, time 58.21069955825806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3887
goal_identified
=== ep: 3888, time 54.99661564826965, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3888
goal_identified
=== ep: 3889, time 56.87206745147705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3889
goal_identified
=== ep: 3890, time 59.79226064682007, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3890
goal_identified
=== ep: 3891, time 57.61893892288208, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3891
goal_identified
=== ep: 3892, time 53.95810794830322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3892
goal_identified
=== ep: 3893, time 50.34280300140381, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3893
goal_identified
=== ep: 3894, time 47.98372411727905, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3894
=== ep: 3895, time 51.386698484420776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3895
goal_identified
=== ep: 3896, time 56.323238134384155, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3896
goal_identified
=== ep: 3897, time 58.34731912612915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3897
goal_identified
goal_identified
=== ep: 3898, time 59.91598725318909, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3898
goal_identified
goal_identified
=== ep: 3899, time 52.6142053604126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3899
=== ep: 3900, time 49.96410155296326, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3900
goal_identified
goal_identified
=== ep: 3901, time 49.58292269706726, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3901
=== ep: 3902, time 51.356252908706665, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3902
goal_identified
=== ep: 3903, time 54.32309579849243, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3903
goal_identified
goal_identified
=== ep: 3904, time 60.061153411865234, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3904
goal_identified
goal_identified
=== ep: 3905, time 57.91264486312866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3905
goal_identified
goal_identified
goal_identified
=== ep: 3906, time 58.64537453651428, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3906
=== ep: 3907, time 56.85606813430786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3907
goal_identified
=== ep: 3908, time 51.91942644119263, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3908
=== ep: 3909, time 48.18530750274658, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3909
goal_identified
=== ep: 3910, time 50.667845010757446, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3910
goal_identified
goal_identified
=== ep: 3911, time 52.64457845687866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3911
=== ep: 3912, time 56.27269005775452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3912
=== ep: 3913, time 59.669435262680054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3913
=== ep: 3914, time 58.18330931663513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3914
=== ep: 3915, time 55.55405521392822, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3915
goal_identified
=== ep: 3916, time 48.92970371246338, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3916
goal_identified
goal_identified
=== ep: 3917, time 48.41787505149841, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3917
goal_identified
=== ep: 3918, time 51.42716097831726, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3918
goal_identified
goal_identified
goal_identified
=== ep: 3919, time 55.99441933631897, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3919
goal_identified
goal_identified
goal_identified
=== ep: 3920, time 58.812530517578125, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3920
=== ep: 3921, time 56.044013261795044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3921
goal_identified
=== ep: 3922, time 52.07891058921814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3922
goal_identified
goal_identified
=== ep: 3923, time 47.61335897445679, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3923
goal_identified
goal_identified
=== ep: 3924, time 49.856871128082275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3924
goal_identified
goal_identified
=== ep: 3925, time 58.203736543655396, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3925
=== ep: 3926, time 53.33976197242737, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3926
=== ep: 3927, time 57.15667366981506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3927
goal_identified
=== ep: 3928, time 58.82498002052307, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3928
goal_identified
=== ep: 3929, time 57.13892674446106, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3929
=== ep: 3930, time 54.001429319381714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3930
=== ep: 3931, time 51.193702936172485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3931
=== ep: 3932, time 50.35145378112793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3932
=== ep: 3933, time 58.674726724624634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3933
=== ep: 3934, time 53.24612021446228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3934
=== ep: 3935, time 55.203969955444336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 59.793171644210815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3936
=== ep: 3937, time 59.82120442390442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3937
=== ep: 3938, time 56.79825186729431, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3938
goal_identified
=== ep: 3939, time 52.54303050041199, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3939
goal_identified
goal_identified
=== ep: 3940, time 49.525843143463135, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3940
=== ep: 3941, time 48.70329475402832, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3941
goal_identified
goal_identified
=== ep: 3942, time 53.321667432785034, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3942
goal_identified
=== ep: 3943, time 55.708356857299805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3943
goal_identified
goal_identified
=== ep: 3944, time 59.135764837265015, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3944
goal_identified
=== ep: 3945, time 59.71681070327759, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3945
goal_identified
=== ep: 3946, time 61.89558267593384, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3946
=== ep: 3947, time 55.124268531799316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3947
=== ep: 3948, time 47.77062177658081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3948
goal_identified
goal_identified
=== ep: 3949, time 50.04286551475525, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3949
=== ep: 3950, time 53.69334554672241, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3950
goal_identified
goal_identified
=== ep: 3951, time 56.79461145401001, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3951
goal_identified
goal_identified
=== ep: 3952, time 58.276869773864746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3952
goal_identified
=== ep: 3953, time 56.82989144325256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3953
goal_identified
=== ep: 3954, time 54.132383823394775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3954
=== ep: 3955, time 47.1834921836853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3955
goal_identified
goal_identified
=== ep: 3956, time 49.46103024482727, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3956
goal_identified
goal_identified
=== ep: 3957, time 53.51651859283447, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3957
goal_identified
goal_identified
goal_identified
=== ep: 3958, time 56.99065446853638, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3958
goal_identified
goal_identified
=== ep: 3959, time 58.78361654281616, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3959
=== ep: 3960, time 58.11461853981018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3960
goal_identified
=== ep: 3961, time 53.28717255592346, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3961
goal_identified
=== ep: 3962, time 48.81100654602051, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3962
=== ep: 3963, time 49.851088523864746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3963
goal_identified
=== ep: 3964, time 55.536829710006714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3964
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3965, time 59.082858085632324, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3331
goal_identified
goal_identified
=== ep: 3966, time 58.923760652542114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3966
=== ep: 3967, time 60.46379518508911, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3967
goal_identified
goal_identified
=== ep: 3968, time 55.53496956825256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3968
=== ep: 3969, time 52.37645721435547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3969
goal_identified
goal_identified
goal_identified
=== ep: 3970, time 47.76575231552124, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3970
goal_identified
goal_identified
goal_identified
=== ep: 3971, time 51.45889401435852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3971
=== ep: 3972, time 54.31105184555054, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3972
goal_identified
goal_identified
=== ep: 3973, time 58.19447135925293, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3973
goal_identified
=== ep: 3974, time 59.32255029678345, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3974
goal_identified
goal_identified
=== ep: 3975, time 56.299410343170166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3975
=== ep: 3976, time 52.918805837631226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3976
goal_identified
=== ep: 3977, time 48.586687326431274, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3977
goal_identified
goal_identified
=== ep: 3978, time 48.89078712463379, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3978
goal_identified
goal_identified
goal_identified
=== ep: 3979, time 53.527416944503784, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3979
=== ep: 3980, time 58.36326241493225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3980
goal_identified
goal_identified
goal_identified
=== ep: 3981, time 57.37332201004028, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3981
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3982, time 53.800063133239746, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3333
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3983, time 50.80507826805115, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3444
=== ep: 3984, time 52.76939654350281, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3984
goal_identified
goal_identified
goal_identified
=== ep: 3985, time 51.34364724159241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3985
goal_identified
goal_identified
=== ep: 3986, time 55.384859800338745, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3986
=== ep: 3987, time 56.7610399723053, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3987
goal_identified
goal_identified
=== ep: 3988, time 60.1794331073761, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3988
goal_identified
goal_identified
goal_identified
=== ep: 3989, time 65.92957949638367, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3989
goal_identified
=== ep: 3990, time 60.07788014411926, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3990
goal_identified
goal_identified
=== ep: 3991, time 59.315526723861694, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3991
goal_identified
=== ep: 3992, time 55.630722761154175, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3992
goal_identified
=== ep: 3993, time 52.15283966064453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3993
goal_identified
goal_identified
=== ep: 3994, time 50.18974018096924, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3994
goal_identified
=== ep: 3995, time 55.64092397689819, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3995
goal_identified
=== ep: 3996, time 58.21196746826172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3996
goal_identified
goal_identified
=== ep: 3997, time 59.93839120864868, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3997
=== ep: 3998, time 60.22398018836975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3998
=== ep: 3999, time 56.563631534576416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
