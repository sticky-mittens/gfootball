==> Playing in 11_vs_11_stochastic.
No DISPLAY defined, doing off-screen rendering
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 6 layers and 500 hidden units.
=== ep: 0, time 120.55173182487488, eps 0.9, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
=== ep: 1, time 116.82852625846863, eps 0.8561552526261419, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
goal_identified
=== ep: 2, time 123.27730941772461, eps 0.8144488388143276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
goal_identified
=== ep: 3, time 120.72331357002258, eps 0.774776470806127, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
goal_identified
=== ep: 4, time 116.79440927505493, eps 0.7370389470171057, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
=== ep: 5, time 122.01757764816284, eps 0.701141903981193, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
goal_identified
=== ep: 6, time 120.58334231376648, eps 0.6669955803928644, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
goal_identified
goal_identified
=== ep: 7, time 120.67126202583313, eps 0.6345145926571234, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
=== ep: 8, time 121.22362184524536, eps 0.6036177213860398, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
=== ep: 9, time 125.96301198005676, eps 0.5742277083079742, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
=== ep: 10, time 115.50809741020203, eps 0.5462710630816575, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
goal_identified
goal_identified
=== ep: 11, time 120.49529457092285, eps 0.5196778795320575, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 12, time 115.48685669898987, eps 0.49438166084852986, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 13, time 122.01082754135132, eps 0.47031915330815344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
goal_identified
goal_identified
=== ep: 14, time 122.96815299987793, eps 0.4474301881084772, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
=== ep: 15, time 124.7874174118042, eps 0.42565753091417224, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
=== ep: 16, time 124.06983709335327, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
goal_identified
=== ep: 17, time 117.34972143173218, eps 0.3852460238219053, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
=== ep: 18, time 123.97180318832397, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
=== ep: 19, time 121.16021800041199, eps 0.3486801800855966, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
goal_identified
=== ep: 20, time 123.48776507377625, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 21, time 117.00313019752502, eps 0.31559403645092865, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
=== ep: 22, time 125.50707244873047, eps 0.3002511042445735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
goal_identified
=== ep: 23, time 121.84056305885315, eps 0.2856564556717689, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 24, time 122.86434888839722, eps 0.27177359650906974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
=== ep: 25, time 118.12323236465454, eps 0.2585678123773109, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
=== ep: 26, time 122.84350562095642, eps 0.24600608193757734, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 27, time 123.17303133010864, eps 0.23405699432065646, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
=== ep: 28, time 125.79425525665283, eps 0.22269067058350425, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 29, time 122.40575385093689, eps 0.2118786889963241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
=== ep: 30, time 120.93793892860413, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
=== ep: 31, time 118.64493179321289, eps 0.191810928470242, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
=== ep: 32, time 125.11569881439209, eps 0.1825049696771952, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
=== ep: 33, time 124.34537863731384, eps 0.17365286785005798, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 34, time 123.59933710098267, eps 0.16523248812340846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
=== ep: 35, time 122.14123845100403, eps 0.15722277516195018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
goal_identified
goal_identified
=== ep: 36, time 122.8676073551178, eps 0.1496037005112063, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 37, time 123.60023427009583, eps 0.14235621251595124, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
=== ep: 38, time 123.92790484428406, eps 0.13546218868114893, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 39, time 123.42185425758362, eps 0.1289043903562757, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
=== ep: 40, time 125.07417750358582, eps 0.12266641962971482, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 41, time 118.16433262825012, eps 0.116732678325436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
=== ep: 42, time 121.64273810386658, eps 0.11108832899943073, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
=== ep: 43, time 120.8265643119812, eps 0.10571925783837377, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
=== ep: 44, time 126.01588416099548, eps 0.10061203936773815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
goal_identified
goal_identified
goal_identified
=== ep: 45, time 119.71409773826599, eps 0.09575390288111604, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 46, time 119.3718273639679, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
goal_identified
=== ep: 47, time 119.87117075920105, eps 0.08673687683177911, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
goal_identified
=== ep: 48, time 120.47045826911926, eps 0.08255544000718185, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
goal_identified
goal_identified
=== ep: 49, time 129.43052577972412, eps 0.07857793426293408, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
=== ep: 50, time 124.0593535900116, eps 0.07479441376288502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
=== ep: 51, time 119.891774892807, eps 0.0711954177350367, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 52, time 120.60878086090088, eps 0.06777194681468615, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
=== ep: 53, time 120.7050461769104, eps 0.06451544054132621, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 54, time 121.61025261878967, eps 0.06141775595303503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
=== ep: 55, time 121.06228995323181, eps 0.05847114722483011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
=== ep: 56, time 119.9328465461731, eps 0.05566824630007096, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
goal_identified
=== ep: 57, time 122.69457983970642, eps 0.05300204446647978, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
=== ep: 58, time 125.71543550491333, eps 0.050465874830710106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
=== ep: 59, time 127.26254153251648, eps 0.04805339564764071, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
goal_identified
=== ep: 60, time 125.70292615890503, eps 0.045758574462709686, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 61, time 118.15328788757324, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
=== ep: 62, time 120.99758505821228, eps 0.04149923295180846, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
goal_identified
goal_identified
=== ep: 63, time 126.9943060874939, eps 0.03952406205346913, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 64, time 123.53524351119995, eps 0.03764522137655123, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
goal_identified
=== ep: 65, time 121.14857053756714, eps 0.03585801284071809, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
goal_identified
=== ep: 66, time 117.98894429206848, eps 0.034157967493714775, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 67, time 126.40962243080139, eps 0.03254083433665968, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
goal_identified
=== ep: 68, time 126.54343867301941, eps 0.031002569694333147, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
=== ep: 69, time 126.77225375175476, eps 0.02953932710388308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
=== ep: 70, time 120.7804491519928, eps 0.028147447696664333, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
=== ep: 71, time 123.75756740570068, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 128.98666524887085, eps 0.025564026480116013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
goal_identified
goal_identified
=== ep: 73, time 121.6432375907898, eps 0.02436602477210106, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
goal_identified
goal_identified
=== ep: 74, time 123.2677755355835, eps 0.02322645029683511, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
goal_identified
=== ep: 75, time 126.15216827392578, eps 0.02214245352455219, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
goal_identified
goal_identified
=== ep: 76, time 123.57112431526184, eps 0.02111132389869288, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
goal_identified
=== ep: 77, time 127.75500893592834, eps 0.020130483058101077, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
goal_identified
goal_identified
=== ep: 78, time 122.05692195892334, eps 0.019197478389778148, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
goal_identified
goal_identified
=== ep: 79, time 126.29988622665405, eps 0.018309976896072843, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
=== ep: 80, time 126.97640919685364, eps 0.017465759360972027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
goal_identified
=== ep: 81, time 124.26377558708191, eps 0.01666271480090467, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
goal_identified
goal_identified
=== ep: 82, time 130.80661916732788, eps 0.015898835186183367, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
goal_identified
=== ep: 83, time 125.35691261291504, eps 0.015172210419884185, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
=== ep: 84, time 124.68954467773438, eps 0.014481023561609456, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 85, time 123.28193616867065, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
=== ep: 86, time 128.27531051635742, eps 0.013198134551968641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 87, time 121.00838041305542, eps 0.012603224509851407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
=== ep: 88, time 120.48363304138184, eps 0.012037328572858524, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
=== ep: 89, time 121.46789956092834, eps 0.011499031706385502, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
=== ep: 90, time 127.44760751724243, eps 0.010986987887879832, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
goal_identified
=== ep: 91, time 127.30973052978516, eps 0.010499916741083536, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
=== ep: 92, time 123.93106603622437, eps 0.010036600334425595, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
goal_identified
=== ep: 93, time 125.94794750213623, eps 0.00959588013555861, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
goal_identified
=== ep: 94, time 130.19511604309082, eps 0.009176654114424539, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
goal_identified
=== ep: 95, time 124.12253212928772, eps 0.00877787398760545, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
goal_identified
=== ep: 96, time 123.67534112930298, eps 0.008398542597069007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
goal_identified
goal_identified
=== ep: 97, time 127.29092574119568, eps 0.008037711416753971, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
=== ep: 98, time 124.66157126426697, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
=== ep: 99, time 129.94623398780823, eps 0.007367984627217855, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 122.11343383789062, eps 0.007057414352177835, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 101, time 125.1219072341919, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
goal_identified
=== ep: 102, time 130.0354254245758, eps 0.006480975162398559, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 103, time 125.04809141159058, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
goal_identified
=== ep: 104, time 129.42430925369263, eps 0.005959391414263934, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
=== ep: 105, time 127.24894666671753, eps 0.005717519040864065, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
goal_identified
=== ep: 106, time 129.52778244018555, eps 0.005487442922312285, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
=== ep: 107, time 130.62981343269348, eps 0.005268587748470919, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
=== ep: 108, time 126.9384970664978, eps 0.005060406267408787, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
goal_identified
=== ep: 109, time 126.63785099983215, eps 0.004862377916986354, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
=== ep: 110, time 126.34525489807129, eps 0.004674007523179196, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 123.04792642593384, eps 0.004494824061885041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 126.25542283058167, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
goal_identified
goal_identified
=== ep: 113, time 122.46636462211609, eps 0.0041622475806460035, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
goal_identified
goal_identified
=== ep: 114, time 123.21113276481628, eps 0.0040080229462666735, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 124.56403827667236, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
=== ep: 116, time 124.11415386199951, eps 0.003721771716092858, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
goal_identified
=== ep: 117, time 124.07044887542725, eps 0.0035890293431213305, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
goal_identified
=== ep: 118, time 124.58930158615112, eps 0.0034627608920727634, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
=== ep: 119, time 127.41106081008911, eps 0.00334265062604924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
goal_identified
goal_identified
goal_identified
=== ep: 120, time 126.58460259437561, eps 0.0032283982068230565, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
goal_identified
=== ep: 121, time 126.7853422164917, eps 0.0031197179438347193, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 125.23338031768799, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
goal_identified
goal_identified
=== ep: 123, time 124.45474982261658, eps 0.0029180001112638996, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
goal_identified
=== ep: 124, time 126.95280623435974, eps 0.002824458142029865, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
goal_identified
=== ep: 125, time 126.25205636024475, eps 0.0027354782684687108, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
=== ep: 126, time 129.1168360710144, eps 0.0026508379945489875, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
=== ep: 127, time 124.79617094993591, eps 0.0025703256754987464, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
=== ep: 128, time 129.94203901290894, eps 0.0024937399885833667, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
=== ep: 129, time 127.3386743068695, eps 0.0024208894296938593, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
=== ep: 130, time 128.5589907169342, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
goal_identified
=== ep: 131, time 125.40160703659058, eps 0.002285673922878779, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
goal_identified
=== ep: 132, time 126.79044103622437, eps 0.0022229708657555565, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 117.71984267234802, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 123.880610704422, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 126.70495629310608, eps 0.002052620779091266, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 127.56728148460388, eps 0.0020012838579124784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
goal_identified
goal_identified
goal_identified
=== ep: 137, time 126.28750109672546, eps 0.0019524506679239415, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
goal_identified
=== ep: 138, time 122.68176984786987, eps 0.001905999100714611, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
goal_identified
=== ep: 139, time 129.16591715812683, eps 0.001861813003170924, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 126.60425972938538, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 126.98496317863464, eps 0.0017798006526189953, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
goal_identified
=== ep: 142, time 128.01996994018555, eps 0.0017417693260160481, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
goal_identified
=== ep: 143, time 129.22932958602905, eps 0.0017055928090985275, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
goal_identified
goal_identified
=== ep: 144, time 128.1526381969452, eps 0.0016711806417306348, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 128.48507475852966, eps 0.0016384467755694515, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 125.29091167449951, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 128.50168371200562, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
=== ep: 148, time 125.75620412826538, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 129.19006490707397, eps 0.0015227160093621863, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
=== ep: 150, time 123.46679425239563, eps 0.0014972228487629025, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 125.54969763755798, eps 0.0014729730042773413, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 126.6127815246582, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 124.35956525802612, eps 0.00142796367199102, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
=== ep: 154, time 128.18412709236145, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
=== ep: 155, time 121.24417853355408, eps 0.001387237543977543, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 127.68559908866882, eps 0.0013683517461028282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 127.8160970211029, eps 0.0013503870194592265, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
goal_identified
=== ep: 158, time 124.95433354377747, eps 0.0013332984428727204, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
goal_identified
=== ep: 159, time 134.2575922012329, eps 0.001317043286000802, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 160, time 129.94728922843933, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 124.14863514900208, eps 0.0012868726283106018, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 129.90325903892517, eps 0.0012728816851329014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
goal_identified
goal_identified
goal_identified
=== ep: 163, time 126.043692111969, eps 0.0012595730883057546, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
=== ep: 164, time 125.20651173591614, eps 0.001246913559404956, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
=== ep: 165, time 130.50581622123718, eps 0.0012348714430141991, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
=== ep: 166, time 128.2731351852417, eps 0.0012234166275700486, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
goal_identified
=== ep: 167, time 126.2999336719513, eps 0.001212520470067348, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 168, time 128.6507637500763, eps 0.0012021557244367845, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 135.59339570999146, eps 0.0011922964734155277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
goal_identified
=== ep: 170, time 126.67371129989624, eps 0.001182918063740569, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 128.43799662590027, eps 0.0011739970445027263, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 172, time 127.25192141532898, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 125.5811448097229, eps 0.001157439036493735, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
=== ep: 174, time 128.69924688339233, eps 0.0011497606440778825, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 175, time 126.9381787776947, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
goal_identified
=== ep: 176, time 128.71234464645386, eps 0.0011355090345108335, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 128.53935050964355, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 178, time 131.79537558555603, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
=== ep: 179, time 129.75417757034302, eps 0.001116633706881133, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 126.10973358154297, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
goal_identified
goal_identified
=== ep: 181, time 124.75344824790955, eps 0.001105534542190287, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
=== ep: 182, time 129.95438480377197, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 133.31043434143066, eps 0.0010954916026690664, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
=== ep: 184, time 126.1041693687439, eps 0.001090834422251547, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 185, time 128.61517691612244, eps 0.0010864043752031938, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
=== ep: 186, time 126.93797159194946, eps 0.0010821903840988777, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
goal_identified
goal_identified
=== ep: 187, time 127.62046432495117, eps 0.0010781819117658682, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
goal_identified
=== ep: 188, time 129.22250938415527, eps 0.0010743689349354123, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 133.86940693855286, eps 0.0010707419191793434, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
goal_identified
=== ep: 190, time 130.5722484588623, eps 0.0010672917950690429, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
goal_identified
goal_identified
=== ep: 191, time 129.34042143821716, eps 0.0010640099354971456, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
goal_identified
=== ep: 192, time 131.02645111083984, eps 0.0010608881341052777, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 132.53490614891052, eps 0.0010579185847638855, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 194, time 127.40626549720764, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
=== ep: 195, time 118.61626815795898, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
goal_identified
=== ep: 196, time 125.07141709327698, eps 0.001049850987889527, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
goal_identified
=== ep: 197, time 136.11346173286438, eps 0.0010474197265209469, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
goal_identified
goal_identified
=== ep: 198, time 125.10616564750671, eps 0.0010451070391685015, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
=== ep: 199, time 133.0983109474182, eps 0.001042907142909185, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
goal_identified
=== ep: 200, time 124.83002209663391, eps 0.001040814536856474, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 127.37420797348022, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 124.76473689079285, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
goal_identified
=== ep: 203, time 129.24258947372437, eps 0.0010351293974264616, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
goal_identified
=== ep: 204, time 125.0448944568634, eps 0.00103341611649703, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
goal_identified
=== ep: 205, time 130.21278023719788, eps 0.0010317863932645186, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 132.80574488639832, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
goal_identified
=== ep: 207, time 126.87283682823181, eps 0.0010287615180101426, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
goal_identified
goal_identified
=== ep: 208, time 131.70668172836304, eps 0.001027358802224555, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
goal_identified
goal_identified
=== ep: 209, time 136.99037504196167, eps 0.0010260244976950921, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
goal_identified
=== ep: 210, time 124.71929597854614, eps 0.0010247552679654227, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
=== ep: 211, time 130.8256459236145, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
goal_identified
=== ep: 212, time 126.25116753578186, eps 0.0010223994927486214, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
=== ep: 213, time 129.9221546649933, eps 0.001021307056596379, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
goal_identified
=== ep: 214, time 132.16726446151733, eps 0.0010202678991839778, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 34/34)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
=== ep: 215, time 129.166601896286, eps 0.0010192794220766138, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
=== ep: 216, time 127.73075819015503, eps 0.0010183391535666436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
=== ep: 217, time 129.45142936706543, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 218, time 132.68463492393494, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
goal_identified
goal_identified
=== ep: 219, time 136.68019199371338, eps 0.0010157846557556941, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
=== ep: 220, time 134.00968289375305, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
goal_identified
=== ep: 221, time 135.1449258327484, eps 0.0010142825471585687, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
goal_identified
=== ep: 222, time 131.2953429222107, eps 0.0010135859791140496, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
=== ep: 223, time 133.08182096481323, eps 0.0010129233830939361, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
=== ep: 224, time 130.49017882347107, eps 0.0010122931022630473, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
goal_identified
=== ep: 225, time 133.48547053337097, eps 0.001011693560591007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 136.5881278514862, eps 0.0010111232589113477, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
=== ep: 227, time 133.01867246627808, eps 0.0010105807711728136, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
=== ep: 228, time 128.27144408226013, eps 0.0010100647408734893, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 133.27800631523132, eps 0.001009573877668838, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
=== ep: 230, time 132.02979493141174, eps 0.001009106954145169, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 132.58164882659912, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
goal_identified
=== ep: 232, time 129.7641956806183, eps 0.0010082403128748867, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 132.0335521697998, eps 0.0010078384280736842, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 234, time 129.6673641204834, eps 0.001007456143425521, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
=== ep: 235, time 130.50750613212585, eps 0.001007092503019653, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
goal_identified
goal_identified
=== ep: 236, time 131.40035843849182, eps 0.001006746597565654, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
goal_identified
goal_identified
=== ep: 237, time 132.79316329956055, eps 0.001006417562119715, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 238, time 132.97621417045593, eps 0.0010061045739218342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 133.0568401813507, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
=== ep: 240, time 133.01813173294067, eps 0.001005523646905642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
goal_identified
=== ep: 241, time 134.11909914016724, eps 0.001005254255467199, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 131.50631189346313, eps 0.0010049980024042435, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 123.71165180206299, eps 0.0010047542469506416, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
=== ep: 244, time 129.9700791835785, eps 0.0010045223795907931, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
=== ep: 245, time 131.78556656837463, eps 0.001004301820535524, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 132.00271081924438, eps 0.0010040920182723119, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
=== ep: 247, time 131.39590167999268, eps 0.0010038924481862177, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
=== ep: 248, time 128.81554412841797, eps 0.0010037026112480747, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
goal_identified
goal_identified
=== ep: 249, time 133.53883361816406, eps 0.0010035220327666559, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 250, time 131.13327312469482, eps 0.0010033502612016988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
goal_identified
=== ep: 251, time 130.1841242313385, eps 0.001003186867034819, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
goal_identified
=== ep: 252, time 133.46297430992126, eps 0.001003031441695491, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
goal_identified
=== ep: 253, time 132.98141169548035, eps 0.0010028835965394094, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
=== ep: 254, time 131.26210117340088, eps 0.0010027429618766747, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
=== ep: 255, time 128.8512372970581, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 133.3172972202301, eps 0.0010024819345422614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
goal_identified
=== ep: 257, time 134.52794885635376, eps 0.0010023608891662839, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 131.08749556541443, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 124.82319927215576, eps 0.0010021362208574892, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
=== ep: 260, time 124.48167896270752, eps 0.001002032036136876, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
goal_identified
=== ep: 261, time 124.4801778793335, eps 0.0010019329325650452, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 126.41203022003174, eps 0.0010018386623314465, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 125.01544952392578, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
goal_identified
=== ep: 264, time 125.63872361183167, eps 0.0010016636904766263, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
goal_identified
=== ep: 265, time 127.78236556053162, eps 0.0010015825513346283, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 126.9798641204834, eps 0.0010015053693952815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 137.1359508037567, eps 0.0010014319516635345, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 125.94870710372925, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 134.3694188594818, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 124.73646092414856, eps 0.0010012324922186594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
=== ep: 271, time 126.6173083782196, eps 0.001001172382863857, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
=== ep: 272, time 129.66928386688232, eps 0.0010011152050768812, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 130.20355868339539, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 274, time 133.99503445625305, eps 0.0010010090792823456, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 275, time 127.6394214630127, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
goal_identified
=== ep: 276, time 127.91481256484985, eps 0.0010009130526924313, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 132.06301593780518, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
=== ep: 278, time 127.31816005706787, eps 0.0010008261642407504, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 130.43522381782532, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
=== ep: 280, time 130.73049306869507, eps 0.0010007475443184742, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 126.70365929603577, eps 0.001000711086151851, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 128.90818333625793, eps 0.0010006764060709957, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 28/28)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 131.02358531951904, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
=== ep: 284, time 122.29700684547424, eps 0.0010006120375228235, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
=== ep: 285, time 134.5961937904358, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 286, time 128.11928367614746, eps 0.0010005537944518927, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 129.5277156829834, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
=== ep: 288, time 124.3804063796997, eps 0.0010005010939419733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
=== ep: 289, time 133.41291213035583, eps 0.001000476655302044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
=== ep: 290, time 127.76002216339111, eps 0.0010004534085486486, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 133.56257438659668, eps 0.0010004312955527947, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
goal_identified
goal_identified
=== ep: 292, time 132.4423623085022, eps 0.0010004102610204745, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 293, time 127.63635516166687, eps 0.0010003902523544011, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
goal_identified
=== ep: 294, time 127.11896514892578, eps 0.0010003712195224871, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
=== ep: 295, time 124.44255185127258, eps 0.0010003531149327387, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 296, time 135.43436980247498, eps 0.0010003358933142518, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
goal_identified
=== ep: 297, time 132.70401406288147, eps 0.0010003195116040093, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 129.34792304039001, eps 0.0010003039288392032, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
goal_identified
goal_identified
=== ep: 299, time 127.64993143081665, eps 0.0010002891060548044, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
=== ep: 300, time 130.00526332855225, eps 0.0010002750061861312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
=== ep: 301, time 127.12720394134521, eps 0.0010002615939761676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
=== ep: 302, time 125.97998070716858, eps 0.001000248835887403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
goal_identified
goal_identified
=== ep: 303, time 126.83741188049316, eps 0.0010002367000179694, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
goal_identified
=== ep: 304, time 131.23150491714478, eps 0.0010002251560218723, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
goal_identified
=== ep: 305, time 130.56116795539856, eps 0.0010002141750331084, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
goal_identified
goal_identified
=== ep: 306, time 130.17929697036743, eps 0.0010002037295934862, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
goal_identified
=== ep: 307, time 132.9886965751648, eps 0.0010001937935839656, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
goal_identified
goal_identified
=== ep: 308, time 133.41333436965942, eps 0.0010001843421593476, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
goal_identified
=== ep: 309, time 133.36548376083374, eps 0.0010001753516861473, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 310, time 131.28851771354675, eps 0.0010001667996834991, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 130.51844716072083, eps 0.001000158664766942, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 312, time 123.89735817909241, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
goal_identified
goal_identified
=== ep: 313, time 136.0326280593872, eps 0.001000143565818053, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
goal_identified
=== ep: 314, time 132.3256094455719, eps 0.0010001365640304844, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 129.42412567138672, eps 0.0010001299037241253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
goal_identified
goal_identified
=== ep: 316, time 131.09329748153687, eps 0.0010001235682447402, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
goal_identified
=== ep: 317, time 125.6944830417633, eps 0.0010001175417503308, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
goal_identified
=== ep: 318, time 123.3894510269165, eps 0.0010001118091715218, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 134.93284034729004, eps 0.0010001063561738807, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
=== ep: 320, time 131.42599177360535, eps 0.0010001011691220727, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
goal_identified
=== ep: 321, time 129.22287273406982, eps 0.0010000962350457665, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
=== ep: 322, time 127.3982183933258, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
goal_identified
=== ep: 323, time 130.60371255874634, eps 0.0010000870770703358, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 127.00232338905334, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
goal_identified
=== ep: 325, time 133.54458022117615, eps 0.0010000787905914928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 136.88093900680542, eps 0.0010000749479290019, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
goal_identified
=== ep: 327, time 129.88019943237305, eps 0.001000071292675372, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 126.09687423706055, eps 0.001000067815690565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
=== ep: 329, time 136.4419219493866, eps 0.0010000645082803084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
=== ep: 330, time 131.4880473613739, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 128.3821849822998, eps 0.0010000583695057963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 128.6625111103058, eps 0.0010000555227914069, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 125.56743693351746, eps 0.0010000528149129166, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
=== ep: 334, time 131.09394097328186, eps 0.0010000502390992187, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
goal_identified
=== ep: 335, time 130.97202610969543, eps 0.0010000477889094373, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
goal_identified
goal_identified
=== ep: 336, time 131.40309357643127, eps 0.0010000454582168217, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
goal_identified
=== ep: 337, time 135.22674322128296, eps 0.001000043241193426, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 338, time 131.78709316253662, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
=== ep: 339, time 134.23590183258057, eps 0.0010000391262498123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
=== ep: 340, time 137.31544709205627, eps 0.001000037218040092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 133.9678909778595, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 127.46417617797852, eps 0.0010000336762753012, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 132.8351457118988, eps 0.001000032033863974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 344, time 128.05663871765137, eps 0.0010000304715539925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
goal_identified
=== ep: 345, time 131.24948644638062, eps 0.001000028985438768, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
goal_identified
=== ep: 346, time 124.64846777915955, eps 0.001000027571802238, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
=== ep: 347, time 132.98370790481567, eps 0.0010000262271095755, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
=== ep: 348, time 132.02512097358704, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
goal_identified
=== ep: 349, time 133.98259377479553, eps 0.0010000237312701107, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
goal_identified
=== ep: 350, time 129.29364609718323, eps 0.00100002257388241, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 128.54603147506714, eps 0.0010000214729411737, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 129.96660614013672, eps 0.0010000204256934752, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
goal_identified
=== ep: 353, time 131.45152640342712, eps 0.0010000194295206493, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
goal_identified
goal_identified
=== ep: 354, time 134.07141709327698, eps 0.0010000184819317455, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 355, time 130.64431762695312, eps 0.001000017580557298, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
goal_identified
=== ep: 356, time 132.62011647224426, eps 0.001000016723143401, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
=== ep: 357, time 132.05320596694946, eps 0.0010000159075460732, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 129.50940680503845, eps 0.0010000151317258964, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
goal_identified
=== ep: 359, time 136.66342568397522, eps 0.0010000143937429161, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
=== ep: 360, time 130.0260682106018, eps 0.0010000136917517905, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
=== ep: 361, time 134.43200516700745, eps 0.001000013023997176, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
goal_identified
=== ep: 362, time 138.4307689666748, eps 0.0010000123888093385, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 131.80335235595703, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 127.57482862472534, eps 0.0010000112098582543, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 138.5872724056244, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 131/131)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 131.61316418647766, eps 0.0010000101430991996, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
=== ep: 367, time 130.88801836967468, eps 0.0010000096484144142, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 130.7104525566101, eps 0.0010000091778556905, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 139.12881994247437, eps 0.0010000087302463867, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
goal_identified
goal_identified
=== ep: 370, time 131.39568090438843, eps 0.001000008304467246, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
goal_identified
=== ep: 371, time 134.02784824371338, eps 0.0010000078994535993, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
=== ep: 372, time 130.19891810417175, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
=== ep: 373, time 133.19284653663635, eps 0.0010000071477211988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
goal_identified
goal_identified
=== ep: 374, time 135.19780659675598, eps 0.0010000067991227223, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
goal_identified
goal_identified
goal_identified
=== ep: 375, time 128.29018378257751, eps 0.0010000064675255943, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
=== ep: 376, time 136.05142283439636, eps 0.001000006152100649, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 132.1535029411316, eps 0.0010000058520591598, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
goal_identified
goal_identified
=== ep: 378, time 133.101176738739, eps 0.0010000055666508666, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 379, time 134.2322223186493, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
=== ep: 380, time 129.40937113761902, eps 0.0010000050369139975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
goal_identified
goal_identified
=== ep: 381, time 135.91606998443604, eps 0.001000004791260803, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 382, time 131.57551646232605, eps 0.0010000045575882562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 134.28253078460693, eps 0.001000004335312054, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
=== ep: 384, time 132.189067363739, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 32/32)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
goal_identified
=== ep: 385, time 139.52466082572937, eps 0.0010000039227525655, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
goal_identified
=== ep: 386, time 130.44652009010315, eps 0.0010000037314376652, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 132.45739364624023, eps 0.001000003549453303, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
goal_identified
=== ep: 388, time 128.69659972190857, eps 0.0010000033763444226, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 389, time 131.43108916282654, eps 0.001000003211678162, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 132.37549376487732, eps 0.0010000030550427698, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
=== ep: 391, time 129.550852060318, eps 0.0010000029060465757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 129.0388560295105, eps 0.0010000027643170119, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
goal_identified
goal_identified
goal_identified
=== ep: 393, time 139.14585375785828, eps 0.0010000026294996803, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
goal_identified
=== ep: 394, time 134.48114919662476, eps 0.0010000025012574677, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
goal_identified
goal_identified
=== ep: 395, time 136.0600082874298, eps 0.0010000023792697014, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
goal_identified
=== ep: 396, time 129.39385414123535, eps 0.0010000022632313489, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 129.62041115760803, eps 0.0010000021528522535, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
=== ep: 398, time 134.2114918231964, eps 0.00100000204785641, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 46/46)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
goal_identified
=== ep: 399, time 141.47973227500916, eps 0.0010000019479812744, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 135.70093297958374, eps 0.0010000018529771066, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
goal_identified
goal_identified
=== ep: 401, time 140.30152344703674, eps 0.0010000017626063467, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
goal_identified
=== ep: 402, time 128.43297719955444, eps 0.0010000016766430208, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 132.576810836792, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
=== ep: 404, time 132.36803722381592, eps 0.001000001517089342, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
goal_identified
=== ep: 405, time 135.33932209014893, eps 0.0010000014431000217, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 132.03356218338013, eps 0.001000001372719203, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
goal_identified
=== ep: 407, time 134.6652774810791, eps 0.0010000013057708975, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 132.6715075969696, eps 0.0010000012420876994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
goal_identified
=== ep: 409, time 131.81485414505005, eps 0.0010000011815103674, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
goal_identified
=== ep: 410, time 135.25840663909912, eps 0.001000001123887427, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 411, time 138.73531889915466, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 135.45776748657227, eps 0.0010000010169353975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 126.8477852344513, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
goal_identified
=== ep: 414, time 130.50698947906494, eps 0.0010000009201611994, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
=== ep: 415, time 130.19565606117249, eps 0.0010000008752844081, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
goal_identified
=== ep: 416, time 131.12187910079956, eps 0.0010000008325962838, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
=== ep: 417, time 128.69738936424255, eps 0.001000000791990084, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
=== ep: 418, time 130.05749797821045, eps 0.0010000007533642718, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
goal_identified
=== ep: 419, time 135.4042227268219, eps 0.0010000007166222626, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
goal_identified
=== ep: 420, time 135.16016960144043, eps 0.0010000006816721825, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
goal_identified
goal_identified
=== ep: 421, time 134.00104546546936, eps 0.001000000648426638, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
goal_identified
=== ep: 422, time 133.46571588516235, eps 0.0010000006168024976, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 127.6306881904602, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 6/6)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 424, time 132.06566452980042, eps 0.0010000005581059794, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 425, time 133.4880175590515, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
=== ep: 426, time 137.04400038719177, eps 0.0010000005049951733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 128.92520356178284, eps 0.001000000480366268, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
goal_identified
goal_identified
=== ep: 428, time 130.15574860572815, eps 0.0010000004569385287, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
goal_identified
=== ep: 429, time 139.8450903892517, eps 0.0010000004346533736, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
=== ep: 430, time 133.25197052955627, eps 0.0010000004134550786, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 132.07561230659485, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 10/10)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
goal_identified
=== ep: 432, time 131.07290482521057, eps 0.0010000003741096257, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 130.2510049343109, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 128.84298276901245, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
=== ep: 435, time 128.2293245792389, eps 0.001000000321999139, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
goal_identified
=== ep: 436, time 136.73552894592285, eps 0.0010000003062950555, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 131.90687227249146, eps 0.0010000002913568694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
=== ep: 438, time 130.27300906181335, eps 0.0010000002771472273, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
goal_identified
=== ep: 439, time 138.8396589756012, eps 0.0010000002636305976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
=== ep: 440, time 130.3484320640564, eps 0.0010000002507731815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
goal_identified
goal_identified
goal_identified
=== ep: 441, time 133.5691077709198, eps 0.0010000002385428292, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
goal_identified
=== ep: 442, time 131.2099688053131, eps 0.0010000002269089582, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
=== ep: 443, time 129.60711336135864, eps 0.0010000002158424776, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
=== ep: 444, time 130.65471410751343, eps 0.0010000002053157158, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
goal_identified
=== ep: 445, time 136.46958589553833, eps 0.0010000001953023503, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
=== ep: 446, time 130.91506099700928, eps 0.001000000185777342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
goal_identified
goal_identified
=== ep: 447, time 130.3164520263672, eps 0.0010000001767168742, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
=== ep: 448, time 133.17488026618958, eps 0.0010000001680982905, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
goal_identified
goal_identified
=== ep: 449, time 139.17313194274902, eps 0.0010000001599000403, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 128.97660636901855, eps 0.0010000001521016232, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
=== ep: 451, time 134.7182834148407, eps 0.0010000001446835395, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
=== ep: 452, time 131.4098358154297, eps 0.0010000001376272401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 133.49664878845215, eps 0.0010000001309150804, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
=== ep: 454, time 138.57080698013306, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
=== ep: 455, time 134.25557923316956, eps 0.0010000001184568633, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 24/24)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
=== ep: 456, time 135.35366249084473, eps 0.0010000001126796538, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
goal_identified
=== ep: 457, time 125.70693898200989, eps 0.0010000001071842023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 128.04246830940247, eps 0.001000000101956767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 138.7201099395752, eps 0.001000000096984277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
goal_identified
=== ep: 460, time 133.1676104068756, eps 0.001000000092254298, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
=== ep: 461, time 134.35082340240479, eps 0.0010000000877550027, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 135.5152382850647, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 136.5830957889557, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 130.37232279777527, eps 0.0010000000755314307, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
=== ep: 465, time 127.18044948577881, eps 0.0010000000718477194, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 131.07275080680847, eps 0.0010000000683436647, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 135.2063639163971, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
=== ep: 468, time 130.92005491256714, eps 0.0010000000618399052, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 137.94049167633057, eps 0.0010000000588239375, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 470, time 136.69909596443176, eps 0.0010000000559550603, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
goal_identified
goal_identified
=== ep: 471, time 134.24793410301208, eps 0.0010000000532260998, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
=== ep: 472, time 132.14678931236267, eps 0.0010000000506302322, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
=== ep: 473, time 133.77138996124268, eps 0.0010000000481609666, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
goal_identified
=== ep: 474, time 132.56231784820557, eps 0.0010000000458121286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 129.21131587028503, eps 0.0010000000435778447, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 476, time 132.87861490249634, eps 0.001000000041452528, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
=== ep: 477, time 133.32343339920044, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
goal_identified
=== ep: 478, time 134.82998657226562, eps 0.0010000000375077985, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
=== ep: 479, time 137.70987820625305, eps 0.0010000000356785216, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 139.347918510437, eps 0.0010000000339384595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 129.4045853614807, eps 0.0010000000322832614, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 133.59633994102478, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 127.98043060302734, eps 0.001000000029211103, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 135.1243233680725, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
=== ep: 485, time 130.4743504524231, eps 0.0010000000264312988, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
goal_identified
goal_identified
=== ep: 486, time 137.52236485481262, eps 0.0010000000251422292, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
goal_identified
=== ep: 487, time 139.94088459014893, eps 0.0010000000239160282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
=== ep: 488, time 131.85978198051453, eps 0.00100000002274963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
=== ep: 489, time 139.66012406349182, eps 0.0010000000216401172, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
goal_identified
=== ep: 490, time 141.1347165107727, eps 0.0010000000205847162, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 134.5037031173706, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
goal_identified
=== ep: 492, time 131.18130564689636, eps 0.0010000000186258216, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
goal_identified
goal_identified
=== ep: 493, time 136.42925572395325, eps 0.0010000000177174295, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 130.23564267158508, eps 0.0010000000168533404, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
=== ep: 495, time 134.36274671554565, eps 0.0010000000160313932, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 134.16596007347107, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 139.98703789710999, eps 0.0010000000145058043, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 139.6943051815033, eps 0.001000000013798348, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
goal_identified
goal_identified
goal_identified
=== ep: 499, time 144.00346422195435, eps 0.0010000000131253947, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
goal_identified
goal_identified
=== ep: 500, time 130.86733198165894, eps 0.0010000000124852615, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 133.8149230480194, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
goal_identified
=== ep: 502, time 129.91395497322083, eps 0.0010000000112971319, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 132.9568099975586, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 130.81261563301086, eps 0.0010000000102220676, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 129.57048296928406, eps 0.0010000000097235315, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
=== ep: 506, time 137.43852400779724, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 135.90877771377563, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 133.86223030090332, eps 0.0010000000083691212, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
goal_identified
=== ep: 509, time 140.2556116580963, eps 0.0010000000079609542, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
=== ep: 510, time 137.45336651802063, eps 0.001000000007572694, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
goal_identified
=== ep: 511, time 135.79920315742493, eps 0.0010000000072033692, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
goal_identified
=== ep: 512, time 130.66006350517273, eps 0.001000000006852057, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
=== ep: 513, time 135.1996626853943, eps 0.001000000006517878, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
=== ep: 514, time 133.12067985534668, eps 0.0010000000061999974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 140.51857614517212, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
goal_identified
goal_identified
=== ep: 516, time 132.46582508087158, eps 0.0010000000056099897, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 517, time 133.14327573776245, eps 0.0010000000053363872, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
goal_identified
goal_identified
=== ep: 518, time 131.3731586933136, eps 0.0010000000050761286, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
=== ep: 519, time 133.66150784492493, eps 0.001000000004828563, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
=== ep: 520, time 137.76693725585938, eps 0.001000000004593071, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 133.79816102981567, eps 0.0010000000043690644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 133.51868629455566, eps 0.0010000000041559827, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
=== ep: 523, time 137.59312415122986, eps 0.0010000000039532928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
goal_identified
=== ep: 524, time 136.5622889995575, eps 0.0010000000037604885, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
=== ep: 525, time 128.35462355613708, eps 0.0010000000035770874, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 131.2152247428894, eps 0.0010000000034026306, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 136.3083016872406, eps 0.0010000000032366824, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
goal_identified
=== ep: 528, time 135.89093613624573, eps 0.0010000000030788276, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
goal_identified
=== ep: 529, time 137.0386061668396, eps 0.0010000000029286714, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
=== ep: 530, time 129.3902235031128, eps 0.0010000000027858384, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 132.27350521087646, eps 0.0010000000026499714, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 532, time 136.2966113090515, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
goal_identified
goal_identified
=== ep: 533, time 132.71663618087769, eps 0.0010000000023977934, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
goal_identified
goal_identified
=== ep: 534, time 131.0616991519928, eps 0.0010000000022808515, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
goal_identified
=== ep: 535, time 132.28619170188904, eps 0.0010000000021696133, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
=== ep: 536, time 136.41291761398315, eps 0.0010000000020637999, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
goal_identified
=== ep: 537, time 134.22469329833984, eps 0.0010000000019631471, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
=== ep: 538, time 144.5633201599121, eps 0.0010000000018674034, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
goal_identified
=== ep: 539, time 140.90298295021057, eps 0.001000000001776329, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
goal_identified
=== ep: 540, time 133.91384363174438, eps 0.0010000000016896964, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
goal_identified
goal_identified
=== ep: 541, time 137.444185256958, eps 0.001000000001607289, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
=== ep: 542, time 135.40705752372742, eps 0.0010000000015289005, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
=== ep: 543, time 132.99642062187195, eps 0.0010000000014543352, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
=== ep: 544, time 144.38264274597168, eps 0.0010000000013834064, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 144.64047193527222, eps 0.001000000001315937, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 136.8678834438324, eps 0.0010000000012517578, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 40/40)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
goal_identified
goal_identified
=== ep: 547, time 137.34215641021729, eps 0.001000000001190709, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
=== ep: 548, time 137.48105835914612, eps 0.0010000000011326374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 139.27917528152466, eps 0.001000000001077398, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 133.50958371162415, eps 0.0010000000010248527, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 143.89021253585815, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 146.66289448738098, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
goal_identified
goal_identified
=== ep: 553, time 137.26624631881714, eps 0.0010000000008820989, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
=== ep: 554, time 132.16372847557068, eps 0.0010000000008390784, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 132.26533460617065, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
goal_identified
=== ep: 556, time 136.22214198112488, eps 0.0010000000007592295, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
goal_identified
goal_identified
=== ep: 557, time 130.43312072753906, eps 0.0010000000007222014, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
goal_identified
=== ep: 558, time 131.0020318031311, eps 0.0010000000006869794, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
=== ep: 559, time 137.2608926296234, eps 0.001000000000653475, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 138.7897551059723, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 137.3136785030365, eps 0.0010000000005912885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 562, time 136.417053937912, eps 0.0010000000005624511, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
goal_identified
goal_identified
goal_identified
=== ep: 563, time 133.95479011535645, eps 0.00100000000053502, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
goal_identified
=== ep: 564, time 133.5575714111328, eps 0.001000000000508927, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
=== ep: 565, time 137.33643674850464, eps 0.001000000000484106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
=== ep: 566, time 139.57791471481323, eps 0.001000000000460496, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
goal_identified
=== ep: 567, time 135.71918988227844, eps 0.0010000000004380374, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
goal_identified
=== ep: 568, time 141.42197728157043, eps 0.001000000000416674, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 142.1597921848297, eps 0.0010000000003963527, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
=== ep: 570, time 134.66099619865417, eps 0.0010000000003770222, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
=== ep: 571, time 137.29120206832886, eps 0.0010000000003586346, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 136.5365822315216, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
goal_identified
=== ep: 573, time 141.68918681144714, eps 0.001000000000324506, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
=== ep: 574, time 135.92166233062744, eps 0.0010000000003086798, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 134.67952156066895, eps 0.0010000000002936252, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
=== ep: 576, time 133.45900893211365, eps 0.001000000000279305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
goal_identified
goal_identified
=== ep: 577, time 140.3158082962036, eps 0.0010000000002656831, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 578, time 136.65117359161377, eps 0.0010000000002527256, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
goal_identified
=== ep: 579, time 143.6130166053772, eps 0.0010000000002404, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
goal_identified
=== ep: 580, time 136.748309135437, eps 0.0010000000002286756, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
=== ep: 581, time 131.8280258178711, eps 0.0010000000002175229, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 18/18)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
=== ep: 582, time 140.07624292373657, eps 0.0010000000002069142, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
goal_identified
=== ep: 583, time 131.04727864265442, eps 0.0010000000001968228, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 584, time 137.57158088684082, eps 0.0010000000001872237, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
goal_identified
=== ep: 585, time 137.82638502120972, eps 0.0010000000001780928, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 138.4318790435791, eps 0.001000000000169407, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
goal_identified
=== ep: 587, time 133.69445395469666, eps 0.001000000000161145, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
=== ep: 588, time 133.88616561889648, eps 0.0010000000001532858, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
=== ep: 589, time 140.8174705505371, eps 0.00100000000014581, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
goal_identified
=== ep: 590, time 139.45934796333313, eps 0.0010000000001386988, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 139.18213367462158, eps 0.0010000000001319344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
goal_identified
=== ep: 592, time 134.96927189826965, eps 0.0010000000001255, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 133.10178112983704, eps 0.0010000000001193791, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 132.62581300735474, eps 0.001000000000113557, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
=== ep: 595, time 136.55288195610046, eps 0.0010000000001080186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
=== ep: 596, time 147.31447339057922, eps 0.0010000000001027505, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
=== ep: 597, time 135.72285270690918, eps 0.0010000000000977393, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 130.70222926139832, eps 0.0010000000000929725, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
goal_identified
=== ep: 599, time 136.62304997444153, eps 0.0010000000000884382, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
goal_identified
=== ep: 600, time 133.50806403160095, eps 0.001000000000084125, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 601, time 131.85536527633667, eps 0.0010000000000800222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 137.25834465026855, eps 0.0010000000000761195, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
goal_identified
goal_identified
=== ep: 603, time 134.72517108917236, eps 0.0010000000000724072, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
=== ep: 604, time 134.0143439769745, eps 0.0010000000000688757, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 131.81742119789124, eps 0.0010000000000655166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 20/20)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 606, time 137.17246770858765, eps 0.0010000000000623215, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
=== ep: 607, time 138.49400067329407, eps 0.001000000000059282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
=== ep: 608, time 136.64161801338196, eps 0.0010000000000563907, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
=== ep: 609, time 135.31782793998718, eps 0.0010000000000536405, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 135.51850628852844, eps 0.0010000000000510245, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 140.3985297679901, eps 0.0010000000000485358, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
goal_identified
=== ep: 612, time 133.4744851589203, eps 0.0010000000000461688, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 613, time 137.5607726573944, eps 0.0010000000000439171, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
goal_identified
=== ep: 614, time 132.70368337631226, eps 0.0010000000000417752, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
goal_identified
=== ep: 615, time 138.13466668128967, eps 0.0010000000000397378, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
goal_identified
goal_identified
=== ep: 616, time 137.38661360740662, eps 0.0010000000000377999, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
=== ep: 617, time 133.90249371528625, eps 0.0010000000000359563, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
=== ep: 618, time 136.96038007736206, eps 0.0010000000000342027, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 144.92230343818665, eps 0.0010000000000325345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
=== ep: 620, time 138.94449520111084, eps 0.001000000000030948, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
=== ep: 621, time 132.6906967163086, eps 0.0010000000000294385, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
goal_identified
goal_identified
goal_identified
=== ep: 622, time 140.54947996139526, eps 0.0010000000000280028, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 131.4467670917511, eps 0.0010000000000266371, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
goal_identified
=== ep: 624, time 136.01472449302673, eps 0.001000000000025338, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 135.30432271957397, eps 0.0010000000000241023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 8/8)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
=== ep: 626, time 135.35606026649475, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
goal_identified
=== ep: 627, time 134.21640276908875, eps 0.0010000000000218085, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
=== ep: 628, time 132.0885682106018, eps 0.001000000000020745, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 144.06547451019287, eps 0.0010000000000197332, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 50/50)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
goal_identified
=== ep: 630, time 136.18603587150574, eps 0.0010000000000187708, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
goal_identified
=== ep: 631, time 138.48968744277954, eps 0.0010000000000178553, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 632, time 140.692857503891, eps 0.0010000000000169845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
=== ep: 633, time 142.75399374961853, eps 0.0010000000000161562, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
=== ep: 634, time 135.88545203208923, eps 0.0010000000000153684, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 136.0196635723114, eps 0.0010000000000146188, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
=== ep: 636, time 135.56228590011597, eps 0.0010000000000139058, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
goal_identified
goal_identified
=== ep: 637, time 141.72965693473816, eps 0.0010000000000132275, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
goal_identified
goal_identified
=== ep: 638, time 133.52460479736328, eps 0.0010000000000125824, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
goal_identified
=== ep: 639, time 141.12477374076843, eps 0.0010000000000119687, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
goal_identified
goal_identified
=== ep: 640, time 137.53207778930664, eps 0.001000000000011385, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
=== ep: 641, time 138.93345308303833, eps 0.00100000000001083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 26/26)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
goal_identified
=== ep: 642, time 144.03657817840576, eps 0.0010000000000103017, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
=== ep: 643, time 134.00878238677979, eps 0.0010000000000097993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 136.20955538749695, eps 0.0010000000000093213, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
goal_identified
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
goal_identified
=== ep: 645, time 139.72983360290527, eps 0.0010000000000088666, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
=== ep: 646, time 136.27876138687134, eps 0.0010000000000084342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
goal_identified
goal_identified
=== ep: 647, time 135.1931004524231, eps 0.001000000000008023, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
=== ep: 648, time 134.739563703537, eps 0.0010000000000076317, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 139.45042324066162, eps 0.0010000000000072594, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 137.83859968185425, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 141.9613492488861, eps 0.0010000000000065686, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 136.54236602783203, eps 0.0010000000000062483, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 141.92346906661987, eps 0.0010000000000059436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 135.39920592308044, eps 0.0010000000000056537, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
goal_identified
=== ep: 655, time 136.67528891563416, eps 0.0010000000000053779, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 133.55540418624878, eps 0.0010000000000051157, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
=== ep: 657, time 134.70857191085815, eps 0.0010000000000048661, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
goal_identified
goal_identified
=== ep: 658, time 139.21893763542175, eps 0.001000000000004629, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
=== ep: 659, time 138.7325427532196, eps 0.0010000000000044032, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
=== ep: 660, time 134.49362421035767, eps 0.0010000000000041883, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 134.61930537223816, eps 0.001000000000003984, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 130.63816499710083, eps 0.0010000000000037897, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
goal_identified
=== ep: 663, time 135.27276420593262, eps 0.001000000000003605, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
goal_identified
=== ep: 664, time 132.91873216629028, eps 0.0010000000000034291, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
goal_identified
=== ep: 665, time 140.90907788276672, eps 0.001000000000003262, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
=== ep: 666, time 139.37043476104736, eps 0.0010000000000031028, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 138.7587752342224, eps 0.0010000000000029514, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
goal_identified
=== ep: 668, time 139.64342665672302, eps 0.0010000000000028075, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 147.01052331924438, eps 0.0010000000000026706, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 138.10716938972473, eps 0.0010000000000025403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 671, time 138.02840662002563, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
goal_identified
=== ep: 672, time 141.38654208183289, eps 0.0010000000000022985, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 44/44)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
=== ep: 673, time 139.95263743400574, eps 0.0010000000000021864, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
goal_identified
=== ep: 674, time 149.58369064331055, eps 0.00100000000000208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 134.0978548526764, eps 0.0010000000000019785, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
goal_identified
=== ep: 676, time 142.18436241149902, eps 0.001000000000001882, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
goal_identified
=== ep: 677, time 139.38891315460205, eps 0.0010000000000017903, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
=== ep: 678, time 137.47730708122253, eps 0.0010000000000017029, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
goal_identified
goal_identified
=== ep: 679, time 148.17772889137268, eps 0.0010000000000016198, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
goal_identified
=== ep: 680, time 133.83458828926086, eps 0.0010000000000015409, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
goal_identified
=== ep: 681, time 140.21208357810974, eps 0.0010000000000014656, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 139.5465726852417, eps 0.0010000000000013943, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
goal_identified
goal_identified
goal_identified
=== ep: 683, time 134.1299798488617, eps 0.0010000000000013262, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
goal_identified
=== ep: 684, time 139.339421749115, eps 0.0010000000000012616, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
=== ep: 685, time 138.09059047698975, eps 0.0010000000000012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
goal_identified
=== ep: 686, time 140.0546112060547, eps 0.0010000000000011415, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
goal_identified
=== ep: 687, time 138.8604998588562, eps 0.0010000000000010857, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
goal_identified
goal_identified
goal_identified
=== ep: 688, time 135.03710746765137, eps 0.0010000000000010328, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
=== ep: 689, time 146.2060194015503, eps 0.0010000000000009825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
=== ep: 690, time 142.13137698173523, eps 0.0010000000000009346, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 138.92890858650208, eps 0.001000000000000889, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 137.20808744430542, eps 0.0010000000000008457, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 141.42623257637024, eps 0.0010000000000008045, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
=== ep: 694, time 137.98289012908936, eps 0.0010000000000007653, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
goal_identified
=== ep: 695, time 137.54138350486755, eps 0.0010000000000007277, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
=== ep: 696, time 145.07118368148804, eps 0.0010000000000006924, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
=== ep: 697, time 131.6498086452484, eps 0.0010000000000006586, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
goal_identified
goal_identified
goal_identified
=== ep: 698, time 139.65170979499817, eps 0.0010000000000006265, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 143.48959016799927, eps 0.001000000000000596, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
goal_identified
=== ep: 700, time 138.52930450439453, eps 0.0010000000000005668, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
=== ep: 701, time 139.35214710235596, eps 0.0010000000000005393, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
=== ep: 702, time 137.20029473304749, eps 0.0010000000000005128, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 141.65705227851868, eps 0.001000000000000488, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
goal_identified
=== ep: 704, time 141.12053799629211, eps 0.001000000000000464, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
=== ep: 705, time 138.91942477226257, eps 0.0010000000000004415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
=== ep: 706, time 141.84760355949402, eps 0.00100000000000042, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
=== ep: 707, time 144.62442660331726, eps 0.0010000000000003994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
goal_identified
=== ep: 708, time 144.00471806526184, eps 0.00100000000000038, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
goal_identified
=== ep: 709, time 144.58644199371338, eps 0.0010000000000003615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
goal_identified
goal_identified
goal_identified
=== ep: 710, time 140.98808240890503, eps 0.0010000000000003437, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
goal_identified
=== ep: 711, time 149.38868236541748, eps 0.001000000000000327, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
goal_identified
goal_identified
=== ep: 712, time 138.8483648300171, eps 0.0010000000000003112, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
goal_identified
=== ep: 713, time 147.3829860687256, eps 0.001000000000000296, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
goal_identified
=== ep: 714, time 137.72563123703003, eps 0.0010000000000002815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
goal_identified
=== ep: 715, time 145.3003339767456, eps 0.0010000000000002678, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
goal_identified
=== ep: 716, time 145.07397150993347, eps 0.0010000000000002548, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
goal_identified
goal_identified
=== ep: 717, time 146.21532082557678, eps 0.0010000000000002422, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
=== ep: 718, time 135.76575922966003, eps 0.0010000000000002305, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
goal_identified
goal_identified
=== ep: 719, time 147.8774073123932, eps 0.0010000000000002192, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 42/42)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
goal_identified
goal_identified
=== ep: 720, time 141.9041931629181, eps 0.0010000000000002086, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
goal_identified
=== ep: 721, time 140.7940113544464, eps 0.0010000000000001984, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
goal_identified
=== ep: 722, time 143.80305314064026, eps 0.0010000000000001887, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 22/22)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
goal_identified
=== ep: 723, time 146.56963634490967, eps 0.0010000000000001796, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
=== ep: 724, time 149.6280701160431, eps 0.0010000000000001707, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 140.5309145450592, eps 0.0010000000000001624, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
goal_identified
=== ep: 726, time 137.32198357582092, eps 0.0010000000000001544, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 141.03010034561157, eps 0.001000000000000147, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
=== ep: 728, time 136.74812984466553, eps 0.0010000000000001399, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 729, time 143.50172066688538, eps 0.001000000000000133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
=== ep: 730, time 138.83328557014465, eps 0.0010000000000001264, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
goal_identified
goal_identified
goal_identified
=== ep: 731, time 145.00163459777832, eps 0.0010000000000001204, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
goal_identified
=== ep: 732, time 150.36745762825012, eps 0.0010000000000001145, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
goal_identified
=== ep: 733, time 148.55583095550537, eps 0.0010000000000001089, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
goal_identified
=== ep: 734, time 149.18439936637878, eps 0.0010000000000001037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
=== ep: 735, time 147.7365539073944, eps 0.0010000000000000985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
goal_identified
=== ep: 736, time 138.79612755775452, eps 0.0010000000000000937, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 737, time 143.91082787513733, eps 0.0010000000000000891, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
=== ep: 738, time 140.9760286808014, eps 0.0010000000000000848, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
goal_identified
goal_identified
goal_identified
=== ep: 739, time 146.42434453964233, eps 0.0010000000000000807, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
goal_identified
goal_identified
=== ep: 740, time 143.19000458717346, eps 0.0010000000000000768, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 149.16748595237732, eps 0.001000000000000073, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 38/38)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
=== ep: 742, time 139.41890573501587, eps 0.0010000000000000694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
=== ep: 743, time 142.02634191513062, eps 0.001000000000000066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
goal_identified
=== ep: 744, time 144.96416091918945, eps 0.001000000000000063, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
=== ep: 745, time 143.82324409484863, eps 0.0010000000000000599, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
goal_identified
=== ep: 746, time 145.00302934646606, eps 0.0010000000000000568, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
goal_identified
goal_identified
=== ep: 747, time 138.6182610988617, eps 0.001000000000000054, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 147.12207007408142, eps 0.0010000000000000514, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
goal_identified
goal_identified
=== ep: 749, time 150.5891134738922, eps 0.001000000000000049, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
=== ep: 750, time 143.71847414970398, eps 0.0010000000000000466, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 139.629052400589, eps 0.0010000000000000443, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
goal_identified
goal_identified
=== ep: 752, time 149.79128170013428, eps 0.001000000000000042, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
=== ep: 753, time 135.08691310882568, eps 0.0010000000000000401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
goal_identified
=== ep: 754, time 142.5755796432495, eps 0.0010000000000000382, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
=== ep: 755, time 140.44504952430725, eps 0.0010000000000000362, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 756, time 141.5898540019989, eps 0.0010000000000000345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
=== ep: 757, time 147.70532083511353, eps 0.0010000000000000328, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 142.91320991516113, eps 0.0010000000000000312, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 156.46740627288818, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
goal_identified
goal_identified
=== ep: 760, time 142.20177674293518, eps 0.0010000000000000282, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
goal_identified
=== ep: 761, time 139.91675686836243, eps 0.001000000000000027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
=== ep: 762, time 148.22760558128357, eps 0.0010000000000000256, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 763, time 143.14518690109253, eps 0.0010000000000000243, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 141.16696882247925, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 36/36)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
=== ep: 765, time 142.9711663722992, eps 0.001000000000000022, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
goal_identified
goal_identified
=== ep: 766, time 143.73866200447083, eps 0.0010000000000000208, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
goal_identified
=== ep: 767, time 139.9533667564392, eps 0.00100000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
=== ep: 768, time 141.26877617835999, eps 0.0010000000000000189, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
=== ep: 769, time 146.687575340271, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 770, time 142.29956531524658, eps 0.0010000000000000172, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
=== ep: 771, time 140.22229385375977, eps 0.0010000000000000163, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
goal_identified
=== ep: 772, time 143.96363949775696, eps 0.0010000000000000154, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 151.6589961051941, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
=== ep: 774, time 142.1312427520752, eps 0.0010000000000000141, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
goal_identified
=== ep: 775, time 139.4976990222931, eps 0.0010000000000000132, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
goal_identified
=== ep: 776, time 153.16246032714844, eps 0.0010000000000000126, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
goal_identified
=== ep: 777, time 145.2701644897461, eps 0.0010000000000000122, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
goal_identified
goal_identified
=== ep: 778, time 148.91670060157776, eps 0.0010000000000000115, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 151.6194567680359, eps 0.0010000000000000109, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
goal_identified
=== ep: 780, time 139.98273921012878, eps 0.0010000000000000104, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
=== ep: 781, time 142.9315881729126, eps 0.00100000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 4/4)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
goal_identified
goal_identified
goal_identified
=== ep: 782, time 142.8433802127838, eps 0.0010000000000000093, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 146.88751006126404, eps 0.001000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
=== ep: 784, time 142.6017143726349, eps 0.0010000000000000085, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 149.87261652946472, eps 0.001000000000000008, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
=== ep: 786, time 137.82432055473328, eps 0.0010000000000000076, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 133.87379002571106, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
=== ep: 788, time 143.8279368877411, eps 0.001000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
goal_identified
goal_identified
=== ep: 789, time 140.79830861091614, eps 0.0010000000000000067, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
goal_identified
=== ep: 790, time 144.6049485206604, eps 0.0010000000000000063, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
=== ep: 791, time 138.51644730567932, eps 0.001000000000000006, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
goal_identified
=== ep: 792, time 130.1508173942566, eps 0.0010000000000000057, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
=== ep: 793, time 130.1682415008545, eps 0.0010000000000000054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
goal_identified
=== ep: 794, time 134.41780757904053, eps 0.0010000000000000052, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 136.26444125175476, eps 0.001000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
goal_identified
=== ep: 796, time 145.20685720443726, eps 0.0010000000000000048, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
=== ep: 797, time 136.4595696926117, eps 0.0010000000000000044, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
=== ep: 798, time 145.042870759964, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 30/30)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
goal_identified
goal_identified
=== ep: 799, time 144.19871163368225, eps 0.0010000000000000041, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
