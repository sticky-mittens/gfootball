==> Playing in 11_vs_11_easy_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 0, time 29.452003955841064, eps 0.9, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
goal_identified
goal_identified
=== ep: 1, time 28.24980854988098, eps 0.8561552526261419, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
goal_identified
=== ep: 2, time 29.38714098930359, eps 0.8144488388143276, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
goal_identified
goal_identified
=== ep: 3, time 31.590104341506958, eps 0.774776470806127, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
=== ep: 4, time 31.918378114700317, eps 0.7370389470171057, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
goal_identified
goal_identified
goal_identified
=== ep: 5, time 32.700807332992554, eps 0.701141903981193, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
goal_identified
goal_identified
goal_identified
=== ep: 6, time 33.54412007331848, eps 0.6669955803928644, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 7, time 34.72385120391846, eps 0.6345145926571234, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
=== ep: 8, time 34.715359926223755, eps 0.6036177213860398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
goal_identified
goal_identified
=== ep: 9, time 35.91422080993652, eps 0.5742277083079742, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
goal_identified
goal_identified
goal_identified
=== ep: 10, time 38.59647178649902, eps 0.5462710630816575, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
goal_identified
goal_identified
goal_identified
=== ep: 11, time 38.43027925491333, eps 0.5196778795320575, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 12, time 44.79476833343506, eps 0.49438166084852986, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
goal_identified
=== ep: 13, time 43.105754137039185, eps 0.47031915330815344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 14, time 44.40517807006836, eps 0.4474301881084772, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
goal_identified
goal_identified
=== ep: 15, time 45.98491382598877, eps 0.42565753091417224, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
goal_identified
goal_identified
goal_identified
=== ep: 16, time 39.51276111602783, eps 0.4049467387413822, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
goal_identified
goal_identified
goal_identified
=== ep: 17, time 46.417884349823, eps 0.3852460238219053, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
goal_identified
goal_identified
goal_identified
=== ep: 18, time 47.04935646057129, eps 0.3665061241067986, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
goal_identified
goal_identified
=== ep: 19, time 49.20052170753479, eps 0.3486801800855966, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 4
goal_identified
=== ep: 20, time 42.92870020866394, eps 0.3317236176131267, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
=== ep: 21, time 45.54686737060547, eps 0.31559403645092865, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
goal_identified
=== ep: 22, time 43.141329288482666, eps 0.3002511042445735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 23, time 51.659810304641724, eps 0.2856564556717689, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
goal_identified
goal_identified
=== ep: 24, time 45.0072455406189, eps 0.27177359650906974, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
goal_identified
=== ep: 25, time 50.74062466621399, eps 0.2585678123773109, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
=== ep: 26, time 47.813886880874634, eps 0.24600608193757734, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
goal_identified
=== ep: 27, time 51.352585315704346, eps 0.23405699432065646, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
goal_identified
goal_identified
goal_identified
=== ep: 28, time 48.6674222946167, eps 0.22269067058350425, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
goal_identified
goal_identified
=== ep: 29, time 43.73673915863037, eps 0.2118786889963241, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 30, time 46.97532844543457, eps 0.2015940139734384, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
=== ep: 31, time 49.26491117477417, eps 0.191810928470242, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
=== ep: 32, time 57.78001689910889, eps 0.1825049696771952, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 12/12)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 33, time 52.30213952064514, eps 0.17365286785005798, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
=== ep: 34, time 47.82359004020691, eps 0.16523248812340846, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 35, time 49.82910227775574, eps 0.15722277516195018, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
=== ep: 36, time 54.34510016441345, eps 0.1496037005112063, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
goal_identified
=== ep: 37, time 53.781665563583374, eps 0.14235621251595124, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
goal_identified
goal_identified
=== ep: 38, time 55.542104959487915, eps 0.13546218868114893, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
=== ep: 39, time 47.5724093914032, eps 0.1289043903562757, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
goal_identified
=== ep: 40, time 52.697686433792114, eps 0.12266641962971482, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 41, time 42.727951765060425, eps 0.116732678325436, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
goal_identified
=== ep: 42, time 56.24609971046448, eps 0.11108832899943073, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 43, time 52.600182056427, eps 0.10571925783837377, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
goal_identified
=== ep: 44, time 53.66344356536865, eps 0.10061203936773815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
=== ep: 45, time 54.16932487487793, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
=== ep: 46, time 60.48067498207092, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
goal_identified
goal_identified
=== ep: 47, time 59.384390354156494, eps 0.08673687683177911, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
=== ep: 48, time 53.810631275177, eps 0.08255544000718185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 14/14)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
goal_identified
=== ep: 49, time 53.70965909957886, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
goal_identified
goal_identified
goal_identified
=== ep: 50, time 50.23135709762573, eps 0.07479441376288502, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
goal_identified
goal_identified
=== ep: 51, time 56.266278982162476, eps 0.0711954177350367, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 52, time 47.53694176673889, eps 0.06777194681468615, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 53, time 51.799017906188965, eps 0.06451544054132621, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 54, time 54.353723764419556, eps 0.06141775595303503, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
goal_identified
goal_identified
=== ep: 55, time 63.60764789581299, eps 0.05847114722483011, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
goal_identified
goal_identified
=== ep: 56, time 62.339531898498535, eps 0.05566824630007096, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
goal_identified
=== ep: 57, time 61.812251567840576, eps 0.05300204446647978, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
=== ep: 58, time 42.697325468063354, eps 0.050465874830710106, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 59, time 58.87608456611633, eps 0.04805339564764071, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
goal_identified
=== ep: 60, time 48.28130292892456, eps 0.045758574462709686, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
goal_identified
goal_identified
=== ep: 61, time 62.73795700073242, eps 0.043575673027635695, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 62, time 53.91158652305603, eps 0.04149923295180846, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 63, time 62.86267066001892, eps 0.03952406205346913, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 64, time 71.50892329216003, eps 0.03764522137655123, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 65, time 64.45626425743103, eps 0.03585801284071809, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 66, time 67.22656679153442, eps 0.034157967493714775, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 67, time 65.14998722076416, eps 0.03254083433665968, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
goal_identified
=== ep: 68, time 61.27516484260559, eps 0.031002569694333147, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 69, time 56.48422861099243, eps 0.02953932710388308, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
goal_identified
goal_identified
=== ep: 70, time 61.75278115272522, eps 0.028147447696664333, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 71, time 71.05967903137207, eps 0.026823451049161253, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 72, time 74.94724321365356, eps 0.025564026480116013, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 73, time 77.3746383190155, eps 0.02436602477210106, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
goal_identified
goal_identified
=== ep: 74, time 78.00488901138306, eps 0.02322645029683511, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 75, time 69.68342065811157, eps 0.02214245352455219, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 76, time 74.97177362442017, eps 0.02111132389869288, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
goal_identified
=== ep: 77, time 74.4018886089325, eps 0.020130483058101077, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 78, time 66.9728102684021, eps 0.019197478389778148, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
goal_identified
goal_identified
goal_identified
=== ep: 79, time 74.49619269371033, eps 0.018309976896072843, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 80, time 80.53275561332703, eps 0.017465759360972027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 81, time 81.57771897315979, eps 0.01666271480090467, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
=== ep: 82, time 68.28208804130554, eps 0.015898835186183367, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
goal_identified
goal_identified
goal_identified
=== ep: 83, time 73.59646844863892, eps 0.015172210419884185, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
=== ep: 84, time 70.7510347366333, eps 0.014481023561609456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
goal_identified
goal_identified
=== ep: 85, time 81.96507573127747, eps 0.01382354628419033, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
goal_identified
goal_identified
goal_identified
=== ep: 86, time 83.64159870147705, eps 0.013198134551968641, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 87, time 77.20861434936523, eps 0.012603224509851407, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
goal_identified
goal_identified
goal_identified
=== ep: 88, time 72.31638145446777, eps 0.012037328572858524, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 89, time 68.29624080657959, eps 0.011499031706385502, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 90, time 74.62828016281128, eps 0.010986987887879832, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
goal_identified
goal_identified
goal_identified
=== ep: 91, time 76.76971745491028, eps 0.010499916741083536, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
goal_identified
goal_identified
=== ep: 92, time 70.62678623199463, eps 0.010036600334425595, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 93, time 72.35767030715942, eps 0.00959588013555861, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 94, time 78.62659645080566, eps 0.009176654114424539, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
goal_identified
=== ep: 95, time 57.34242844581604, eps 0.00877787398760545, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 96, time 70.08971953392029, eps 0.008398542597069007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 97, time 72.72950839996338, eps 0.008037711416753971, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
=== ep: 98, time 70.86030197143555, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 99, time 73.50270557403564, eps 0.007367984627217855, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
goal_identified
=== ep: 100, time 76.83264708518982, eps 0.007057414352177835, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
goal_identified
goal_identified
goal_identified
=== ep: 101, time 78.28608703613281, eps 0.006761990768184489, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
goal_identified
=== ep: 102, time 81.75286364555359, eps 0.006480975162398559, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 103, time 73.27871513366699, eps 0.006213664849431085, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 104, time 76.41047859191895, eps 0.005959391414263934, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 105, time 75.52061367034912, eps 0.005717519040864065, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
=== ep: 106, time 83.09824204444885, eps 0.005487442922312285, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
=== ep: 107, time 88.8768265247345, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
goal_identified
=== ep: 108, time 70.73406982421875, eps 0.005060406267408787, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 109, time 62.571773529052734, eps 0.004862377916986354, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
goal_identified
goal_identified
goal_identified
=== ep: 110, time 75.00600171089172, eps 0.004674007523179196, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 111, time 70.46216034889221, eps 0.004494824061885041, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 112, time 66.42400884628296, eps 0.0043243794811181555, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
goal_identified
goal_identified
=== ep: 113, time 78.07338547706604, eps 0.0041622475806460035, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 90.41523885726929, eps 0.0040080229462666735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
=== ep: 115, time 77.67076826095581, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 116, time 66.00986838340759, eps 0.003721771716092858, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
goal_identified
=== ep: 117, time 75.63011312484741, eps 0.0035890293431213305, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
goal_identified
goal_identified
goal_identified
=== ep: 118, time 71.51891851425171, eps 0.0034627608920727634, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 119, time 77.84552359580994, eps 0.00334265062604924, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 120, time 74.80126023292542, eps 0.0032283982068230565, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 121, time 82.03576993942261, eps 0.0031197179438347193, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 122, time 65.59006309509277, eps 0.0030163380798177374, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
goal_identified
goal_identified
=== ep: 123, time 80.71115326881409, eps 0.0029180001112638996, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 124, time 75.75368309020996, eps 0.002824458142029865, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 125, time 79.6248505115509, eps 0.0027354782684687108, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 126, time 81.51413464546204, eps 0.0026508379945489875, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 127, time 81.82015037536621, eps 0.0025703256754987464, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 128, time 78.07106852531433, eps 0.0024937399885833667, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 129, time 71.97157502174377, eps 0.0024208894296938593, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
goal_identified
=== ep: 130, time 69.75005412101746, eps 0.0023515918344868374, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
goal_identified
goal_identified
=== ep: 131, time 58.25531792640686, eps 0.002285673922878779, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 132, time 73.51670217514038, eps 0.0022229708657555565, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
goal_identified
goal_identified
goal_identified
=== ep: 133, time 79.81998944282532, eps 0.0021633258728137976, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
goal_identified
goal_identified
=== ep: 134, time 87.68126463890076, eps 0.0021065898005034594, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
goal_identified
goal_identified
=== ep: 135, time 79.68211698532104, eps 0.002052620779091266, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 136, time 73.25730514526367, eps 0.0020012838579124784, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 137, time 79.42046451568604, eps 0.0019524506679239415, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 138, time 76.93031859397888, eps 0.001905999100714611, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 139, time 79.53285956382751, eps 0.001861813003170924, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
goal_identified
goal_identified
goal_identified
=== ep: 140, time 73.40403890609741, eps 0.0018197818870335101, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
goal_identified
goal_identified
goal_identified
=== ep: 141, time 85.67106103897095, eps 0.0017798006526189953, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 142, time 70.13660883903503, eps 0.0017417693260160481, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
goal_identified
goal_identified
goal_identified
=== ep: 143, time 79.35296869277954, eps 0.0017055928090985275, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
goal_identified
goal_identified
=== ep: 144, time 68.12387275695801, eps 0.0016711806417306348, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 145, time 71.33245754241943, eps 0.0016384467755694515, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 146, time 70.0405113697052, eps 0.0016073093588992661, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
goal_identified
=== ep: 147, time 87.09853434562683, eps 0.0015776905319596466, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
goal_identified
goal_identified
goal_identified
=== ep: 148, time 78.52620840072632, eps 0.0015495162322554856, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 149, time 76.47774934768677, eps 0.0015227160093621863, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
goal_identified
goal_identified
goal_identified
=== ep: 150, time 78.28796362876892, eps 0.0014972228487629025, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 151, time 81.28516006469727, eps 0.0014729730042773413, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
goal_identified
goal_identified
goal_identified
=== ep: 152, time 73.21070504188538, eps 0.001449905838663109, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 153, time 75.92299127578735, eps 0.00142796367199102, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 154, time 81.7141945362091, eps 0.0014070916374152305, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 155, time 76.26570415496826, eps 0.001387237543977543, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 156, time 73.32446956634521, eps 0.0013683517461028282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 157, time 67.56304049491882, eps 0.0013503870194592265, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
goal_identified
goal_identified
goal_identified
=== ep: 158, time 79.3039608001709, eps 0.0013332984428727204, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
goal_identified
goal_identified
goal_identified
=== ep: 159, time 70.79952025413513, eps 0.001317043286000802, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
goal_identified
=== ep: 160, time 73.71784138679504, eps 0.0013015809024843582, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
goal_identified
goal_identified
goal_identified
=== ep: 161, time 75.72739052772522, eps 0.0012868726283106018, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
goal_identified
goal_identified
=== ep: 162, time 79.03465986251831, eps 0.0012728816851329014, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 163, time 78.22954392433167, eps 0.0012595730883057546, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 164, time 83.90622115135193, eps 0.001246913559404956, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
goal_identified
=== ep: 165, time 59.32128167152405, eps 0.0012348714430141991, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 166, time 76.60773301124573, eps 0.0012234166275700486, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 167, time 69.44008946418762, eps 0.001212520470067348, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
goal_identified
goal_identified
=== ep: 168, time 76.29150223731995, eps 0.0012021557244367845, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 169, time 73.7542290687561, eps 0.0011922964734155277, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 170, time 79.12143898010254, eps 0.001182918063740569, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 171, time 84.37586688995361, eps 0.0011739970445027263, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
goal_identified
goal_identified
=== ep: 172, time 77.26692318916321, eps 0.0011655111085071537, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
goal_identified
goal_identified
=== ep: 173, time 74.36507391929626, eps 0.001157439036493735, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
goal_identified
goal_identified
=== ep: 174, time 72.47532844543457, eps 0.0011497606440778825, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
goal_identified
goal_identified
goal_identified
=== ep: 175, time 73.89444780349731, eps 0.0011424567312790603, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 176, time 78.36757206916809, eps 0.0011355090345108335, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
goal_identified
goal_identified
goal_identified
=== ep: 177, time 76.42145562171936, eps 0.0011289001809123877, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
goal_identified
goal_identified
goal_identified
=== ep: 178, time 84.29405426979065, eps 0.0011226136449073282, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 179, time 80.93668484687805, eps 0.001116633706881133, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
goal_identified
goal_identified
goal_identified
=== ep: 180, time 65.63747525215149, eps 0.001110945413873925, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 181, time 72.5740385055542, eps 0.001105534542190287, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 182, time 83.17673397064209, eps 0.0011003875618326132, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 183, time 83.54788613319397, eps 0.0010954916026690664, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 184, time 77.68726181983948, eps 0.001090834422251547, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 185, time 73.33784294128418, eps 0.0010864043752031938, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
goal_identified
=== ep: 186, time 83.54392409324646, eps 0.0010821903840988777, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
=== ep: 187, time 72.31743383407593, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 188, time 79.49921607971191, eps 0.0010743689349354123, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 189, time 82.83724093437195, eps 0.0010707419191793434, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
goal_identified
goal_identified
goal_identified
=== ep: 190, time 82.16582155227661, eps 0.0010672917950690429, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
goal_identified
goal_identified
goal_identified
=== ep: 191, time 77.1254334449768, eps 0.0010640099354971456, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 192, time 77.96498823165894, eps 0.0010608881341052777, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
goal_identified
goal_identified
goal_identified
=== ep: 193, time 75.63055562973022, eps 0.0010579185847638855, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 194, time 79.65410470962524, eps 0.0010550938620528466, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
goal_identified
goal_identified
goal_identified
=== ep: 195, time 80.0014386177063, eps 0.001052406902694051, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
goal_identified
goal_identified
goal_identified
=== ep: 196, time 89.03219246864319, eps 0.001049850987889527, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
goal_identified
goal_identified
goal_identified
=== ep: 197, time 76.32506036758423, eps 0.0010474197265209469, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
goal_identified
goal_identified
=== ep: 198, time 65.71290135383606, eps 0.0010451070391685015, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
goal_identified
=== ep: 199, time 64.95935797691345, eps 0.001042907142909185, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
goal_identified
goal_identified
=== ep: 200, time 75.50481486320496, eps 0.001040814536856474, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 201, time 75.4882481098175, eps 0.0010388239884052469, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
=== ep: 202, time 82.39416861534119, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
goal_identified
goal_identified
goal_identified
=== ep: 203, time 82.82949686050415, eps 0.0010351293974264616, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
goal_identified
=== ep: 204, time 87.03286027908325, eps 0.00103341611649703, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
goal_identified
goal_identified
=== ep: 205, time 78.42907285690308, eps 0.0010317863932645186, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
goal_identified
goal_identified
goal_identified
=== ep: 206, time 79.21669554710388, eps 0.0010302361525719613, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
goal_identified
goal_identified
goal_identified
=== ep: 207, time 77.42277503013611, eps 0.0010287615180101426, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
goal_identified
=== ep: 208, time 67.8675184249878, eps 0.001027358802224555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
goal_identified
goal_identified
=== ep: 209, time 84.29817390441895, eps 0.0010260244976950921, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
goal_identified
goal_identified
=== ep: 210, time 79.08283162117004, eps 0.0010247552679654227, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
goal_identified
=== ep: 211, time 77.97886681556702, eps 0.00102354793930011, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 212, time 73.00326943397522, eps 0.0010223994927486214, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
goal_identified
=== ep: 213, time 71.41459631919861, eps 0.001021307056596379, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 214, time 76.1380717754364, eps 0.0010202678991839778, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 215, time 79.03594517707825, eps 0.0010192794220766138, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
goal_identified
goal_identified
goal_identified
=== ep: 216, time 86.46334266662598, eps 0.0010183391535666436, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
goal_identified
goal_identified
goal_identified
=== ep: 217, time 69.43146347999573, eps 0.0010174447424930286, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
goal_identified
goal_identified
goal_identified
=== ep: 218, time 64.02633118629456, eps 0.0010165939523622068, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
goal_identified
=== ep: 219, time 78.8505871295929, eps 0.0010157846557556941, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 220, time 76.7296793460846, eps 0.001015014829010431, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 221, time 77.27677726745605, eps 0.0010142825471585687, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
=== ep: 222, time 105.00024461746216, eps 0.0010135859791140496, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
goal_identified
goal_identified
goal_identified
=== ep: 223, time 80.34872555732727, eps 0.0010129233830939361, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 224, time 81.7812283039093, eps 0.0010122931022630473, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 225, time 81.46119022369385, eps 0.001011693560591007, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 226, time 74.54756164550781, eps 0.0010111232589113477, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
goal_identified
goal_identified
goal_identified
=== ep: 227, time 81.14431953430176, eps 0.0010105807711728136, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 228, time 82.39655876159668, eps 0.0010100647408734893, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
goal_identified
goal_identified
goal_identified
=== ep: 229, time 86.60217785835266, eps 0.001009573877668838, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 230, time 77.4455316066742, eps 0.001009106954145169, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 231, time 71.09954237937927, eps 0.0010086628027504636, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 232, time 76.33259296417236, eps 0.0010082403128748867, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
goal_identified
goal_identified
=== ep: 233, time 79.61191821098328, eps 0.0010078384280736842, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
goal_identified
goal_identified
=== ep: 234, time 76.30618381500244, eps 0.001007456143425521, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 235, time 84.80186295509338, eps 0.001007092503019653, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
goal_identified
goal_identified
goal_identified
=== ep: 236, time 79.15984416007996, eps 0.001006746597565654, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
goal_identified
=== ep: 237, time 80.36335730552673, eps 0.001006417562119715, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
goal_identified
goal_identified
goal_identified
=== ep: 238, time 77.22013568878174, eps 0.0010061045739218342, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
goal_identified
goal_identified
goal_identified
=== ep: 239, time 72.7998559474945, eps 0.0010058068503384884, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 240, time 80.77114367485046, eps 0.001005523646905642, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
goal_identified
goal_identified
goal_identified
=== ep: 241, time 77.91275310516357, eps 0.001005254255467199, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
goal_identified
goal_identified
=== ep: 242, time 83.12807393074036, eps 0.0010049980024042435, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 243, time 77.3861517906189, eps 0.0010047542469506416, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 244, time 78.18537592887878, eps 0.0010045223795907931, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 245, time 68.83729529380798, eps 0.001004301820535524, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 246, time 78.18834829330444, eps 0.0010040920182723119, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
goal_identified
=== ep: 247, time 76.18578100204468, eps 0.0010038924481862177, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
goal_identified
goal_identified
=== ep: 248, time 84.402508020401, eps 0.0010037026112480747, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 249, time 84.10769605636597, eps 0.0010035220327666559, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 250, time 77.48491835594177, eps 0.0010033502612016988, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 251, time 68.54949688911438, eps 0.001003186867034819, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 252, time 78.50844430923462, eps 0.001003031441695491, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
goal_identified
goal_identified
=== ep: 253, time 63.69557023048401, eps 0.0010028835965394094, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
goal_identified
goal_identified
=== ep: 254, time 69.10541868209839, eps 0.0010027429618766747, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 255, time 82.26340532302856, eps 0.0010026091860473767, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
=== ep: 256, time 82.82354521751404, eps 0.0010024819345422614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
goal_identified
=== ep: 257, time 77.2201337814331, eps 0.0010023608891662839, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 258, time 65.27879643440247, eps 0.001002245747242954, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
goal_identified
goal_identified
=== ep: 259, time 65.19391131401062, eps 0.0010021362208574892, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
goal_identified
goal_identified
=== ep: 260, time 76.0188992023468, eps 0.001002032036136876, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 261, time 76.57936096191406, eps 0.0010019329325650452, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 262, time 83.90026497840881, eps 0.0010018386623314465, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 263, time 88.05900073051453, eps 0.0010017489897113931, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 264, time 77.95420551300049, eps 0.0010016636904766263, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 265, time 74.88616442680359, eps 0.0010015825513346283, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
goal_identified
goal_identified
=== ep: 266, time 78.3511209487915, eps 0.0010015053693952815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 267, time 80.83914756774902, eps 0.0010014319516635345, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 268, time 67.06722283363342, eps 0.0010013621145568167, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
goal_identified
=== ep: 269, time 89.6329243183136, eps 0.0010012956834459848, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
goal_identified
=== ep: 270, time 77.8564305305481, eps 0.0010012324922186594, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 82.0452139377594, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
goal_identified
=== ep: 272, time 79.27796030044556, eps 0.0010011152050768812, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 273, time 69.03932476043701, eps 0.0010010608158834819, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
goal_identified
goal_identified
goal_identified
=== ep: 274, time 75.98582243919373, eps 0.0010010090792823456, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
goal_identified
=== ep: 275, time 80.64348673820496, eps 0.0010009598659050213, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 276, time 89.18545579910278, eps 0.0010009130526924313, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 277, time 65.58017563819885, eps 0.0010008685225871602, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
goal_identified
goal_identified
goal_identified
=== ep: 278, time 77.15515327453613, eps 0.0010008261642407504, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 279, time 77.51877284049988, eps 0.001000785871735272, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
goal_identified
goal_identified
=== ep: 280, time 78.38636708259583, eps 0.0010007475443184742, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 281, time 83.48910212516785, eps 0.001000711086151851, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
goal_identified
goal_identified
goal_identified
=== ep: 282, time 86.27746605873108, eps 0.0010006764060709957, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 283, time 82.58254790306091, eps 0.001000643417357642, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 284, time 68.10739135742188, eps 0.0010006120375228235, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
goal_identified
goal_identified
=== ep: 285, time 79.36201810836792, eps 0.0010005821881006083, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 286, time 74.75758194923401, eps 0.0010005537944518927, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 287, time 80.9365451335907, eps 0.0010005267855777657, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
goal_identified
goal_identified
goal_identified
=== ep: 288, time 72.86403274536133, eps 0.0010005010939419733, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 289, time 83.33128952980042, eps 0.001000476655302044, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
goal_identified
goal_identified
goal_identified
=== ep: 290, time 71.24232363700867, eps 0.0010004534085486486, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 291, time 81.20851683616638, eps 0.0010004312955527947, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
goal_identified
=== ep: 292, time 70.23340916633606, eps 0.0010004102610204745, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 293, time 84.15086197853088, eps 0.0010003902523544011, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
goal_identified
goal_identified
goal_identified
=== ep: 294, time 71.51374769210815, eps 0.0010003712195224871, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 295, time 74.8017144203186, eps 0.0010003531149327387, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
goal_identified
goal_identified
=== ep: 296, time 71.2769775390625, eps 0.0010003358933142518, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 297, time 79.27451062202454, eps 0.0010003195116040093, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 298, time 82.46797704696655, eps 0.0010003039288392032, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
goal_identified
=== ep: 299, time 86.916100025177, eps 0.0010002891060548044, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 300, time 79.86633229255676, eps 0.0010002750061861312, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 301, time 74.97977590560913, eps 0.0010002615939761676, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
goal_identified
goal_identified
=== ep: 302, time 74.21989679336548, eps 0.001000248835887403, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 303, time 66.96527624130249, eps 0.0010002367000179694, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 304, time 72.83849668502808, eps 0.0010002251560218723, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 305, time 74.50130558013916, eps 0.0010002141750331084, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 306, time 81.72463274002075, eps 0.0010002037295934862, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 307, time 76.74261212348938, eps 0.0010001937935839656, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
goal_identified
goal_identified
=== ep: 308, time 81.0156672000885, eps 0.0010001843421593476, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
goal_identified
=== ep: 309, time 76.59328436851501, eps 0.0010001753516861473, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
goal_identified
goal_identified
goal_identified
=== ep: 310, time 73.28483748435974, eps 0.0010001667996834991, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
goal_identified
=== ep: 311, time 91.54320645332336, eps 0.001000158664766942, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
goal_identified
goal_identified
goal_identified
=== ep: 312, time 78.10253262519836, eps 0.0010001509265949466, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
goal_identified
goal_identified
goal_identified
=== ep: 313, time 80.81039881706238, eps 0.001000143565818053, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 314, time 69.20507264137268, eps 0.0010001365640304844, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 315, time 71.29969668388367, eps 0.0010001299037241253, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 316, time 71.6489429473877, eps 0.0010001235682447402, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
goal_identified
goal_identified
=== ep: 317, time 77.57650899887085, eps 0.0010001175417503308, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 318, time 78.78231024742126, eps 0.0010001118091715218, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
goal_identified
goal_identified
goal_identified
=== ep: 319, time 76.2650191783905, eps 0.0010001063561738807, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 320, time 79.20049238204956, eps 0.0010001011691220727, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 321, time 69.34375619888306, eps 0.0010000962350457665, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 322, time 81.681720495224, eps 0.0010000915416072012, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 323, time 82.7981185913086, eps 0.0010000870770703358, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 324, time 78.81365513801575, eps 0.0010000828302715028, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
goal_identified
goal_identified
=== ep: 325, time 78.26383686065674, eps 0.0010000787905914928, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 326, time 73.46402907371521, eps 0.0010000749479290019, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
goal_identified
goal_identified
=== ep: 327, time 76.86579418182373, eps 0.001000071292675372, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
goal_identified
goal_identified
=== ep: 328, time 75.16495060920715, eps 0.001000067815690565, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 329, time 84.86615920066833, eps 0.0010000645082803084, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
goal_identified
=== ep: 330, time 79.64363765716553, eps 0.0010000613621743532, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
=== ep: 331, time 74.41301584243774, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 332, time 71.58692073822021, eps 0.0010000555227914069, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
=== ep: 333, time 73.26189661026001, eps 0.0010000528149129166, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 334, time 74.9351646900177, eps 0.0010000502390992187, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 335, time 77.83461785316467, eps 0.0010000477889094373, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 336, time 87.20548844337463, eps 0.0010000454582168217, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
=== ep: 337, time 73.58082580566406, eps 0.001000043241193426, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 338, time 71.08903527259827, eps 0.0010000411322955373, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 339, time 75.0887508392334, eps 0.0010000391262498123, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
goal_identified
goal_identified
=== ep: 340, time 78.54559063911438, eps 0.001000037218040092, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 341, time 73.45773482322693, eps 0.0010000354028948577, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 342, time 84.34114027023315, eps 0.0010000336762753012, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 343, time 74.60554337501526, eps 0.001000032033863974, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 344, time 78.2351405620575, eps 0.0010000304715539925, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 345, time 82.3947274684906, eps 0.001000028985438768, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 346, time 75.10130906105042, eps 0.001000027571802238, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
goal_identified
goal_identified
=== ep: 347, time 79.85496687889099, eps 0.0010000262271095755, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
goal_identified
goal_identified
goal_identified
=== ep: 348, time 72.69448852539062, eps 0.0010000249479983478, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 349, time 71.83921360969543, eps 0.0010000237312701107, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
goal_identified
goal_identified
goal_identified
=== ep: 350, time 65.58623361587524, eps 0.00100002257388241, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
goal_identified
goal_identified
goal_identified
=== ep: 351, time 74.3236620426178, eps 0.0010000214729411737, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
goal_identified
goal_identified
=== ep: 352, time 62.14995813369751, eps 0.0010000204256934752, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
goal_identified
goal_identified
=== ep: 353, time 72.4717857837677, eps 0.0010000194295206493, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 354, time 79.84175229072571, eps 0.0010000184819317455, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
goal_identified
goal_identified
=== ep: 355, time 81.41731715202332, eps 0.001000017580557298, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 356, time 79.4590265750885, eps 0.001000016723143401, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
goal_identified
goal_identified
goal_identified
=== ep: 357, time 66.33781814575195, eps 0.0010000159075460732, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
goal_identified
goal_identified
goal_identified
=== ep: 358, time 78.96510004997253, eps 0.0010000151317258964, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
goal_identified
goal_identified
=== ep: 359, time 83.29888010025024, eps 0.0010000143937429161, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 360, time 78.10246443748474, eps 0.0010000136917517905, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 361, time 68.78306651115417, eps 0.001000013023997176, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 74.66499352455139, eps 0.0010000123888093385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
goal_identified
goal_identified
goal_identified
=== ep: 363, time 81.50347638130188, eps 0.0010000117845999773, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
goal_identified
=== ep: 364, time 70.86874103546143, eps 0.0010000112098582543, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 365, time 70.52219319343567, eps 0.001000010663147016, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
goal_identified
goal_identified
goal_identified
=== ep: 366, time 77.35293889045715, eps 0.0010000101430991996, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
goal_identified
goal_identified
goal_identified
=== ep: 367, time 75.621657371521, eps 0.0010000096484144142, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 368, time 78.35523390769958, eps 0.0010000091778556905, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
goal_identified
goal_identified
=== ep: 369, time 70.67111420631409, eps 0.0010000087302463867, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 370, time 67.03061604499817, eps 0.001000008304467246, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 371, time 73.32035827636719, eps 0.0010000078994535993, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
goal_identified
goal_identified
=== ep: 372, time 73.0785129070282, eps 0.0010000075141927012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
goal_identified
goal_identified
=== ep: 373, time 58.8990581035614, eps 0.0010000071477211988, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 374, time 78.68309593200684, eps 0.0010000067991227223, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 375, time 58.84876465797424, eps 0.0010000064675255943, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 376, time 73.31993770599365, eps 0.001000006152100649, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 377, time 68.86061763763428, eps 0.0010000058520591598, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 378, time 76.70336580276489, eps 0.0010000055666508666, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
goal_identified
goal_identified
=== ep: 379, time 67.78720545768738, eps 0.0010000052951621003, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 380, time 63.49985194206238, eps 0.0010000050369139975, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 381, time 67.32858633995056, eps 0.001000004791260803, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
goal_identified
goal_identified
goal_identified
=== ep: 382, time 68.0713472366333, eps 0.0010000045575882562, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 383, time 70.73728036880493, eps 0.001000004335312054, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
goal_identified
goal_identified
goal_identified
=== ep: 384, time 74.08618950843811, eps 0.0010000041238763903, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 385, time 62.82171154022217, eps 0.0010000039227525655, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
goal_identified
goal_identified
goal_identified
=== ep: 386, time 77.59599542617798, eps 0.0010000037314376652, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 387, time 75.60094285011292, eps 0.001000003549453303, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 388, time 66.91793370246887, eps 0.0010000033763444226, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 389, time 77.9650559425354, eps 0.001000003211678162, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 390, time 75.53215074539185, eps 0.0010000030550427698, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 391, time 80.8355610370636, eps 0.0010000029060465757, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 392, time 69.67350268363953, eps 0.0010000027643170119, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
goal_identified
goal_identified
goal_identified
=== ep: 393, time 80.47846722602844, eps 0.0010000026294996803, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 394, time 76.55312871932983, eps 0.0010000025012574677, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 395, time 79.50519251823425, eps 0.0010000023792697014, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
goal_identified
goal_identified
=== ep: 396, time 70.06266951560974, eps 0.0010000022632313489, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 397, time 77.96871280670166, eps 0.0010000021528522535, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
goal_identified
goal_identified
goal_identified
=== ep: 398, time 74.45988583564758, eps 0.00100000204785641, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
goal_identified
goal_identified
goal_identified
=== ep: 399, time 72.8478536605835, eps 0.0010000019479812744, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 81.89634561538696, eps 0.0010000018529771066, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 401, time 75.35352039337158, eps 0.0010000017626063467, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 402, time 76.40782260894775, eps 0.0010000016766430208, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
goal_identified
goal_identified
goal_identified
=== ep: 403, time 74.1404447555542, eps 0.0010000015948721758, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 404, time 73.80713152885437, eps 0.001000001517089342, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 405, time 80.38510632514954, eps 0.0010000014431000217, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 406, time 68.61589884757996, eps 0.001000001372719203, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
goal_identified
goal_identified
goal_identified
=== ep: 407, time 76.95974540710449, eps 0.0010000013057708975, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
=== ep: 408, time 81.20460486412048, eps 0.0010000012420876994, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
goal_identified
goal_identified
=== ep: 409, time 78.25078701972961, eps 0.0010000011815103674, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 410, time 79.37940049171448, eps 0.001000001123887427, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
goal_identified
goal_identified
goal_identified
=== ep: 411, time 76.77081346511841, eps 0.0010000010690747903, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 412, time 79.48465967178345, eps 0.0010000010169353975, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 413, time 81.35898280143738, eps 0.0010000009673388729, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
goal_identified
goal_identified
goal_identified
=== ep: 414, time 68.87909746170044, eps 0.0010000009201611994, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 415, time 82.74738788604736, eps 0.0010000008752844081, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
goal_identified
goal_identified
=== ep: 416, time 77.59212493896484, eps 0.0010000008325962838, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
goal_identified
goal_identified
=== ep: 417, time 78.24441909790039, eps 0.001000000791990084, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 418, time 77.95492553710938, eps 0.0010000007533642718, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
goal_identified
goal_identified
goal_identified
=== ep: 419, time 79.61622834205627, eps 0.0010000007166222626, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 420, time 76.00520753860474, eps 0.0010000006816721825, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
goal_identified
goal_identified
=== ep: 421, time 72.58393692970276, eps 0.001000000648426638, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
goal_identified
goal_identified
goal_identified
=== ep: 422, time 79.10904860496521, eps 0.0010000006168024976, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 423, time 75.7749547958374, eps 0.0010000005867206849, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
goal_identified
goal_identified
goal_identified
=== ep: 424, time 77.6736421585083, eps 0.0010000005581059794, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 425, time 78.41296172142029, eps 0.0010000005308868295, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 426, time 69.908522605896, eps 0.0010000005049951733, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
goal_identified
=== ep: 427, time 83.35061573982239, eps 0.001000000480366268, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 428, time 76.95474767684937, eps 0.0010000004569385287, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 429, time 71.89674687385559, eps 0.0010000004346533736, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 430, time 81.38048362731934, eps 0.0010000004134550786, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 431, time 76.08134841918945, eps 0.0010000003932906364, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 432, time 74.57567954063416, eps 0.0010000003741096257, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 433, time 67.43269872665405, eps 0.001000000355864084, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
goal_identified
goal_identified
goal_identified
=== ep: 434, time 75.29057574272156, eps 0.0010000003385083878, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
goal_identified
goal_identified
goal_identified
=== ep: 435, time 76.97913670539856, eps 0.001000000321999139, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 436, time 74.35524225234985, eps 0.0010000003062950555, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
goal_identified
goal_identified
=== ep: 437, time 75.51112794876099, eps 0.0010000002913568694, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 438, time 78.94101929664612, eps 0.0010000002771472273, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
goal_identified
=== ep: 439, time 83.92427778244019, eps 0.0010000002636305976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
goal_identified
goal_identified
=== ep: 440, time 72.97681784629822, eps 0.0010000002507731815, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 441, time 80.3926203250885, eps 0.0010000002385428292, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
goal_identified
goal_identified
=== ep: 442, time 77.6961362361908, eps 0.0010000002269089582, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 443, time 74.14548087120056, eps 0.0010000002158424776, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 444, time 82.73651194572449, eps 0.0010000002053157158, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 445, time 76.51464438438416, eps 0.0010000001953023503, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 446, time 66.80294489860535, eps 0.001000000185777342, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 447, time 82.5115020275116, eps 0.0010000001767168742, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
goal_identified
goal_identified
=== ep: 448, time 80.56546783447266, eps 0.0010000001680982905, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 449, time 79.27421593666077, eps 0.0010000001599000403, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
goal_identified
goal_identified
goal_identified
=== ep: 450, time 75.66665983200073, eps 0.0010000001521016232, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 451, time 75.35267639160156, eps 0.0010000001446835395, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
goal_identified
goal_identified
=== ep: 452, time 78.66251611709595, eps 0.0010000001376272401, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 453, time 70.14348292350769, eps 0.0010000001309150804, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 454, time 80.59400272369385, eps 0.0010000001245302765, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 455, time 67.22453284263611, eps 0.0010000001184568633, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 456, time 72.44632315635681, eps 0.0010000001126796538, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
goal_identified
goal_identified
=== ep: 457, time 81.51780676841736, eps 0.0010000001071842023, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 458, time 76.3441481590271, eps 0.001000000101956767, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
goal_identified
goal_identified
goal_identified
=== ep: 459, time 68.24481916427612, eps 0.001000000096984277, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 460, time 74.20680928230286, eps 0.001000000092254298, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
goal_identified
goal_identified
goal_identified
=== ep: 461, time 80.96318459510803, eps 0.0010000000877550027, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 462, time 77.25809574127197, eps 0.0010000000834751407, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 463, time 74.6532769203186, eps 0.00100000007940401, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 464, time 73.74281930923462, eps 0.0010000000755314307, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
goal_identified
goal_identified
=== ep: 465, time 70.4664249420166, eps 0.0010000000718477194, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
goal_identified
goal_identified
goal_identified
=== ep: 466, time 77.94862508773804, eps 0.0010000000683436647, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
goal_identified
goal_identified
goal_identified
=== ep: 467, time 83.01105403900146, eps 0.001000000065010505, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 71.60381174087524, eps 0.0010000000618399052, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
goal_identified
=== ep: 469, time 69.2269241809845, eps 0.0010000000588239375, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
goal_identified
goal_identified
=== ep: 470, time 79.09225702285767, eps 0.0010000000559550603, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 471, time 82.06734490394592, eps 0.0010000000532260998, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
goal_identified
goal_identified
goal_identified
=== ep: 472, time 70.09924936294556, eps 0.0010000000506302322, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 473, time 74.00754904747009, eps 0.0010000000481609666, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 474, time 76.88365840911865, eps 0.0010000000458121286, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
goal_identified
goal_identified
=== ep: 475, time 76.83148670196533, eps 0.0010000000435778447, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 476, time 78.30260276794434, eps 0.001000000041452528, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
=== ep: 477, time 74.17701244354248, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
goal_identified
goal_identified
goal_identified
=== ep: 478, time 78.03729248046875, eps 0.0010000000375077985, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 479, time 71.7869484424591, eps 0.0010000000356785216, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
goal_identified
=== ep: 480, time 83.88827753067017, eps 0.0010000000339384595, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
goal_identified
goal_identified
=== ep: 481, time 79.46876454353333, eps 0.0010000000322832614, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 482, time 78.25165796279907, eps 0.0010000000307087882, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
goal_identified
=== ep: 483, time 77.72896885871887, eps 0.001000000029211103, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 484, time 78.55189871788025, eps 0.0010000000277864607, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
goal_identified
goal_identified
goal_identified
=== ep: 485, time 79.18420171737671, eps 0.0010000000264312988, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
goal_identified
goal_identified
=== ep: 486, time 78.1243588924408, eps 0.0010000000251422292, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 487, time 73.26774144172668, eps 0.0010000000239160282, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
goal_identified
goal_identified
=== ep: 488, time 82.22316861152649, eps 0.00100000002274963, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 489, time 66.49762296676636, eps 0.0010000000216401172, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
goal_identified
goal_identified
=== ep: 490, time 81.0059335231781, eps 0.0010000000205847162, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 491, time 78.62210392951965, eps 0.0010000000195807877, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 492, time 70.38351655006409, eps 0.0010000000186258216, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
goal_identified
goal_identified
=== ep: 493, time 77.5280909538269, eps 0.0010000000177174295, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 494, time 74.76123547554016, eps 0.0010000000168533404, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 495, time 77.90030169487, eps 0.0010000000160313932, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
goal_identified
goal_identified
=== ep: 496, time 65.0165433883667, eps 0.001000000015249533, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
goal_identified
goal_identified
=== ep: 497, time 78.66420269012451, eps 0.0010000000145058043, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
=== ep: 498, time 69.53900170326233, eps 0.001000000013798348, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 499, time 72.97457981109619, eps 0.0010000000131253947, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 500, time 66.45471477508545, eps 0.0010000000124852615, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 501, time 79.7467725276947, eps 0.0010000000118763482, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 502, time 65.21750450134277, eps 0.0010000000112971319, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
goal_identified
goal_identified
goal_identified
=== ep: 503, time 65.81260871887207, eps 0.0010000000107461642, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
goal_identified
=== ep: 504, time 75.58163356781006, eps 0.0010000000102220676, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 505, time 71.04554390907288, eps 0.0010000000097235315, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 506, time 73.34552121162415, eps 0.0010000000092493092, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 507, time 71.14366722106934, eps 0.0010000000087982152, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 508, time 74.9281542301178, eps 0.0010000000083691212, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 509, time 72.12860202789307, eps 0.0010000000079609542, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 510, time 73.31500768661499, eps 0.001000000007572694, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 511, time 74.17094111442566, eps 0.0010000000072033692, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
=== ep: 512, time 70.9892795085907, eps 0.001000000006852057, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 513, time 72.64825129508972, eps 0.001000000006517878, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 514, time 57.655107498168945, eps 0.0010000000061999974, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
goal_identified
goal_identified
=== ep: 515, time 57.69859027862549, eps 0.0010000000058976199, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 516, time 57.09614109992981, eps 0.0010000000056099897, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 517, time 49.23873019218445, eps 0.0010000000053363872, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 518, time 61.84329295158386, eps 0.0010000000050761286, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
goal_identified
goal_identified
goal_identified
=== ep: 519, time 58.253320932388306, eps 0.001000000004828563, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 49.72667741775513, eps 0.001000000004593071, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 521, time 64.18262243270874, eps 0.0010000000043690644, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 522, time 49.839195251464844, eps 0.0010000000041559827, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 523, time 53.99372363090515, eps 0.0010000000039532928, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 524, time 51.71240305900574, eps 0.0010000000037604885, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 525, time 57.78037452697754, eps 0.0010000000035770874, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 526, time 53.979233264923096, eps 0.0010000000034026306, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
goal_identified
=== ep: 527, time 56.43306350708008, eps 0.0010000000032366824, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
goal_identified
=== ep: 528, time 54.17308044433594, eps 0.0010000000030788276, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 529, time 65.39080595970154, eps 0.0010000000029286714, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
goal_identified
goal_identified
goal_identified
=== ep: 530, time 57.84594702720642, eps 0.0010000000027858384, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 531, time 52.43787479400635, eps 0.0010000000026499714, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 532, time 67.5781147480011, eps 0.0010000000025207308, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 533, time 57.38918662071228, eps 0.0010000000023977934, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
goal_identified
goal_identified
goal_identified
=== ep: 534, time 51.28467345237732, eps 0.0010000000022808515, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 535, time 52.22535467147827, eps 0.0010000000021696133, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
goal_identified
goal_identified
goal_identified
=== ep: 536, time 58.476903438568115, eps 0.0010000000020637999, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
goal_identified
goal_identified
goal_identified
=== ep: 537, time 51.81420159339905, eps 0.0010000000019631471, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
goal_identified
goal_identified
=== ep: 538, time 57.41335654258728, eps 0.0010000000018674034, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
goal_identified
goal_identified
goal_identified
=== ep: 539, time 56.507811307907104, eps 0.001000000001776329, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 540, time 59.114893674850464, eps 0.0010000000016896964, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 541, time 64.46960592269897, eps 0.001000000001607289, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 542, time 48.71809387207031, eps 0.0010000000015289005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 543, time 57.63064885139465, eps 0.0010000000014543352, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
goal_identified
goal_identified
=== ep: 544, time 56.536409854888916, eps 0.0010000000013834064, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 545, time 47.62042236328125, eps 0.001000000001315937, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 54.448140144348145, eps 0.0010000000012517578, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
goal_identified
=== ep: 547, time 61.056588888168335, eps 0.001000000001190709, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 548, time 47.392579793930054, eps 0.0010000000011326374, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 549, time 61.285956144332886, eps 0.001000000001077398, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 550, time 66.52630639076233, eps 0.0010000000010248527, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 57.332517862319946, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 552, time 60.74501657485962, eps 0.001000000000927325, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 553, time 58.334372758865356, eps 0.0010000000008820989, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
goal_identified
=== ep: 554, time 60.121206283569336, eps 0.0010000000008390784, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
goal_identified
goal_identified
goal_identified
=== ep: 555, time 60.25186014175415, eps 0.001000000000798156, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 556, time 59.042951583862305, eps 0.0010000000007592295, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 557, time 63.03054738044739, eps 0.0010000000007222014, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
goal_identified
goal_identified
goal_identified
=== ep: 558, time 46.739763498306274, eps 0.0010000000006869794, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 559, time 64.11378622055054, eps 0.001000000000653475, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
goal_identified
goal_identified
goal_identified
=== ep: 560, time 58.409852504730225, eps 0.0010000000006216046, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 561, time 48.42918157577515, eps 0.0010000000005912885, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
goal_identified
goal_identified
=== ep: 562, time 61.23102259635925, eps 0.0010000000005624511, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 563, time 55.606884241104126, eps 0.00100000000053502, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 564, time 55.04731583595276, eps 0.001000000000508927, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 565, time 59.96177363395691, eps 0.001000000000484106, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
goal_identified
goal_identified
goal_identified
=== ep: 566, time 64.90430569648743, eps 0.001000000000460496, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 567, time 51.54603600502014, eps 0.0010000000004380374, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 568, time 62.48159313201904, eps 0.001000000000416674, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 569, time 63.31014132499695, eps 0.0010000000003963527, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 570, time 53.60358285903931, eps 0.0010000000003770222, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
goal_identified
goal_identified
goal_identified
=== ep: 571, time 55.35127329826355, eps 0.0010000000003586346, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
goal_identified
goal_identified
goal_identified
=== ep: 572, time 50.87685465812683, eps 0.0010000000003411438, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
goal_identified
goal_identified
=== ep: 573, time 57.658069372177124, eps 0.001000000000324506, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
goal_identified
=== ep: 574, time 54.19726800918579, eps 0.0010000000003086798, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 16/16)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 575, time 59.27985882759094, eps 0.0010000000002936252, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 576, time 54.69955039024353, eps 0.001000000000279305, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
goal_identified
goal_identified
=== ep: 577, time 64.75026226043701, eps 0.0010000000002656831, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
goal_identified
goal_identified
goal_identified
=== ep: 578, time 56.97770619392395, eps 0.0010000000002527256, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
goal_identified
goal_identified
goal_identified
=== ep: 579, time 56.52519965171814, eps 0.0010000000002404, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 580, time 66.75398850440979, eps 0.0010000000002286756, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 581, time 57.63288116455078, eps 0.0010000000002175229, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
goal_identified
goal_identified
goal_identified
=== ep: 582, time 56.98495674133301, eps 0.0010000000002069142, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 583, time 63.084203243255615, eps 0.0010000000001968228, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
goal_identified
goal_identified
goal_identified
=== ep: 584, time 64.29667735099792, eps 0.0010000000001872237, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 585, time 64.67867255210876, eps 0.0010000000001780928, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 57.62881398200989, eps 0.001000000000169407, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 587, time 63.77518916130066, eps 0.001000000000161145, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
goal_identified
goal_identified
goal_identified
=== ep: 588, time 68.84025454521179, eps 0.0010000000001532858, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 589, time 55.43739128112793, eps 0.00100000000014581, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 590, time 62.454221963882446, eps 0.0010000000001386988, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 591, time 70.36641716957092, eps 0.0010000000001319344, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 592, time 55.97038698196411, eps 0.0010000000001255, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 593, time 58.268409729003906, eps 0.0010000000001193791, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 594, time 64.60220241546631, eps 0.001000000000113557, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 595, time 60.25252366065979, eps 0.0010000000001080186, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 596, time 48.194016456604004, eps 0.0010000000001027505, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
goal_identified
goal_identified
goal_identified
=== ep: 597, time 51.20977759361267, eps 0.0010000000000977393, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 598, time 56.88610029220581, eps 0.0010000000000929725, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 599, time 53.49645447731018, eps 0.0010000000000884382, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 600, time 64.65747284889221, eps 0.001000000000084125, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
goal_identified
=== ep: 601, time 65.69358277320862, eps 0.0010000000000800222, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 602, time 52.72396636009216, eps 0.0010000000000761195, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
goal_identified
goal_identified
goal_identified
=== ep: 603, time 65.74198031425476, eps 0.0010000000000724072, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
goal_identified
=== ep: 604, time 59.55510354042053, eps 0.0010000000000688757, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 605, time 63.058979511260986, eps 0.0010000000000655166, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
goal_identified
goal_identified
goal_identified
=== ep: 606, time 66.59517478942871, eps 0.0010000000000623215, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 60.848122119903564, eps 0.001000000000059282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
goal_identified
goal_identified
=== ep: 608, time 65.07788729667664, eps 0.0010000000000563907, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
goal_identified
goal_identified
goal_identified
=== ep: 609, time 62.33442187309265, eps 0.0010000000000536405, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 610, time 57.849148988723755, eps 0.0010000000000510245, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
goal_identified
goal_identified
=== ep: 611, time 68.78790998458862, eps 0.0010000000000485358, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
goal_identified
goal_identified
=== ep: 612, time 64.95833802223206, eps 0.0010000000000461688, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 613, time 48.477388858795166, eps 0.0010000000000439171, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
goal_identified
goal_identified
=== ep: 614, time 63.168256759643555, eps 0.0010000000000417752, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 615, time 56.54207682609558, eps 0.0010000000000397378, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 615
goal_identified
goal_identified
=== ep: 616, time 51.19251465797424, eps 0.0010000000000377999, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 49.57887673377991, eps 0.0010000000000359563, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 618, time 58.61147665977478, eps 0.0010000000000342027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 619, time 52.071500062942505, eps 0.0010000000000325345, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
goal_identified
=== ep: 620, time 59.446295976638794, eps 0.001000000000030948, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
goal_identified
goal_identified
goal_identified
=== ep: 621, time 60.7683367729187, eps 0.0010000000000294385, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 622, time 56.79270315170288, eps 0.0010000000000280028, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
goal_identified
goal_identified
goal_identified
=== ep: 623, time 64.99949979782104, eps 0.0010000000000266371, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 624, time 55.62669110298157, eps 0.001000000000025338, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
goal_identified
goal_identified
=== ep: 625, time 58.720032930374146, eps 0.0010000000000241023, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 626, time 59.416882276535034, eps 0.0010000000000229268, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 627, time 57.88483214378357, eps 0.0010000000000218085, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 628, time 48.00286030769348, eps 0.001000000000020745, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 629, time 55.912636518478394, eps 0.0010000000000197332, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
goal_identified
goal_identified
=== ep: 630, time 55.66297793388367, eps 0.0010000000000187708, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 631, time 51.63310384750366, eps 0.0010000000000178553, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 632, time 68.4719181060791, eps 0.0010000000000169845, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
=== ep: 633, time 53.405301570892334, eps 0.0010000000000161562, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
goal_identified
goal_identified
goal_identified
=== ep: 634, time 60.27547907829285, eps 0.0010000000000153684, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
goal_identified
goal_identified
=== ep: 635, time 67.56824278831482, eps 0.0010000000000146188, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 636, time 56.312398195266724, eps 0.0010000000000139058, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 637, time 63.293211460113525, eps 0.0010000000000132275, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 638, time 50.71242952346802, eps 0.0010000000000125824, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
goal_identified
=== ep: 639, time 51.830652952194214, eps 0.0010000000000119687, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
=== ep: 640, time 54.6545991897583, eps 0.001000000000011385, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 641, time 51.017990827560425, eps 0.00100000000001083, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
goal_identified
goal_identified
goal_identified
=== ep: 642, time 57.43341898918152, eps 0.0010000000000103017, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 643, time 58.04100060462952, eps 0.0010000000000097993, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
goal_identified
=== ep: 644, time 56.71023774147034, eps 0.0010000000000093213, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
goal_identified
goal_identified
goal_identified
=== ep: 645, time 55.44712018966675, eps 0.0010000000000088666, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
goal_identified
goal_identified
=== ep: 646, time 66.67782282829285, eps 0.0010000000000084342, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
goal_identified
goal_identified
goal_identified
=== ep: 647, time 61.39030623435974, eps 0.001000000000008023, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 648, time 57.25149607658386, eps 0.0010000000000076317, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 649, time 68.59713864326477, eps 0.0010000000000072594, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 650, time 52.998026847839355, eps 0.0010000000000069055, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
goal_identified
goal_identified
goal_identified
=== ep: 651, time 63.25206112861633, eps 0.0010000000000065686, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
goal_identified
=== ep: 652, time 69.45336580276489, eps 0.0010000000000062483, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 653, time 59.75245118141174, eps 0.0010000000000059436, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
goal_identified
=== ep: 654, time 58.64089393615723, eps 0.0010000000000056537, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 655, time 58.089555978775024, eps 0.0010000000000053779, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 656, time 58.38562035560608, eps 0.0010000000000051157, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 657, time 59.2143292427063, eps 0.0010000000000048661, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 658, time 48.17211079597473, eps 0.001000000000004629, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
goal_identified
goal_identified
=== ep: 659, time 57.20084524154663, eps 0.0010000000000044032, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 660, time 46.6247296333313, eps 0.0010000000000041883, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 661, time 57.88924789428711, eps 0.001000000000003984, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 662, time 53.19954967498779, eps 0.0010000000000037897, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 663, time 63.557974338531494, eps 0.001000000000003605, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 664, time 60.42387318611145, eps 0.0010000000000034291, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 665, time 52.65921092033386, eps 0.001000000000003262, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 666, time 69.16018915176392, eps 0.0010000000000031028, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 56.94273257255554, eps 0.0010000000000029514, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 668, time 57.07066035270691, eps 0.0010000000000028075, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 669, time 63.23442840576172, eps 0.0010000000000026706, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
goal_identified
goal_identified
goal_identified
=== ep: 670, time 49.189619302749634, eps 0.0010000000000025403, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 671, time 58.36392593383789, eps 0.0010000000000024165, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 672, time 54.746201276779175, eps 0.0010000000000022985, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 673, time 47.30891036987305, eps 0.0010000000000021864, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 674, time 56.159501791000366, eps 0.00100000000000208, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
goal_identified
goal_identified
goal_identified
=== ep: 675, time 59.22287154197693, eps 0.0010000000000019785, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 676, time 48.27881836891174, eps 0.001000000000001882, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
goal_identified
=== ep: 677, time 64.17970418930054, eps 0.0010000000000017903, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 678, time 49.09171223640442, eps 0.0010000000000017029, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 679, time 61.19546055793762, eps 0.0010000000000016198, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 680, time 58.75130343437195, eps 0.0010000000000015409, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
goal_identified
goal_identified
=== ep: 681, time 59.009835958480835, eps 0.0010000000000014656, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 682, time 61.48654532432556, eps 0.0010000000000013943, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 683, time 54.68758773803711, eps 0.0010000000000013262, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
goal_identified
goal_identified
goal_identified
=== ep: 684, time 58.228687047958374, eps 0.0010000000000012616, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 685, time 56.82533311843872, eps 0.0010000000000012, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
goal_identified
goal_identified
goal_identified
=== ep: 686, time 55.81100058555603, eps 0.0010000000000011415, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 687, time 48.732696771621704, eps 0.0010000000000010857, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 688, time 63.313334226608276, eps 0.0010000000000010328, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 689, time 42.481526374816895, eps 0.0010000000000009825, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
goal_identified
goal_identified
=== ep: 690, time 63.93286895751953, eps 0.0010000000000009346, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 691, time 55.686442375183105, eps 0.001000000000000889, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 692, time 52.63332557678223, eps 0.0010000000000008457, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 693, time 64.71237015724182, eps 0.0010000000000008045, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 694, time 47.00300216674805, eps 0.0010000000000007653, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
goal_identified
goal_identified
=== ep: 695, time 61.68054747581482, eps 0.0010000000000007277, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
goal_identified
goal_identified
=== ep: 696, time 56.900803327560425, eps 0.0010000000000006924, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 697, time 51.42188739776611, eps 0.0010000000000006586, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
goal_identified
goal_identified
=== ep: 698, time 56.06693482398987, eps 0.0010000000000006265, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
goal_identified
goal_identified
=== ep: 699, time 55.29628324508667, eps 0.001000000000000596, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 700, time 57.50496172904968, eps 0.0010000000000005668, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 701, time 56.4925274848938, eps 0.0010000000000005393, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
goal_identified
goal_identified
=== ep: 702, time 63.90394997596741, eps 0.0010000000000005128, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 702
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 703, time 56.549914836883545, eps 0.001000000000000488, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 704, time 59.7830491065979, eps 0.001000000000000464, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
goal_identified
goal_identified
goal_identified
=== ep: 705, time 60.12844133377075, eps 0.0010000000000004415, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 706, time 54.43039894104004, eps 0.00100000000000042, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 707, time 65.52004480361938, eps 0.0010000000000003994, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 708, time 48.04841423034668, eps 0.00100000000000038, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 709, time 57.95861506462097, eps 0.0010000000000003615, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 710, time 54.20956087112427, eps 0.0010000000000003437, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 711, time 51.20073223114014, eps 0.001000000000000327, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
goal_identified
goal_identified
=== ep: 712, time 62.07952857017517, eps 0.0010000000000003112, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 713, time 55.187615394592285, eps 0.001000000000000296, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 714, time 52.85523009300232, eps 0.0010000000000002815, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
goal_identified
goal_identified
goal_identified
=== ep: 715, time 51.91174125671387, eps 0.0010000000000002678, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 716, time 55.6004433631897, eps 0.0010000000000002548, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
goal_identified
goal_identified
goal_identified
=== ep: 717, time 50.093907594680786, eps 0.0010000000000002422, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 718, time 59.03023171424866, eps 0.0010000000000002305, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
goal_identified
=== ep: 719, time 54.90226483345032, eps 0.0010000000000002192, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
goal_identified
goal_identified
goal_identified
=== ep: 720, time 51.29951477050781, eps 0.0010000000000002086, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
goal_identified
=== ep: 721, time 64.76664471626282, eps 0.0010000000000001984, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 722, time 48.094773292541504, eps 0.0010000000000001887, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 723, time 61.09011220932007, eps 0.0010000000000001796, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
goal_identified
goal_identified
goal_identified
=== ep: 724, time 61.71456503868103, eps 0.0010000000000001707, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 725, time 43.97040295600891, eps 0.0010000000000001624, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
goal_identified
goal_identified
goal_identified
=== ep: 726, time 63.533018350601196, eps 0.0010000000000001544, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 727, time 43.83807873725891, eps 0.001000000000000147, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
goal_identified
goal_identified
goal_identified
=== ep: 728, time 64.98878645896912, eps 0.0010000000000001399, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 729, time 63.65436887741089, eps 0.001000000000000133, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 730, time 54.392544746398926, eps 0.0010000000000001264, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 731, time 62.438074350357056, eps 0.0010000000000001204, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 732, time 58.66808009147644, eps 0.0010000000000001145, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
goal_identified
goal_identified
goal_identified
=== ep: 733, time 61.71646022796631, eps 0.0010000000000001089, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
goal_identified
goal_identified
=== ep: 734, time 56.462024211883545, eps 0.0010000000000001037, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 735, time 58.54923462867737, eps 0.0010000000000000985, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
goal_identified
goal_identified
goal_identified
=== ep: 736, time 63.162429094314575, eps 0.0010000000000000937, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
goal_identified
goal_identified
goal_identified
=== ep: 737, time 52.215455293655396, eps 0.0010000000000000891, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
goal_identified
goal_identified
=== ep: 738, time 52.14827251434326, eps 0.0010000000000000848, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 739, time 56.81861352920532, eps 0.0010000000000000807, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
goal_identified
goal_identified
goal_identified
=== ep: 740, time 53.45660138130188, eps 0.0010000000000000768, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 741, time 60.70293569564819, eps 0.001000000000000073, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 742, time 51.322314977645874, eps 0.0010000000000000694, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
goal_identified
goal_identified
=== ep: 743, time 63.274274587631226, eps 0.001000000000000066, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 744, time 61.260906457901, eps 0.001000000000000063, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
goal_identified
goal_identified
=== ep: 745, time 63.857706785202026, eps 0.0010000000000000599, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
=== ep: 746, time 57.897337198257446, eps 0.0010000000000000568, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 747, time 63.79664731025696, eps 0.001000000000000054, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 748, time 70.52722382545471, eps 0.0010000000000000514, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
goal_identified
goal_identified
=== ep: 749, time 64.0295991897583, eps 0.001000000000000049, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 750, time 56.83334732055664, eps 0.0010000000000000466, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
goal_identified
goal_identified
goal_identified
=== ep: 751, time 72.3214521408081, eps 0.0010000000000000443, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 752, time 72.92885947227478, eps 0.001000000000000042, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 753, time 59.54327607154846, eps 0.0010000000000000401, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
goal_identified
goal_identified
=== ep: 754, time 60.54065823554993, eps 0.0010000000000000382, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
goal_identified
goal_identified
goal_identified
=== ep: 755, time 67.2554121017456, eps 0.0010000000000000362, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 756, time 58.760143756866455, eps 0.0010000000000000345, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 757, time 58.871524810791016, eps 0.0010000000000000328, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 758, time 57.56743764877319, eps 0.0010000000000000312, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 759, time 57.17665195465088, eps 0.0010000000000000297, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 760, time 59.236042976379395, eps 0.0010000000000000282, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 761, time 47.224730014801025, eps 0.001000000000000027, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
goal_identified
goal_identified
goal_identified
=== ep: 762, time 56.25137138366699, eps 0.0010000000000000256, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 763, time 51.384060859680176, eps 0.0010000000000000243, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 764, time 58.33522319793701, eps 0.0010000000000000232, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 765, time 55.676716327667236, eps 0.001000000000000022, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 766, time 52.96855401992798, eps 0.0010000000000000208, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
goal_identified
=== ep: 767, time 58.41562080383301, eps 0.00100000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
goal_identified
goal_identified
=== ep: 768, time 43.4044394493103, eps 0.0010000000000000189, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 769, time 64.06332969665527, eps 0.001000000000000018, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
goal_identified
goal_identified
goal_identified
=== ep: 770, time 46.553855895996094, eps 0.0010000000000000172, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 771, time 59.90547204017639, eps 0.0010000000000000163, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
goal_identified
goal_identified
goal_identified
=== ep: 772, time 60.71469283103943, eps 0.0010000000000000154, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
=== ep: 773, time 44.51713490486145, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 774, time 62.193503618240356, eps 0.0010000000000000141, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 775, time 60.37647557258606, eps 0.0010000000000000132, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
goal_identified
goal_identified
=== ep: 776, time 50.57328653335571, eps 0.0010000000000000126, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 777, time 59.89307141304016, eps 0.0010000000000000122, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 778, time 60.83201575279236, eps 0.0010000000000000115, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
goal_identified
goal_identified
goal_identified
=== ep: 779, time 55.565858125686646, eps 0.0010000000000000109, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 780, time 59.34839868545532, eps 0.0010000000000000104, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
goal_identified
goal_identified
goal_identified
=== ep: 781, time 61.08799719810486, eps 0.00100000000000001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
goal_identified
goal_identified
=== ep: 782, time 58.372445583343506, eps 0.0010000000000000093, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
goal_identified
goal_identified
goal_identified
=== ep: 783, time 52.943859338760376, eps 0.001000000000000009, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 784, time 63.87644028663635, eps 0.0010000000000000085, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
goal_identified
goal_identified
=== ep: 785, time 63.164427042007446, eps 0.001000000000000008, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
goal_identified
goal_identified
goal_identified
=== ep: 786, time 54.49742007255554, eps 0.0010000000000000076, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
goal_identified
goal_identified
=== ep: 787, time 62.0139365196228, eps 0.0010000000000000074, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
goal_identified
goal_identified
goal_identified
=== ep: 788, time 55.85849118232727, eps 0.001000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 789, time 55.89068841934204, eps 0.0010000000000000067, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
goal_identified
goal_identified
goal_identified
=== ep: 790, time 54.63828444480896, eps 0.0010000000000000063, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
goal_identified
goal_identified
=== ep: 791, time 45.53173828125, eps 0.001000000000000006, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
goal_identified
goal_identified
goal_identified
=== ep: 792, time 65.09893679618835, eps 0.0010000000000000057, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 793, time 54.76752424240112, eps 0.0010000000000000054, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 794, time 56.952364921569824, eps 0.0010000000000000052, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
goal_identified
goal_identified
goal_identified
=== ep: 795, time 67.13412308692932, eps 0.001000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
goal_identified
goal_identified
goal_identified
=== ep: 796, time 52.01012897491455, eps 0.0010000000000000048, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 797, time 63.23742747306824, eps 0.0010000000000000044, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
goal_identified
goal_identified
goal_identified
=== ep: 798, time 68.8568217754364, eps 0.0010000000000000041, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
goal_identified
goal_identified
=== ep: 799, time 53.71082639694214, eps 0.0010000000000000041, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 800, time 59.86533713340759, eps 0.001000000000000004, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 801, time 58.6791729927063, eps 0.0010000000000000037, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
goal_identified
goal_identified
=== ep: 802, time 53.99612832069397, eps 0.0010000000000000035, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 803, time 63.96636366844177, eps 0.0010000000000000033, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 804, time 56.29611897468567, eps 0.001000000000000003, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 805, time 57.04454851150513, eps 0.001000000000000003, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
goal_identified
goal_identified
goal_identified
=== ep: 806, time 58.2168755531311, eps 0.0010000000000000028, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 807, time 50.93966221809387, eps 0.0010000000000000026, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 808, time 59.51310992240906, eps 0.0010000000000000026, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
goal_identified
goal_identified
=== ep: 809, time 53.08580160140991, eps 0.0010000000000000024, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 810, time 59.741947174072266, eps 0.0010000000000000024, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 811, time 55.49989914894104, eps 0.0010000000000000022, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 812, time 53.73467707633972, eps 0.0010000000000000022, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
goal_identified
goal_identified
=== ep: 813, time 66.62307119369507, eps 0.001000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 814, time 53.04581832885742, eps 0.001000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
goal_identified
goal_identified
goal_identified
=== ep: 815, time 59.77130389213562, eps 0.0010000000000000018, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 816, time 66.05797362327576, eps 0.0010000000000000018, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 817, time 43.66657900810242, eps 0.0010000000000000018, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
goal_identified
goal_identified
goal_identified
=== ep: 818, time 63.50944995880127, eps 0.0010000000000000015, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 819, time 48.099379777908325, eps 0.0010000000000000015, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 820, time 61.18883228302002, eps 0.0010000000000000013, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 821, time 65.97893357276917, eps 0.0010000000000000013, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 822, time 54.14695143699646, eps 0.0010000000000000013, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
goal_identified
goal_identified
goal_identified
=== ep: 823, time 58.30715036392212, eps 0.0010000000000000013, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
goal_identified
goal_identified
goal_identified
=== ep: 824, time 61.35589838027954, eps 0.001000000000000001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 825, time 55.687949657440186, eps 0.001000000000000001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 826, time 57.74985909461975, eps 0.001000000000000001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 827, time 51.64473485946655, eps 0.001000000000000001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 828, time 58.4609260559082, eps 0.0010000000000000009, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
goal_identified
goal_identified
goal_identified
=== ep: 829, time 53.46400189399719, eps 0.0010000000000000009, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 830, time 55.91187500953674, eps 0.0010000000000000009, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 831, time 63.8485631942749, eps 0.0010000000000000009, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
goal_identified
goal_identified
goal_identified
=== ep: 832, time 48.33973503112793, eps 0.0010000000000000009, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 833, time 63.371108293533325, eps 0.0010000000000000007, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 834, time 63.451385736465454, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 835, time 47.633466958999634, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 836, time 63.817495822906494, eps 0.0010000000000000007, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
goal_identified
goal_identified
goal_identified
=== ep: 837, time 52.76408839225769, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 838, time 53.54408597946167, eps 0.0010000000000000007, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
goal_identified
goal_identified
goal_identified
=== ep: 839, time 56.9808144569397, eps 0.0010000000000000007, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 840, time 52.145857095718384, eps 0.0010000000000000005, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 841, time 53.93380379676819, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
goal_identified
=== ep: 842, time 48.00762319564819, eps 0.0010000000000000005, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 843, time 56.14888525009155, eps 0.0010000000000000005, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 844, time 51.71723747253418, eps 0.0010000000000000005, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
goal_identified
goal_identified
goal_identified
=== ep: 845, time 57.17728877067566, eps 0.0010000000000000005, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 846, time 54.65023446083069, eps 0.0010000000000000005, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 847, time 54.699082374572754, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 848, time 54.33432674407959, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
goal_identified
goal_identified
goal_identified
=== ep: 849, time 48.996734619140625, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
goal_identified
goal_identified
=== ep: 850, time 60.745956897735596, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 851, time 46.30723977088928, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
goal_identified
goal_identified
goal_identified
=== ep: 852, time 59.65298771858215, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
goal_identified
=== ep: 853, time 59.334142208099365, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
goal_identified
=== ep: 854, time 49.944822788238525, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
=== ep: 855, time 68.13848733901978, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
goal_identified
=== ep: 856, time 55.58055329322815, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 857, time 59.9180371761322, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
goal_identified
goal_identified
=== ep: 858, time 65.98913955688477, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 859, time 44.572845220565796, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 860, time 64.32809543609619, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
goal_identified
goal_identified
goal_identified
=== ep: 861, time 49.99704146385193, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 862, time 59.88451075553894, eps 0.0010000000000000002, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
goal_identified
goal_identified
=== ep: 863, time 64.42500448226929, eps 0.0010000000000000002, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 864, time 45.44534993171692, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 865, time 64.45004749298096, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 866, time 39.692368030548096, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 867, time 60.90148329734802, eps 0.0010000000000000002, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 868, time 47.822932720184326, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
goal_identified
goal_identified
goal_identified
=== ep: 869, time 58.54558253288269, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 870, time 62.210187911987305, eps 0.0010000000000000002, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 871, time 46.69011616706848, eps 0.0010000000000000002, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 872, time 63.93617391586304, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
goal_identified
goal_identified
goal_identified
=== ep: 873, time 52.73388051986694, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
goal_identified
goal_identified
=== ep: 874, time 54.18588876724243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 875, time 51.48648929595947, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
goal_identified
=== ep: 876, time 53.96519160270691, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 877, time 60.933175802230835, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 878, time 52.1336715221405, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 879, time 61.950432538986206, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 880, time 62.411619901657104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 881, time 54.1847198009491, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 882, time 59.58761143684387, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 883, time 56.259493350982666, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 884, time 55.86055326461792, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
goal_identified
goal_identified
goal_identified
=== ep: 885, time 57.651904821395874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 886, time 51.00603532791138, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
goal_identified
goal_identified
goal_identified
=== ep: 887, time 58.452627182006836, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 888, time 54.694353103637695, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
goal_identified
goal_identified
=== ep: 889, time 54.150583267211914, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
goal_identified
goal_identified
goal_identified
=== ep: 890, time 59.767531871795654, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
goal_identified
goal_identified
=== ep: 891, time 57.81319832801819, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 892, time 58.81483173370361, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
goal_identified
goal_identified
goal_identified
=== ep: 893, time 55.47848582267761, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 894, time 57.842655181884766, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 895, time 55.69322752952576, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 896, time 54.59066462516785, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 897, time 54.009140968322754, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 898, time 51.77915120124817, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 899, time 61.290828227996826, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
goal_identified
goal_identified
goal_identified
=== ep: 900, time 53.71287751197815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 901, time 58.08078908920288, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
goal_identified
goal_identified
goal_identified
=== ep: 902, time 59.37672472000122, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
goal_identified
goal_identified
goal_identified
=== ep: 903, time 54.715826749801636, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 904, time 56.57358527183533, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
goal_identified
goal_identified
goal_identified
=== ep: 905, time 42.74865198135376, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 906, time 56.78006649017334, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 907, time 45.480851888656616, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 908, time 63.54924917221069, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 909, time 63.09278130531311, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
=== ep: 910, time 54.95073199272156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
goal_identified
goal_identified
goal_identified
=== ep: 911, time 63.307246923446655, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 912, time 61.12518811225891, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
goal_identified
goal_identified
goal_identified
=== ep: 913, time 47.117323875427246, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
=== ep: 914, time 56.393982887268066, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
goal_identified
goal_identified
goal_identified
=== ep: 915, time 41.71522641181946, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
goal_identified
goal_identified
=== ep: 916, time 62.02936339378357, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
goal_identified
goal_identified
=== ep: 917, time 58.2767653465271, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 918, time 52.8766610622406, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
goal_identified
goal_identified
goal_identified
=== ep: 919, time 65.83781552314758, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 920, time 44.75373101234436, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
goal_identified
goal_identified
goal_identified
=== ep: 921, time 58.231632471084595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
goal_identified
goal_identified
=== ep: 922, time 48.39444541931152, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
goal_identified
=== ep: 923, time 54.768876791000366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
goal_identified
=== ep: 924, time 64.21455383300781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
goal_identified
goal_identified
goal_identified
=== ep: 925, time 52.098642110824585, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 926, time 63.40112090110779, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 927, time 58.934826612472534, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 928, time 61.32130575180054, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
goal_identified
goal_identified
=== ep: 929, time 65.96872186660767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
goal_identified
goal_identified
=== ep: 930, time 58.44733715057373, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
goal_identified
goal_identified
=== ep: 931, time 58.42790412902832, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 932, time 63.83074903488159, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 933, time 51.56512522697449, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 934, time 53.948484659194946, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 935, time 56.63913345336914, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 936, time 47.71002650260925, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
goal_identified
goal_identified
=== ep: 937, time 60.531733751297, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
goal_identified
goal_identified
goal_identified
=== ep: 938, time 56.772722005844116, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 939, time 57.37552618980408, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
goal_identified
goal_identified
=== ep: 940, time 63.45523500442505, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
goal_identified
goal_identified
goal_identified
=== ep: 941, time 60.32917881011963, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
goal_identified
goal_identified
goal_identified
=== ep: 942, time 51.91000747680664, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
goal_identified
goal_identified
=== ep: 943, time 63.94131922721863, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
goal_identified
goal_identified
goal_identified
=== ep: 944, time 59.07695126533508, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 945, time 58.86602425575256, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
=== ep: 946, time 65.29564619064331, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
goal_identified
goal_identified
=== ep: 947, time 58.16511607170105, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
goal_identified
goal_identified
=== ep: 948, time 60.473217725753784, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 949, time 52.09135437011719, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
goal_identified
goal_identified
=== ep: 950, time 47.88363194465637, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 951, time 53.96977400779724, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 952, time 53.34667348861694, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
goal_identified
goal_identified
goal_identified
=== ep: 953, time 58.19988298416138, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
=== ep: 954, time 50.433269739151, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
goal_identified
goal_identified
=== ep: 955, time 61.73286771774292, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
goal_identified
=== ep: 956, time 53.94492769241333, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 957, time 50.45297384262085, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 958, time 56.82879590988159, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 959, time 54.40175175666809, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
goal_identified
goal_identified
=== ep: 960, time 57.17080783843994, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 961, time 54.46519494056702, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
goal_identified
goal_identified
goal_identified
=== ep: 962, time 61.51771783828735, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
=== ep: 963, time 59.89376997947693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
goal_identified
goal_identified
=== ep: 964, time 52.562899351119995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
goal_identified
goal_identified
goal_identified
=== ep: 965, time 61.38927245140076, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 966, time 45.20861744880676, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 967, time 64.34706616401672, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
=== ep: 968, time 53.13475489616394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 969, time 61.506535053253174, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 970, time 63.893287897109985, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 971, time 49.29158353805542, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
goal_identified
goal_identified
=== ep: 972, time 64.24279856681824, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 973, time 54.46343970298767, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
goal_identified
goal_identified
=== ep: 974, time 52.692099809646606, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 975, time 54.021567583084106, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
goal_identified
goal_identified
goal_identified
=== ep: 976, time 54.83235216140747, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
goal_identified
=== ep: 977, time 54.83982181549072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 978, time 56.94430184364319, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
=== ep: 979, time 62.13521957397461, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
goal_identified
goal_identified
=== ep: 980, time 56.85967206954956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 981, time 65.43959283828735, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 982, time 62.84091091156006, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
goal_identified
goal_identified
goal_identified
=== ep: 983, time 56.82050371170044, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 984, time 60.076958417892456, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 985, time 48.84549069404602, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
goal_identified
goal_identified
=== ep: 986, time 53.37631440162659, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 987, time 50.181925535202026, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
goal_identified
goal_identified
goal_identified
=== ep: 988, time 60.199472427368164, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
goal_identified
=== ep: 989, time 51.1996750831604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
goal_identified
goal_identified
=== ep: 990, time 60.649654150009155, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 991, time 52.8737907409668, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 992, time 57.47826814651489, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 993, time 45.44430208206177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
goal_identified
=== ep: 994, time 52.295915842056274, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
goal_identified
goal_identified
=== ep: 995, time 55.49153113365173, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 996, time 61.21911358833313, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
goal_identified
goal_identified
goal_identified
=== ep: 997, time 58.98126459121704, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
goal_identified
=== ep: 998, time 54.25574350357056, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 999, time 63.57770538330078, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
goal_identified
goal_identified
=== ep: 1000, time 51.996201515197754, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1001, time 56.80986523628235, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1002, time 50.875091552734375, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
goal_identified
goal_identified
=== ep: 1003, time 60.751891136169434, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1004, time 52.37709879875183, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1005, time 58.057992935180664, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
goal_identified
goal_identified
goal_identified
=== ep: 1006, time 64.14544034004211, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1007, time 54.232675552368164, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
goal_identified
goal_identified
goal_identified
=== ep: 1008, time 62.511427879333496, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
goal_identified
=== ep: 1009, time 56.292709827423096, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1010, time 58.23965835571289, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
goal_identified
goal_identified
goal_identified
=== ep: 1011, time 57.803765296936035, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1012, time 57.04931616783142, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1013, time 55.10661220550537, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1014, time 58.761855125427246, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1015, time 57.29319739341736, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1016, time 56.01925182342529, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
goal_identified
goal_identified
=== ep: 1017, time 64.60945272445679, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
goal_identified
goal_identified
=== ep: 1018, time 54.17458391189575, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
goal_identified
=== ep: 1019, time 62.90332078933716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
goal_identified
goal_identified
goal_identified
=== ep: 1020, time 63.39114713668823, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
goal_identified
goal_identified
goal_identified
=== ep: 1021, time 48.20851254463196, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1022, time 64.19747281074524, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
goal_identified
goal_identified
=== ep: 1023, time 42.78219795227051, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1024, time 63.65992069244385, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1025, time 42.28269720077515, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1026, time 62.772226095199585, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
goal_identified
goal_identified
goal_identified
=== ep: 1027, time 47.703948736190796, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1028, time 60.63630676269531, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1029, time 59.49999928474426, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
goal_identified
goal_identified
goal_identified
=== ep: 1030, time 56.014121770858765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
goal_identified
=== ep: 1031, time 67.473459482193, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1032, time 61.5951292514801, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
=== ep: 1033, time 63.31220555305481, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
goal_identified
goal_identified
=== ep: 1034, time 63.83738946914673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1035, time 57.22982597351074, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1036, time 67.28334450721741, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
goal_identified
goal_identified
=== ep: 1037, time 66.48123049736023, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
goal_identified
goal_identified
=== ep: 1038, time 58.90877890586853, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1039, time 64.70404815673828, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1040, time 60.79190754890442, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
goal_identified
=== ep: 1041, time 55.798407554626465, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1042, time 54.07531404495239, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1043, time 44.14356207847595, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
goal_identified
goal_identified
=== ep: 1044, time 59.267274379730225, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1045, time 53.2428081035614, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1046, time 59.96532893180847, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1047, time 59.16115093231201, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1048, time 56.81651711463928, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
goal_identified
goal_identified
goal_identified
=== ep: 1049, time 55.12284064292908, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1050, time 49.65455651283264, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
goal_identified
goal_identified
=== ep: 1051, time 57.39687919616699, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1052, time 48.43686604499817, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
goal_identified
goal_identified
=== ep: 1053, time 63.213279485702515, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
goal_identified
goal_identified
=== ep: 1054, time 51.0768620967865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
goal_identified
goal_identified
=== ep: 1055, time 59.425599098205566, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1056, time 61.53288292884827, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1057, time 50.3497998714447, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1058, time 62.53148698806763, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1059, time 50.30281162261963, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
goal_identified
goal_identified
=== ep: 1060, time 56.502333641052246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1061, time 56.73478364944458, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
goal_identified
goal_identified
=== ep: 1062, time 53.3885703086853, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
goal_identified
goal_identified
=== ep: 1063, time 64.01930284500122, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
goal_identified
goal_identified
=== ep: 1064, time 49.10019087791443, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
goal_identified
goal_identified
=== ep: 1065, time 64.07032680511475, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1066, time 57.92013216018677, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
goal_identified
=== ep: 1067, time 55.97451043128967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1068, time 65.62589836120605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
goal_identified
=== ep: 1069, time 58.96854543685913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1070, time 55.44009733200073, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1071, time 63.3021605014801, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1072, time 47.89811897277832, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1072
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1073, time 59.16829752922058, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
goal_identified
goal_identified
=== ep: 1074, time 48.18389821052551, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1075, time 54.45008158683777, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1076, time 56.677093744277954, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1077, time 50.52148747444153, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
goal_identified
goal_identified
goal_identified
=== ep: 1078, time 63.682541847229004, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
goal_identified
goal_identified
=== ep: 1079, time 53.500547885894775, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
goal_identified
goal_identified
=== ep: 1080, time 61.7848846912384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1081, time 63.57563519477844, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
goal_identified
goal_identified
=== ep: 1082, time 54.26095247268677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
goal_identified
goal_identified
goal_identified
=== ep: 1083, time 64.29221391677856, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
goal_identified
goal_identified
=== ep: 1084, time 58.36420035362244, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
goal_identified
goal_identified
goal_identified
=== ep: 1085, time 59.83639597892761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
goal_identified
goal_identified
=== ep: 1086, time 56.54965257644653, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1087, time 56.351176261901855, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1088, time 63.2060444355011, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1089, time 47.47850751876831, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1090, time 62.455127000808716, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
goal_identified
goal_identified
goal_identified
=== ep: 1091, time 50.29021692276001, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
goal_identified
goal_identified
=== ep: 1092, time 59.37710976600647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
goal_identified
goal_identified
=== ep: 1093, time 60.98830199241638, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1094, time 52.6650824546814, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
goal_identified
goal_identified
=== ep: 1095, time 64.25929307937622, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
goal_identified
=== ep: 1096, time 57.71412920951843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1097, time 55.240320682525635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1098, time 63.01707363128662, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
goal_identified
goal_identified
goal_identified
=== ep: 1099, time 54.92170238494873, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
goal_identified
goal_identified
=== ep: 1100, time 59.24992322921753, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1101, time 59.740803718566895, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
goal_identified
goal_identified
goal_identified
=== ep: 1102, time 50.22695302963257, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1103, time 57.42393612861633, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1104, time 55.820383071899414, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1105, time 54.74132323265076, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
goal_identified
goal_identified
=== ep: 1106, time 59.797513008117676, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
goal_identified
=== ep: 1107, time 53.0570387840271, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
goal_identified
goal_identified
=== ep: 1108, time 59.98368263244629, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1109, time 61.83209538459778, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
goal_identified
goal_identified
=== ep: 1110, time 56.37352204322815, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1111, time 63.45356798171997, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
goal_identified
goal_identified
=== ep: 1112, time 61.65712332725525, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
goal_identified
goal_identified
=== ep: 1113, time 55.656086444854736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1114, time 62.471219301223755, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1115, time 56.45579719543457, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1116, time 55.75613021850586, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1117, time 55.84229302406311, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1118, time 52.47555661201477, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1119, time 51.94048190116882, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
goal_identified
goal_identified
goal_identified
=== ep: 1120, time 56.26904487609863, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1121, time 56.95260286331177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
goal_identified
goal_identified
goal_identified
=== ep: 1122, time 54.31947159767151, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1123, time 67.34260535240173, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1124, time 63.13905572891235, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1125, time 59.971237659454346, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1126, time 65.76265144348145, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
goal_identified
goal_identified
=== ep: 1127, time 60.65634560585022, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
goal_identified
goal_identified
=== ep: 1128, time 64.41426467895508, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1129, time 67.18727588653564, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
goal_identified
goal_identified
goal_identified
=== ep: 1130, time 51.2603874206543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1131, time 67.1677520275116, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1132, time 55.55293297767639, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1133, time 51.511603116989136, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1134, time 54.99361562728882, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1135, time 47.801217555999756, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1136, time 61.97714591026306, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
goal_identified
goal_identified
goal_identified
=== ep: 1137, time 57.343037605285645, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1138, time 57.93933820724487, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1139, time 64.64425253868103, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
goal_identified
goal_identified
=== ep: 1140, time 57.54770755767822, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1141, time 56.1519672870636, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1142, time 57.70178842544556, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
goal_identified
goal_identified
goal_identified
=== ep: 1143, time 48.75424814224243, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1144, time 55.662012338638306, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
goal_identified
goal_identified
goal_identified
=== ep: 1145, time 51.48319363594055, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1146, time 54.49363565444946, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1147, time 57.555026054382324, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1148, time 55.226892948150635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1149, time 58.9373254776001, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
goal_identified
goal_identified
=== ep: 1150, time 56.990262269973755, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
goal_identified
goal_identified
=== ep: 1151, time 56.61036014556885, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1152, time 60.06740069389343, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
goal_identified
goal_identified
goal_identified
=== ep: 1153, time 58.47178626060486, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1154, time 53.88414788246155, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
goal_identified
goal_identified
goal_identified
=== ep: 1155, time 61.23597764968872, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1156, time 54.09238815307617, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
goal_identified
goal_identified
=== ep: 1157, time 55.5323371887207, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1158, time 55.47460174560547, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
goal_identified
goal_identified
=== ep: 1159, time 49.878753900527954, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
goal_identified
goal_identified
=== ep: 1160, time 62.230308532714844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1161, time 52.62777018547058, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1162, time 60.13174843788147, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
goal_identified
goal_identified
goal_identified
=== ep: 1163, time 57.00563907623291, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1164, time 60.6544930934906, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
goal_identified
goal_identified
=== ep: 1165, time 60.285746812820435, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1166, time 51.48136305809021, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1167, time 62.17631554603577, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1168, time 49.18779230117798, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1169, time 59.30421710014343, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1170, time 51.083158016204834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
goal_identified
goal_identified
=== ep: 1171, time 59.22440958023071, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
goal_identified
goal_identified
goal_identified
=== ep: 1172, time 51.03573131561279, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1173, time 60.664735317230225, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1174, time 59.05033326148987, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1175, time 49.020658016204834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1176, time 61.64517092704773, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
goal_identified
goal_identified
=== ep: 1177, time 43.75713849067688, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1178, time 63.209827184677124, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
goal_identified
goal_identified
goal_identified
=== ep: 1179, time 47.864795207977295, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1180, time 63.34430551528931, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1181, time 60.379812240600586, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
goal_identified
goal_identified
goal_identified
=== ep: 1182, time 58.293946743011475, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
goal_identified
goal_identified
=== ep: 1183, time 63.92978644371033, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1184, time 65.48701119422913, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
goal_identified
goal_identified
=== ep: 1185, time 57.96334910392761, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
goal_identified
goal_identified
=== ep: 1186, time 61.40319013595581, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1187, time 65.96474814414978, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
goal_identified
goal_identified
goal_identified
=== ep: 1188, time 61.39131689071655, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
goal_identified
goal_identified
=== ep: 1189, time 60.299965381622314, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1190, time 67.7108793258667, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1191, time 62.7920298576355, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1192, time 57.374364614486694, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1193, time 64.48909759521484, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
goal_identified
=== ep: 1194, time 59.44330072402954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
goal_identified
goal_identified
goal_identified
=== ep: 1195, time 53.677653551101685, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1196, time 57.743141889572144, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
goal_identified
goal_identified
=== ep: 1197, time 54.393779039382935, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1198, time 52.87893342971802, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
goal_identified
goal_identified
goal_identified
=== ep: 1199, time 57.52825999259949, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
goal_identified
goal_identified
goal_identified
=== ep: 1200, time 54.1855034828186, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
goal_identified
goal_identified
=== ep: 1201, time 53.683825731277466, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
goal_identified
=== ep: 1202, time 62.35847783088684, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
goal_identified
=== ep: 1203, time 49.882444858551025, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1204, time 63.53528904914856, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1205, time 55.39488172531128, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
goal_identified
=== ep: 1206, time 51.62335157394409, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
goal_identified
goal_identified
goal_identified
=== ep: 1207, time 55.68838047981262, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
goal_identified
=== ep: 1208, time 52.28710746765137, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1209, time 51.589415311813354, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1210, time 53.38650918006897, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1211, time 63.75349164009094, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1212, time 48.4396436214447, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1213, time 68.1846227645874, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
goal_identified
goal_identified
goal_identified
=== ep: 1214, time 56.062851428985596, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1215, time 56.24180197715759, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
=== ep: 1216, time 62.37282133102417, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1217, time 45.476848125457764, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
goal_identified
goal_identified
=== ep: 1218, time 62.85633707046509, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1219, time 48.936450242996216, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1220, time 59.65771269798279, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
goal_identified
=== ep: 1221, time 57.801058769226074, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
goal_identified
goal_identified
=== ep: 1222, time 61.06169295310974, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1223, time 63.24222111701965, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
goal_identified
goal_identified
goal_identified
=== ep: 1224, time 55.375344038009644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1225, time 62.48173975944519, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1226, time 61.054906368255615, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
=== ep: 1227, time 62.97415065765381, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
goal_identified
goal_identified
goal_identified
=== ep: 1228, time 56.62266707420349, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1229, time 62.19292449951172, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
goal_identified
goal_identified
goal_identified
=== ep: 1230, time 66.118403673172, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
goal_identified
goal_identified
goal_identified
=== ep: 1231, time 45.101653814315796, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
goal_identified
goal_identified
goal_identified
=== ep: 1232, time 65.36240291595459, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
goal_identified
goal_identified
goal_identified
=== ep: 1233, time 47.96812295913696, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1234, time 58.356199741363525, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
goal_identified
goal_identified
goal_identified
=== ep: 1235, time 56.95257925987244, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1236, time 51.72927904129028, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1236
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1237, time 61.684144020080566, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
goal_identified
goal_identified
goal_identified
=== ep: 1238, time 57.91902184486389, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
goal_identified
=== ep: 1239, time 65.87803220748901, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
goal_identified
goal_identified
=== ep: 1240, time 61.803391218185425, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
goal_identified
goal_identified
goal_identified
=== ep: 1241, time 59.996809005737305, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1242, time 66.61106848716736, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
goal_identified
=== ep: 1243, time 61.27857327461243, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
goal_identified
goal_identified
goal_identified
=== ep: 1244, time 58.91448378562927, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
goal_identified
goal_identified
goal_identified
=== ep: 1245, time 66.40736842155457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1246, time 65.32502341270447, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1247, time 61.03206825256348, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1248, time 60.6219744682312, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
goal_identified
goal_identified
=== ep: 1249, time 63.931156396865845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
goal_identified
goal_identified
goal_identified
=== ep: 1250, time 60.3946897983551, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
goal_identified
goal_identified
goal_identified
=== ep: 1251, time 55.168461561203, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
goal_identified
goal_identified
=== ep: 1252, time 63.058650493621826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
goal_identified
goal_identified
=== ep: 1253, time 57.523463010787964, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1254, time 46.161043643951416, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
=== ep: 1255, time 60.51778221130371, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1256, time 50.21843385696411, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1257, time 62.8301842212677, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1258, time 62.52003574371338, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
goal_identified
goal_identified
goal_identified
=== ep: 1259, time 60.791927099227905, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1260, time 65.88632297515869, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1261, time 54.11535429954529, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1262, time 66.80109739303589, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1263, time 66.20726704597473, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
goal_identified
goal_identified
=== ep: 1264, time 52.143370389938354, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1265, time 67.42955470085144, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
goal_identified
goal_identified
=== ep: 1266, time 67.11658525466919, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1267, time 60.99591541290283, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1268, time 63.99001502990723, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
goal_identified
goal_identified
=== ep: 1269, time 62.13934516906738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1270, time 63.97466588020325, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1271, time 63.62541723251343, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1272, time 58.469422578811646, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1273, time 62.95285415649414, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 54.49532103538513, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1275, time 64.40270686149597, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
goal_identified
goal_identified
goal_identified
=== ep: 1276, time 55.4265615940094, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1277, time 58.958250284194946, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1278, time 66.03868269920349, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
goal_identified
=== ep: 1279, time 46.02855658531189, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1280, time 65.90111589431763, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
goal_identified
=== ep: 1281, time 51.786768436431885, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
goal_identified
=== ep: 1282, time 62.73697543144226, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1283, time 60.84244394302368, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
goal_identified
=== ep: 1284, time 50.4185733795166, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1285, time 61.2566933631897, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1286, time 55.833327531814575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
goal_identified
goal_identified
goal_identified
=== ep: 1287, time 65.01325345039368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
goal_identified
goal_identified
=== ep: 1288, time 57.76274371147156, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1289, time 60.31283664703369, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
goal_identified
=== ep: 1290, time 68.59320187568665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1291, time 61.547560691833496, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1292, time 59.710450410842896, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
goal_identified
goal_identified
=== ep: 1293, time 61.5995078086853, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1293
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1294, time 62.09117889404297, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
goal_identified
goal_identified
goal_identified
=== ep: 1295, time 49.21644067764282, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1296, time 56.46156692504883, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1297, time 55.97668123245239, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1298, time 51.863911628723145, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1299, time 60.57769179344177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1300, time 57.813772439956665, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1301, time 54.61697196960449, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
goal_identified
goal_identified
goal_identified
=== ep: 1302, time 65.14726424217224, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1303, time 63.432982444763184, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1304, time 59.02864646911621, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1305, time 63.91333746910095, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1306, time 58.163567781448364, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1307, time 66.38368582725525, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
=== ep: 1308, time 61.5232355594635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
goal_identified
goal_identified
goal_identified
=== ep: 1309, time 51.47401165962219, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1310, time 66.20743584632874, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
goal_identified
goal_identified
=== ep: 1311, time 42.6378014087677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1312, time 63.506458044052124, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
goal_identified
goal_identified
goal_identified
=== ep: 1313, time 48.88176727294922, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1314, time 60.22945737838745, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1315, time 55.60246753692627, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1316, time 59.784438371658325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
goal_identified
goal_identified
=== ep: 1317, time 60.32818078994751, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1318, time 52.09267544746399, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1319, time 62.05275559425354, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
=== ep: 1320, time 53.30210304260254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
goal_identified
goal_identified
goal_identified
=== ep: 1321, time 51.96689295768738, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1322, time 57.609313011169434, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1323, time 54.692909479141235, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1324, time 58.77958559989929, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
goal_identified
goal_identified
=== ep: 1325, time 55.94148564338684, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
goal_identified
goal_identified
=== ep: 1326, time 44.12375545501709, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
goal_identified
goal_identified
goal_identified
=== ep: 1327, time 55.295764684677124, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1328, time 58.87404465675354, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
goal_identified
goal_identified
goal_identified
=== ep: 1329, time 59.79293394088745, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1330, time 59.10615015029907, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1331, time 64.84379816055298, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
goal_identified
goal_identified
goal_identified
=== ep: 1332, time 58.48193383216858, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
goal_identified
=== ep: 1333, time 58.27216458320618, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1334, time 54.66445589065552, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
goal_identified
goal_identified
goal_identified
=== ep: 1335, time 59.765920639038086, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1336, time 48.540706396102905, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1337, time 56.51539611816406, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1338, time 62.612855195999146, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1339, time 51.56370830535889, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
goal_identified
goal_identified
goal_identified
=== ep: 1340, time 65.7925317287445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1341, time 68.76619982719421, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
goal_identified
=== ep: 1342, time 56.55294752120972, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
goal_identified
goal_identified
=== ep: 1343, time 65.01438283920288, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1344, time 60.934699296951294, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1345, time 61.63721203804016, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1346, time 63.750598669052124, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1347, time 41.233638525009155, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1348, time 61.14617133140564, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1349, time 46.9659526348114, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
goal_identified
goal_identified
goal_identified
=== ep: 1350, time 61.764203786849976, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
goal_identified
goal_identified
=== ep: 1351, time 60.67664074897766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1352, time 55.9619197845459, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1353, time 56.25216197967529, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1354, time 56.52681803703308, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
goal_identified
goal_identified
=== ep: 1355, time 52.78589677810669, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1356, time 54.51720714569092, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1357, time 53.61433720588684, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1358, time 56.88100838661194, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
=== ep: 1359, time 62.704657316207886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1360, time 62.36372256278992, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
goal_identified
goal_identified
goal_identified
=== ep: 1361, time 66.7307493686676, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
goal_identified
goal_identified
goal_identified
=== ep: 1362, time 66.14793944358826, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
goal_identified
goal_identified
=== ep: 1363, time 54.19905924797058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
goal_identified
goal_identified
goal_identified
=== ep: 1364, time 64.71235847473145, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1365, time 61.13029742240906, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
goal_identified
goal_identified
goal_identified
=== ep: 1366, time 53.911683082580566, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1367, time 64.91552448272705, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
goal_identified
goal_identified
goal_identified
=== ep: 1368, time 48.945759534835815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1369, time 58.676645040512085, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
goal_identified
goal_identified
goal_identified
=== ep: 1370, time 53.46245002746582, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1371, time 54.58629488945007, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1372, time 61.89433813095093, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
goal_identified
goal_identified
goal_identified
=== ep: 1373, time 58.24142026901245, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
goal_identified
goal_identified
=== ep: 1374, time 64.25998902320862, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
goal_identified
goal_identified
goal_identified
=== ep: 1375, time 53.53272294998169, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1376, time 66.95167660713196, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1376
goal_identified
=== ep: 1377, time 64.73906445503235, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1378, time 54.98015761375427, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1379, time 68.22145128250122, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
goal_identified
goal_identified
=== ep: 1380, time 50.65163493156433, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1381, time 62.923601150512695, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1382, time 59.265974283218384, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1383, time 57.420090436935425, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
=== ep: 1384, time 57.354026794433594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
goal_identified
goal_identified
=== ep: 1385, time 50.921356439590454, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1386, time 63.15355372428894, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1387, time 44.53449559211731, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1388, time 65.67684364318848, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
goal_identified
goal_identified
=== ep: 1389, time 51.64534258842468, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1390, time 64.0905876159668, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
goal_identified
goal_identified
=== ep: 1391, time 64.69043040275574, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1392, time 54.95052981376648, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1393, time 64.18041038513184, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
goal_identified
goal_identified
=== ep: 1394, time 52.91445565223694, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
goal_identified
goal_identified
=== ep: 1395, time 58.86599063873291, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
goal_identified
goal_identified
goal_identified
=== ep: 1396, time 39.88704061508179, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
goal_identified
=== ep: 1397, time 65.46996736526489, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
goal_identified
goal_identified
goal_identified
=== ep: 1398, time 55.62325096130371, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1399, time 60.642569065093994, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1400, time 65.12597465515137, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
goal_identified
goal_identified
goal_identified
=== ep: 1401, time 52.417126178741455, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
goal_identified
goal_identified
=== ep: 1402, time 59.29869747161865, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1403, time 53.51317071914673, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1404, time 65.36290287971497, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
goal_identified
goal_identified
goal_identified
=== ep: 1405, time 50.91482853889465, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1406, time 57.52627468109131, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1407, time 58.11849045753479, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1408, time 47.83257865905762, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
goal_identified
goal_identified
goal_identified
=== ep: 1409, time 62.59405565261841, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
goal_identified
=== ep: 1410, time 50.43773341178894, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1411, time 63.71612548828125, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1412, time 60.76706552505493, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
goal_identified
goal_identified
=== ep: 1413, time 63.38818979263306, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1414, time 61.58125591278076, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1415, time 51.38908529281616, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
goal_identified
goal_identified
=== ep: 1416, time 62.53860545158386, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1417, time 43.41689920425415, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1418, time 64.2487142086029, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1419, time 52.572036027908325, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1420, time 55.01937937736511, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1421, time 55.90605354309082, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1422, time 58.038379430770874, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1423, time 56.08672094345093, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
goal_identified
=== ep: 1424, time 53.296114444732666, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
goal_identified
goal_identified
=== ep: 1425, time 65.02676844596863, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1426, time 48.79975438117981, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1427, time 61.879714012145996, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1428, time 61.64001798629761, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1429, time 47.21317195892334, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
goal_identified
goal_identified
=== ep: 1430, time 60.23797917366028, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1431, time 43.74500751495361, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
goal_identified
goal_identified
goal_identified
=== ep: 1432, time 56.10809683799744, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1433, time 56.18567156791687, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
goal_identified
=== ep: 1434, time 65.90035939216614, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1435, time 62.78633213043213, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1436, time 56.74844288825989, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
=== ep: 1437, time 69.48521256446838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
goal_identified
goal_identified
=== ep: 1438, time 65.52335906028748, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1439, time 60.12896537780762, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1440, time 66.22269821166992, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
=== ep: 1441, time 60.741424798965454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1442, time 64.71013736724854, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1443, time 68.62227487564087, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1444, time 55.60831832885742, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1445, time 65.97419691085815, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1446, time 67.61080884933472, eps 0.001, sum reward: 10, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
goal_identified
goal_identified
goal_identified
=== ep: 1447, time 56.60730981826782, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1448, time 65.58748745918274, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1449, time 58.53059530258179, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1450, time 61.41515350341797, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1451, time 65.23416042327881, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
goal_identified
goal_identified
goal_identified
=== ep: 1452, time 47.12962508201599, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
goal_identified
=== ep: 1453, time 65.76420259475708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
goal_identified
goal_identified
=== ep: 1454, time 43.98871827125549, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1454
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1455, time 63.133270502090454, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1456, time 50.18454456329346, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1457, time 57.733595848083496, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
goal_identified
goal_identified
goal_identified
=== ep: 1458, time 57.805728912353516, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
goal_identified
goal_identified
goal_identified
=== ep: 1459, time 58.90016984939575, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
goal_identified
goal_identified
goal_identified
=== ep: 1460, time 58.340031147003174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1461, time 53.56375193595886, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
goal_identified
goal_identified
goal_identified
=== ep: 1462, time 65.33820271492004, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1463, time 48.47364401817322, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1464, time 63.21145272254944, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
goal_identified
goal_identified
goal_identified
=== ep: 1465, time 57.61189317703247, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1466, time 55.65575647354126, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1467, time 63.312371015548706, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
goal_identified
goal_identified
goal_identified
=== ep: 1468, time 48.242422580718994, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1469, time 61.25461149215698, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1470, time 54.43841218948364, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
goal_identified
goal_identified
goal_identified
=== ep: 1471, time 58.55554819107056, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1472, time 54.82891631126404, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1473, time 60.7609167098999, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1474, time 61.162113666534424, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1475, time 52.238211154937744, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1476, time 59.487194538116455, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
goal_identified
=== ep: 1477, time 55.32018494606018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1478, time 56.9255006313324, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
goal_identified
goal_identified
goal_identified
=== ep: 1479, time 57.426955461502075, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
goal_identified
goal_identified
goal_identified
=== ep: 1480, time 57.50085520744324, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1481, time 50.17964577674866, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1482, time 56.932661294937134, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1483, time 49.83336687088013, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
goal_identified
goal_identified
goal_identified
=== ep: 1484, time 55.41271185874939, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1485, time 61.60785150527954, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1486, time 47.21844816207886, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1487, time 61.844080686569214, eps 0.001, sum reward: 6, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1488, time 47.6713125705719, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1489, time 54.794336557388306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
goal_identified
goal_identified
=== ep: 1490, time 57.42652225494385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1491, time 48.706562519073486, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1492, time 63.91267466545105, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
goal_identified
goal_identified
=== ep: 1493, time 45.61430335044861, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
goal_identified
goal_identified
goal_identified
=== ep: 1494, time 61.57820773124695, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
goal_identified
=== ep: 1495, time 57.7729115486145, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1496, time 55.200358867645264, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1497, time 59.87811279296875, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
goal_identified
goal_identified
=== ep: 1498, time 55.084967374801636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1499, time 61.72688174247742, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
goal_identified
goal_identified
goal_identified
=== ep: 1500, time 51.500887393951416, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1501, time 63.62749171257019, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1502, time 54.16249370574951, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 57.18200206756592, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1504, time 63.218719720840454, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1505, time 48.26363515853882, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1506, time 61.12010335922241, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1507, time 51.85743975639343, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1508, time 53.71848392486572, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1509, time 58.49237942695618, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1510, time 54.368778228759766, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1511, time 59.385162353515625, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
goal_identified
goal_identified
=== ep: 1512, time 59.20833611488342, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1513, time 63.981350898742676, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1514, time 61.8162739276886, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
=== ep: 1515, time 60.96777939796448, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1516, time 65.75682163238525, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1516
goal_identified
goal_identified
goal_identified
=== ep: 1517, time 62.19674301147461, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1518, time 59.989606857299805, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1519, time 61.49151039123535, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
goal_identified
goal_identified
=== ep: 1520, time 62.94894289970398, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1521, time 53.955445528030396, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
goal_identified
goal_identified
goal_identified
=== ep: 1522, time 62.845093965530396, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1523, time 56.76470232009888, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
goal_identified
goal_identified
goal_identified
=== ep: 1524, time 57.86029028892517, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
goal_identified
goal_identified
=== ep: 1525, time 58.300116777420044, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
goal_identified
goal_identified
=== ep: 1526, time 58.23632550239563, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
goal_identified
=== ep: 1527, time 64.4562554359436, eps 0.001, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
goal_identified
goal_identified
=== ep: 1528, time 50.910258293151855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
goal_identified
goal_identified
goal_identified
=== ep: 1529, time 61.72176909446716, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1530, time 59.12891221046448, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1531, time 54.20602011680603, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
goal_identified
goal_identified
goal_identified
=== ep: 1532, time 64.58642196655273, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1533, time 54.03709173202515, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1534, time 56.943565368652344, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
goal_identified
goal_identified
=== ep: 1535, time 61.66935610771179, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
goal_identified
goal_identified
goal_identified
=== ep: 1536, time 49.89934825897217, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
goal_identified
=== ep: 1537, time 64.81994700431824, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1538, time 56.28519105911255, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
goal_identified
goal_identified
=== ep: 1539, time 59.39678716659546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1540, time 60.55367684364319, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
goal_identified
goal_identified
goal_identified
=== ep: 1541, time 58.199546337127686, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1542, time 63.89111089706421, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1543, time 45.638283014297485, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
goal_identified
goal_identified
goal_identified
=== ep: 1544, time 65.93130898475647, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 45.2826874256134, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
goal_identified
goal_identified
goal_identified
=== ep: 1546, time 66.90967130661011, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
goal_identified
goal_identified
goal_identified
=== ep: 1547, time 61.159184217453, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1548, time 55.87432932853699, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1549, time 69.57376480102539, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1550, time 57.179115533828735, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
goal_identified
goal_identified
goal_identified
=== ep: 1551, time 62.660627603530884, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
goal_identified
goal_identified
goal_identified
=== ep: 1552, time 60.51097369194031, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
goal_identified
goal_identified
=== ep: 1553, time 58.298640966415405, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1554, time 65.32453036308289, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
goal_identified
goal_identified
=== ep: 1555, time 51.54389476776123, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
goal_identified
goal_identified
goal_identified
=== ep: 1556, time 53.36888813972473, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
goal_identified
=== ep: 1557, time 52.54779815673828, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1558, time 56.01237511634827, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
goal_identified
goal_identified
goal_identified
=== ep: 1559, time 59.168570041656494, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1560, time 58.668720960617065, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1561, time 55.496018409729004, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
goal_identified
goal_identified
goal_identified
=== ep: 1562, time 52.744181871414185, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
goal_identified
goal_identified
goal_identified
=== ep: 1563, time 47.53984570503235, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1564, time 56.11489939689636, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1565, time 60.84344840049744, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1566, time 54.46214485168457, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1567, time 61.00287866592407, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1568, time 61.35094332695007, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1569, time 54.767699241638184, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
goal_identified
goal_identified
=== ep: 1570, time 61.499420404434204, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1571, time 60.347891330718994, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1572, time 52.19981551170349, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
goal_identified
goal_identified
goal_identified
=== ep: 1573, time 62.0665922164917, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
goal_identified
goal_identified
=== ep: 1574, time 52.00206160545349, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1575, time 60.112717628479004, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1576, time 58.52630853652954, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
=== ep: 1577, time 57.7645845413208, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
goal_identified
goal_identified
=== ep: 1578, time 58.32167363166809, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
goal_identified
goal_identified
=== ep: 1579, time 54.9318413734436, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
goal_identified
=== ep: 1580, time 65.30353689193726, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1581, time 52.64083528518677, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1582, time 56.673155069351196, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1583, time 58.90369510650635, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
=== ep: 1584, time 49.83161759376526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1585, time 64.04926562309265, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
goal_identified
goal_identified
goal_identified
=== ep: 1586, time 52.89285397529602, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
goal_identified
=== ep: 1587, time 60.82830286026001, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1588, time 63.91863465309143, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
goal_identified
goal_identified
=== ep: 1589, time 52.558772802352905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1590, time 63.402283668518066, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1591, time 62.31694555282593, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
goal_identified
goal_identified
=== ep: 1592, time 56.804758071899414, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
=== ep: 1593, time 56.11998796463013, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1594, time 56.322043895721436, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
goal_identified
goal_identified
=== ep: 1595, time 66.25947141647339, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1596, time 49.56352114677429, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1597, time 64.26162433624268, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1598, time 62.49098587036133, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1599, time 51.391451358795166, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1600, time 65.4869647026062, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1600
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1601, time 48.254438638687134, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
=== ep: 1602, time 61.53587508201599, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
goal_identified
goal_identified
goal_identified
=== ep: 1603, time 54.44685506820679, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
goal_identified
goal_identified
goal_identified
=== ep: 1604, time 57.96107316017151, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1605, time 54.686490535736084, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 133/133)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1606, time 57.63076329231262, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
goal_identified
=== ep: 1607, time 63.43497037887573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1608, time 45.64103555679321, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1609, time 64.57653307914734, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1610, time 49.08567810058594, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1611, time 60.83675932884216, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1612, time 59.10794401168823, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
goal_identified
=== ep: 1613, time 52.14247941970825, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1614, time 62.663820028305054, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1615, time 53.039846420288086, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1615
goal_identified
goal_identified
goal_identified
=== ep: 1616, time 62.04830574989319, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
goal_identified
goal_identified
goal_identified
=== ep: 1617, time 47.03549122810364, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
goal_identified
goal_identified
=== ep: 1618, time 62.747920989990234, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
goal_identified
goal_identified
goal_identified
=== ep: 1619, time 44.457658767700195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1620, time 61.05523490905762, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
goal_identified
goal_identified
=== ep: 1621, time 55.81612467765808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1622, time 53.04652190208435, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
goal_identified
goal_identified
=== ep: 1623, time 60.26348543167114, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
goal_identified
goal_identified
=== ep: 1624, time 44.72247385978699, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
goal_identified
goal_identified
goal_identified
=== ep: 1625, time 56.21155285835266, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
goal_identified
goal_identified
goal_identified
=== ep: 1626, time 52.168970584869385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
goal_identified
goal_identified
=== ep: 1627, time 63.75259804725647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1627
goal_identified
goal_identified
goal_identified
=== ep: 1628, time 53.236942768096924, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
goal_identified
goal_identified
goal_identified
=== ep: 1629, time 64.6786835193634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
goal_identified
goal_identified
goal_identified
=== ep: 1630, time 61.43551540374756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
goal_identified
goal_identified
goal_identified
=== ep: 1631, time 52.35826134681702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1632, time 64.76900553703308, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
goal_identified
goal_identified
=== ep: 1633, time 48.00927686691284, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1634, time 62.64668345451355, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1635, time 58.75290775299072, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
goal_identified
goal_identified
=== ep: 1636, time 57.223660707473755, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
goal_identified
=== ep: 1637, time 58.904637575149536, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1638, time 58.753878116607666, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1639, time 67.24321508407593, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1640, time 52.12110662460327, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1641, time 64.02300643920898, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1642, time 60.53239893913269, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1643, time 49.61748790740967, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1644, time 65.68308353424072, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1645, time 52.745039224624634, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1646, time 64.38911056518555, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1647, time 58.99914216995239, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
goal_identified
goal_identified
=== ep: 1648, time 65.57363295555115, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
goal_identified
=== ep: 1649, time 70.01246571540833, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1650, time 52.547120571136475, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1651, time 70.23525071144104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 68.0044858455658, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
goal_identified
goal_identified
goal_identified
=== ep: 1653, time 58.37529158592224, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1654, time 63.752209186553955, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
goal_identified
goal_identified
=== ep: 1655, time 48.58144450187683, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
goal_identified
=== ep: 1656, time 61.19573783874512, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
goal_identified
goal_identified
=== ep: 1657, time 46.633156299591064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1658, time 60.34266018867493, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1659, time 59.92543339729309, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1660, time 57.22454023361206, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1661, time 61.41630220413208, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
goal_identified
goal_identified
goal_identified
=== ep: 1662, time 57.59556221961975, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1663, time 66.29912328720093, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
goal_identified
goal_identified
=== ep: 1664, time 61.2446084022522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1665, time 54.31223392486572, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1666, time 68.96290016174316, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1667, time 61.654502868652344, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1668, time 54.653133153915405, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
goal_identified
goal_identified
=== ep: 1669, time 64.9245114326477, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1670, time 53.11408472061157, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1671, time 53.70571994781494, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1672, time 48.36185812950134, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
goal_identified
goal_identified
goal_identified
=== ep: 1673, time 60.507463693618774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
goal_identified
goal_identified
=== ep: 1674, time 52.816983461380005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1675, time 56.66302418708801, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1676, time 61.96368169784546, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1677, time 53.40223169326782, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1678, time 56.49405598640442, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1679, time 52.184138774871826, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
goal_identified
goal_identified
goal_identified
=== ep: 1680, time 53.78461980819702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1681, time 55.30682325363159, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1682, time 63.93105340003967, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1683, time 60.50874304771423, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1684, time 58.067100524902344, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1685, time 58.63091278076172, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1686, time 60.195265769958496, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
goal_identified
goal_identified
goal_identified
=== ep: 1687, time 64.41658234596252, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1688, time 50.795154333114624, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1689, time 49.9666314125061, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1690, time 54.435543060302734, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1691, time 51.45986771583557, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
goal_identified
goal_identified
goal_identified
=== ep: 1692, time 62.75715970993042, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
goal_identified
goal_identified
goal_identified
=== ep: 1693, time 59.41625237464905, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
goal_identified
=== ep: 1694, time 61.236249685287476, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
goal_identified
goal_identified
goal_identified
=== ep: 1695, time 53.019187688827515, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
goal_identified
goal_identified
goal_identified
=== ep: 1696, time 54.373581409454346, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
goal_identified
goal_identified
goal_identified
=== ep: 1697, time 53.407609701156616, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
goal_identified
goal_identified
goal_identified
=== ep: 1698, time 47.882720708847046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1699, time 64.05283880233765, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1700, time 62.09965467453003, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1701, time 58.88819718360901, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1702, time 63.31942319869995, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1703, time 60.70524883270264, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
goal_identified
goal_identified
goal_identified
=== ep: 1704, time 60.86664700508118, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1705, time 58.86840295791626, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1706, time 51.01782178878784, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1707, time 59.807920932769775, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
goal_identified
=== ep: 1708, time 45.63041067123413, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
goal_identified
goal_identified
=== ep: 1709, time 57.55761528015137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
goal_identified
goal_identified
goal_identified
=== ep: 1710, time 59.64914011955261, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1711, time 51.69709753990173, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1712, time 64.6027238368988, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1713, time 62.477965116500854, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
goal_identified
goal_identified
goal_identified
=== ep: 1714, time 65.68519043922424, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
goal_identified
goal_identified
goal_identified
=== ep: 1715, time 65.83525490760803, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1716, time 51.7617974281311, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1717, time 62.97944784164429, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
goal_identified
goal_identified
goal_identified
=== ep: 1718, time 58.93023657798767, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
goal_identified
goal_identified
=== ep: 1719, time 49.29131197929382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1720, time 61.21040654182434, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
goal_identified
goal_identified
=== ep: 1721, time 50.67171764373779, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
goal_identified
goal_identified
=== ep: 1722, time 56.458416223526, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1723, time 54.18257188796997, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1724, time 65.1446442604065, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1725, time 61.89895009994507, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1726, time 50.98654246330261, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
=== ep: 1727, time 65.34622001647949, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
goal_identified
goal_identified
=== ep: 1728, time 57.28378748893738, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1729, time 54.504154205322266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
goal_identified
goal_identified
=== ep: 1730, time 55.32481098175049, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
goal_identified
goal_identified
=== ep: 1731, time 52.0182945728302, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1732, time 51.345701694488525, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
goal_identified
goal_identified
goal_identified
=== ep: 1733, time 54.221057176589966, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
goal_identified
goal_identified
goal_identified
=== ep: 1734, time 69.36319088935852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
goal_identified
goal_identified
=== ep: 1735, time 57.64945697784424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
goal_identified
goal_identified
goal_identified
=== ep: 1736, time 58.525917530059814, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1737, time 63.69342613220215, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
goal_identified
goal_identified
goal_identified
=== ep: 1738, time 52.52015542984009, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
goal_identified
goal_identified
goal_identified
=== ep: 1739, time 52.40845322608948, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
goal_identified
goal_identified
=== ep: 1740, time 48.91092586517334, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1741, time 61.39038634300232, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1742, time 47.85664939880371, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1743, time 67.3374412059784, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1744, time 54.82632303237915, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1745, time 61.74570417404175, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1746, time 59.034120321273804, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1747, time 62.73937392234802, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1748, time 69.05897355079651, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
goal_identified
goal_identified
goal_identified
=== ep: 1749, time 63.58947944641113, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1750, time 53.28014874458313, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1751, time 59.2170627117157, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1752, time 55.099445104599, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1753, time 56.2049355506897, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1754, time 46.47757434844971, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
goal_identified
goal_identified
goal_identified
=== ep: 1755, time 55.36756730079651, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1756, time 52.605180501937866, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1757, time 53.643747329711914, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1758, time 56.45327806472778, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1759, time 59.34320116043091, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
goal_identified
goal_identified
goal_identified
=== ep: 1760, time 44.315101861953735, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1761, time 60.169841289520264, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1762, time 53.763200998306274, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1763, time 58.637463331222534, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1764, time 62.92205286026001, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
goal_identified
goal_identified
=== ep: 1765, time 60.073811769485474, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
=== ep: 1766, time 66.7368893623352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1767, time 58.60044717788696, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1768, time 54.26587390899658, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1769, time 64.10683727264404, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
goal_identified
goal_identified
=== ep: 1770, time 44.87338161468506, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
goal_identified
goal_identified
=== ep: 1771, time 55.20688247680664, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1772, time 56.62269568443298, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
goal_identified
goal_identified
=== ep: 1773, time 67.24030995368958, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
goal_identified
goal_identified
=== ep: 1774, time 56.23203945159912, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
goal_identified
goal_identified
=== ep: 1775, time 63.49249839782715, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
goal_identified
goal_identified
=== ep: 1776, time 64.87047219276428, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
goal_identified
goal_identified
goal_identified
=== ep: 1777, time 59.17907166481018, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
goal_identified
goal_identified
goal_identified
=== ep: 1778, time 65.06410694122314, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1779, time 46.637009382247925, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1780, time 58.35330533981323, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
goal_identified
goal_identified
goal_identified
=== ep: 1781, time 52.33399558067322, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
goal_identified
goal_identified
goal_identified
=== ep: 1782, time 52.271740436553955, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1783, time 59.400832176208496, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
goal_identified
goal_identified
=== ep: 1784, time 55.614402770996094, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1785, time 55.63329076766968, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
goal_identified
goal_identified
=== ep: 1786, time 49.641138553619385, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
goal_identified
goal_identified
goal_identified
=== ep: 1787, time 58.208313941955566, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1788, time 46.24333906173706, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 58.696754693984985, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1790, time 62.19190692901611, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1791, time 61.234092712402344, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1792, time 62.04706001281738, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1793, time 59.86346983909607, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1794, time 63.5856716632843, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1795, time 61.7874116897583, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
goal_identified
goal_identified
=== ep: 1796, time 50.52301597595215, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
goal_identified
goal_identified
goal_identified
=== ep: 1797, time 56.859222412109375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1798, time 51.154378175735474, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1799, time 58.19553232192993, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1800, time 50.08535075187683, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1801, time 65.52979254722595, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
goal_identified
goal_identified
goal_identified
=== ep: 1802, time 53.899542570114136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1803, time 58.06552743911743, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
goal_identified
goal_identified
goal_identified
=== ep: 1804, time 47.08879065513611, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
goal_identified
goal_identified
=== ep: 1805, time 59.871981382369995, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
goal_identified
goal_identified
=== ep: 1806, time 38.61469483375549, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
goal_identified
goal_identified
goal_identified
=== ep: 1807, time 63.31447434425354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
goal_identified
goal_identified
=== ep: 1808, time 60.86834359169006, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
goal_identified
goal_identified
goal_identified
=== ep: 1809, time 64.88464117050171, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1810, time 61.25686287879944, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
goal_identified
goal_identified
goal_identified
=== ep: 1811, time 59.4910352230072, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
goal_identified
goal_identified
=== ep: 1812, time 62.308311462402344, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
goal_identified
=== ep: 1813, time 57.916224002838135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
goal_identified
goal_identified
goal_identified
=== ep: 1814, time 64.90395522117615, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 41.52029824256897, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
goal_identified
goal_identified
goal_identified
=== ep: 1816, time 61.32055401802063, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1817, time 48.50617718696594, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1818, time 58.465301752090454, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1819, time 53.48236799240112, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
goal_identified
goal_identified
=== ep: 1820, time 66.19934391975403, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
goal_identified
goal_identified
=== ep: 1821, time 62.14191937446594, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
goal_identified
goal_identified
goal_identified
=== ep: 1822, time 55.375226736068726, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1823, time 58.16032266616821, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1824, time 53.53421354293823, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
goal_identified
goal_identified
=== ep: 1825, time 54.783913135528564, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1826, time 46.606223821640015, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
goal_identified
goal_identified
goal_identified
=== ep: 1827, time 61.96301746368408, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1828, time 60.26803231239319, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
goal_identified
=== ep: 1829, time 63.243056774139404, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1830, time 57.8308482170105, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
goal_identified
=== ep: 1831, time 60.905778646469116, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1832, time 64.04517889022827, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
goal_identified
goal_identified
goal_identified
=== ep: 1833, time 62.032163858413696, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
goal_identified
goal_identified
goal_identified
=== ep: 1834, time 59.7246994972229, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
goal_identified
goal_identified
=== ep: 1835, time 46.17307901382446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
goal_identified
goal_identified
goal_identified
=== ep: 1836, time 54.846253395080566, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1837, time 50.672186613082886, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1838, time 54.35783648490906, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
goal_identified
goal_identified
goal_identified
=== ep: 1839, time 55.04588270187378, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1840, time 62.6494574546814, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1841, time 54.903138875961304, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1842, time 46.94326686859131, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1843, time 55.46226358413696, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
goal_identified
goal_identified
=== ep: 1844, time 54.622623682022095, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1845, time 56.71840143203735, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1846, time 59.030978202819824, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1847, time 65.06617426872253, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1847
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1848, time 62.349937438964844, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1849, time 59.176207304000854, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1850, time 60.39202952384949, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1851, time 58.513134479522705, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
goal_identified
goal_identified
=== ep: 1852, time 64.21853113174438, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
goal_identified
goal_identified
goal_identified
=== ep: 1853, time 56.57725715637207, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1854, time 48.66935157775879, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1855, time 53.91261291503906, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1856, time 60.982332706451416, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1857, time 59.21021389961243, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
goal_identified
goal_identified
=== ep: 1858, time 54.509687662124634, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
goal_identified
goal_identified
=== ep: 1859, time 61.62875199317932, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1860, time 61.44722270965576, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1861, time 66.19112491607666, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1862, time 56.09754943847656, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1863, time 51.74994373321533, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1864, time 56.167715072631836, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1865, time 57.36310863494873, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1866, time 49.75285768508911, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
goal_identified
goal_identified
goal_identified
=== ep: 1867, time 56.55956053733826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
goal_identified
goal_identified
goal_identified
=== ep: 1868, time 59.6509485244751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1869, time 58.398630142211914, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
goal_identified
goal_identified
goal_identified
=== ep: 1870, time 52.86069297790527, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1871, time 58.722312688827515, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1872, time 63.29518127441406, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
goal_identified
=== ep: 1873, time 53.25759983062744, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1874, time 56.75757932662964, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1875, time 59.73933029174805, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1876, time 61.76395392417908, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1877, time 55.14198970794678, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1878, time 55.298370599746704, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
goal_identified
goal_identified
goal_identified
=== ep: 1879, time 57.76150298118591, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
goal_identified
goal_identified
=== ep: 1880, time 55.463725090026855, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1881, time 49.79133081436157, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
goal_identified
goal_identified
goal_identified
=== ep: 1882, time 57.65360879898071, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
goal_identified
goal_identified
=== ep: 1883, time 60.52642822265625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1884, time 47.25277519226074, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
goal_identified
goal_identified
=== ep: 1885, time 57.829585552215576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1886, time 54.31393623352051, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1887, time 59.2819550037384, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1888, time 45.25327253341675, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1889, time 61.62488579750061, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 49.183332204818726, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
goal_identified
=== ep: 1891, time 55.95637607574463, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1892, time 52.92765951156616, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
goal_identified
goal_identified
=== ep: 1893, time 63.354353189468384, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1894, time 56.60168766975403, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
goal_identified
goal_identified
=== ep: 1895, time 55.079068183898926, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1896, time 58.52883219718933, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1897, time 64.94979453086853, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1898, time 54.29219460487366, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1899, time 57.03167223930359, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1900, time 59.08487010002136, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1900
goal_identified
goal_identified
=== ep: 1901, time 62.667036294937134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1901
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1902, time 62.638972759246826, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1902
goal_identified
goal_identified
goal_identified
=== ep: 1903, time 47.09718728065491, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1904, time 61.43865156173706, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1904
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1905, time 53.163440465927124, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1905
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1906, time 58.53399991989136, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1906
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1907, time 53.433879137039185, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1907
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1908, time 67.4132194519043, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1908
goal_identified
=== ep: 1909, time 55.9504656791687, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1909
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1910, time 64.2619698047638, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1910
goal_identified
goal_identified
goal_identified
=== ep: 1911, time 54.57407093048096, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1911
goal_identified
goal_identified
goal_identified
=== ep: 1912, time 64.28561115264893, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1913, time 62.84971356391907, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1913
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1914, time 65.43414545059204, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1914
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1915, time 68.05748724937439, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1915
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1916, time 56.76690721511841, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1916
goal_identified
goal_identified
=== ep: 1917, time 60.38914155960083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1918, time 45.138702630996704, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1918
goal_identified
goal_identified
goal_identified
=== ep: 1919, time 61.71349310874939, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1920, time 48.4984929561615, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1920
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1921, time 60.5251407623291, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1921
goal_identified
goal_identified
goal_identified
=== ep: 1922, time 57.59201693534851, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1922
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1923, time 66.77963042259216, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1923
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1924, time 63.21885418891907, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1925, time 55.14168691635132, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1925
goal_identified
goal_identified
goal_identified
=== ep: 1926, time 54.28897166252136, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1926
goal_identified
goal_identified
goal_identified
=== ep: 1927, time 53.5399124622345, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1928, time 51.3940646648407, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1929, time 53.34995245933533, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1930, time 59.082751989364624, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1930
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1931, time 63.77016472816467, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1931
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1932, time 59.56422758102417, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1932
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1933, time 53.500463247299194, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1934, time 55.262815713882446, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1934
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1935, time 56.6801962852478, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1935
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1936, time 50.99905061721802, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1936
goal_identified
goal_identified
goal_identified
=== ep: 1937, time 54.8466694355011, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1937
goal_identified
=== ep: 1938, time 61.947303771972656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1938
goal_identified
goal_identified
=== ep: 1939, time 61.21524381637573, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1939
goal_identified
goal_identified
goal_identified
=== ep: 1940, time 65.29647827148438, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1940
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1941, time 51.22083020210266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1941
goal_identified
goal_identified
=== ep: 1942, time 59.76347541809082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1942
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1943, time 50.48096036911011, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1943
goal_identified
goal_identified
=== ep: 1944, time 59.92819690704346, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1944
goal_identified
goal_identified
=== ep: 1945, time 42.28513312339783, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1945
goal_identified
goal_identified
goal_identified
=== ep: 1946, time 65.67214941978455, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1946
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1947, time 61.049397468566895, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1947
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1948, time 62.104761838912964, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1948
goal_identified
goal_identified
=== ep: 1949, time 57.468425035476685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1949
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1950, time 56.03956842422485, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1950
goal_identified
goal_identified
goal_identified
=== ep: 1951, time 59.884252309799194, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1951
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1952, time 54.19397687911987, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1953, time 53.042940855026245, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1953
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1954, time 50.17378067970276, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
goal_identified
goal_identified
goal_identified
=== ep: 1955, time 65.07744407653809, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1956, time 60.05328440666199, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1956
goal_identified
=== ep: 1957, time 64.37277865409851, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1958, time 53.377662897109985, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1958
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1959, time 60.50942277908325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1959
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1960, time 57.63067817687988, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1960
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1961, time 47.48940944671631, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1961
goal_identified
goal_identified
=== ep: 1962, time 51.4252347946167, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1962
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1963, time 61.60825276374817, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1963
goal_identified
goal_identified
goal_identified
=== ep: 1964, time 59.54479646682739, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1964
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1965, time 62.503591775894165, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1965
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1966, time 53.97274899482727, eps 0.001, sum reward: 6, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1966
goal_identified
goal_identified
goal_identified
=== ep: 1967, time 66.28044056892395, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1967
goal_identified
goal_identified
goal_identified
=== ep: 1968, time 58.65760684013367, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1968
goal_identified
=== ep: 1969, time 66.07962942123413, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1969
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1970, time 62.62159752845764, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1970
goal_identified
goal_identified
=== ep: 1971, time 62.461387395858765, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1971
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1972, time 69.58988523483276, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1972
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1973, time 56.446430683135986, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1973
goal_identified
goal_identified
=== ep: 1974, time 60.60707187652588, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1974
goal_identified
goal_identified
goal_identified
=== ep: 1975, time 45.05141520500183, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1975
goal_identified
goal_identified
goal_identified
=== ep: 1976, time 62.313616037368774, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1976
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1977, time 49.459216833114624, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1978, time 56.545159101486206, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1978
goal_identified
=== ep: 1979, time 55.032801151275635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1979
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1980, time 64.35487127304077, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1980
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1981, time 63.601354360580444, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1981
goal_identified
goal_identified
=== ep: 1982, time 51.62916851043701, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1982
goal_identified
goal_identified
=== ep: 1983, time 48.08419489860535, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1983
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1984, time 53.375988245010376, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1984
goal_identified
goal_identified
goal_identified
=== ep: 1985, time 56.81785821914673, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1985
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1986, time 56.45457696914673, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1987, time 59.94770383834839, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1987
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1988, time 63.557153940200806, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1988
goal_identified
=== ep: 1989, time 61.15843224525452, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1989
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1990, time 57.695852518081665, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1990
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1991, time 49.55857825279236, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1992, time 55.178696155548096, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1992
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1993, time 56.35341787338257, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1993
goal_identified
goal_identified
goal_identified
=== ep: 1994, time 54.528300046920776, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1994
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1995, time 58.21765375137329, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1995
goal_identified
goal_identified
goal_identified
=== ep: 1996, time 56.56452989578247, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1996
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1997, time 64.03468632698059, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1997
goal_identified
goal_identified
goal_identified
=== ep: 1998, time 58.53104257583618, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1998
goal_identified
goal_identified
goal_identified
=== ep: 1999, time 57.18343472480774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1999
goal_identified
goal_identified
=== ep: 2000, time 45.26801800727844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2000
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2001, time 58.08851933479309, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2001
goal_identified
goal_identified
goal_identified
=== ep: 2002, time 55.882580041885376, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2002
goal_identified
goal_identified
=== ep: 2003, time 58.60622525215149, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2004, time 55.05958795547485, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2004
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2005, time 63.804070711135864, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2005
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2006, time 54.9606192111969, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2006
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2007, time 55.09317350387573, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2007
goal_identified
goal_identified
goal_identified
=== ep: 2008, time 43.967551946640015, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2008
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2009, time 62.29581546783447, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2009
goal_identified
goal_identified
goal_identified
=== ep: 2010, time 53.667375802993774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2010
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2011, time 58.08874583244324, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2012, time 56.19922184944153, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2012
goal_identified
goal_identified
=== ep: 2013, time 62.67203068733215, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2013
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2014, time 59.77085065841675, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2014
goal_identified
=== ep: 2015, time 52.73263931274414, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2015
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2016, time 46.876200914382935, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2016
goal_identified
goal_identified
=== ep: 2017, time 56.61987543106079, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2017
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2018, time 61.11819815635681, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2018
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2019, time 54.83138418197632, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2019
goal_identified
goal_identified
=== ep: 2020, time 57.36719632148743, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2020
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2021, time 65.48126983642578, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2021
goal_identified
goal_identified
=== ep: 2022, time 62.030951261520386, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2022
goal_identified
goal_identified
=== ep: 2023, time 66.9786365032196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2023
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2024, time 60.03279781341553, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2024
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2025, time 57.791818380355835, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2025
goal_identified
goal_identified
=== ep: 2026, time 60.28977060317993, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2026
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2027, time 57.87648010253906, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2028, time 65.5638678073883, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2028
goal_identified
goal_identified
goal_identified
=== ep: 2029, time 49.136536598205566, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2029
goal_identified
=== ep: 2030, time 52.77148938179016, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2030
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2031, time 53.48377013206482, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2031
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2032, time 65.56501817703247, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2032
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2033, time 49.064528703689575, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2033
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2034, time 63.95195484161377, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2034
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2035, time 52.68878793716431, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2035
goal_identified
goal_identified
=== ep: 2036, time 68.25991177558899, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2036
goal_identified
goal_identified
=== ep: 2037, time 59.83410978317261, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2038, time 60.31638240814209, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2039, time 49.239935874938965, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2039
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2040, time 61.9423041343689, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2040
goal_identified
goal_identified
=== ep: 2041, time 56.355088233947754, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2041
goal_identified
goal_identified
goal_identified
=== ep: 2042, time 54.94766139984131, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2042
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2043, time 49.223355293273926, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2043
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2044, time 59.56332969665527, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2044
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2045, time 61.608163833618164, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2045
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2046, time 50.02367043495178, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2047, time 55.75013852119446, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2047
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2048, time 54.68821477890015, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2048
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2049, time 59.18530869483948, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2049
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2050, time 43.78864884376526, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2050
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2051, time 59.474753856658936, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2051
goal_identified
=== ep: 2052, time 57.37523031234741, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2052
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2053, time 58.44907855987549, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2053
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2054, time 55.77472400665283, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2054
goal_identified
goal_identified
goal_identified
=== ep: 2055, time 56.9123899936676, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2055
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2056, time 62.089335680007935, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2056
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2057, time 65.4568681716919, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2058, time 60.71335983276367, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2058
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2059, time 59.39641261100769, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2059
goal_identified
goal_identified
=== ep: 2060, time 52.91008186340332, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2060
goal_identified
goal_identified
goal_identified
=== ep: 2061, time 61.467804193496704, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2061
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2062, time 57.77138614654541, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2062
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2063, time 54.67065787315369, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2063
=== ep: 2064, time 44.49949645996094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2064
goal_identified
goal_identified
=== ep: 2065, time 58.973973751068115, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2065
goal_identified
goal_identified
=== ep: 2066, time 61.881824254989624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2066
goal_identified
goal_identified
goal_identified
=== ep: 2067, time 52.77732992172241, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2067
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2068, time 55.5428786277771, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2068
goal_identified
goal_identified
goal_identified
=== ep: 2069, time 53.908350467681885, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2069
goal_identified
goal_identified
goal_identified
=== ep: 2070, time 59.281585454940796, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2070
goal_identified
=== ep: 2071, time 46.10586190223694, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2071
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2072, time 56.920480728149414, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2072
goal_identified
goal_identified
goal_identified
=== ep: 2073, time 57.88583827018738, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2073
goal_identified
goal_identified
=== ep: 2074, time 67.77673935890198, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2074
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2075, time 54.27874565124512, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2075
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2076, time 59.571234941482544, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2076
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2077, time 47.616809606552124, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2077
goal_identified
=== ep: 2078, time 61.505115270614624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2079, time 43.99122953414917, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2079
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2080, time 61.378928661346436, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2080
goal_identified
=== ep: 2081, time 51.24274468421936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2082, time 67.5746431350708, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2082
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2083, time 60.57742428779602, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2083
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2084, time 59.88716435432434, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2084
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2085, time 50.41926074028015, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2085
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2086, time 51.32501792907715, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2086
goal_identified
goal_identified
goal_identified
=== ep: 2087, time 58.195048570632935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2087
goal_identified
goal_identified
goal_identified
=== ep: 2088, time 51.958251953125, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2088
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2089, time 52.06509304046631, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2089
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2090, time 57.39215135574341, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2090
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2091, time 59.252732276916504, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2091
goal_identified
goal_identified
=== ep: 2092, time 50.281978368759155, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2092
goal_identified
goal_identified
=== ep: 2093, time 49.35564827919006, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2093
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2094, time 57.81184196472168, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2094
goal_identified
=== ep: 2095, time 57.48132634162903, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2095
=== ep: 2096, time 59.359517335891724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2096
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2097, time 56.63677382469177, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2097
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2098, time 60.21836519241333, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2098
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2099, time 60.2758526802063, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2099
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2100, time 63.020410776138306, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2100
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2101, time 68.03582000732422, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2101
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2102, time 58.835739850997925, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2102
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2103, time 61.63052272796631, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2103
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2104, time 44.70018982887268, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2105, time 59.95562481880188, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2105
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2106, time 49.31446146965027, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2106
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2107, time 58.55821204185486, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2107
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2108, time 52.315359115600586, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2108
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2109, time 66.51566505432129, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2109
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2110, time 63.53754949569702, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2110
goal_identified
goal_identified
goal_identified
=== ep: 2111, time 59.30730652809143, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2111
goal_identified
=== ep: 2112, time 51.52611780166626, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2112
goal_identified
goal_identified
goal_identified
=== ep: 2113, time 52.50921320915222, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2113
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2114, time 56.675899505615234, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2114
goal_identified
goal_identified
goal_identified
=== ep: 2115, time 53.1116156578064, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2116, time 54.40378212928772, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2116
goal_identified
goal_identified
goal_identified
=== ep: 2117, time 58.89023804664612, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2117
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2118, time 67.1758964061737, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2118
goal_identified
=== ep: 2119, time 56.82304096221924, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2119
goal_identified
goal_identified
goal_identified
=== ep: 2120, time 60.17345428466797, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2120
goal_identified
goal_identified
goal_identified
=== ep: 2121, time 42.16956925392151, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2121
goal_identified
goal_identified
=== ep: 2122, time 59.47049617767334, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2122
goal_identified
=== ep: 2123, time 52.978673219680786, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2123
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2124, time 61.326932191848755, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2124
goal_identified
=== ep: 2125, time 53.12946271896362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2125
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2126, time 66.75516843795776, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2126
goal_identified
goal_identified
goal_identified
=== ep: 2127, time 61.34422516822815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2127
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2128, time 66.7593367099762, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2128
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2129, time 65.54843926429749, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2129
goal_identified
goal_identified
goal_identified
=== ep: 2130, time 63.06916809082031, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2130
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2131, time 64.60485434532166, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2131
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2132, time 59.52619504928589, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2132
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2133, time 58.217137813568115, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2133
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2134, time 59.917787313461304, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2134
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2135, time 65.61177277565002, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2135
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2136, time 62.5507869720459, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2136
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2137, time 61.709219217300415, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2137
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2138, time 55.27143859863281, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2139, time 54.76028227806091, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2139
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2140, time 57.19118404388428, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2140
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2141, time 61.878052711486816, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2141
goal_identified
goal_identified
=== ep: 2142, time 59.18740105628967, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2142
goal_identified
goal_identified
goal_identified
=== ep: 2143, time 50.042646646499634, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2144, time 59.86947321891785, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2144
goal_identified
goal_identified
=== ep: 2145, time 55.02044701576233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2145
goal_identified
goal_identified
goal_identified
=== ep: 2146, time 57.2859251499176, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2146
goal_identified
=== ep: 2147, time 49.35499572753906, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2148, time 64.76448059082031, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2148
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2149, time 53.30409550666809, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2149
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2150, time 59.229891777038574, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2150
goal_identified
goal_identified
=== ep: 2151, time 50.16026830673218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2151
goal_identified
goal_identified
=== ep: 2152, time 67.64562678337097, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2152
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2153, time 56.57437205314636, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2154, time 58.209019899368286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2154
=== ep: 2155, time 45.56848430633545, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2155
goal_identified
goal_identified
goal_identified
=== ep: 2156, time 62.45852589607239, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2156
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2157, time 43.49624705314636, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2157
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2158, time 56.22978901863098, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2159, time 53.51107692718506, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2159
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2160, time 66.19875836372375, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2161, time 46.998188972473145, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2162, time 60.64207148551941, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2162
goal_identified
goal_identified
=== ep: 2163, time 51.19631600379944, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2163
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2164, time 59.04370403289795, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2164
goal_identified
goal_identified
goal_identified
=== ep: 2165, time 46.54490542411804, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2165
goal_identified
goal_identified
=== ep: 2166, time 64.28361082077026, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2167, time 59.374340295791626, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2167
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2168, time 60.26282572746277, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2169, time 57.41799211502075, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2169
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2170, time 57.26923155784607, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2170
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2171, time 66.69615888595581, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2171
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2172, time 62.80135703086853, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2173, time 65.99700903892517, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2173
goal_identified
=== ep: 2174, time 63.00251603126526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2174
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2175, time 62.888665437698364, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2175
goal_identified
goal_identified
goal_identified
=== ep: 2176, time 61.08531832695007, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2176
goal_identified
=== ep: 2177, time 64.03658151626587, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2177
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2178, time 57.76312446594238, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2178
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2179, time 62.05775713920593, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2180, time 62.85134696960449, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2180
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2181, time 62.518656730651855, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2181
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2182, time 66.58304500579834, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2182
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2183, time 60.59256148338318, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2184, time 55.69965648651123, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2184
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2185, time 53.38955235481262, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2185
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2186, time 58.261775732040405, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2186
goal_identified
goal_identified
=== ep: 2187, time 58.23574709892273, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2187
goal_identified
goal_identified
goal_identified
=== ep: 2188, time 49.02884864807129, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2188
goal_identified
goal_identified
=== ep: 2189, time 58.70319938659668, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2189
goal_identified
goal_identified
=== ep: 2190, time 59.541932821273804, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2190
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2191, time 67.21399450302124, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2191
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2192, time 52.21555733680725, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2192
goal_identified
goal_identified
=== ep: 2193, time 60.95043659210205, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2193
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2194, time 52.88101267814636, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2194
goal_identified
goal_identified
=== ep: 2195, time 66.13062071800232, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2195
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2196, time 52.70068168640137, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2196
goal_identified
goal_identified
goal_identified
=== ep: 2197, time 58.590627908706665, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2197
goal_identified
goal_identified
=== ep: 2198, time 51.4671790599823, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2198
goal_identified
goal_identified
goal_identified
=== ep: 2199, time 65.21284246444702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2199
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2200, time 53.196786403656006, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2200
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2201, time 57.492114305496216, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2201
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2202, time 50.22819185256958, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2202
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2203, time 65.4338047504425, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2203
goal_identified
goal_identified
goal_identified
=== ep: 2204, time 55.60888457298279, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2204
goal_identified
goal_identified
=== ep: 2205, time 60.450376987457275, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2205
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2206, time 53.27965807914734, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2206
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2207, time 66.43568515777588, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2207
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2208, time 64.9679183959961, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2208
goal_identified
goal_identified
=== ep: 2209, time 60.084094762802124, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2209
goal_identified
goal_identified
goal_identified
=== ep: 2210, time 60.25910949707031, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2210
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2211, time 49.93680000305176, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2212, time 63.82020282745361, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2212
=== ep: 2213, time 53.951878786087036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2213
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2214, time 58.876455783843994, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2214
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2215, time 38.53624725341797, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2215
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2216, time 62.444047927856445, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
goal_identified
goal_identified
=== ep: 2217, time 51.795984745025635, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2217
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2218, time 58.848037004470825, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
goal_identified
goal_identified
=== ep: 2219, time 53.718520402908325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2219
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2220, time 63.95552468299866, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2220
goal_identified
goal_identified
goal_identified
=== ep: 2221, time 61.71126079559326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2221
goal_identified
=== ep: 2222, time 57.48570013046265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2222
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2223, time 47.402498722076416, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2223
goal_identified
goal_identified
=== ep: 2224, time 56.37025570869446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2224
goal_identified
goal_identified
goal_identified
=== ep: 2225, time 60.87345600128174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2225
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2226, time 58.55088543891907, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2226
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2227, time 56.8044810295105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2228, time 59.966251611709595, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2228
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2229, time 57.48444890975952, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2229
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2230, time 65.39484000205994, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2230
goal_identified
goal_identified
=== ep: 2231, time 64.474454164505, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2231
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2232, time 60.837743043899536, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2232
goal_identified
goal_identified
goal_identified
=== ep: 2233, time 62.83463144302368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2233
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2234, time 54.37778282165527, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2235, time 65.68339705467224, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2235
goal_identified
=== ep: 2236, time 58.57635164260864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2236
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2237, time 66.16094660758972, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2237
goal_identified
=== ep: 2238, time 66.33644199371338, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2238
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2239, time 59.17301416397095, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2239
goal_identified
goal_identified
=== ep: 2240, time 60.99839496612549, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2241, time 49.580570936203, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2241
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2242, time 65.86942052841187, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2242
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2243, time 51.48747181892395, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2243
goal_identified
goal_identified
=== ep: 2244, time 64.7659432888031, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 43.71983361244202, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2245
goal_identified
goal_identified
goal_identified
=== ep: 2246, time 67.29679369926453, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2246
goal_identified
goal_identified
goal_identified
=== ep: 2247, time 55.45633602142334, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2247
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2248, time 64.37514925003052, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2248
goal_identified
goal_identified
=== ep: 2249, time 50.525195360183716, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2249
goal_identified
goal_identified
=== ep: 2250, time 63.71518158912659, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2250
goal_identified
goal_identified
goal_identified
=== ep: 2251, time 56.780577421188354, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2251
goal_identified
goal_identified
goal_identified
=== ep: 2252, time 68.89895248413086, eps 0.001, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2252
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2253, time 66.42739248275757, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2253
goal_identified
goal_identified
goal_identified
=== ep: 2254, time 61.042900800704956, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2254
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2255, time 61.89222812652588, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2255
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2256, time 51.35964798927307, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2256
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2257, time 64.50155997276306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2257
goal_identified
goal_identified
goal_identified
=== ep: 2258, time 54.504706144332886, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2258
goal_identified
goal_identified
=== ep: 2259, time 61.53650164604187, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2259
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2260, time 44.66747760772705, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2260
goal_identified
goal_identified
=== ep: 2261, time 63.05606126785278, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2261
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2262, time 58.95504951477051, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1899
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2263, time 64.240713596344, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2263
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2264, time 55.75816559791565, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2264
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2265, time 59.247647762298584, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2265
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2266, time 64.04142570495605, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2266
goal_identified
goal_identified
goal_identified
=== ep: 2267, time 65.62941384315491, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2267
goal_identified
goal_identified
=== ep: 2268, time 62.1638081073761, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2268
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2269, time 61.440821409225464, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1924
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2270, time 54.18585133552551, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2270
goal_identified
goal_identified
=== ep: 2271, time 58.63276648521423, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2271
goal_identified
goal_identified
goal_identified
=== ep: 2272, time 64.16059064865112, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2272
goal_identified
goal_identified
goal_identified
=== ep: 2273, time 58.98587679862976, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2273
goal_identified
goal_identified
goal_identified
=== ep: 2274, time 61.14517402648926, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2274
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2275, time 46.224297761917114, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2275
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2276, time 63.832051277160645, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2276
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2277, time 55.26811242103577, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2277
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2278, time 62.669076442718506, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1929
goal_identified
goal_identified
goal_identified
=== ep: 2279, time 47.71397805213928, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2279
goal_identified
goal_identified
goal_identified
=== ep: 2280, time 65.69260954856873, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2280
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2281, time 62.04054021835327, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2281
goal_identified
goal_identified
=== ep: 2282, time 63.31128215789795, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2282
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2283, time 55.21826410293579, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2283
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2284, time 57.195122480392456, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2284
goal_identified
goal_identified
=== ep: 2285, time 59.3121132850647, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2285
goal_identified
goal_identified
=== ep: 2286, time 54.88559436798096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2286
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2287, time 50.70076823234558, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2287
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2288, time 59.87115406990051, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2288
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2289, time 63.68768286705017, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2289
goal_identified
=== ep: 2290, time 56.04001188278198, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2290
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2291, time 60.66549253463745, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2291
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2292, time 54.919946908950806, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2292
goal_identified
=== ep: 2293, time 67.63092565536499, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2293
goal_identified
goal_identified
goal_identified
=== ep: 2294, time 59.33202910423279, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2294
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2295, time 62.634835720062256, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2295
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2296, time 47.90616226196289, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2296
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2297, time 64.26000928878784, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2297
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2298, time 56.70297908782959, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2298
goal_identified
goal_identified
goal_identified
=== ep: 2299, time 56.534446239471436, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2299
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2300, time 51.60557699203491, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2300
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2301, time 64.2823395729065, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2301
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2302, time 61.185001611709595, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2303, time 54.14553618431091, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2303
goal_identified
goal_identified
goal_identified
=== ep: 2304, time 56.684263467788696, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2304
goal_identified
goal_identified
goal_identified
=== ep: 2305, time 59.68589425086975, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2305
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2306, time 65.450368642807, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2306
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2307, time 56.02713346481323, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2307
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2308, time 53.46229410171509, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2308
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2309, time 54.89348649978638, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2309
goal_identified
goal_identified
=== ep: 2310, time 61.16108155250549, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2311, time 55.02037787437439, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2311
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2312, time 52.609095335006714, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2312
goal_identified
goal_identified
goal_identified
=== ep: 2313, time 58.48438119888306, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2313
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2314, time 61.32008600234985, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2314
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2315, time 51.816813468933105, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2315
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2316, time 50.81466746330261, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2316
goal_identified
goal_identified
=== ep: 2317, time 58.24148106575012, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2317
goal_identified
goal_identified
goal_identified
=== ep: 2318, time 58.02286100387573, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2318
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2319, time 50.752525806427, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2319
goal_identified
goal_identified
goal_identified
=== ep: 2320, time 58.23871731758118, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2320
goal_identified
=== ep: 2321, time 59.1159725189209, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2322, time 57.52745604515076, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2322
goal_identified
goal_identified
goal_identified
=== ep: 2323, time 48.168116092681885, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2324, time 62.30473852157593, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2324
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2325, time 58.70033288002014, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2325
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2326, time 64.63074684143066, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 130/130)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2326
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2327, time 47.35484170913696, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2327
goal_identified
=== ep: 2328, time 62.63533806800842, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2328
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2329, time 53.59827160835266, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2329
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2330, time 64.95623350143433, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2330
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2331, time 45.39931559562683, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2331
goal_identified
goal_identified
=== ep: 2332, time 61.982524394989014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2332
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2333, time 57.163228034973145, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2333
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2334, time 64.99583268165588, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2334
goal_identified
goal_identified
=== ep: 2335, time 50.25743055343628, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2336, time 62.375848054885864, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2336
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2337, time 56.21080493927002, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2338, time 64.46836924552917, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2338
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2339, time 53.8615357875824, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2339
goal_identified
goal_identified
goal_identified
=== ep: 2340, time 56.67796754837036, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2340
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2341, time 53.19442677497864, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2341
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2342, time 63.941622495651245, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2342
goal_identified
goal_identified
goal_identified
=== ep: 2343, time 50.79413294792175, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2343
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2344, time 57.91674447059631, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2344
goal_identified
goal_identified
goal_identified
=== ep: 2345, time 56.94030165672302, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2345
goal_identified
goal_identified
goal_identified
=== ep: 2346, time 64.85272574424744, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2346
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2347, time 60.54896259307861, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2347
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2348, time 53.737388372421265, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2348
=== ep: 2349, time 60.93559527397156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2349
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2350, time 60.71864700317383, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2350
goal_identified
goal_identified
goal_identified
=== ep: 2351, time 63.90123414993286, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2351
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2352, time 58.496801137924194, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2352
goal_identified
goal_identified
=== ep: 2353, time 52.31414079666138, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2353
goal_identified
goal_identified
goal_identified
=== ep: 2354, time 57.89704179763794, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2354
goal_identified
goal_identified
goal_identified
=== ep: 2355, time 60.134669065475464, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2355
goal_identified
goal_identified
goal_identified
=== ep: 2356, time 54.54989290237427, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2356
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2357, time 55.730703592300415, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2357
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2358, time 56.897894620895386, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2358
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2359, time 60.80481815338135, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 136/136)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2359
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2360, time 50.45285701751709, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2360
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2361, time 53.330904960632324, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2361
goal_identified
goal_identified
=== ep: 2362, time 58.283538579940796, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2362
goal_identified
goal_identified
goal_identified
=== ep: 2363, time 63.1612606048584, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2363
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2364, time 47.24646306037903, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2364
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2365, time 55.791271924972534, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2365
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2366, time 53.419119358062744, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2366
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2367, time 60.50169849395752, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2367
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2368, time 49.09955883026123, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2368
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2369, time 61.02894306182861, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2369
goal_identified
=== ep: 2370, time 60.92363095283508, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2370
goal_identified
goal_identified
goal_identified
=== ep: 2371, time 67.73247838020325, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2371
goal_identified
=== ep: 2372, time 49.675859451293945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2373, time 62.79642939567566, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2373
goal_identified
goal_identified
=== ep: 2374, time 52.85361289978027, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2374
goal_identified
goal_identified
=== ep: 2375, time 66.96807527542114, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2375
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2376, time 55.72174024581909, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2376
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2377, time 56.89697575569153, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2377
goal_identified
goal_identified
goal_identified
=== ep: 2378, time 51.92142629623413, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2378
goal_identified
goal_identified
goal_identified
=== ep: 2379, time 62.261831521987915, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2379
goal_identified
goal_identified
=== ep: 2380, time 54.22590732574463, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2380
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2381, time 56.830718755722046, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2381
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2382, time 58.322399616241455, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2382
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2383, time 63.61317467689514, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2383
goal_identified
goal_identified
goal_identified
=== ep: 2384, time 65.05477476119995, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2384
goal_identified
goal_identified
goal_identified
=== ep: 2385, time 54.89219903945923, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2385
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2386, time 59.667304039001465, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2386
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2387, time 57.568899393081665, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2387
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2388, time 64.88149785995483, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2388
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2389, time 63.06094169616699, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2390, time 60.534550189971924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2390
goal_identified
=== ep: 2391, time 50.35269045829773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2391
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2392, time 56.54163098335266, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2392
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2393, time 57.933398962020874, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2393
goal_identified
goal_identified
=== ep: 2394, time 52.03541898727417, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2394
goal_identified
goal_identified
goal_identified
=== ep: 2395, time 53.771281242370605, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2395
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2396, time 59.41175723075867, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2396
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2397, time 57.60636281967163, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2397
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2398, time 60.0797221660614, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2398
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2399, time 47.667054414749146, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2399
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2400, time 57.55549597740173, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2400
goal_identified
goal_identified
=== ep: 2401, time 48.88803744316101, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2402, time 60.05075120925903, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2402
goal_identified
goal_identified
=== ep: 2403, time 56.77301645278931, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2403
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2404, time 65.75125479698181, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2404
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2405, time 57.81071972846985, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2406, time 67.41019630432129, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2406
goal_identified
goal_identified
=== ep: 2407, time 58.28785729408264, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 67.93985223770142, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2408
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2409, time 63.73798680305481, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1933
goal_identified
=== ep: 2410, time 65.36702585220337, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2411, time 60.51827597618103, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1954
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 53.71561813354492, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2412
goal_identified
goal_identified
goal_identified
=== ep: 2413, time 56.109713077545166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2414, time 58.24835228919983, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2038
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2415, time 53.59391164779663, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2416, time 53.64020586013794, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 58.39058017730713, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2417
goal_identified
goal_identified
goal_identified
=== ep: 2418, time 65.39947485923767, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2418
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2419, time 55.55031156539917, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2419
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2420, time 55.62806844711304, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2420
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2421, time 54.44214916229248, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2421
goal_identified
goal_identified
goal_identified
=== ep: 2422, time 61.65819334983826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2422
goal_identified
goal_identified
goal_identified
=== ep: 2423, time 51.37371373176575, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2423
goal_identified
goal_identified
=== ep: 2424, time 51.98069739341736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2424
goal_identified
goal_identified
=== ep: 2425, time 60.11139893531799, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2426, time 60.53644132614136, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2158
goal_identified
goal_identified
goal_identified
=== ep: 2427, time 62.06800580024719, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2427
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2428, time 57.39779806137085, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2428
goal_identified
goal_identified
=== ep: 2429, time 62.147533655166626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2429
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2430, time 59.83354711532593, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2160
goal_identified
goal_identified
goal_identified
=== ep: 2431, time 67.46109247207642, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2432, time 59.75533699989319, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2432
goal_identified
goal_identified
goal_identified
=== ep: 2433, time 68.05101990699768, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2434, time 50.96392369270325, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2434
goal_identified
goal_identified
=== ep: 2435, time 59.56231713294983, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2435
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2436, time 54.391902446746826, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2436
goal_identified
goal_identified
=== ep: 2437, time 62.093663692474365, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2437
goal_identified
goal_identified
goal_identified
=== ep: 2438, time 47.593709230422974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2438
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2439, time 59.772547006607056, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2439
goal_identified
goal_identified
=== ep: 2440, time 61.26514434814453, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2441, time 60.066458225250244, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2441
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2442, time 55.50116968154907, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2442
goal_identified
goal_identified
goal_identified
=== ep: 2443, time 60.17962026596069, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2443
goal_identified
goal_identified
goal_identified
=== ep: 2444, time 63.45299792289734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2444
goal_identified
goal_identified
=== ep: 2445, time 66.24675941467285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2446, time 57.06287932395935, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2446
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2447, time 57.287992000579834, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2447
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2448, time 54.48275446891785, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2448
goal_identified
goal_identified
goal_identified
=== ep: 2449, time 60.910627603530884, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2449
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2450, time 47.411970376968384, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2450
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2451, time 62.718080282211304, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2451
goal_identified
=== ep: 2452, time 60.89500379562378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2452
goal_identified
goal_identified
=== ep: 2453, time 62.41058325767517, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2453
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2454, time 55.48469066619873, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2454
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2455, time 58.03670787811279, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2455
goal_identified
goal_identified
=== ep: 2456, time 60.740639209747314, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2456
goal_identified
goal_identified
=== ep: 2457, time 60.539135217666626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2457
goal_identified
goal_identified
=== ep: 2458, time 52.991589069366455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2458
goal_identified
goal_identified
=== ep: 2459, time 53.08726501464844, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2459
goal_identified
=== ep: 2460, time 57.94302225112915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2460
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2461, time 55.30977749824524, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2461
goal_identified
goal_identified
goal_identified
=== ep: 2462, time 54.207329750061035, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2462
goal_identified
=== ep: 2463, time 59.33353018760681, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2463
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2464, time 62.82514262199402, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2464
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2465, time 54.65537667274475, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2465
goal_identified
=== ep: 2466, time 52.39661192893982, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2466
goal_identified
goal_identified
goal_identified
=== ep: 2467, time 58.68927621841431, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2467
goal_identified
goal_identified
goal_identified
=== ep: 2468, time 56.781901597976685, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2468
goal_identified
goal_identified
goal_identified
=== ep: 2469, time 56.805598974227905, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2469
goal_identified
goal_identified
goal_identified
=== ep: 2470, time 55.63630533218384, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2470
goal_identified
goal_identified
=== ep: 2471, time 65.06124663352966, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2471
goal_identified
goal_identified
goal_identified
=== ep: 2472, time 61.68317985534668, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2473, time 65.02651071548462, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2473
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2474, time 57.044577836990356, eps 0.001, sum reward: 7, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2218
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2475, time 59.79543876647949, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2475
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2476, time 63.33669376373291, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2269
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2477, time 65.56480193138123, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2477
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2478, time 70.61647748947144, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2478
goal_identified
goal_identified
goal_identified
=== ep: 2479, time 54.01805925369263, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2479
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2480, time 65.80430817604065, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2480
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2481, time 52.08147978782654, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2481
goal_identified
goal_identified
goal_identified
=== ep: 2482, time 68.66762185096741, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2482
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2483, time 53.862133264541626, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2409
goal_identified
goal_identified
=== ep: 2484, time 55.39350342750549, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2484
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2485, time 46.61100673675537, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2486, time 63.275092124938965, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2486
goal_identified
goal_identified
=== ep: 2487, time 48.53673577308655, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2487
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2488, time 61.30689859390259, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2488
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2489, time 59.563578605651855, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2489
goal_identified
=== ep: 2490, time 62.24068737030029, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2490
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2491, time 54.322378158569336, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2491
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2492, time 57.6793646812439, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2414
goal_identified
goal_identified
goal_identified
=== ep: 2493, time 59.329052209854126, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2493
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2494, time 55.18768787384033, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2494
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2495, time 49.207234621047974, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2495
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2496, time 58.53645157814026, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2430
goal_identified
goal_identified
goal_identified
=== ep: 2497, time 63.95886206626892, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2497
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2498, time 63.6213014125824, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2498
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2499, time 58.22291088104248, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2499
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2500, time 66.86786818504333, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2500
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2501, time 56.803205490112305, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2501
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2502, time 66.99010705947876, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 58.280075788497925, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2503
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2504, time 65.72193932533264, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2504
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2505, time 61.78077507019043, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2505
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2506, time 63.01424026489258, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2506
goal_identified
=== ep: 2507, time 58.80641317367554, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2507
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2508, time 48.266398429870605, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2508
goal_identified
goal_identified
goal_identified
=== ep: 2509, time 60.885565996170044, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2509
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2510, time 51.67584776878357, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2510
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2511, time 62.63075876235962, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2512, time 56.55136585235596, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2512
goal_identified
goal_identified
=== ep: 2513, time 67.67526936531067, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2513
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2514, time 61.42249417304993, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2514
goal_identified
goal_identified
goal_identified
=== ep: 2515, time 63.40173816680908, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2515
goal_identified
=== ep: 2516, time 54.38575315475464, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2516
goal_identified
goal_identified
=== ep: 2517, time 61.51449465751648, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2517
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2518, time 63.688708543777466, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2518
goal_identified
goal_identified
goal_identified
=== ep: 2519, time 55.51002311706543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2520, time 53.891916275024414, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2520
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2521, time 56.815146923065186, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2521
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2522, time 58.07150459289551, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2522
goal_identified
=== ep: 2523, time 51.273467779159546, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2523
goal_identified
goal_identified
goal_identified
=== ep: 2524, time 54.24173927307129, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2524
goal_identified
goal_identified
goal_identified
=== ep: 2525, time 61.22413969039917, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2525
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2526, time 58.08148980140686, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2476
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2527, time 58.80607867240906, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2527
goal_identified
goal_identified
goal_identified
=== ep: 2528, time 52.09836411476135, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2529, time 65.33719301223755, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2529
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2530, time 48.054654359817505, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2530
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2531, time 56.01471161842346, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2531
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2532, time 53.24295496940613, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2532
goal_identified
=== ep: 2533, time 66.14033269882202, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2533
goal_identified
goal_identified
goal_identified
=== ep: 2534, time 50.15868258476257, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2534
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2535, time 64.7188949584961, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2535
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2536, time 54.445544719696045, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2536
goal_identified
goal_identified
goal_identified
=== ep: 2537, time 71.11418771743774, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2537
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2538, time 60.26287031173706, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2538
goal_identified
goal_identified
goal_identified
=== ep: 2539, time 61.886799335479736, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2539
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2540, time 51.37217688560486, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2540
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2541, time 63.22570061683655, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2542, time 59.89491558074951, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2542
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2543, time 58.264750242233276, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2543
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2544, time 50.12703776359558, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2544
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2545, time 62.911248445510864, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2545
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2546, time 63.97441816329956, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2546
goal_identified
goal_identified
goal_identified
=== ep: 2547, time 57.54674410820007, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2547
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2548, time 63.10857176780701, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2548
goal_identified
=== ep: 2549, time 56.89051365852356, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2549
goal_identified
goal_identified
=== ep: 2550, time 62.346248149871826, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2550
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2551, time 63.051488637924194, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2551
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2552, time 61.238216400146484, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2552
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2553, time 58.67321491241455, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2553
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2554, time 58.96339726448059, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2554
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2555, time 63.70108127593994, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2555
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2556, time 69.92191171646118, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2557, time 63.96920585632324, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2558, time 65.96566343307495, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2558
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2559, time 62.28338384628296, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2559
goal_identified
goal_identified
goal_identified
=== ep: 2560, time 59.65608477592468, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2560
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2561, time 62.74051213264465, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2561
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2562, time 63.1051185131073, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2562
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2563, time 70.18273830413818, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2563
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2564, time 56.99021363258362, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2564
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2565, time 64.11401867866516, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2565
goal_identified
=== ep: 2566, time 51.60368299484253, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2566
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2567, time 68.180335521698, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2567
goal_identified
goal_identified
goal_identified
=== ep: 2568, time 58.3401734828949, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2568
goal_identified
goal_identified
goal_identified
=== ep: 2569, time 59.190369606018066, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 46.85241222381592, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2570
goal_identified
goal_identified
=== ep: 2571, time 61.05480408668518, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2572, time 61.6617534160614, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 126/126)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2572
goal_identified
goal_identified
=== ep: 2573, time 56.38079261779785, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2574, time 57.77201223373413, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2574
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2575, time 64.34213614463806, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2575
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2576, time 67.0749728679657, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2576
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2577, time 67.4079647064209, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2577
goal_identified
goal_identified
=== ep: 2578, time 62.61655235290527, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2578
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2579, time 58.612850189208984, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2579
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2580, time 55.5818772315979, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2580
goal_identified
goal_identified
=== ep: 2581, time 63.65206336975098, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2581
goal_identified
goal_identified
goal_identified
=== ep: 2582, time 65.63292384147644, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2582
goal_identified
=== ep: 2583, time 53.46956467628479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2583
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2584, time 66.02021050453186, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2584
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2585, time 52.8098840713501, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2585
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2586, time 68.84727954864502, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2586
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2587, time 61.69640612602234, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2587
goal_identified
goal_identified
goal_identified
=== ep: 2588, time 64.84768986701965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2588
goal_identified
goal_identified
=== ep: 2589, time 45.60586714744568, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2589
goal_identified
=== ep: 2590, time 65.2913269996643, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2591, time 52.01554226875305, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2591
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2592, time 63.02361583709717, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2592
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2593, time 46.61439251899719, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2593
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2594, time 69.86939263343811, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2594
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2595, time 64.02818655967712, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2595
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2596, time 66.89919447898865, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2483
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2597, time 65.25123739242554, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2597
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2598, time 62.7448935508728, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2599, time 65.07722210884094, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2599
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2600, time 65.11510872840881, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2600
goal_identified
goal_identified
goal_identified
=== ep: 2601, time 60.611923933029175, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2601
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2602, time 65.04463934898376, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2602
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2603, time 67.43207168579102, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2603
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2604, time 64.96249294281006, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2604
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2605, time 71.50343298912048, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2605
goal_identified
goal_identified
=== ep: 2606, time 69.68497776985168, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2606
goal_identified
goal_identified
goal_identified
=== ep: 2607, time 66.07162117958069, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2607
goal_identified
goal_identified
=== ep: 2608, time 72.28618693351746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2608
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2609, time 64.12784337997437, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2609
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2610, time 58.87706708908081, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2610
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2611, time 60.509624004364014, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2611
goal_identified
goal_identified
goal_identified
=== ep: 2612, time 52.66835880279541, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2612
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2613, time 59.428425312042236, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2613
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2614, time 47.8160343170166, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2614
goal_identified
goal_identified
goal_identified
=== ep: 2615, time 56.50198841094971, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2615
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2616, time 62.643919229507446, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2616
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2617, time 69.99447083473206, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2617
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2618, time 56.43029546737671, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2618
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2619, time 63.06371021270752, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2619
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2620, time 53.3426034450531, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2620
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2621, time 66.19498825073242, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2621
=== ep: 2622, time 50.33331251144409, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2622
goal_identified
goal_identified
goal_identified
=== ep: 2623, time 55.56108999252319, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2623
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2624, time 56.860454082489014, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2624
goal_identified
goal_identified
goal_identified
=== ep: 2625, time 61.363606691360474, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2625
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2626, time 54.74313974380493, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2626
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2627, time 56.684776306152344, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2627
goal_identified
goal_identified
goal_identified
=== ep: 2628, time 64.2502236366272, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2628
goal_identified
goal_identified
=== ep: 2629, time 63.39477729797363, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2629
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2630, time 58.82310438156128, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2630
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2631, time 60.473453998565674, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2631
goal_identified
=== ep: 2632, time 61.319533586502075, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2632
goal_identified
goal_identified
=== ep: 2633, time 70.2604591846466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2633
goal_identified
goal_identified
goal_identified
=== ep: 2634, time 63.06764030456543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2634
goal_identified
goal_identified
=== ep: 2635, time 67.20352554321289, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2635
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2636, time 53.78486633300781, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2636
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2637, time 64.69810032844543, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2638, time 58.853163957595825, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2638
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2639, time 62.48089957237244, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2640, time 51.56933879852295, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2640
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2641, time 59.0297372341156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2641
goal_identified
goal_identified
goal_identified
=== ep: 2642, time 62.45416021347046, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2642
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2643, time 58.20264148712158, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2643
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2644, time 60.68350958824158, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2645, time 57.179128885269165, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2645
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2646, time 67.917653799057, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2646
goal_identified
=== ep: 2647, time 61.464752435684204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2647
goal_identified
goal_identified
goal_identified
=== ep: 2648, time 59.011106729507446, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2648
goal_identified
goal_identified
goal_identified
=== ep: 2649, time 57.15653872489929, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2649
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2650, time 59.92627692222595, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2650
goal_identified
goal_identified
=== ep: 2651, time 67.84629559516907, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2651
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2652, time 67.78501391410828, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2652
goal_identified
=== ep: 2653, time 59.520012855529785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2653
goal_identified
goal_identified
goal_identified
=== ep: 2654, time 60.41307592391968, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2654
goal_identified
goal_identified
goal_identified
=== ep: 2655, time 56.34202265739441, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2655
goal_identified
goal_identified
=== ep: 2656, time 65.45828056335449, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2656
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2657, time 46.338399171829224, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2657
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2658, time 58.832348108291626, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2658
=== ep: 2659, time 61.234548568725586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2659
goal_identified
goal_identified
goal_identified
=== ep: 2660, time 66.81678342819214, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2660
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2661, time 48.0291485786438, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2661
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2662, time 63.03758692741394, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2662
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2663, time 52.093764305114746, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2664, time 62.233404874801636, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2665, time 44.22592544555664, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2665
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2666, time 67.41516733169556, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2666
goal_identified
goal_identified
goal_identified
=== ep: 2667, time 52.882481813430786, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2667
goal_identified
goal_identified
goal_identified
=== ep: 2668, time 61.93071269989014, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2668
goal_identified
=== ep: 2669, time 51.31782007217407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2669
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2670, time 66.68796586990356, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2670
goal_identified
goal_identified
=== ep: 2671, time 53.137367963790894, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2671
goal_identified
goal_identified
goal_identified
=== ep: 2672, time 55.14838933944702, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2672
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2673, time 55.8266236782074, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2673
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2674, time 66.64720392227173, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2674
goal_identified
goal_identified
=== ep: 2675, time 54.45281195640564, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2675
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2676, time 58.58849239349365, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2676
goal_identified
goal_identified
goal_identified
=== ep: 2677, time 62.248311042785645, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2677
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2678, time 60.308860063552856, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2678
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2679, time 58.260311126708984, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2679
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2680, time 55.57798933982849, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2680
goal_identified
goal_identified
=== ep: 2681, time 62.5162889957428, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2681
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2682, time 65.15732336044312, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2682
goal_identified
=== ep: 2683, time 52.400023221969604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2683
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2684, time 59.131958961486816, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2684
goal_identified
goal_identified
=== ep: 2685, time 58.8352324962616, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2685
goal_identified
=== ep: 2686, time 67.74477314949036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2686
=== ep: 2687, time 51.35646033287048, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2687
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2688, time 64.36062431335449, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2688
goal_identified
goal_identified
=== ep: 2689, time 55.86015343666077, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2689
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2690, time 68.40308809280396, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2690
goal_identified
goal_identified
=== ep: 2691, time 53.88468623161316, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2691
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2692, time 58.90092706680298, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2496
goal_identified
goal_identified
goal_identified
=== ep: 2693, time 55.40086579322815, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2694, time 61.199291467666626, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2694
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2695, time 53.450867652893066, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2695
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2696, time 55.554502964019775, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2696
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2697, time 64.77520942687988, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2697
goal_identified
goal_identified
goal_identified
=== ep: 2698, time 68.12190628051758, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2698
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2699, time 58.75966477394104, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2699
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2700, time 60.16939330101013, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2700
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2701, time 55.43146324157715, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2702, time 67.96045684814453, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2702
goal_identified
goal_identified
=== ep: 2703, time 68.81355118751526, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2703
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2704, time 60.49497389793396, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2704
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2705, time 64.01630759239197, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2705
goal_identified
goal_identified
goal_identified
=== ep: 2706, time 46.95710062980652, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2706
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2707, time 61.81033182144165, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2707
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2708, time 47.87915277481079, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2709, time 60.94621157646179, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2709
goal_identified
goal_identified
=== ep: 2710, time 56.83583879470825, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2710
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2711, time 65.34721875190735, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2711
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2712, time 63.718631744384766, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2712
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2713, time 66.21047735214233, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2596
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2714, time 60.717772483825684, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2714
goal_identified
goal_identified
=== ep: 2715, time 62.28662991523743, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2715
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2716, time 51.15734076499939, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2716
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2717, time 64.34635972976685, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2717
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2718, time 46.02050518989563, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2718
=== ep: 2719, time 60.31596803665161, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2720, time 57.45900273323059, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
goal_identified
== current size of memory is eps 21 > 20.0 and we are deleting ep 2720
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2721, time 69.65230417251587, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2722, time 54.373109102249146, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2723, time 64.19321775436401, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2723
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2724, time 60.9080913066864, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2724
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2725, time 64.93011856079102, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2725
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2726, time 66.17132568359375, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2726
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2727, time 66.97741508483887, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2727
goal_identified
goal_identified
goal_identified
=== ep: 2728, time 62.6015260219574, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2728
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2729, time 64.65741443634033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2729
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2730, time 68.43306970596313, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2692
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2731, time 59.60688018798828, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2731
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2732, time 68.98511981964111, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2732
goal_identified
goal_identified
goal_identified
=== ep: 2733, time 61.049912452697754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2733
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2734, time 65.1385886669159, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2734
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2735, time 58.40370225906372, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2735
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2736, time 49.81256651878357, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2736
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2737, time 58.0322048664093, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2737
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2738, time 54.46404767036438, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2738
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2739, time 59.693279504776, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2739
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2740, time 60.26363182067871, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2740
goal_identified
goal_identified
goal_identified
=== ep: 2741, time 64.13997006416321, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2741
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2742, time 65.67802166938782, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2742
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2743, time 62.87186288833618, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2743
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2744, time 61.60863494873047, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2744
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2745, time 61.4635546207428, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2745
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2746, time 58.74955201148987, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2746
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2747, time 67.43315553665161, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2747
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2748, time 40.858128786087036, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2748
goal_identified
goal_identified
=== ep: 2749, time 61.31564807891846, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2749
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2750, time 56.49572825431824, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2713
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2751, time 65.08719301223755, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2751
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2752, time 53.17069411277771, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2753, time 68.09085655212402, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2753
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2754, time 60.723288774490356, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2755, time 64.95639181137085, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2755
goal_identified
=== ep: 2756, time 50.89951777458191, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2756
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2757, time 50.95918560028076, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2757
goal_identified
goal_identified
=== ep: 2758, time 58.750977516174316, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2758
goal_identified
goal_identified
goal_identified
=== ep: 2759, time 58.391791343688965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2759
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2760, time 59.894585847854614, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2760
goal_identified
goal_identified
=== ep: 2761, time 66.23017597198486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2761
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2762, time 62.727867603302, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2762
goal_identified
goal_identified
goal_identified
=== ep: 2763, time 72.53686571121216, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2763
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2764, time 64.4620714187622, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2764
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2765, time 64.65721821784973, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2765
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2766, time 65.48021626472473, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2766
goal_identified
goal_identified
goal_identified
=== ep: 2767, time 59.970269203186035, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2767
goal_identified
goal_identified
goal_identified
=== ep: 2768, time 62.22923421859741, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2768
goal_identified
goal_identified
goal_identified
=== ep: 2769, time 65.57387089729309, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2769
goal_identified
goal_identified
goal_identified
=== ep: 2770, time 59.22221231460571, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2770
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2771, time 54.20839166641235, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2771
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2772, time 55.74071741104126, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2772
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2773, time 61.54983615875244, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2773
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2774, time 52.54645848274231, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2774
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2775, time 58.84899353981018, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2776, time 58.7355751991272, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2776
goal_identified
goal_identified
=== ep: 2777, time 59.52308201789856, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2777
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2778, time 54.69353747367859, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2778
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2779, time 56.06291365623474, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2779
goal_identified
goal_identified
goal_identified
=== ep: 2780, time 59.38168811798096, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2780
goal_identified
goal_identified
goal_identified
=== ep: 2781, time 48.8848876953125, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2781
goal_identified
goal_identified
goal_identified
=== ep: 2782, time 59.087721824645996, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2782
goal_identified
goal_identified
=== ep: 2783, time 60.090343952178955, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2783
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2784, time 58.949891805648804, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2784
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2785, time 56.97929096221924, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2785
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2786, time 59.59168100357056, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2786
goal_identified
goal_identified
=== ep: 2787, time 60.49637985229492, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2788, time 46.0235481262207, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2788
goal_identified
goal_identified
=== ep: 2789, time 61.636752128601074, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2789
goal_identified
goal_identified
goal_identified
=== ep: 2790, time 57.203484773635864, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2790
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2791, time 64.50808000564575, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2792, time 55.923744916915894, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2792
goal_identified
goal_identified
=== ep: 2793, time 66.500723361969, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2793
goal_identified
goal_identified
=== ep: 2794, time 66.18920874595642, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2795, time 64.85300326347351, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2795
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2796, time 66.57500410079956, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2796
goal_identified
=== ep: 2797, time 57.70433568954468, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2797
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2798, time 62.143677949905396, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2798
goal_identified
goal_identified
goal_identified
=== ep: 2799, time 63.86885213851929, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2799
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2800, time 62.26193046569824, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2800
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2801, time 55.6323504447937, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2801
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2802, time 58.10782718658447, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2802
goal_identified
goal_identified
goal_identified
=== ep: 2803, time 60.86778259277344, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2803
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2804, time 68.14535069465637, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2804
goal_identified
=== ep: 2805, time 47.68526601791382, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2805
goal_identified
goal_identified
goal_identified
=== ep: 2806, time 63.88457798957825, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2806
goal_identified
goal_identified
goal_identified
=== ep: 2807, time 56.707093715667725, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2807
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2808, time 64.68347549438477, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2808
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2809, time 47.250492095947266, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2809
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2810, time 65.23597311973572, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2810
goal_identified
goal_identified
=== ep: 2811, time 52.75747036933899, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2811
goal_identified
=== ep: 2812, time 62.6302375793457, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2812
goal_identified
=== ep: 2813, time 50.19251871109009, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2813
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2814, time 68.84578156471252, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2814
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2815, time 63.49662184715271, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2815
goal_identified
goal_identified
=== ep: 2816, time 63.407126665115356, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2816
goal_identified
goal_identified
goal_identified
=== ep: 2817, time 61.60513257980347, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2817
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2818, time 56.09862279891968, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2730
goal_identified
goal_identified
=== ep: 2819, time 63.69951343536377, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2819
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2820, time 56.09922456741333, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2820
goal_identified
goal_identified
=== ep: 2821, time 49.42664384841919, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2821
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2822, time 57.034104347229004, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2822
goal_identified
goal_identified
goal_identified
=== ep: 2823, time 57.72141361236572, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2823
goal_identified
goal_identified
=== ep: 2824, time 58.79123306274414, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2824
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2825, time 53.74515128135681, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2825
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2826, time 66.65560555458069, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2826
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2827, time 61.455766916275024, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2827
goal_identified
goal_identified
goal_identified
=== ep: 2828, time 55.52084708213806, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2828
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2829, time 56.04195976257324, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2750
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2830, time 62.64425730705261, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2830
goal_identified
goal_identified
=== ep: 2831, time 62.04666781425476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2831
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2832, time 47.81219744682312, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2833, time 62.75777578353882, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2829
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2834, time 56.09384512901306, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2834
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2835, time 58.64637231826782, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2835
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2836, time 49.24381971359253, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2836
goal_identified
goal_identified
=== ep: 2837, time 61.743178606033325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2838, time 49.55264472961426, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2838
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2839, time 58.246941566467285, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2839
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2840, time 61.96324014663696, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2840
goal_identified
goal_identified
=== ep: 2841, time 56.298752784729004, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2841
goal_identified
goal_identified
=== ep: 2842, time 51.85372972488403, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2843, time 54.205278635025024, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2843
goal_identified
goal_identified
=== ep: 2844, time 63.17419505119324, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2844
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2845, time 52.85376048088074, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2845
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2846, time 63.65952229499817, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2847, time 58.52040505409241, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2848, time 66.48858070373535, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2848
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2849, time 41.1639518737793, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2849
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2850, time 60.84510612487793, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2850
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2851, time 53.18514442443848, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2851
goal_identified
goal_identified
goal_identified
=== ep: 2852, time 62.43840312957764, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2852
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2853, time 51.25923728942871, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2853
goal_identified
goal_identified
goal_identified
=== ep: 2854, time 64.18631148338318, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2854
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2855, time 60.51144790649414, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2855
goal_identified
goal_identified
=== ep: 2856, time 51.61375546455383, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2856
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2857, time 53.60682129859924, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2857
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2858, time 59.45830154418945, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2859, time 54.70051050186157, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2859
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2860, time 56.408875703811646, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 60.03101110458374, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2861
goal_identified
goal_identified
goal_identified
=== ep: 2862, time 62.559550523757935, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2862
goal_identified
goal_identified
goal_identified
=== ep: 2863, time 57.93306541442871, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2863
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2864, time 57.77856516838074, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2864
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2865, time 50.975067138671875, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2865
goal_identified
goal_identified
=== ep: 2866, time 63.32773756980896, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2866
goal_identified
goal_identified
goal_identified
=== ep: 2867, time 45.9672532081604, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2867
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2868, time 60.07130265235901, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2868
goal_identified
goal_identified
goal_identified
=== ep: 2869, time 58.12305045127869, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2869
=== ep: 2870, time 62.84906721115112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2870
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2871, time 48.04909706115723, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2871
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2872, time 62.03942608833313, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2872
goal_identified
goal_identified
goal_identified
=== ep: 2873, time 51.461222887039185, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2873
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2874, time 54.20637059211731, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2874
goal_identified
goal_identified
goal_identified
=== ep: 2875, time 57.906954288482666, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2876, time 67.75332140922546, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2876
goal_identified
goal_identified
goal_identified
=== ep: 2877, time 65.53376460075378, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2878, time 58.20219087600708, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2833
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2879, time 60.65348434448242, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2879
goal_identified
goal_identified
goal_identified
=== ep: 2880, time 65.12416434288025, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2881, time 64.81104588508606, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2881
goal_identified
goal_identified
goal_identified
=== ep: 2882, time 72.15553689002991, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2882
goal_identified
goal_identified
goal_identified
=== ep: 2883, time 60.42204141616821, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2883
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2884, time 63.96531558036804, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2885, time 60.27003717422485, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2885
goal_identified
goal_identified
=== ep: 2886, time 61.125880002975464, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2886
goal_identified
goal_identified
goal_identified
=== ep: 2887, time 59.650575399398804, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2887
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2888, time 47.94338822364807, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2888
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2889, time 53.96774959564209, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2889
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2890, time 62.5529088973999, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2890
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2891, time 63.88379716873169, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2891
goal_identified
=== ep: 2892, time 63.26584720611572, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2892
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2893, time 52.69065499305725, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2893
goal_identified
goal_identified
goal_identified
=== ep: 2894, time 63.97056317329407, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2894
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2895, time 54.759297370910645, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2896, time 52.82944202423096, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2897, time 51.56462216377258, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2897
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2898, time 68.41819477081299, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2899, time 56.622962951660156, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2899
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2900, time 63.3798987865448, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2900
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2901, time 58.938034534454346, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2901
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2902, time 68.84291315078735, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2902
goal_identified
goal_identified
goal_identified
=== ep: 2903, time 69.17102718353271, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2903
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2904, time 64.33540487289429, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2904
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2905, time 64.7481780052185, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2905
goal_identified
goal_identified
goal_identified
=== ep: 2906, time 61.326685428619385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2906
goal_identified
goal_identified
goal_identified
=== ep: 2907, time 60.18628120422363, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2907
goal_identified
goal_identified
goal_identified
=== ep: 2908, time 65.93165183067322, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2908
goal_identified
goal_identified
=== ep: 2909, time 64.92957210540771, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2909
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2910, time 54.655054569244385, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2910
goal_identified
goal_identified
goal_identified
=== ep: 2911, time 62.002352476119995, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2911
goal_identified
goal_identified
=== ep: 2912, time 49.09671664237976, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2912
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2913, time 58.75494742393494, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2884
goal_identified
goal_identified
=== ep: 2914, time 53.02150249481201, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2914
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2915, time 68.8932056427002, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2915
goal_identified
goal_identified
goal_identified
=== ep: 2916, time 61.12064552307129, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 67.87184953689575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2917
goal_identified
goal_identified
goal_identified
=== ep: 2918, time 62.41347002983093, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2918
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2919, time 59.965736389160156, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2920, time 63.48285937309265, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2920
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2921, time 69.18597149848938, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2921
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2922, time 60.30182886123657, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2922
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2923, time 58.64220714569092, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2923
goal_identified
goal_identified
goal_identified
=== ep: 2924, time 48.498164892196655, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2924
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2925, time 64.33963966369629, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2925
goal_identified
goal_identified
goal_identified
=== ep: 2926, time 52.41691255569458, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2926
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2927, time 61.25638794898987, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2927
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2928, time 58.63860321044922, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2928
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2929, time 64.83118844032288, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2929
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2930, time 62.401548862457275, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2930
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2931, time 58.09373331069946, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2931
goal_identified
goal_identified
goal_identified
=== ep: 2932, time 62.45732831954956, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2932
=== ep: 2933, time 62.62615466117859, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2933
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2934, time 52.680333614349365, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2934
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2935, time 52.70700454711914, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2935
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2936, time 55.41964602470398, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2936
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2937, time 64.49644041061401, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2937
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2938, time 57.92443227767944, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2913
goal_identified
goal_identified
goal_identified
=== ep: 2939, time 64.69221591949463, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2939
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2940, time 64.25979161262512, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2940
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2941, time 66.96109056472778, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2941
goal_identified
goal_identified
=== ep: 2942, time 68.11699485778809, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2942
goal_identified
goal_identified
=== ep: 2943, time 57.61279606819153, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2943
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2944, time 60.819501638412476, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2944
goal_identified
goal_identified
goal_identified
=== ep: 2945, time 57.296067237854004, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2945
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2946, time 57.75735831260681, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2946
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2947, time 51.83230948448181, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2947
goal_identified
goal_identified
=== ep: 2948, time 52.74455142021179, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2948
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2949, time 63.43356418609619, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2949
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2950, time 57.9262535572052, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2950
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2951, time 53.89192843437195, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2951
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2952, time 61.496851444244385, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2952
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2953, time 55.214547872543335, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2953
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2954, time 51.84799146652222, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2954
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2955, time 55.41263270378113, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2955
goal_identified
=== ep: 2956, time 65.22238874435425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2957, time 56.54013752937317, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2957
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2958, time 63.922159910202026, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2958
goal_identified
goal_identified
goal_identified
=== ep: 2959, time 58.68804121017456, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2959
goal_identified
goal_identified
goal_identified
=== ep: 2960, time 68.75763297080994, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2960
goal_identified
goal_identified
goal_identified
=== ep: 2961, time 66.49897503852844, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2961
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2962, time 60.04611563682556, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2962
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2963, time 48.666884660720825, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2963
goal_identified
goal_identified
=== ep: 2964, time 55.92557406425476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2964
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2965, time 58.55654191970825, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2965
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2966, time 62.02190375328064, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2966
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2967, time 51.36563229560852, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2967
goal_identified
goal_identified
goal_identified
=== ep: 2968, time 70.12158298492432, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2968
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2969, time 54.47142553329468, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2969
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2970, time 59.626399993896484, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2970
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2971, time 41.39293575286865, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2971
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2972, time 67.56351351737976, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2972
goal_identified
=== ep: 2973, time 53.850996255874634, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2974, time 62.282780170440674, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2974
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2975, time 51.41141366958618, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2976, time 60.86615061759949, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2976
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2977, time 48.30897903442383, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2938
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2978, time 56.995625734329224, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2978
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2979, time 61.95972180366516, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2977
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2980, time 66.3423638343811, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2980
goal_identified
=== ep: 2981, time 62.56613063812256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2981
goal_identified
goal_identified
goal_identified
=== ep: 2982, time 60.48402190208435, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2982
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2983, time 50.26377844810486, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2983
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2984, time 61.34683322906494, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2984
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2985, time 47.99669528007507, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2985
goal_identified
=== ep: 2986, time 58.02480506896973, eps 0.001, sum reward: 1, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2987, time 59.530643701553345, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2987
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2988, time 68.67162227630615, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2988
goal_identified
goal_identified
=== ep: 2989, time 54.31019330024719, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2989
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2990, time 63.053784132003784, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2990
goal_identified
goal_identified
goal_identified
=== ep: 2991, time 55.22888231277466, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2991
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2992, time 68.25654458999634, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2992
goal_identified
goal_identified
goal_identified
=== ep: 2993, time 60.284860610961914, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2993
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2994, time 57.869317293167114, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2994
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2995, time 54.36919021606445, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2995
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2996, time 61.82296872138977, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2996
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2997, time 57.89991545677185, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2997
goal_identified
goal_identified
goal_identified
=== ep: 2998, time 50.06349015235901, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2998
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2999, time 58.12745213508606, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2999
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3000, time 59.37363123893738, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3000
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3001, time 52.07241678237915, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3001
goal_identified
goal_identified
goal_identified
=== ep: 3002, time 60.70653748512268, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3002
goal_identified
goal_identified
=== ep: 3003, time 60.75575828552246, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3003
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3004, time 63.25732398033142, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3004
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3005, time 48.086477756500244, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3005
goal_identified
goal_identified
=== ep: 3006, time 61.684266328811646, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3006
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3007, time 57.62022280693054, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3007
goal_identified
goal_identified
goal_identified
=== ep: 3008, time 61.69457459449768, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3008
goal_identified
goal_identified
goal_identified
=== ep: 3009, time 55.54573631286621, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3009
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3010, time 67.36605072021484, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3010
goal_identified
goal_identified
goal_identified
=== ep: 3011, time 63.66598987579346, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3011
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3012, time 68.00871920585632, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3012
goal_identified
goal_identified
=== ep: 3013, time 63.34876990318298, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3013
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3014, time 56.24639582633972, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3014
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3015, time 61.552714347839355, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3015
goal_identified
goal_identified
goal_identified
=== ep: 3016, time 63.81574869155884, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3016
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3017, time 54.80978035926819, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3017
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3018, time 55.031177282333374, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3018
goal_identified
goal_identified
=== ep: 3019, time 49.17308568954468, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3019
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3020, time 67.44098091125488, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3020
goal_identified
goal_identified
goal_identified
=== ep: 3021, time 51.8729510307312, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3021
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3022, time 56.23087668418884, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3022
goal_identified
goal_identified
goal_identified
=== ep: 3023, time 54.55603861808777, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3023
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3024, time 64.51157975196838, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3024
goal_identified
=== ep: 3025, time 53.67792367935181, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3025
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3026, time 57.73544144630432, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3026
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3027, time 57.26384234428406, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3027
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3028, time 65.78038263320923, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3028
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3029, time 62.70867323875427, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3029
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3030, time 57.68260669708252, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3030
goal_identified
goal_identified
=== ep: 3031, time 57.90941047668457, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3031
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3032, time 63.883769035339355, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3032
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3033, time 62.050089836120605, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3033
goal_identified
goal_identified
goal_identified
=== ep: 3034, time 68.48446488380432, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3034
goal_identified
goal_identified
goal_identified
=== ep: 3035, time 45.32952666282654, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3035
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3036, time 61.31678247451782, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3036
goal_identified
goal_identified
goal_identified
=== ep: 3037, time 51.96007227897644, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3037
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3038, time 66.06087136268616, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3038
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3039, time 48.362064361572266, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3039
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3040, time 62.3399932384491, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 59.66810941696167, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3041
goal_identified
goal_identified
goal_identified
=== ep: 3042, time 63.454124450683594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3042
goal_identified
goal_identified
goal_identified
=== ep: 3043, time 47.06515908241272, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3043
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3044, time 55.49933218955994, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3044
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3045, time 62.38860535621643, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3045
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3046, time 63.779114723205566, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3046
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3047, time 60.75975823402405, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3047
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3048, time 62.37070441246033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3048
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3049, time 56.908623456954956, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3049
goal_identified
goal_identified
goal_identified
=== ep: 3050, time 66.61857748031616, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3050
goal_identified
=== ep: 3051, time 57.83177876472473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3051
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3052, time 67.19165396690369, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3052
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3053, time 54.49391508102417, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3053
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3054, time 59.60625767707825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3054
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3055, time 43.76575469970703, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3055
goal_identified
goal_identified
goal_identified
=== ep: 3056, time 63.54984736442566, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3056
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3057, time 54.125970125198364, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3057
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3058, time 63.869943141937256, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3058
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3059, time 56.06473755836487, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3059
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3060, time 60.33895993232727, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3060
goal_identified
goal_identified
goal_identified
=== ep: 3061, time 59.780965089797974, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3061
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3062, time 52.75881910324097, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3062
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3063, time 52.265466928482056, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3063
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3064, time 59.18350434303284, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3064
=== ep: 3065, time 61.91199469566345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3065
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3066, time 66.56304502487183, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3066
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3067, time 53.604214668273926, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3067
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3068, time 62.74958777427673, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3068
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3069, time 54.52265429496765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3069
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3070, time 68.0961081981659, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3070
goal_identified
goal_identified
goal_identified
=== ep: 3071, time 58.293253660202026, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3071
goal_identified
goal_identified
goal_identified
=== ep: 3072, time 57.44788432121277, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3072
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3073, time 45.828752517700195, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3073
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3074, time 69.25297260284424, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3074
goal_identified
=== ep: 3075, time 57.78087115287781, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3075
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3076, time 64.38671231269836, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3076
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3077, time 53.97062706947327, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3077
goal_identified
=== ep: 3078, time 66.2641270160675, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3078
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3079, time 60.36000347137451, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3079
goal_identified
goal_identified
goal_identified
=== ep: 3080, time 65.63614702224731, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3080
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3081, time 64.87129211425781, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3081
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3082, time 56.47050881385803, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3082
goal_identified
=== ep: 3083, time 53.75115966796875, eps 0.001, sum reward: 1, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3083
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3084, time 57.047070026397705, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3084
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3085, time 60.17076849937439, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3085
goal_identified
goal_identified
=== ep: 3086, time 57.27777051925659, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3086
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3087, time 54.036566495895386, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3087
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3088, time 63.485782623291016, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3088
goal_identified
goal_identified
goal_identified
=== ep: 3089, time 62.47183918952942, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3089
goal_identified
goal_identified
=== ep: 3090, time 67.1802704334259, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3090
goal_identified
goal_identified
=== ep: 3091, time 54.15633583068848, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3091
goal_identified
goal_identified
goal_identified
=== ep: 3092, time 62.29891228675842, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3092
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3093, time 54.72387886047363, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3093
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3094, time 67.58280158042908, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3094
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3095, time 61.20029973983765, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3095
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3096, time 62.13601779937744, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3096
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3097, time 46.67145919799805, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3097
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3098, time 63.821857929229736, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3098
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3099, time 53.68545317649841, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3099
goal_identified
goal_identified
goal_identified
=== ep: 3100, time 62.210026025772095, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3100
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3101, time 54.13118243217468, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3101
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3102, time 62.8527307510376, eps 0.001, sum reward: 7, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3102
goal_identified
goal_identified
=== ep: 3103, time 62.008686780929565, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3103
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3104, time 65.1534435749054, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3104
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3105, time 66.68415880203247, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3105
goal_identified
goal_identified
goal_identified
=== ep: 3106, time 65.16334247589111, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3106
goal_identified
goal_identified
goal_identified
=== ep: 3107, time 52.08524131774902, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3107
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3108, time 57.42401933670044, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3108
goal_identified
goal_identified
goal_identified
=== ep: 3109, time 54.62845587730408, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3109
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3110, time 61.94458818435669, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3110
goal_identified
=== ep: 3111, time 47.30261492729187, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3111
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3112, time 67.71920895576477, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3112
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3113, time 57.1208975315094, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3113
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3114, time 66.84402346611023, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3114
goal_identified
goal_identified
goal_identified
=== ep: 3115, time 53.39270901679993, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3116, time 57.849873304367065, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3116
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3117, time 49.36174392700195, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3117
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3118, time 64.4586033821106, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3118
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3119, time 47.218889236450195, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3119
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3120, time 60.2665913105011, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3120
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3121, time 60.96334791183472, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3121
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3122, time 65.75149250030518, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3122
goal_identified
goal_identified
goal_identified
=== ep: 3123, time 61.42226719856262, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3123
goal_identified
goal_identified
=== ep: 3124, time 55.678287982940674, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3124
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3125, time 55.51077198982239, eps 0.001, sum reward: 5, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3125
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3126, time 60.16145348548889, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3126
goal_identified
goal_identified
goal_identified
=== ep: 3127, time 57.19727683067322, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3127
goal_identified
goal_identified
goal_identified
=== ep: 3128, time 55.15236258506775, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3128
goal_identified
goal_identified
=== ep: 3129, time 51.26405572891235, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3129
goal_identified
goal_identified
goal_identified
=== ep: 3130, time 64.3208703994751, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3130
goal_identified
goal_identified
goal_identified
=== ep: 3131, time 60.81908392906189, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3131
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3132, time 63.27605080604553, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3132
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3133, time 44.28248333930969, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3133
goal_identified
goal_identified
goal_identified
=== ep: 3134, time 63.610068559646606, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3134
goal_identified
goal_identified
goal_identified
=== ep: 3135, time 55.20107173919678, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3135
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3136, time 59.669264793395996, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3136
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3137, time 51.88487219810486, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3137
goal_identified
goal_identified
goal_identified
=== ep: 3138, time 67.91161179542542, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3138
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3139, time 58.81328558921814, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3139
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3140, time 67.66323113441467, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3140
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3141, time 54.70601010322571, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3141
goal_identified
goal_identified
goal_identified
=== ep: 3142, time 63.012160778045654, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3142
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3143, time 57.23434567451477, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3143
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3144, time 65.92597532272339, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3144
goal_identified
goal_identified
goal_identified
=== ep: 3145, time 65.40957570075989, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3145
goal_identified
goal_identified
goal_identified
=== ep: 3146, time 53.12766790390015, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3146
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3147, time 48.50892519950867, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3147
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3148, time 61.088149309158325, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3148
goal_identified
goal_identified
goal_identified
=== ep: 3149, time 62.579283714294434, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3149
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3150, time 62.78225660324097, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3150
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3151, time 58.95020914077759, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3151
goal_identified
goal_identified
goal_identified
=== ep: 3152, time 61.87604236602783, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3152
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3153, time 58.23292851448059, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3153
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3154, time 64.13550400733948, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3154
goal_identified
=== ep: 3155, time 62.53092885017395, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3155
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3156, time 64.25976824760437, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3156
goal_identified
goal_identified
=== ep: 3157, time 66.59216809272766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3157
goal_identified
goal_identified
goal_identified
=== ep: 3158, time 50.002765417099, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3158
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3159, time 52.12544584274292, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3160, time 54.444575548172, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3160
goal_identified
goal_identified
=== ep: 3161, time 74.47545909881592, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3161
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3162, time 55.95489525794983, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3162
goal_identified
goal_identified
goal_identified
=== ep: 3163, time 62.24961709976196, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3163
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3164, time 49.674768924713135, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3164
goal_identified
goal_identified
=== ep: 3165, time 66.22509002685547, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3165
goal_identified
=== ep: 3166, time 55.72120904922485, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 59.4504554271698, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3167
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3168, time 50.32919192314148, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3168
goal_identified
goal_identified
=== ep: 3169, time 67.9533760547638, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3169
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3170, time 64.20460271835327, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3170
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3171, time 61.74496579170227, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3171
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3172, time 57.21480178833008, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3172
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3173, time 56.43674993515015, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3173
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3174, time 65.51904034614563, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3174
goal_identified
goal_identified
goal_identified
=== ep: 3175, time 60.96276116371155, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3175
goal_identified
goal_identified
=== ep: 3176, time 55.491580963134766, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3176
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3177, time 58.36968278884888, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3178, time 59.260369539260864, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3178
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3179, time 61.98144555091858, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3179
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3180, time 49.88708448410034, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3181, time 59.09883642196655, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3181
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3182, time 59.28649377822876, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3182
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3183, time 60.161155700683594, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3183
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3184, time 50.28407692909241, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3184
goal_identified
goal_identified
goal_identified
=== ep: 3185, time 62.41252326965332, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3185
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3186, time 64.36638164520264, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 132/132)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3186
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3187, time 58.30024337768555, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3187
goal_identified
goal_identified
=== ep: 3188, time 59.908039569854736, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3188
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3189, time 58.29676127433777, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3189
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3190, time 68.81630182266235, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3190
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3191, time 57.64976119995117, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3191
goal_identified
goal_identified
goal_identified
=== ep: 3192, time 63.405577659606934, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3192
goal_identified
goal_identified
goal_identified
=== ep: 3193, time 47.74607872962952, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3193
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3194, time 67.11465454101562, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3194
goal_identified
goal_identified
goal_identified
=== ep: 3195, time 50.92977571487427, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3195
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3196, time 59.84453821182251, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3196
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3197, time 54.81597924232483, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3197
=== ep: 3198, time 67.42953372001648, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3198
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3199, time 52.35792064666748, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3199
goal_identified
goal_identified
goal_identified
=== ep: 3200, time 54.30211019515991, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3200
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3201, time 55.72069215774536, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3201
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3202, time 61.3895308971405, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3202
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3203, time 57.84534955024719, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3203
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3204, time 56.82479429244995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3204
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3205, time 64.91735887527466, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3205
goal_identified
goal_identified
goal_identified
=== ep: 3206, time 67.93298554420471, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3206
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3207, time 59.24570918083191, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3207
goal_identified
goal_identified
=== ep: 3208, time 62.00243949890137, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3208
goal_identified
goal_identified
goal_identified
=== ep: 3209, time 45.433252573013306, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3209
goal_identified
goal_identified
goal_identified
=== ep: 3210, time 66.71224474906921, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3210
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3211, time 55.43419337272644, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3211
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3212, time 60.888428926467896, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3212
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3213, time 51.47270846366882, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3213
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3214, time 70.41938996315002, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3214
goal_identified
=== ep: 3215, time 61.960278034210205, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3215
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3216, time 66.58725881576538, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3216
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3217, time 55.17039179801941, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3217
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3218, time 60.94636535644531, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3218
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3219, time 58.77114391326904, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3219
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3220, time 59.61963152885437, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3221, time 54.53433585166931, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3221
goal_identified
=== ep: 3222, time 55.74717831611633, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3222
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3223, time 66.99839901924133, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3223
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3224, time 63.772518157958984, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3224
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3225, time 59.753942012786865, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3225
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3226, time 55.518163204193115, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3226
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3227, time 57.51589798927307, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3227
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3228, time 64.17284798622131, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3228
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3229, time 52.18949246406555, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3229
goal_identified
goal_identified
=== ep: 3230, time 60.18769145011902, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3230
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3231, time 58.20230484008789, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3231
goal_identified
goal_identified
goal_identified
=== ep: 3232, time 65.83179783821106, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3232
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3233, time 59.860748291015625, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3233
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3234, time 55.34792757034302, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3235, time 56.901145935058594, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3235
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3236, time 61.741337299346924, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3236
goal_identified
goal_identified
goal_identified
=== ep: 3237, time 59.07998561859131, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3237
goal_identified
goal_identified
goal_identified
=== ep: 3238, time 54.82129716873169, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3238
goal_identified
goal_identified
=== ep: 3239, time 62.66920781135559, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3239
goal_identified
goal_identified
goal_identified
=== ep: 3240, time 58.96116065979004, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3240
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3241, time 58.05735230445862, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3242, time 51.299888610839844, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3242
goal_identified
goal_identified
goal_identified
=== ep: 3243, time 63.369725465774536, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3243
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3244, time 63.30168843269348, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
goal_identified
=== ep: 3245, time 63.537203550338745, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3245
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3246, time 60.191651344299316, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3246
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3247, time 58.49393916130066, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3247
goal_identified
goal_identified
goal_identified
=== ep: 3248, time 64.2157073020935, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3248
goal_identified
goal_identified
=== ep: 3249, time 64.47477769851685, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3249
goal_identified
goal_identified
=== ep: 3250, time 53.86647391319275, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3250
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3251, time 54.03234362602234, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3251
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3252, time 59.89301371574402, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3252
goal_identified
goal_identified
goal_identified
=== ep: 3253, time 65.17467737197876, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3253
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3254, time 53.679808616638184, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3254
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3255, time 63.57225060462952, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3255
goal_identified
goal_identified
=== ep: 3256, time 57.13214468955994, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3256
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3257, time 72.19363784790039, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3258, time 56.81015372276306, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3258
goal_identified
goal_identified
goal_identified
=== ep: 3259, time 56.37699341773987, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3259
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3260, time 47.43930459022522, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3260
goal_identified
=== ep: 3261, time 63.66482591629028, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3261
goal_identified
goal_identified
goal_identified
=== ep: 3262, time 54.8757586479187, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3262
goal_identified
goal_identified
=== ep: 3263, time 60.76311731338501, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3263
goal_identified
goal_identified
goal_identified
=== ep: 3264, time 60.89007568359375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3264
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3265, time 64.28290486335754, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3265
goal_identified
goal_identified
goal_identified
=== ep: 3266, time 62.53458857536316, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3266
goal_identified
goal_identified
=== ep: 3267, time 54.332672119140625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3267
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3268, time 53.475730895996094, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3268
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3269, time 60.080859422683716, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3269
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3270, time 52.751596212387085, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3270
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3271, time 59.6549859046936, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3271
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3272, time 59.48664999008179, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3272
goal_identified
goal_identified
=== ep: 3273, time 69.83432173728943, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3273
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3274, time 64.34386777877808, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3274
goal_identified
goal_identified
=== ep: 3275, time 64.50274276733398, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3275
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3276, time 57.21734285354614, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3276
goal_identified
goal_identified
=== ep: 3277, time 54.58716082572937, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3277
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3278, time 62.658507347106934, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3278
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3279, time 59.2427875995636, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3279
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3280, time 62.5525119304657, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2168
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3281, time 56.38568568229675, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3281
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3282, time 69.43945407867432, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3282
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3283, time 65.8510639667511, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3283
goal_identified
goal_identified
goal_identified
=== ep: 3284, time 66.87242603302002, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3284
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3285, time 62.534772872924805, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3285
goal_identified
=== ep: 3286, time 55.30121946334839, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3286
goal_identified
goal_identified
goal_identified
=== ep: 3287, time 59.732685804367065, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3287
goal_identified
goal_identified
goal_identified
=== ep: 3288, time 58.1470685005188, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3288
goal_identified
goal_identified
=== ep: 3289, time 51.37620782852173, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3289
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3290, time 58.3085081577301, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3290
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3291, time 64.21077466011047, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3291
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3292, time 69.85743308067322, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3292
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3293, time 62.44223380088806, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3293
goal_identified
goal_identified
goal_identified
=== ep: 3294, time 56.20728302001953, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3294
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3295, time 56.39094281196594, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3295
goal_identified
=== ep: 3296, time 58.50831437110901, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3296
goal_identified
goal_identified
goal_identified
=== ep: 3297, time 60.11364507675171, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3297
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3298, time 57.08631443977356, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3298
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3299, time 61.42727446556091, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3299
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3300, time 60.317110538482666, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3300
goal_identified
goal_identified
=== ep: 3301, time 71.0942234992981, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3301
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3302, time 58.50355625152588, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3302
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3303, time 68.38110017776489, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3303
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3304, time 59.59859895706177, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3304
goal_identified
goal_identified
=== ep: 3305, time 64.40912246704102, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3305
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3306, time 60.29326033592224, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3306
goal_identified
goal_identified
goal_identified
=== ep: 3307, time 63.103501081466675, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3307
goal_identified
goal_identified
goal_identified
=== ep: 3308, time 65.7508590221405, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3308
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3309, time 59.82380175590515, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3309
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3310, time 57.222718715667725, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3310
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3311, time 52.17452669143677, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3311
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3312, time 61.982547998428345, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3312
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3313, time 69.59141993522644, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3313
goal_identified
=== ep: 3314, time 56.748103618621826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3314
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3315, time 60.555670738220215, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3315
goal_identified
goal_identified
goal_identified
=== ep: 3316, time 62.1810245513916, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3316
goal_identified
goal_identified
goal_identified
=== ep: 3317, time 63.18158793449402, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 67.01084351539612, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3318
goal_identified
goal_identified
goal_identified
=== ep: 3319, time 47.49012804031372, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3319
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3320, time 55.84586524963379, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3320
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3321, time 58.18389368057251, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3321
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3322, time 67.00957727432251, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3322
goal_identified
goal_identified
goal_identified
=== ep: 3323, time 53.100746154785156, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3323
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3324, time 64.01818108558655, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3324
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3325, time 57.87518286705017, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3325
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3326, time 66.64863276481628, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3326
goal_identified
goal_identified
goal_identified
=== ep: 3327, time 50.626788854599, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3327
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3328, time 55.26053071022034, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3328
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3329, time 56.91850972175598, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3329
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3330, time 64.45463299751282, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3330
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3331, time 59.08905100822449, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3331
goal_identified
goal_identified
goal_identified
=== ep: 3332, time 57.634618282318115, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3332
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3333, time 61.88104462623596, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3333
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3334, time 60.3287627696991, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3334
goal_identified
goal_identified
goal_identified
=== ep: 3335, time 58.38161087036133, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3335
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3336, time 47.1762490272522, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3336
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3337, time 59.600441455841064, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3337
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3338, time 64.25209736824036, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3338
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3339, time 59.74852395057678, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3339
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3340, time 63.12990069389343, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3340
goal_identified
goal_identified
=== ep: 3341, time 49.765031576156616, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3341
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3342, time 65.67783856391907, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3342
goal_identified
goal_identified
goal_identified
=== ep: 3343, time 50.114105224609375, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3343
goal_identified
goal_identified
goal_identified
=== ep: 3344, time 60.277413845062256, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3344
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3345, time 52.04577708244324, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3345
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3346, time 71.5454535484314, eps 0.001, sum reward: 9, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3346
goal_identified
goal_identified
goal_identified
=== ep: 3347, time 68.336266040802, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3347
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3348, time 65.82875061035156, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3348
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3349, time 66.33010220527649, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3349
goal_identified
goal_identified
=== ep: 3350, time 57.35746192932129, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3350
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3351, time 62.48402690887451, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3351
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3352, time 59.00146675109863, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3352
goal_identified
goal_identified
=== ep: 3353, time 62.9391713142395, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3353
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3354, time 61.75215482711792, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3354
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3355, time 49.646501779556274, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3355
goal_identified
goal_identified
goal_identified
=== ep: 3356, time 58.23941445350647, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3356
goal_identified
goal_identified
=== ep: 3357, time 61.312326192855835, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3357
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3358, time 63.9585964679718, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3358
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3359, time 57.47062420845032, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3359
goal_identified
goal_identified
goal_identified
=== ep: 3360, time 55.879223346710205, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3360
goal_identified
goal_identified
goal_identified
=== ep: 3361, time 59.102171659469604, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3361
goal_identified
=== ep: 3362, time 58.79218149185181, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3362
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3363, time 51.342326164245605, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3363
goal_identified
goal_identified
goal_identified
=== ep: 3364, time 59.01582717895508, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3364
goal_identified
goal_identified
=== ep: 3365, time 65.78643941879272, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3365
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3366, time 62.27460169792175, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3366
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3367, time 71.93955779075623, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3367
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3368, time 48.99974727630615, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3368
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3369, time 64.48602724075317, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3369
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3370, time 55.12924933433533, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3370
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3371, time 67.8543746471405, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3371
goal_identified
goal_identified
goal_identified
=== ep: 3372, time 51.997586488723755, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3373, time 59.726717710494995, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3373
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3374, time 54.774090051651, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3374
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3375, time 64.43794918060303, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3375
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3376, time 55.56566548347473, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3376
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3377, time 55.72625231742859, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3377
goal_identified
goal_identified
goal_identified
=== ep: 3378, time 60.04663038253784, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3378
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3379, time 62.812761306762695, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3379
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3380, time 60.87818241119385, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3380
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3381, time 54.929922103881836, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3381
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3382, time 58.05708575248718, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3382
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3383, time 65.90531706809998, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3383
goal_identified
goal_identified
goal_identified
=== ep: 3384, time 58.15612244606018, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3384
goal_identified
goal_identified
goal_identified
=== ep: 3385, time 62.71059584617615, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3385
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3386, time 54.07090950012207, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3386
goal_identified
goal_identified
=== ep: 3387, time 64.50066566467285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3387
goal_identified
goal_identified
goal_identified
=== ep: 3388, time 63.90358591079712, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3388
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3389, time 68.15216135978699, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3389
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3390, time 71.1524305343628, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3390
goal_identified
goal_identified
goal_identified
=== ep: 3391, time 63.854697704315186, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3391
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3392, time 59.86961340904236, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3392
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3393, time 54.899131774902344, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3393
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3394, time 55.76572608947754, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3394
goal_identified
goal_identified
goal_identified
=== ep: 3395, time 58.115029096603394, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3395
goal_identified
goal_identified
goal_identified
=== ep: 3396, time 58.8384428024292, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3397, time 57.71624517440796, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3397
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3398, time 67.92877006530762, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3398
goal_identified
goal_identified
goal_identified
=== ep: 3399, time 60.97974944114685, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3399
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3400, time 69.31769943237305, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3400
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3401, time 64.12667036056519, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3401
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3402, time 63.755033016204834, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3402
goal_identified
goal_identified
goal_identified
=== ep: 3403, time 62.758907079696655, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3403
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3404, time 45.81771779060364, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3404
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3405, time 63.220332622528076, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3405
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3406, time 64.67658400535583, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3406
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3407, time 62.71315050125122, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3407
goal_identified
goal_identified
goal_identified
=== ep: 3408, time 60.70450448989868, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3408
=== ep: 3409, time 60.520089864730835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3409
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3410, time 58.03040528297424, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3410
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3411, time 64.18050217628479, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3411
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3412, time 50.0064594745636, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3412
=== ep: 3413, time 57.76923060417175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3413
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3414, time 48.511478424072266, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3414
goal_identified
goal_identified
=== ep: 3415, time 69.84556341171265, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3415
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3416, time 64.10903143882751, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3416
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3417, time 72.27517676353455, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3417
goal_identified
goal_identified
goal_identified
=== ep: 3418, time 70.74112129211426, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3418
goal_identified
goal_identified
=== ep: 3419, time 65.85989093780518, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3419
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3420, time 62.47333097457886, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3420
goal_identified
goal_identified
goal_identified
=== ep: 3421, time 56.84858560562134, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3421
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3422, time 65.29291987419128, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3422
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3423, time 58.73884081840515, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3423
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3424, time 62.88190507888794, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3424
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3425, time 46.04048538208008, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3425
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3426, time 63.0379433631897, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3426
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3427, time 61.52969431877136, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3427
goal_identified
goal_identified
=== ep: 3428, time 69.14603924751282, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3428
goal_identified
goal_identified
=== ep: 3429, time 63.25423717498779, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3429
goal_identified
goal_identified
=== ep: 3430, time 62.311596155166626, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3430
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3431, time 62.377208948135376, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3431
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3432, time 48.35697889328003, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3432
goal_identified
goal_identified
goal_identified
=== ep: 3433, time 59.008601665496826, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3433
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3434, time 52.34286952018738, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3434
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3435, time 62.427703857421875, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3435
goal_identified
goal_identified
=== ep: 3436, time 59.09516453742981, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3436
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3437, time 60.91761088371277, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3437
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3438, time 66.06701850891113, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3438
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3439, time 60.08151984214783, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3439
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3440, time 46.891974449157715, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3441, time 55.86490201950073, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3441
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3442, time 65.896249294281, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3442
goal_identified
goal_identified
=== ep: 3443, time 67.3696916103363, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3443
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3444, time 60.98275828361511, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3444
goal_identified
goal_identified
=== ep: 3445, time 70.28363633155823, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3445
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3446, time 57.82824730873108, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3446
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3447, time 59.664841651916504, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3447
=== ep: 3448, time 45.78861165046692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3448
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3449, time 62.169798851013184, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3449
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3450, time 60.9745659828186, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3450
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3451, time 61.71567440032959, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3451
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3452, time 63.844536781311035, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3452
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3453, time 64.34681010246277, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3453
=== ep: 3454, time 57.061386585235596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3454
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3455, time 62.38036632537842, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3455
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3456, time 47.3655846118927, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3456
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3457, time 61.09559392929077, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3457
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3458, time 48.98326373100281, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3458
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3459, time 66.06812024116516, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3459
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3460, time 56.945680141448975, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3460
goal_identified
goal_identified
goal_identified
=== ep: 3461, time 67.88494086265564, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3461
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3462, time 65.79025864601135, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3462
goal_identified
goal_identified
goal_identified
=== ep: 3463, time 64.08424973487854, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3463
goal_identified
goal_identified
=== ep: 3464, time 64.23578095436096, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3464
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3465, time 62.619210720062256, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3465
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3466, time 51.362396240234375, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3466
goal_identified
goal_identified
goal_identified
=== ep: 3467, time 56.89047884941101, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3467
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3468, time 53.47170448303223, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3468
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3469, time 61.47311735153198, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3469
goal_identified
goal_identified
goal_identified
=== ep: 3470, time 59.69821238517761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3470
goal_identified
goal_identified
=== ep: 3471, time 61.665581941604614, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3471
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3472, time 64.5060043334961, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3472
goal_identified
goal_identified
=== ep: 3473, time 60.81143856048584, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3473
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3474, time 70.43407011032104, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3474
goal_identified
goal_identified
goal_identified
=== ep: 3475, time 63.016716957092285, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3475
goal_identified
goal_identified
=== ep: 3476, time 68.2337007522583, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3476
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3477, time 61.19556498527527, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3477
goal_identified
goal_identified
goal_identified
=== ep: 3478, time 57.98828482627869, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3478
goal_identified
goal_identified
=== ep: 3479, time 56.57353353500366, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3479
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3480, time 69.24167251586914, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3480
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3481, time 57.82592725753784, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3481
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3482, time 56.486175298690796, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3482
goal_identified
goal_identified
goal_identified
=== ep: 3483, time 53.69873404502869, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3483
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3484, time 67.16123414039612, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3484
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3485, time 67.96876931190491, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3485
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3486, time 59.19679927825928, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3487, time 58.07264733314514, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3487
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3488, time 53.00665521621704, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3488
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3489, time 63.54554891586304, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3489
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3490, time 58.15515923500061, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3490
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3491, time 50.760573625564575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3491
goal_identified
=== ep: 3492, time 59.87706279754639, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3492
goal_identified
goal_identified
goal_identified
=== ep: 3493, time 58.227535247802734, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3493
goal_identified
=== ep: 3494, time 62.25481081008911, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3494
goal_identified
goal_identified
goal_identified
=== ep: 3495, time 55.22084188461304, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3495
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3496, time 59.393263816833496, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3496
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3497, time 66.16988515853882, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3497
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3498, time 60.08237075805664, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3498
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3499, time 66.97443771362305, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3499
goal_identified
goal_identified
goal_identified
=== ep: 3500, time 40.892754554748535, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3500
goal_identified
goal_identified
goal_identified
=== ep: 3501, time 62.193079233169556, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3501
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3502, time 63.34843587875366, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3502
goal_identified
goal_identified
goal_identified
=== ep: 3503, time 65.98865818977356, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3503
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3504, time 59.18712615966797, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3504
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3505, time 51.17565941810608, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3505
goal_identified
goal_identified
=== ep: 3506, time 55.3818998336792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3506
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3507, time 58.88911461830139, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3507
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3508, time 55.41139197349548, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3508
goal_identified
goal_identified
=== ep: 3509, time 57.87294864654541, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3509
goal_identified
goal_identified
=== ep: 3510, time 58.108132123947144, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3510
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3511, time 66.25869035720825, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3511
goal_identified
=== ep: 3512, time 50.000932693481445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3512
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3513, time 55.05230522155762, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3513
goal_identified
goal_identified
goal_identified
=== ep: 3514, time 52.881247997283936, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3514
goal_identified
goal_identified
goal_identified
=== ep: 3515, time 68.04608750343323, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3515
goal_identified
goal_identified
goal_identified
=== ep: 3516, time 62.63896036148071, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3516
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3517, time 68.9321677684784, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3517
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3518, time 66.12610626220703, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3518
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3519, time 52.00431251525879, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3519
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3520, time 51.9794237613678, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3520
goal_identified
=== ep: 3521, time 63.63398861885071, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3521
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3522, time 59.43433952331543, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3522
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3523, time 67.51247024536133, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3523
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3524, time 60.35510849952698, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3524
goal_identified
goal_identified
=== ep: 3525, time 59.07053542137146, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3525
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3526, time 58.799509048461914, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3526
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3527, time 61.30494046211243, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3527
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3528, time 63.87811803817749, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3528
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3529, time 49.57367444038391, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3529
goal_identified
=== ep: 3530, time 61.91849613189697, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3530
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3531, time 58.159515619277954, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3531
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3532, time 63.73863911628723, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3532
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3533, time 66.72569513320923, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3533
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3534, time 63.815343141555786, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3534
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3535, time 45.27141523361206, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3535
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3536, time 63.29421782493591, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3536
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3537, time 53.14663815498352, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3537
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3538, time 64.22762560844421, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3538
goal_identified
=== ep: 3539, time 52.19719433784485, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3539
goal_identified
goal_identified
goal_identified
=== ep: 3540, time 62.769280672073364, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3540
goal_identified
goal_identified
goal_identified
=== ep: 3541, time 62.19667053222656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3541
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3542, time 56.913445472717285, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3542
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3543, time 47.5385525226593, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3543
goal_identified
goal_identified
=== ep: 3544, time 59.98913502693176, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3544
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3545, time 61.09248375892639, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3545
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3546, time 73.44079899787903, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3546
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3547, time 69.08216571807861, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3547
goal_identified
goal_identified
goal_identified
=== ep: 3548, time 63.86237835884094, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3548
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3549, time 64.81073331832886, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3549
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3550, time 51.02758765220642, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3550
goal_identified
goal_identified
goal_identified
=== ep: 3551, time 56.827850103378296, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3551
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3552, time 58.99224638938904, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3552
goal_identified
=== ep: 3553, time 53.24992752075195, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3553
goal_identified
goal_identified
=== ep: 3554, time 63.6559534072876, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3554
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3555, time 59.429290771484375, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3555
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3556, time 62.669758319854736, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3556
goal_identified
goal_identified
goal_identified
=== ep: 3557, time 64.41236209869385, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3557
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3558, time 62.629374265670776, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2183
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3559, time 69.7013087272644, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3559
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3560, time 65.98609781265259, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3560
goal_identified
goal_identified
goal_identified
=== ep: 3561, time 60.10764193534851, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3561
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3562, time 56.015849351882935, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3562
goal_identified
=== ep: 3563, time 60.87270140647888, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3563
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3564, time 58.29887390136719, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3564
goal_identified
goal_identified
goal_identified
=== ep: 3565, time 65.96252584457397, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3565
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3566, time 48.663286447525024, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3566
=== ep: 3567, time 62.02109408378601, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3567
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3568, time 60.29858946800232, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3568
goal_identified
goal_identified
goal_identified
=== ep: 3569, time 63.83446192741394, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3569
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3570, time 53.006756067276, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3570
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3571, time 52.30456757545471, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3571
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3572, time 63.905999422073364, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3572
goal_identified
goal_identified
=== ep: 3573, time 61.237656593322754, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3573
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3574, time 63.10574007034302, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3574
goal_identified
goal_identified
goal_identified
=== ep: 3575, time 59.53905272483826, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3575
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3576, time 55.18989133834839, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3576
goal_identified
goal_identified
goal_identified
=== ep: 3577, time 66.6458580493927, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3577
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3578, time 65.41236853599548, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3579, time 57.22614312171936, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3579
goal_identified
goal_identified
=== ep: 3580, time 58.78036189079285, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3580
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3581, time 55.070231199264526, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3581
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3582, time 66.55328488349915, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3582
goal_identified
goal_identified
=== ep: 3583, time 52.19380497932434, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3583
goal_identified
goal_identified
goal_identified
=== ep: 3584, time 57.383686780929565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3584
goal_identified
goal_identified
goal_identified
=== ep: 3585, time 55.91142153739929, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3585
goal_identified
=== ep: 3586, time 62.42102813720703, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3586
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3587, time 60.45415186882019, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 55.63131308555603, eps 0.001, sum reward: 8, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2262
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3589, time 59.03050684928894, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3589
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3590, time 62.63384437561035, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3590
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3591, time 60.628278493881226, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 61.997178077697754, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3592
goal_identified
=== ep: 3593, time 54.308897256851196, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3593
goal_identified
=== ep: 3594, time 69.63641166687012, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3594
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3595, time 64.02160143852234, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3595
goal_identified
goal_identified
goal_identified
=== ep: 3596, time 68.29837083816528, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3596
goal_identified
goal_identified
=== ep: 3597, time 65.86017322540283, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3597
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3598, time 55.65967392921448, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3598
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3599, time 61.39214015007019, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3599
goal_identified
goal_identified
goal_identified
=== ep: 3600, time 61.08752751350403, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3600
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3601, time 60.45743250846863, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3601
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3602, time 58.003347396850586, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3602
goal_identified
goal_identified
goal_identified
=== ep: 3603, time 49.54631972312927, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3603
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3604, time 65.54663300514221, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3604
goal_identified
goal_identified
=== ep: 3605, time 59.08706831932068, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3605
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3606, time 55.88333797454834, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3606
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3607, time 53.35228633880615, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3607
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3608, time 63.59665489196777, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3608
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3609, time 61.03387260437012, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3609
goal_identified
goal_identified
=== ep: 3610, time 52.48401379585266, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3610
goal_identified
=== ep: 3611, time 58.69286608695984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3611
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3612, time 61.965864181518555, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2278
goal_identified
=== ep: 3613, time 65.82086277008057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3613
goal_identified
goal_identified
=== ep: 3614, time 64.78717017173767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3614
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3615, time 58.73864936828613, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3615
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3616, time 56.14059853553772, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3616
goal_identified
goal_identified
=== ep: 3617, time 52.15410494804382, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3617
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3618, time 58.792205572128296, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3618
goal_identified
goal_identified
goal_identified
=== ep: 3619, time 56.18640613555908, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3619
=== ep: 3620, time 59.220229148864746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3620
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3621, time 59.83837032318115, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3621
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3622, time 61.729976415634155, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3622
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3623, time 65.35986995697021, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3623
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3624, time 47.4687237739563, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3624
goal_identified
goal_identified
goal_identified
=== ep: 3625, time 56.21849346160889, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3625
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3626, time 60.9864182472229, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3626
goal_identified
goal_identified
goal_identified
=== ep: 3627, time 66.06510066986084, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3627
goal_identified
goal_identified
=== ep: 3628, time 71.22354650497437, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3628
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3629, time 72.10033679008484, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3629
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3630, time 56.399752616882324, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 125/125)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3630
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3631, time 62.25528812408447, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3631
goal_identified
goal_identified
goal_identified
=== ep: 3632, time 48.0020534992218, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3632
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3633, time 64.07769417762756, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3633
goal_identified
goal_identified
=== ep: 3634, time 49.41568422317505, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3634
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3635, time 58.52473568916321, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3635
goal_identified
goal_identified
goal_identified
=== ep: 3636, time 60.02569556236267, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3636
goal_identified
goal_identified
=== ep: 3637, time 65.53224587440491, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3637
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3638, time 63.80347919464111, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3638
goal_identified
goal_identified
=== ep: 3639, time 65.68539142608643, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3640, time 47.418391704559326, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3640
goal_identified
goal_identified
goal_identified
=== ep: 3641, time 63.383646965026855, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3641
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3642, time 55.9259934425354, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3642
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3643, time 62.80758476257324, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3643
goal_identified
goal_identified
=== ep: 3644, time 53.57881999015808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3644
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3645, time 61.59302735328674, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3646, time 64.72508907318115, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3646
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3647, time 54.449782609939575, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3647
goal_identified
goal_identified
goal_identified
=== ep: 3648, time 56.38679265975952, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3648
goal_identified
=== ep: 3649, time 58.57338094711304, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3649
goal_identified
=== ep: 3650, time 57.59574556350708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3650
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3651, time 62.47861671447754, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 50.05146145820618, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3652
goal_identified
goal_identified
=== ep: 3653, time 65.52456712722778, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3653
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3654, time 62.603808641433716, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3654
goal_identified
goal_identified
goal_identified
=== ep: 3655, time 67.6367917060852, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3655
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3656, time 70.29509258270264, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 135/135)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3656
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3657, time 48.08000135421753, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3657
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3658, time 54.86753559112549, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3658
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3659, time 60.864030599594116, eps 0.001, sum reward: 5, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3659
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3660, time 64.12986302375793, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3660
goal_identified
goal_identified
goal_identified
=== ep: 3661, time 63.99020075798035, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3661
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3662, time 63.510979413986206, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3662
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3663, time 49.386146545410156, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3663
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3664, time 62.55972671508789, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3664
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3665, time 51.45383405685425, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3665
goal_identified
goal_identified
goal_identified
=== ep: 3666, time 62.31259751319885, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3666
goal_identified
goal_identified
goal_identified
=== ep: 3667, time 53.75624656677246, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3667
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3668, time 65.40786027908325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3668
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3669, time 62.69873237609863, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3669
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3670, time 62.59517741203308, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3670
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3671, time 64.55874943733215, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3671
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3672, time 70.06528306007385, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3672
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3673, time 63.173728466033936, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3673
goal_identified
goal_identified
=== ep: 3674, time 67.78127717971802, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3674
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3675, time 64.27679824829102, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3675
goal_identified
goal_identified
=== ep: 3676, time 49.103959798812866, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3676
goal_identified
goal_identified
goal_identified
=== ep: 3677, time 59.42902231216431, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3677
goal_identified
goal_identified
goal_identified
=== ep: 3678, time 64.36200618743896, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3678
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3679, time 58.76836800575256, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3679
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3680, time 63.66785216331482, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3680
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3681, time 61.95697212219238, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3681
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3682, time 57.631065130233765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3682
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3683, time 61.8906135559082, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3683
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3684, time 57.214730739593506, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3684
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3685, time 65.05621480941772, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3685
goal_identified
=== ep: 3686, time 47.88128042221069, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3686
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3687, time 61.087764501571655, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3687
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3688, time 61.03186869621277, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3688
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3689, time 60.46786379814148, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3689
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3690, time 57.988038539886475, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3690
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3691, time 54.82518124580383, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3691
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3692, time 60.0474419593811, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3692
goal_identified
goal_identified
goal_identified
=== ep: 3693, time 67.46257209777832, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3693
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3694, time 50.417567014694214, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3694
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3695, time 57.264297008514404, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3695
goal_identified
goal_identified
goal_identified
=== ep: 3696, time 52.531294107437134, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3696
goal_identified
goal_identified
=== ep: 3697, time 67.76672744750977, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3697
goal_identified
goal_identified
=== ep: 3698, time 64.42323160171509, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3698
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3699, time 54.02696084976196, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3699
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3700, time 52.589008808135986, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3700
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3701, time 55.91578412055969, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3701
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3702, time 56.428783893585205, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3702
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3703, time 62.81383776664734, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3703
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3704, time 52.795291900634766, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3704
goal_identified
goal_identified
=== ep: 3705, time 68.01046395301819, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3705
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3706, time 60.1982741355896, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3706
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3707, time 70.91233110427856, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3707
goal_identified
=== ep: 3708, time 63.75803017616272, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3708
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3709, time 57.990336179733276, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3709
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3710, time 51.823604106903076, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3710
goal_identified
goal_identified
=== ep: 3711, time 59.025670766830444, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3711
goal_identified
goal_identified
goal_identified
=== ep: 3712, time 51.80296039581299, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3712
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3713, time 63.10516834259033, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3713
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3714, time 49.09406042098999, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3714
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3715, time 66.99436736106873, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3715
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3716, time 53.99968910217285, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3716
goal_identified
goal_identified
goal_identified
=== ep: 3717, time 55.99145984649658, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3717
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3718, time 51.39043307304382, eps 0.001, sum reward: 9, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2426
goal_identified
goal_identified
goal_identified
=== ep: 3719, time 64.14137363433838, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3719
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3720, time 69.12484073638916, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3720
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3721, time 66.78139543533325, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3721
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3722, time 57.29804801940918, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3722
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3723, time 60.522573709487915, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3723
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3724, time 55.95567345619202, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3724
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3725, time 64.1764407157898, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3725
goal_identified
goal_identified
goal_identified
=== ep: 3726, time 64.24389863014221, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3726
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3727, time 51.372231245040894, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3727
goal_identified
goal_identified
=== ep: 3728, time 60.26309275627136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3728
goal_identified
goal_identified
=== ep: 3729, time 62.714736461639404, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3729
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3730, time 62.23887920379639, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3730
goal_identified
goal_identified
=== ep: 3731, time 59.865702629089355, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3731
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3732, time 46.826037883758545, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3732
goal_identified
goal_identified
=== ep: 3733, time 68.16398572921753, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3733
goal_identified
=== ep: 3734, time 60.96514415740967, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3734
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3735, time 63.69786977767944, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3735
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3736, time 53.137845277786255, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3736
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3737, time 63.944626808166504, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3737
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3738, time 58.51811957359314, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3738
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3739, time 58.80149030685425, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3739
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3740, time 53.877259969711304, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3740
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3741, time 59.22194314002991, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3741
goal_identified
goal_identified
=== ep: 3742, time 68.51605367660522, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3742
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3743, time 67.2710702419281, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3743
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3744, time 59.68710494041443, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2474
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3745, time 56.246554136276245, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3745
goal_identified
goal_identified
goal_identified
=== ep: 3746, time 56.171257734298706, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3746
goal_identified
=== ep: 3747, time 60.86678981781006, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3747
goal_identified
goal_identified
goal_identified
=== ep: 3748, time 60.60476326942444, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3748
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3749, time 50.60021257400513, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3749
goal_identified
=== ep: 3750, time 63.87221431732178, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3750
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3751, time 58.654183864593506, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3751
goal_identified
goal_identified
goal_identified
=== ep: 3752, time 67.51650094985962, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3752
goal_identified
goal_identified
goal_identified
=== ep: 3753, time 57.43378949165344, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3753
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3754, time 48.767454624176025, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3754
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3755, time 59.34886169433594, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3755
goal_identified
goal_identified
goal_identified
=== ep: 3756, time 54.46670746803284, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3756
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3757, time 58.08122491836548, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3757
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3758, time 58.32161259651184, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3758
goal_identified
goal_identified
goal_identified
=== ep: 3759, time 58.823225021362305, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3759
goal_identified
goal_identified
goal_identified
=== ep: 3760, time 61.39465928077698, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3760
goal_identified
goal_identified
goal_identified
=== ep: 3761, time 49.261401653289795, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3761
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3762, time 55.25282335281372, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3762
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3763, time 58.99728322029114, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3763
goal_identified
goal_identified
goal_identified
=== ep: 3764, time 65.34074020385742, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3764
goal_identified
goal_identified
goal_identified
=== ep: 3765, time 60.898054361343384, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3765
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3766, time 45.62427282333374, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3766
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3767, time 62.268402338027954, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3767
goal_identified
goal_identified
goal_identified
=== ep: 3768, time 67.69926357269287, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3768
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3769, time 59.286683797836304, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3769
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3770, time 62.67866539955139, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3770
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3771, time 53.25111198425293, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3771
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3772, time 64.32864809036255, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3772
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3773, time 59.5274658203125, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3773
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3774, time 63.76501488685608, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3774
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3775, time 54.29307746887207, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3775
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3776, time 46.391632318496704, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3776
goal_identified
goal_identified
=== ep: 3777, time 63.12103796005249, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3777
goal_identified
goal_identified
goal_identified
=== ep: 3778, time 63.56979489326477, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3778
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3779, time 57.26529240608215, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3779
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3780, time 63.44508194923401, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3780
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3781, time 51.485353231430054, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3781
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3782, time 65.73278617858887, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3782
goal_identified
=== ep: 3783, time 50.96318316459656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3783
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3784, time 59.44443345069885, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3784
goal_identified
goal_identified
goal_identified
=== ep: 3785, time 53.25035738945007, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3785
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3786, time 69.73646664619446, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3786
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3787, time 65.42182850837708, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 65.25874066352844, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3788
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3789, time 58.06316137313843, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3789
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3790, time 59.70611310005188, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3790
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3791, time 45.46753239631653, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3791
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3792, time 65.91820573806763, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3792
goal_identified
goal_identified
goal_identified
=== ep: 3793, time 53.68035840988159, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3793
goal_identified
goal_identified
goal_identified
=== ep: 3794, time 66.85754060745239, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3794
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3795, time 56.40050148963928, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3795
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3796, time 57.57643532752991, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3796
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3797, time 55.52555847167969, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3797
goal_identified
=== ep: 3798, time 56.1194429397583, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3798
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3799, time 54.87591505050659, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3799
goal_identified
goal_identified
=== ep: 3800, time 54.62814950942993, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3800
goal_identified
goal_identified
goal_identified
=== ep: 3801, time 61.835944175720215, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3801
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3802, time 63.99903440475464, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3802
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3803, time 47.407272815704346, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3803
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3804, time 59.14377021789551, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 137/137)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3804
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3805, time 54.91042160987854, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3805
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3806, time 70.1764600276947, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3806
goal_identified
goal_identified
goal_identified
=== ep: 3807, time 60.775497913360596, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3807
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3808, time 50.92161965370178, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3808
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3809, time 55.864500761032104, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3809
goal_identified
goal_identified
goal_identified
=== ep: 3810, time 58.59258770942688, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3810
goal_identified
goal_identified
goal_identified
=== ep: 3811, time 56.592469930648804, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3811
goal_identified
goal_identified
=== ep: 3812, time 61.04219460487366, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3812
goal_identified
=== ep: 3813, time 53.05816578865051, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3813
goal_identified
goal_identified
goal_identified
=== ep: 3814, time 67.58960056304932, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3814
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3815, time 54.07710075378418, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3815
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3816, time 53.32203555107117, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3816
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3817, time 50.325443267822266, eps 0.001, sum reward: 6, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3817
goal_identified
goal_identified
goal_identified
=== ep: 3818, time 68.51845073699951, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3818
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3819, time 65.94053030014038, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3819
goal_identified
goal_identified
goal_identified
=== ep: 3820, time 57.85463833808899, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3820
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3821, time 50.358803510665894, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3821
goal_identified
goal_identified
goal_identified
=== ep: 3822, time 61.02653670310974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3822
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3823, time 55.9905846118927, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3823
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3824, time 66.40355825424194, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3824
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3825, time 46.68099904060364, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3825
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3826, time 64.32056641578674, eps 0.001, sum reward: 8, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3826
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3827, time 57.26280212402344, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3827
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3828, time 67.3968596458435, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3828
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3829, time 54.083194732666016, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3829
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3830, time 48.181546688079834, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3830
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3831, time 56.704237937927246, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3831
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3832, time 62.52308964729309, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3832
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3833, time 50.93796133995056, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3833
goal_identified
goal_identified
goal_identified
=== ep: 3834, time 58.47117900848389, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3834
goal_identified
goal_identified
goal_identified
=== ep: 3835, time 52.09610867500305, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3835
goal_identified
=== ep: 3836, time 57.853132247924805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3836
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3837, time 54.00520372390747, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3837
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3838, time 51.712451696395874, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3838
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3839, time 52.693854093551636, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3839
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3840, time 46.71553897857666, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3840
goal_identified
goal_identified
=== ep: 3841, time 54.83777952194214, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3841
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3842, time 58.30246138572693, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3842
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3843, time 54.05617380142212, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3843
goal_identified
goal_identified
goal_identified
=== ep: 3844, time 62.555277824401855, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3844
goal_identified
goal_identified
goal_identified
=== ep: 3845, time 58.96978831291199, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3845
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3846, time 48.26198220252991, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3846
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3847, time 49.02322864532471, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3848, time 53.65340852737427, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3848
goal_identified
goal_identified
goal_identified
=== ep: 3849, time 60.009729862213135, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3849
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3850, time 58.4810893535614, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3850
goal_identified
goal_identified
=== ep: 3851, time 49.01019597053528, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3851
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3852, time 51.308332681655884, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3852
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3853, time 53.11350059509277, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3853
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3854, time 52.54970026016235, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3854
goal_identified
goal_identified
=== ep: 3855, time 53.00437927246094, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3855
goal_identified
goal_identified
goal_identified
=== ep: 3856, time 48.0909686088562, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3856
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3857, time 59.72408723831177, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3857
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3858, time 48.50135827064514, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3858
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3859, time 51.18359065055847, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3859
goal_identified
goal_identified
goal_identified
=== ep: 3860, time 51.84615349769592, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3860
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3861, time 56.149762868881226, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3861
goal_identified
goal_identified
goal_identified
=== ep: 3862, time 62.15299582481384, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3862
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3863, time 53.17231321334839, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3863
goal_identified
goal_identified
goal_identified
=== ep: 3864, time 51.809629917144775, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3864
goal_identified
goal_identified
=== ep: 3865, time 51.487799644470215, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3865
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3866, time 55.77901220321655, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3866
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3867, time 58.12255787849426, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3867
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3868, time 45.44570803642273, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3868
goal_identified
goal_identified
goal_identified
=== ep: 3869, time 55.60851287841797, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3869
goal_identified
goal_identified
goal_identified
=== ep: 3870, time 55.83658742904663, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3870
goal_identified
goal_identified
=== ep: 3871, time 54.39643931388855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3871
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3872, time 49.80883431434631, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3872
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3873, time 46.476279973983765, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3873
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3874, time 60.45268774032593, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 138/138)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3874
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3875, time 58.24376368522644, eps 0.001, sum reward: 7, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3875
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3876, time 51.10533785820007, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3876
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3877, time 50.60235261917114, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3877
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3878, time 54.057989835739136, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3878
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3879, time 61.2516553401947, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3879
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3880, time 54.93591547012329, eps 0.001, sum reward: 4, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3880
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3881, time 52.294875144958496, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3881
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3882, time 52.27894449234009, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3882
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3883, time 53.17317819595337, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3883
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3884, time 52.24200201034546, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3884
goal_identified
=== ep: 3885, time 51.47388172149658, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3886, time 47.39586782455444, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3886
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3887, time 61.81657528877258, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3887
goal_identified
goal_identified
goal_identified
=== ep: 3888, time 56.57434892654419, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3888
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3889, time 63.1752450466156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3889
goal_identified
=== ep: 3890, time 62.45376205444336, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3890
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3891, time 53.55815410614014, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3891
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3892, time 44.30797553062439, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3892
goal_identified
goal_identified
goal_identified
=== ep: 3893, time 55.9466872215271, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3893
goal_identified
goal_identified
goal_identified
=== ep: 3894, time 50.1087863445282, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3894
goal_identified
goal_identified
=== ep: 3895, time 60.896804094314575, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3895
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3896, time 50.78878474235535, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3896
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3897, time 57.08665347099304, eps 0.001, sum reward: 8, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3897
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3898, time 53.945008516311646, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3898
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3899, time 54.48790431022644, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3899
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3900, time 59.23551368713379, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3900
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3901, time 53.78125309944153, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3901
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3902, time 49.352593183517456, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3902
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3903, time 50.9935348033905, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3903
goal_identified
goal_identified
goal_identified
=== ep: 3904, time 56.90286874771118, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3904
goal_identified
goal_identified
goal_identified
=== ep: 3905, time 60.069496870040894, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3905
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3906, time 64.93681263923645, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3906
goal_identified
goal_identified
=== ep: 3907, time 50.94151425361633, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3907
goal_identified
goal_identified
goal_identified
=== ep: 3908, time 54.70600485801697, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3908
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3909, time 40.92591428756714, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3909
goal_identified
goal_identified
=== ep: 3910, time 53.3273983001709, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3910
goal_identified
goal_identified
=== ep: 3911, time 46.58014702796936, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3911
goal_identified
goal_identified
goal_identified
=== ep: 3912, time 47.589787006378174, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3912
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3913, time 48.699735164642334, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3913
goal_identified
goal_identified
goal_identified
=== ep: 3914, time 54.27281737327576, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3914
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3915, time 51.91327714920044, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3915
goal_identified
goal_identified
goal_identified
=== ep: 3916, time 56.36446571350098, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3916
goal_identified
goal_identified
goal_identified
=== ep: 3917, time 50.73750567436218, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3917
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3918, time 51.00309610366821, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3918
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3919, time 55.14842677116394, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3919
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3920, time 38.614410400390625, eps 0.001, sum reward: 10, score_diff 10, tot learning steps 10 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2818
goal_identified
goal_identified
=== ep: 3921, time 52.179447412490845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3921
goal_identified
goal_identified
=== ep: 3922, time 49.95307517051697, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3922
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3923, time 56.195319175720215, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3923
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3924, time 51.8469820022583, eps 0.001, sum reward: 6, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3924
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3925, time 47.08501744270325, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3925
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3926, time 47.97761940956116, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3926
goal_identified
goal_identified
goal_identified
=== ep: 3927, time 56.69712591171265, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3927
goal_identified
goal_identified
goal_identified
=== ep: 3928, time 46.19179654121399, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3928
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3929, time 52.77868962287903, eps 0.001, sum reward: 7, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3929
goal_identified
goal_identified
=== ep: 3930, time 42.14156460762024, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3930
goal_identified
=== ep: 3931, time 58.11076593399048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3931
goal_identified
goal_identified
=== ep: 3932, time 58.70669174194336, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3932
goal_identified
goal_identified
goal_identified
=== ep: 3933, time 49.253363847732544, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3933
goal_identified
goal_identified
=== ep: 3934, time 49.93286323547363, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3934
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3935, time 47.35720205307007, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 49.54412817955017, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3936
goal_identified
goal_identified
goal_identified
=== ep: 3937, time 50.91206073760986, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3937
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3938, time 47.66325306892395, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3938
goal_identified
goal_identified
goal_identified
=== ep: 3939, time 52.91279864311218, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3939
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3940, time 53.61833691596985, eps 0.001, sum reward: 10, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3180
goal_identified
goal_identified
=== ep: 3941, time 51.49618721008301, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3941
goal_identified
goal_identified
goal_identified
=== ep: 3942, time 61.79586839675903, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3942
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3943, time 52.618536710739136, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3943
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3944, time 52.93129229545593, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3944
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3945, time 48.18771481513977, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3945
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3946, time 49.9325807094574, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3946
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3947, time 54.04365420341492, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3947
goal_identified
goal_identified
goal_identified
=== ep: 3948, time 55.98456907272339, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3948
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3949, time 50.315430879592896, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3949
goal_identified
goal_identified
=== ep: 3950, time 49.08558249473572, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3951, time 53.620906591415405, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3951
goal_identified
goal_identified
goal_identified
=== ep: 3952, time 51.6618549823761, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3952
goal_identified
goal_identified
=== ep: 3953, time 56.655558586120605, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3953
goal_identified
=== ep: 3954, time 43.01289248466492, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3954
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3955, time 51.114463090896606, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3955
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3956, time 45.709930419921875, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3956
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3957, time 55.25543427467346, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3957
goal_identified
goal_identified
=== ep: 3958, time 57.61376166343689, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3958
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3959, time 56.669180393218994, eps 0.001, sum reward: 5, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3959
goal_identified
goal_identified
goal_identified
=== ep: 3960, time 48.496506690979004, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3960
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3961, time 45.456188678741455, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3961
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3962, time 54.82895469665527, eps 0.001, sum reward: 9, score_diff 9, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3220
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3963, time 45.09315824508667, eps 0.001, sum reward: 8, score_diff 8, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3963
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3964, time 53.69690465927124, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3964
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3965, time 41.96820330619812, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3965
goal_identified
=== ep: 3966, time 59.07921648025513, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3966
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3967, time 52.29735565185547, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3967
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3968, time 49.28727722167969, eps 0.001, sum reward: 6, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3968
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3969, time 44.289246559143066, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3969
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3970, time 50.16508221626282, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3970
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3971, time 55.64592695236206, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3971
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3972, time 59.19991493225098, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3972
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3973, time 56.84449243545532, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3973
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3974, time 46.682117223739624, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3974
goal_identified
goal_identified
goal_identified
=== ep: 3975, time 56.4297776222229, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3975
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3976, time 42.74735713005066, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3976
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3977, time 55.92442345619202, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3977
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3978, time 45.96446347236633, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3978
goal_identified
goal_identified
goal_identified
=== ep: 3979, time 48.489399671554565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3979
goal_identified
goal_identified
=== ep: 3980, time 47.668192863464355, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3980
goal_identified
goal_identified
goal_identified
=== ep: 3981, time 53.57454586029053, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3981
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3982, time 52.56786251068115, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3982
goal_identified
goal_identified
goal_identified
=== ep: 3983, time 52.39442014694214, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3983
goal_identified
goal_identified
=== ep: 3984, time 42.89109468460083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3984
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3985, time 56.26348805427551, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3985
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3986, time 52.42811107635498, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3986
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3987, time 50.32341647148132, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3987
goal_identified
goal_identified
goal_identified
=== ep: 3988, time 53.359071016311646, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3988
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3989, time 48.507405042648315, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3989
goal_identified
goal_identified
goal_identified
=== ep: 3990, time 58.76429724693298, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3990
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3991, time 53.270132541656494, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3991
goal_identified
goal_identified
goal_identified
=== ep: 3992, time 52.872283697128296, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3992
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3993, time 51.7932448387146, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3993
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3994, time 54.503599405288696, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3995, time 44.537211179733276, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3995
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3996, time 54.10005331039429, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3996
goal_identified
goal_identified
=== ep: 3997, time 44.18932747840881, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3997
goal_identified
goal_identified
goal_identified
=== ep: 3998, time 51.96228241920471, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 21 > 20.0 and we are deleting ep 3998
goal_identified
goal_identified
goal_identified
=== ep: 3999, time 48.61530947685242, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
